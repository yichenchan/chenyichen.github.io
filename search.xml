<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>我的 2018 总结</title>
      <link href="/2019/01/02/2018%E6%80%BB%E7%BB%93/"/>
      <url>/2019/01/02/2018%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>今天是 2019 的第二天，写下这篇博文对自己的 2018 作一个总结。</p><a id="more"></a><p>2018 年真的是对自己意义重大的一年，经历了许许多多的事情，人生最快乐的大学四年走到了头，也进入了哈工大继续进行硕士阶段的学习。下面按照时间轴列一下这一年经历的事情和感悟。</p><ul><li>第一件事情当然就是和小丸子在一起啦～17 年就认识，18 年在回家的火车上一激动表了白就在一起了。整个 18 年，都跟她愉快地谈着恋爱，马上就一周年了。说实话和之前两个前女友在一起的时间都没有超过半年的，和小丸子能谈一年对我这个浪子来说真是一种奇迹。我的学校在 811 路的头，她的公司在 811 路尾，每个周三我就坐 811 路坐到底去找她，那颠簸的两个小时车程真的刻骨铭心，幸运的是，她每周都回家，而她的家就在我学校里面。虽然见面很困难，但我们还是一起去玩了很多地方，江滩、科技馆、博物馆…… to do list 一条又一条划掉，这真的是非常幸福的一段时光。</li><li>然后当然是考研复试啦。我还记得是跟小丸子在一起没过几天就出的初试成绩，好像是三百六十多，说真的，当时的我既惊喜又觉得在意料之内，惊喜是因为生性悲观的我觉得自己没希望了，意料之内是因为我觉得我去年的努力完全配得上这个结果。我惊喜地跟妈妈和小丸子分享我的喜悦，当然我没高兴太久，就开始投入了复试<strong>十门课</strong>的复习之中。接着开学后过了一段时间飞去了哈尔滨复试，这是我第一次去哈尔滨，准确的说，是第一次去寒冷的东北。高中的好友李文峰和刘序杰热情地接待了我，真的很感谢他们。经过了笔试和五轮的面试，在放榜的那天，我同样既忐忑又自信地看到了自己的名字，专硕综合第七名，一切尘埃落定。</li><li>接下来就是论文答辩和毕业季了。说来惭愧，论文根本没有用心搞，从 github 抄了一份代码糊弄过去了，好吧其实老师答辩也是糊弄过去了，我也就这么糊弄着毕业了，四年大学生涯从此画下了一个句号。这四年，真的想写一篇长文矫情一番，可是想说的话太多反而无从写起，干脆就让他们留在记忆里吧。毕业前跟沈广建寝室去宜昌和三峡自驾游了一番，大学感情最深的几拨人，沈广建寝室和滑板的兄弟，当然还有后来换的寝室的室友。我是寝室最后一个走的，在最后一次关上 726 的那扇门的时候，本来对毕业季的矫情嗤之以鼻的我也涌起一阵痛苦。最无忧无虑的时光离开了我，开始进入一个新的阶段了。</li><li>毕业那个暑假，为了陪小丸子，在武汉找了一份实习，在猿辅导公司进行英语题库的输入，自以为是的我看不起这份苦力似的工作，消极怠工，在第十二天的时候被炒鱿鱼了，拿到了一千二，我的第一次职场生涯就这样结束了。至今都忘不了上班途中在光谷堵成狗的那几天。后来没办法，又去找了一份家教，教一个高三的体育生英语，由于女孩子很听话，那段课程教得非常开心，不过也许是我人生最后一次做家教了。这段时间第一次体验了同居生活，每天就是早上上班，中午午休，下午学习，晚上一起去附近的财大堕落街吃小吃，非常平淡，非常安稳。对了！之前帮咸鱼领养了两只猫，呆呆和皮蛋，我们在出租屋内帮他养了两个月，从开始的害怕到慢慢接纳再到嫌麻烦再到爱上最后到不舍，我相信我以后一定会养一只猫的。这段时间给了我一种过小日子的浓浓幸福感，让我难以忘怀，我第一次觉得，就一直这样下去也不错嘛。这个独立博客也是暑假搭建起来的，从刚开始的两篇博客发展到现在的几十篇，虽然都是很水的笔记==，希望自己能坚持把这个博客运营下去。暑假到头了，好日子也到头了，离开开学还有十几天，买上了回家的火车，开始了异地恋，把扎进了武汉的根拔了起来，离开了这座城市。</li><li>九月十五号，正式开启硕士阶段，来到了深圳这座我从小待过几年的城市，去之前满心期待，后来现实给我当头一棒，发生了许多不如意的<a href="https://chenyichen.xyz/2018/09/26/%E5%9D%8E%E5%9D%B7%E9%80%89%E5%AF%BC%E5%B8%88/">事情</a>，后来慢慢生活走上正轨，开始了规律的上课生活。刚开始激情满满，每天把课余时间都拿来学习吴恩达的课程，定期健身，可是越往后越开始担心自己转行 AI 以后找不到工作，自己给自己压力，焦虑程度与日俱增，本来已经几乎控制住的强迫症因为焦虑又蹦了出来，开始了第三次的复发。于是乎今年的下半年又在和强迫症抗争的痛苦中度过。这段时间经常会回忆大学发生的许多事情，每当最焦虑的时候，那一幕幕就开始闪回。对我来说，仅仅距离本科毕业才几个月，我已经感觉大学的日子如泡影一般，模模糊糊似乎无法触碰，仿佛过去了十几年，也许是一个阶段已经结束，离开了那个环境，心态已经完全不一样了。国庆节回了一次武汉，和啊祥大米咸鱼几个聚了聚，只要和这些兄弟在一起，就是最无忧无虑的，路上还碰到正川和谭钦，聊了许多近况，心里真的很暖，我终于明白了，之所以大学那么无忧无虑，是因为有这些兄弟在，下一次见到你们会是什么时候呢？不知为何，现在不想交新的朋友了，除了两个舍友，并没有认识其他同学的期待，不过，很开心的是认识了一个印度留学生 sajid，应该算得上是我第一个外国朋友了。这段时间开始把翻墙常态化，开始用推特和 youtube，同时思想开始完全转为自由派，开始了更深的社会思考。</li></ul><p>这就是我的 2018，关键词是毕业，离别和重新开始，一个旧阶段的结束，一个新阶段的开始。这一年自己开始决定走上 AI 之路，未来会怎样我不知道，只希望自己面对任何环境的变化和动荡时能保持一颗平常心，不要逼自己太多。我只想对以后的我说，别那么要强，你可以很平庸，你可以活得不那么焦虑。</p>]]></content>
      
      <categories>
          
          <category> 个人随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
            <tag> 年终总结 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 5 week 2）</title>
      <link href="/2018/12/22/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c5w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/12/22/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c5w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h1 id="Part1-Operations-on-word-vectors"><a href="#Part1-Operations-on-word-vectors" class="headerlink" title="Part1 - Operations on word vectors"></a>Part1 - Operations on word vectors</h1><p>通过这部分我们将学会：</p><ul><li>加载预训练词向量，用 cos 相似度测量相似度</li><li>使用词嵌入解决词语类比问题</li><li>减少词向量中的性别偏差</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> w2v_utils <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><a id="more"></a><p>接下来加载词向量，这次作业使用的是 50 维的 GloVe 向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words, word_to_vec_map = read_glove_vecs(<span class="string">'data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure><p>读取的函数为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_glove_vecs</span><span class="params">(glove_file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(glove_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        words = set()</span><br><span class="line">        word_to_vec_map = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            line = line.strip().split()</span><br><span class="line">            curr_word = line[<span class="number">0</span>]</span><br><span class="line">            words.add(curr_word)</span><br><span class="line">            word_to_vec_map[curr_word] = np.array(line[<span class="number">1</span>:], dtype=np.float64)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> words, word_to_vec_map</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>word：词汇表中的单词 set</li><li>word_to_veec_map：将单词映射到 GloVe 向量的 dict</li></ul><h2 id="cosine-相似度"><a href="#cosine-相似度" class="headerlink" title="cosine 相似度"></a>cosine 相似度</h2><p>用下列公式测量两个词向量的相似度：</p><script type="math/tex; mode=display">\text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_1.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Cosine similarity reflects the degree of similariy between u and v</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        u -- a word vector of shape (n,)          </span></span><br><span class="line"><span class="string">        v -- a word vector of shape (n,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cosine_similarity -- the cosine similarity between u and v defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    distance = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the dot product between u and v (≈1 line)</span></span><br><span class="line">    dot = np.dot(u,v)</span><br><span class="line">    <span class="comment"># Compute the L2 norm of u (≈1 line)</span></span><br><span class="line">    norm_u = np.sqrt(np.sum(np.square(u)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the L2 norm of v (≈1 line)</span></span><br><span class="line">    norm_v = np.sqrt(np.sum(np.square(v)))</span><br><span class="line">    <span class="comment"># Compute the cosine similarity defined by formula (1) (≈1 line)</span></span><br><span class="line">    cosine_similarity = dot/(norm_u*norm_v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure><h2 id="词语类比"><a href="#词语类比" class="headerlink" title="词语类比"></a>词语类比</h2><p>要实现  “<em>a</em> is to <em>b</em> as <em>c</em> is to <strong>__</strong>“ 这种类比任务，我们可以从词向量词汇表里面找出这么一个词使得 $e_b - e_a \approx e_d - e_c$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a, word_b, word_c, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the word analogy task as explained above: a is to b as c is to ____. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_a -- a word, string</span></span><br><span class="line"><span class="string">    word_b -- a word, string</span></span><br><span class="line"><span class="string">    word_c -- a word, string</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary that maps words to their corresponding vectors. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert words to lower case</span></span><br><span class="line">    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the word embeddings v_a, v_b and v_c (≈1-3 lines)</span></span><br><span class="line">    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]</span><br><span class="line">    </span><br><span class="line">    words = word_to_vec_map.keys()</span><br><span class="line">    max_cosine_sim = <span class="number">-100</span>              <span class="comment"># Initialize max_cosine_sim to a large negative number</span></span><br><span class="line">    best_word = <span class="keyword">None</span>                   <span class="comment"># Initialize best_word with None, it will help keep track of the word to output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over the whole word vector set</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:        </span><br><span class="line">        <span class="comment"># to avoid best_word being one of the input words, pass on them.</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> [word_a, word_b, word_c] :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)</span></span><br><span class="line">        cosine_sim = cosine_similarity(word_to_vec_map[w], e_b-e_a+e_c)<span class="comment">#找出使得这个值最小的</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the cosine_sim is more than the max_cosine_sim seen so far,</span></span><br><span class="line">            <span class="comment"># then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> cosine_sim &gt; max_cosine_sim:</span><br><span class="line">            max_cosine_sim = cosine_sim</span><br><span class="line">            best_word = w</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> best_word</span><br></pre></td></tr></table></figure><p>我们进行一下测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">triads_to_try = [(<span class="string">'small'</span>, <span class="string">'smaller'</span>, <span class="string">'big'</span>), (<span class="string">'india'</span>, <span class="string">'delhi'</span>, <span class="string">'japan'</span>), (<span class="string">'man'</span>, <span class="string">'woman'</span>, <span class="string">'boy'</span>), (<span class="string">'small'</span>, <span class="string">'smaller'</span>, <span class="string">'large'</span>)]</span><br><span class="line"><span class="keyword">for</span> triad <span class="keyword">in</span> triads_to_try:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'&#123;&#125; -&gt; &#123;&#125; :: &#123;&#125; -&gt; &#123;&#125;'</span>.format( *triad, complete_analogy(*triad,word_to_vec_map)))</span><br></pre></td></tr></table></figure><p>small -&gt; smaller :: big -&gt; bigger<br>india -&gt; delhi :: japan -&gt; tokyo<br>man -&gt; woman :: boy -&gt; girl<br>small -&gt; smaller :: large -&gt; larger</p><p>效果不错！！</p><h2 id="词向量去偏见"><a href="#词向量去偏见" class="headerlink" title="词向量去偏见"></a>词向量去偏见</h2><p>我们首先要找到代表性别偏见的偏见轴向量 g，做法就是用 “woman” 这个词的词向量减去 “man” 这个词的词向量，即 $g = e_{woman}-e_{man}$，如果要更精确的结果，我们可以计算 $g_1 = e_{mother}-e_{father}$, $g_2 = e_{girl}-e_{boy}$ …… 然后取平均值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g = word_to_vec_map[<span class="string">'woman'</span>] - word_to_vec_map[<span class="string">'man'</span>]</span><br></pre></td></tr></table></figure><p>现在看看男生女生的名字和偏见轴的相似度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">'List of names and their similarities with constructed vector:'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># girls and boys name</span></span><br><span class="line">name_list = [<span class="string">'john'</span>, <span class="string">'marie'</span>, <span class="string">'sophie'</span>, <span class="string">'ronaldo'</span>, <span class="string">'priya'</span>, <span class="string">'rahul'</span>, <span class="string">'danielle'</span>, <span class="string">'reza'</span>, <span class="string">'katy'</span>, <span class="string">'yasmin'</span>,<span class="string">'bill'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> name_list:</span><br><span class="line">    <span class="keyword">print</span> (w, cosine_similarity(word_to_vec_map[w], g))</span><br></pre></td></tr></table></figure><p>List of names and their similarities with constructed vector:<br>john -0.23163356146<br>marie 0.315597935396<br>sophie 0.318687898594<br>ronaldo -0.312447968503<br>priya 0.17632041839<br>rahul -0.169154710392<br>danielle 0.243932992163<br>reza -0.079304296722<br>katy 0.283106865957<br>yasmin 0.233138577679<br>bill -0.0306830313755</p><p>我们发现，男生的名字倾向于有负的相似度，女生名字倾向于有正的相似度，由于男女有别，这个结果很正常，接下来让我们换一些中性的词语。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Other words and their similarities:'</span>)</span><br><span class="line">word_list = [<span class="string">'lipstick'</span>, <span class="string">'guns'</span>, <span class="string">'science'</span>, <span class="string">'arts'</span>, <span class="string">'literature'</span>, <span class="string">'warrior'</span>,<span class="string">'doctor'</span>, <span class="string">'tree'</span>, <span class="string">'receptionist'</span>, </span><br><span class="line">             <span class="string">'technology'</span>,  <span class="string">'fashion'</span>, <span class="string">'teacher'</span>, <span class="string">'engineer'</span>, <span class="string">'pilot'</span>, <span class="string">'computer'</span>, <span class="string">'singer'</span>]</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> word_list:</span><br><span class="line">    <span class="keyword">print</span> (w, cosine_similarity(word_to_vec_map[w], g))</span><br></pre></td></tr></table></figure><p>Other words and their similarities:<br>lipstick 0.276919162564<br>guns -0.18884855679<br>science -0.0608290654093<br>arts 0.00818931238588<br>literature 0.0647250443346<br>warrior -0.209201646411<br>doctor 0.118952894109<br>tree -0.0708939917548<br>receptionist 0.330779417506<br>technology -0.131937324476<br>fashion 0.0356389462577<br>teacher 0.179209234318<br>engineer -0.0803928049452<br>pilot 0.00107644989919<br>computer -0.103303588739<br>singer 0.185005181365</p><p>我们可以发现，“computer” 更接近于 “man”，而 “literature” 更接近于 “woman”！什么鬼？女生就不能学计算机？男生就不能学文学？屁！让我们纠正这个偏见！</p><h3 id="中立化-Neutralize-bias-for-non-gender-specific-words"><a href="#中立化-Neutralize-bias-for-non-gender-specific-words" class="headerlink" title="中立化 Neutralize bias for non-gender specific words"></a>中立化 Neutralize bias for non-gender specific words</h3><p>首先我们的词向量是 50 维的，将它分为两个部分，偏差轴方向 g 和剩下的 49 维向量，称之为 $g_{\perp}$，在线性代数里，我们称它们是正交的，也就是垂直的，所以 $g_{\perp}$ 是非偏差轴。我们将有偏差的词向量投影到非偏差轴上，便获得了中立化后的词向量。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_2.png" alt=""></p><p> 公式为：</p><script type="math/tex; mode=display">词向量在偏差轴上的投影：e^{bias\_component} = \frac{e \cdot g}{||g||_2^2} * g\\词向量在非偏差轴上的投影：e^{debiased} = e - e^{bias\_component}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word, g, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. </span></span><br><span class="line"><span class="string">    This function ensures that gender neutral words are zero in the gender subspace.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        word -- string indicating the word to debias</span></span><br><span class="line"><span class="string">        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)</span></span><br><span class="line"><span class="string">        word_to_vec_map -- dictionary mapping words to their corresponding vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        e_debiased -- neutralized word vector representation of the input "word"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line)</span></span><br><span class="line">    e = word_to_vec_map[word]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute e_biascomponent using the formula give above. (≈ 1 line)</span></span><br><span class="line">    e_biascomponent = (np.dot(e,g)/np.linalg.norm(g)**<span class="number">2</span>) * g</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Neutralize e by substracting e_biascomponent from it </span></span><br><span class="line">    <span class="comment"># e_debiased should be equal to its orthogonal projection. (≈ 1 line)</span></span><br><span class="line">    e_debiased = e - e_biascomponent</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e_debiased</span><br></pre></td></tr></table></figure><p>我们看看“接待员”这个词在中立化前后中立化后与偏差轴的相似度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">e = <span class="string">"receptionist"</span></span><br><span class="line">print(<span class="string">"cosine similarity between "</span> + e + <span class="string">" and g, before neutralizing: "</span>, cosine_similarity(word_to_vec_map[<span class="string">"receptionist"</span>], g))</span><br><span class="line"></span><br><span class="line">e_debiased = neutralize(<span class="string">"receptionist"</span>, g, word_to_vec_map)</span><br><span class="line">print(<span class="string">"cosine similarity between "</span> + e + <span class="string">" and g, after neutralizing: "</span>, cosine_similarity(e_debiased, g))</span><br></pre></td></tr></table></figure><p>cosine similarity between receptionist and g, before neutralizing:  0.330779417506<br>cosine similarity between receptionist and g, after neutralizing:  -3.26732746085e-17</p><h3 id="平均化-Equalization-algorithm-for-gender-specific-words"><a href="#平均化-Equalization-algorithm-for-gender-specific-words" class="headerlink" title="平均化  Equalization algorithm for gender-specific words"></a>平均化  Equalization algorithm for gender-specific words</h3><p>ok，我们已经将与性别无关的词投影到非偏差轴了，那么如果与性别有关的词到这根轴的距离不相等，那么这些词距离这类无性别词的距离就不相等了，造成了相对的偏差，比如 “保姆” 距离 “男演员” 和 “女演员” 的距离。我们要做的就是将男演员对应的词向量和女演员对应的词向量调整为关于非偏差轴对称。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_3.png" alt=""></p><p>作业的公式再一次出现了错误……</p><p>论坛一个大佬的更正后的公式：</p><script type="math/tex; mode=display">\mu = \frac{e_{w1} + e_{w2}}{2}\\ \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\\  \mu_{\perp} = \mu - \mu_{B} \\   e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\\    e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\\    e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {||e_{w1B}-\mu_B||} \\    e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {||e_{w2B}-\mu_B||} \\    e_1 = e_{w1B}^{corrected} + \mu_{\perp} \\    e_2 = e_{w2B}^{corrected} + \mu_{\perp}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_4.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_5.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pair, bias_axis, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Debias gender specific words by following the equalize method described in the figure above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") </span></span><br><span class="line"><span class="string">    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their corresponding vectors</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    e_1 -- word vector corresponding to the first word</span></span><br><span class="line"><span class="string">    e_2 -- word vector corresponding to the second word</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines)</span></span><br><span class="line">    w1, w2 = pair</span><br><span class="line">    e_w1, e_w2 = word_to_vec_map[w1],word_to_vec_map[w2]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)</span></span><br><span class="line">    mu = (e_w1 + e_w2)/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)</span></span><br><span class="line">    mu_B = (np.dot(mu,bias_axis)/(np.linalg.norm(bias_axis)**<span class="number">2</span>)) * bias_axis</span><br><span class="line">    mu_orth = mu - mu_B</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)</span></span><br><span class="line">    e_w1B = (np.dot(e_w1,bias_axis)/(np.linalg.norm(bias_axis)**<span class="number">2</span>)) * bias_axis</span><br><span class="line">    e_w2B = (np.dot(e_w2,bias_axis)/(np.linalg.norm(bias_axis)**<span class="number">2</span>)) * bias_axis</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)</span></span><br><span class="line">    corrected_e_w1B = np.sqrt(abs(<span class="number">1</span>-np.linalg.norm(mu_orth)**<span class="number">2</span>)) * ((e_w1B-mu_B)/np.linalg.norm(e_w1B-mu_B))</span><br><span class="line">    corrected_e_w2B = np.sqrt(abs(<span class="number">1</span>-np.linalg.norm(mu_orth)**<span class="number">2</span>)) * ((e_w2B-mu_B)/np.linalg.norm(e_w2B-mu_B))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)</span></span><br><span class="line">    e1 = corrected_e_w1B + mu_orth</span><br><span class="line">    e2 = corrected_e_w2B + mu_orth                                                           </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e1, e2</span><br></pre></td></tr></table></figure><p>现在看看平均化前后的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"cosine similarities before equalizing:"</span>)</span><br><span class="line">print(<span class="string">"cosine_similarity(word_to_vec_map[\"doctor\"], gender) = "</span>, cosine_similarity(word_to_vec_map[<span class="string">"doctor"</span>], g))</span><br><span class="line">print(<span class="string">"cosine_similarity(word_to_vec_map[\"nurse\"], gender) = "</span>, cosine_similarity(word_to_vec_map[<span class="string">"nurse"</span>], g))</span><br><span class="line">print()</span><br><span class="line">e1, e2 = equalize((<span class="string">"doctor"</span>, <span class="string">"nurse"</span>), g, word_to_vec_map)</span><br><span class="line">print(<span class="string">"cosine similarities after equalizing:"</span>)</span><br><span class="line">print(<span class="string">"cosine_similarity(e1, gender) = "</span>, cosine_similarity(e1, g))</span><br><span class="line">print(<span class="string">"cosine_similarity(e2, gender) = "</span>, cosine_similarity(e2, g))</span><br></pre></td></tr></table></figure><p>cosine similarities before equalizing:<br>cosine_similarity(word_to_vec_map[“doctor”], gender) =  0.118952894109<br>cosine_similarity(word_to_vec_map[“nurse”], gender) =  0.380308796807</p><p>cosine similarities after equalizing:<br>cosine_similarity(e1, gender) =  -0.698086236808<br>cosine_similarity(e2, gender) =  0.698086236808</p><p>参考：The debiasing algorithm is from Bolukbasi et al., 2016, <a href="https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf" target="_blank" rel="noopener">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a></p><h1 id="Part2-Emojify"><a href="#Part2-Emojify" class="headerlink" title="Part2 - Emojify"></a>Part2 - Emojify</h1><p>这部分作业主要就是用两种方法实现句子的 Emoji 化，目的是输入一个句子，输出跟这个句子有关的 Emoji.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> emo_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> emoji</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="版本1-基本模型"><a href="#版本1-基本模型" class="headerlink" title="版本1 - 基本模型"></a>版本1 - 基本模型</h2><h3 id="Emoji-数据集"><a href="#Emoji-数据集" class="headerlink" title="Emoji 数据集"></a>Emoji 数据集</h3><ul><li>X 包含 127 条句子（字符串）</li><li>Y 包含了每个句子对应的标签值，从 0 - 4 的整数，对应了五个 emoji </li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_6.png" alt=""></p><p>加载训练集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, Y_train = read_csv(<span class="string">'data/train_emoji.csv'</span>)</span><br><span class="line">X_test, Y_test = read_csv(<span class="string">'data/tesss.csv'</span>)</span><br></pre></td></tr></table></figure><p>找出具有最长单词数的句子的长度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">maxLen = len(max(X_train, key=len).split())</span><br></pre></td></tr></table></figure><h3 id="版本1-的模型"><a href="#版本1-的模型" class="headerlink" title="版本1 的模型"></a>版本1 的模型</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_7.png" alt=""></p><p>为了能够计算损失函数，将标签值变为 one-hot 向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y_oh_train = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br><span class="line">Y_oh_test = convert_to_one_hot(Y_test, C = <span class="number">5</span>)</span><br></pre></td></tr></table></figure><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p>第一步是加载预训练的 50 维的 Glove 词向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(<span class="string">'data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure><ul><li><code>word_to_index</code> 是将 word 映射到词汇表中 index 的字典（400001个词）</li><li><code>index_to_word</code> 是将 index 映射到 word 的索引值的字典</li><li><code>word_to_vec_map</code> 是将 word 映射到其词向量的字典</li></ul><p>接下来我们实现输入句子输出其所有词向量的平均值的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_avg</span><span class="params">(sentence, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word</span></span><br><span class="line"><span class="string">    and averages its value into a single vector encoding the meaning of the sentence.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    sentence -- string, one training example from X</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Step 1: Split sentence into list of lower case words (≈ 1 line)</span></span><br><span class="line">    words = sentence.lower().split()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the average word vector, should have the same shape as your word vectors.</span></span><br><span class="line">    avg = np.zeros((<span class="number">50</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: average the word vectors. You can loop over the words in the list "words".</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        avg += word_to_vec_map[w]</span><br><span class="line">    avg = avg / len(words)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure><p>以上函数用在前向传播中，接下来是反向传播的公式：</p><script type="math/tex; mode=display">z^{(i)} = W . avg^{(i)} + b\\a^{(i)} = softmax(z^{(i)})\\\mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)</script><p>其中 Yoh 是标签 Y 的 onehot 向量。</p><p>下面是模型函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, word_to_vec_map, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">400</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Model to train word vector representations in numpy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, numpy array of sentences as strings, of shape (m, 1)</span></span><br><span class="line"><span class="string">    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    learning_rate -- learning_rate for the stochastic gradient descent algorithm</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    pred -- vector of predictions, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    W -- weight matrix of the softmax layer, of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">    b -- bias of the softmax layer, of shape (n_y,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define number of training examples</span></span><br><span class="line">    m = Y.shape[<span class="number">0</span>]                          <span class="comment"># number of training examples</span></span><br><span class="line">    n_y = <span class="number">5</span>                                 <span class="comment"># number of classes  </span></span><br><span class="line">    n_h = <span class="number">50</span>                                <span class="comment"># dimensions of the GloVe vectors </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters using Xavier initialization</span></span><br><span class="line">    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)</span><br><span class="line">    b = np.zeros((n_y,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert Y to Y_onehot with n_y classes</span></span><br><span class="line">    Y_oh = convert_to_one_hot(Y, C = n_y) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_iterations):  <span class="comment"># 遍历完所有的 epoch</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):           <span class="comment"># 每一个 epoch 遍历完所有样本，每一个样本更新一次参数，应该是随机梯度下降</span></span><br><span class="line">          </span><br><span class="line">            <span class="comment"># Average the word vectors of the words from the i'th training example</span></span><br><span class="line">            avg = sentence_to_avg(X[i], word_to_vec_map)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagate the avg through the softmax layer</span></span><br><span class="line">            z = np.dot(W,avg)+b</span><br><span class="line">            a = softmax(z)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax)</span></span><br><span class="line">            cost = -np.sum(Y_oh*np.log(a))</span><br><span class="line">          </span><br><span class="line">            <span class="comment"># Compute gradients </span></span><br><span class="line">            dz = a - Y_oh[i]</span><br><span class="line">            dW = np.dot(dz.reshape(n_y,<span class="number">1</span>), avg.reshape(<span class="number">1</span>, n_h))</span><br><span class="line">            db = dz</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters with Stochastic Gradient Descent</span></span><br><span class="line">            W = W - learning_rate * dW</span><br><span class="line">            b = b - learning_rate * db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch: "</span> + str(t) + <span class="string">" --- cost = "</span> + str(cost))</span><br><span class="line">            pred = predict(X, Y, W, b, word_to_vec_map)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred, W, b</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred, W, b = model(X_train, Y_train, word_to_vec_map)</span><br><span class="line">print(pred)</span><br></pre></td></tr></table></figure><p>Epoch: 0 —- cost = 227.527181633<br>Accuracy: 0.348484848485<br>Epoch: 100 —- cost = 418.198641202<br>Accuracy: 0.931818181818<br>Epoch: 200 —- cost = 482.727277095<br>Accuracy: 0.954545454545<br>Epoch: 300 —- cost = 516.659063961<br>Accuracy: 0.969696969697</p><h3 id="在测试集上试验"><a href="#在测试集上试验" class="headerlink" title="在测试集上试验"></a>在测试集上试验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Training set:"</span>)</span><br><span class="line">pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)</span><br><span class="line">print(<span class="string">'Test set:'</span>)</span><br><span class="line">pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)</span><br></pre></td></tr></table></figure><p>Training set:<br>Accuracy: 0.977272727273<br>Test set:<br>Accuracy: 0.857142857143</p><p>自己随便写几个句子进行测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_my_sentences = np.array([<span class="string">"i hate you"</span>, <span class="string">"i love you"</span>, <span class="string">"funny lol"</span>, <span class="string">"lets play with a ball"</span>, <span class="string">"food is ready"</span>, <span class="string">"not feeling happy"</span>])</span><br><span class="line">Y_my_labels = np.array([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">2</span>], [<span class="number">1</span>], [<span class="number">4</span>],[<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)</span><br><span class="line">print_predictions(X_my_sentences, pred)</span><br></pre></td></tr></table></figure><p>Accuracy: 0.666666666667</p><p>i hate you 😞<br>i love you ❤️<br>funny lol 😄<br>lets play with a ball ⚾<br>food is ready 🍴<br>not feeling happy 😄</p><p>我们可以打印一下混淆矩阵，帮助理解哪一类emoji更加对模型难以分辨。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'           '</span>+ label_to_emoji(<span class="number">0</span>)+ <span class="string">'    '</span> + label_to_emoji(<span class="number">1</span>) + <span class="string">'    '</span> +  label_to_emoji(<span class="number">2</span>)+ <span class="string">'    '</span> + label_to_emoji(<span class="number">3</span>)+<span class="string">'   '</span> + label_to_emoji(<span class="number">4</span>))</span><br><span class="line">print(pd.crosstab(Y_test, pred_test.reshape(<span class="number">56</span>,), rownames=[<span class="string">'Actual'</span>], colnames=[<span class="string">'Predicted'</span>], margins=<span class="keyword">True</span>))</span><br><span class="line">plot_confusion_matrix(Y_test, pred_test)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_8.png" alt=""></p><p>这个模型没有考虑词语的前后联系，所以会出现 not feeling happy 😄 这种错误，接下来用 LSTM 模型来实现这个任务。</p><h2 id="版本-2-在-Keras-中使用-LSTM-模型"><a href="#版本-2-在-Keras-中使用-LSTM-模型" class="headerlink" title="版本 2 - 在 Keras 中使用 LSTM 模型"></a>版本 2 - 在 Keras 中使用 LSTM 模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, Dropout, LSTM, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="模型概况"><a href="#模型概况" class="headerlink" title="模型概况"></a>模型概况</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_9.png" alt=""></p><h3 id="Keras-和-minibatch"><a href="#Keras-和-minibatch" class="headerlink" title="Keras 和 minibatch"></a>Keras 和 minibatch</h3><p>数据集中所有的句子长度不是统一的，而在 Keras 中要实现小批量梯度下降，某个 minibatch 中所有句子的长度必须全部相同，这样才能输入 LSTM 层进行训练，为了解决这个问题，我们可以将句子进行<strong>填充</strong>，以最长的句子为基准，不足的部分用零向量进行填充，假设最长的句子有 20 个词，那么 “I love you” 这个句子的词向量为 $(e_{i}, e_{love}, e_{you}, \vec{0}, \vec{0}, \ldots, \vec{0})$.</p><h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h3><p>在 Keras 中，嵌入矩阵是用类似于嵌入层的形式实现的，输入一个由索引值组成的句子，输出句子每个词的词向量。接下来我们会实现一个用预训练词向量初始化过的嵌入层，由于训练集较小，保持嵌入层参数固定不被训练。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_10.png" alt=""></p><p>首先进行数据的预处理，也就是把句子每个词变成 index，长度不够的用 0 进行填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)</span></span><br><span class="line">    X_indices = np.zeros((m,max_len))<span class="comment"># 这一步其实已经做好了零填充</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] = word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure><p>接下来就用我们自己的词嵌入矩阵开始构建 Embedding 层啦，方法是：</p><ul><li>将我们自己的词嵌入矩阵变成 Embedding 层的参数要求的形状<ul><li>先用 0 初始化一个正确的形状的矩阵</li><li>将词向量填入这个矩阵</li></ul></li><li>定义 <a href="https://keras.io/layers/embeddings/" target="_blank" rel="noopener">Embedding 层</a> </li><li>将该层的参数设为我们自己的词嵌入矩阵</li></ul><p>详细的嵌入层设置可以见这个<a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" target="_blank" rel="noopener">博客</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 以下部分都是为了将我们自己的词嵌入矩阵变成 Embedding 层的参数要求的形状</span></span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                    <span class="comment"># Keras embedding 层输入的规定，估计是注意词汇表的索引值第一个是 1 而不是 0，所以要加一</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先用 0 初始化一个正确的形状(vocab_len,emb_dim)的矩阵 </span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))<span class="comment"># 这个是传入Keras embedding 层的参数，它的形状必须跟 embedding.get_weights() 的形状相同</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 按照索引值将正确的词向量填入这个矩阵</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从这里开始构建 Keras 的 embedding 层</span></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. </span></span><br><span class="line">    embedding_layer = Embedding(vocab_len,emb_dim,trainable=<span class="keyword">False</span>)<span class="comment"># 这个将该层调为“不可训练”，保持其参数不变</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))<span class="comment"># 这里不太懂？？</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将该层的参数设为我们自己的词嵌入矩阵</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure><h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><p>一切准备就绪！开始建立整个模型，包括 Input 层 -&gt; LSTM 层（返回所有序列值） -&gt; Dropout 层 -&gt; LSTM 层（返回最后一个值）-&gt; Dropout 层 -&gt; softmax（包括 Dense 层和 softmax 激活层） </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojify_V2</span><span class="params">(input_shape, word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义输入层</span></span><br><span class="line">    sentence_indices = Input(shape=input_shape, dtype=<span class="string">'int32'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建嵌入层（参数是自己训练好的词嵌入矩阵）</span></span><br><span class="line">    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将输入向前传播</span></span><br><span class="line">    embeddings = embedding_layer(sentence_indices)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 继续传播经过一个隐藏单元数为 128 的 LSTM 层</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>, return_sequences=<span class="keyword">True</span>)(embeddings)<span class="comment"># 注意设置 return_sequences=True，返回所有时间序列 </span></span><br><span class="line">    <span class="comment"># 继续经过一个概率值为 0.5 的 Dropout 层</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># 继续传播经过一个隐藏单元数为 128 的 LSTM 层</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>)(X)<span class="comment"># 注意不用设置 return_sequences=True，默认返回最后一个cell的值</span></span><br><span class="line">    <span class="comment"># 继续经过一个概率值为 0.5 的 Dropout 层</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># 这是 softmax 层的第一部分，先通过一个全连接层</span></span><br><span class="line">    X = Dense(<span class="number">5</span>)(X)</span><br><span class="line">    <span class="comment"># 再通过 softmax 激活函数层</span></span><br><span class="line">    X = Activation(<span class="string">'softmax'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建从 sentence_indices 到 X 的模型</span></span><br><span class="line">    model = Model(input=sentence_indices, outputs=X)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>让我们看看模型的 summary，maxLen 为 10：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         (None, 10)                0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">embedding_2 (Embedding)      (None, 10, 50)            20000050  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_1 (LSTM)                (None, 10, 128)           91648     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)          (None, 10, 128)           0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_2 (LSTM)                (None, 128)               131584    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)          (None, 128)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 5)                 645       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">activation_1 (Activation)    (None, 5)                 0         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 20,223,927</span><br><span class="line">Trainable params: 223,877</span><br><span class="line">Non-trainable params: 20,000,050</span><br></pre></td></tr></table></figure><p>其中 20000050  个嵌入层的参数是我们预训练好的，不需要训练，所以只需要更新两个 LSTM 层和一个 Dense 层的 223,877 个参数。</p><p>接下来编译模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><p>模型输入形状(<code>m</code>, <code>max_len</code>) 的索引值，输出形状为(<code>m</code>, <code>number of classes</code>) 的标签值 onehot 向量，接下来将训练集变成相应的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)</span><br><span class="line">Y_train_oh = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br></pre></td></tr></table></figure><p>接下来开始训练，设置 epoch 为 50，batch_size = 32：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train_indices, Y_train_oh, epochs = <span class="number">50</span>, batch_size = <span class="number">32</span>, shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">......    </span><br><span class="line">Epoch 47/50</span><br><span class="line">132/132 [==============================] - 0s - loss: 0.0597 - acc: 0.9773     </span><br><span class="line">Epoch 48/50</span><br><span class="line">132/132 [==============================] - 0s - loss: 0.0428 - acc: 0.9848     </span><br><span class="line">Epoch 49/50</span><br><span class="line">132/132 [==============================] - 0s - loss: 0.0379 - acc: 0.9848     </span><br><span class="line">Epoch 50/50</span><br><span class="line">132/132 [==============================] - 0s - loss: 0.0418 - acc: 0.9773</span><br></pre></td></tr></table></figure><p>让我们在测试集上面试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先测试集同样需要变一下形状</span></span><br><span class="line">X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)</span><br><span class="line">Y_test_oh = convert_to_one_hot(Y_test, C = <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 进行测试</span></span><br><span class="line">loss, acc = model.evaluate(X_test_indices, Y_test_oh)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Test accuracy = "</span>, acc)</span><br></pre></td></tr></table></figure><p>32/56 [================&gt;………….] - ETA: 0s<br>Test accuracy =  0.821428562914</p><p>结果还不错！接下来看看哪些地方搞错了！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">C = <span class="number">5</span></span><br><span class="line">y_test_oh = np.eye(C)[Y_test.reshape(<span class="number">-1</span>)] <span class="comment"># np.eye(C)生成5×5的对角矩阵，np.eye(C)[Y_test.reshape(-1)]取[]中每个元素的索引值</span></span><br><span class="line">                                          <span class="comment"># 注意 np.array[np.array] 将会把 [] 中所有的元素索引出来 </span></span><br><span class="line">X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)</span><br><span class="line">pred = model.predict(X_test_indices)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(X_test)):</span><br><span class="line">    x = X_test_indices</span><br><span class="line">    num = np.argmax(pred[i])<span class="comment"># 将 onehot 变成索引</span></span><br><span class="line">    <span class="keyword">if</span>(num != Y_test[i]):</span><br><span class="line">        print(<span class="string">'Expected emoji:'</span>+ label_to_emoji(Y_test[i]) + <span class="string">' prediction: '</span>+ X_test[i] + label_to_emoji(num).strip())</span><br></pre></td></tr></table></figure><p>Expected emoji:😄 prediction: she got me a nice present    ❤️<br>Expected emoji:😞 prediction: work is hard    😄<br>Expected emoji:😞 prediction: This girl is messing with me    ❤️<br>Expected emoji:😞 prediction: work is horrible    😄<br>Expected emoji:🍴 prediction: any suggestions for dinner    😄<br>Expected emoji:😄 prediction: you brighten my day    ❤️<br>Expected emoji:😞 prediction: she is a bully    😄<br>Expected emoji:😞 prediction: My life is so boring    ❤️<br>Expected emoji:😄 prediction: will you be my valentine    ❤️<br>Expected emoji:😞 prediction: go away    ⚾<br>Expected emoji:🍴 prediction: I did not have breakfast ❤️</p><p>在第一部分我们有一个句子 “not feel good” 总是会失误，原因就是未考虑词语前后联系，看看使用这个模型效果如何：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_test = np.array([<span class="string">'not feeling good'</span>])</span><br><span class="line">X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)</span><br><span class="line">print(x_test[<span class="number">0</span>] +<span class="string">' '</span>+  label_to_emoji(np.argmax(model.predict(X_test_indices))))</span><br></pre></td></tr></table></figure><p>not feeling well 😞</p><p>成功！！</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>词嵌入对于小训练集的 NLP 问题意义重大</li><li>在 Keras 中训练序列化模型需要注意的细节：<ul><li>为了使用 mini-batch，我们必须将所有样本填充至一样的长度</li><li>嵌入层 Embedding() 可以用自己预训练的词嵌入矩阵进行初始化，这些值即可以固定不训练也可以继续在数据集中微调，如果训练集较小，一般不值得训练</li><li>LSTM() 层有一个选项，return_sequences，决定是返回所有  cell 的值还是只最后一个 cell 的值</li><li>使用 Dropout() 层进行正则化</li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 词向量 </tag>
            
            <tag> Debiasing word vectors </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 5 week 2）—— 词嵌入</title>
      <link href="/2018/12/17/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c5w2/"/>
      <url>/2018/12/17/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c5w2/</url>
      <content type="html"><![CDATA[<p>自然语言处理和深度学习是非常重要的组合。使用词向量表示和嵌入层可以在许多行业训练 RNN，例如情感分析，命名实体识别和机器翻译。</p><h2 id="Word-Embeddings-词嵌入介绍"><a href="#Word-Embeddings-词嵌入介绍" class="headerlink" title="Word Embeddings 词嵌入介绍"></a>Word Embeddings 词嵌入介绍</h2><h3 id="Word-representation"><a href="#Word-representation" class="headerlink" title="Word representation"></a>Word representation</h3><p>如何用向量表示一个词？假设我们有一个词汇表 V，数量为 |V| = 10000，V = {a, aaron,…, zulu,\<unk>}，我们可以用 one-hot 编码来表示这些词：</unk></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_1.png" alt=""></p><a id="more"></a><p>但是问题是，对于这两个句子：</p><ul><li>I want a glass of orange ____.</li><li>I want a glass of apple ____.</li></ul><p>对于算法来说，orange juice 是一个非常可能的选择，那么对于 apple 来说，apple juice 也是一个很可能的选择，但是由于 one-hot 编码没能体现出 orange 和 apple 的密切关系（都是水果），所以在这里单词 king 和 orange 的相关程度并不比 apple 低。用词向量表示可以解决这个问题。</p><p>我们先创建一系列特征：性别 gender、皇室 royal、年龄 age、食物 food……用一个数字表示某个词跟这个特征的相关程度，然后形成一个特征向量。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_2.png" alt=""></p><p>于是，我们可以发现 apple 和 orange 具有更加接近的特征向量，在这一点上与实际情况更加符合。</p><p>将这些特征向量可视化，我们可以发现这些词是一组一组分布的。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_3.png" alt=""></p><p>什么是词嵌入？假设特征向量的维度是三维，这形成一个三维的特征空间，每个词在这个空间中占据一个位置，就如同嵌入一样，所以我们称之为“词嵌入”。其实词嵌入相当于给词编码。</p><h3 id="使用词嵌入"><a href="#使用词嵌入" class="headerlink" title="使用词嵌入"></a>使用词嵌入</h3><p>词嵌入对于迁移学习非常有用，即将具有很多数据集的任务 A 迁移到数据集较少的任务 B。</p><ul><li>从大量的文本语料库（十亿到几千亿个单词）中学习单词嵌入，或者下载在线的训练好的词嵌入。</li><li>将这些词嵌入迁移学习到训练集更小的新任务上。（比如十万个词左右）</li><li>可选：继续使用新数据来微调词嵌入。如果第二步训练集太小，通常不会继续微调。</li></ul><h3 id="词嵌入的属性"><a href="#词嵌入的属性" class="headerlink" title="词嵌入的属性"></a>词嵌入的属性</h3><p>我们可以用词向量来进行词的类比：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_4.png" alt=""></p><p>在词向量空间中，代表 man 的这个词的词向量和代表 woman 这个词的词向量之间的距离与 king 向量和 queen 向量之间的距离近似相等，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_5.png" alt=""></p><p>要找到和 man -&gt; woman 类似的一对词 king -&gt; ?，我们可以遍历所有词向量，找到这么一个词 w 满足：$sim(e_w,e_{king}-e_{man}+e_{woman})$ 最大，其中 sim() 是 “cosine 相似度”。</p><script type="math/tex; mode=display">sim(u,v)=\frac{u^Tv}{||u||_2||v||_2}</script><p>它其实表示的是向量 u 和 v 之间的 cos 值，cos 值越大表示两者夹角越小，相似度越大。</p><p>另外 $||u-v||^2$ 也可以表示两者的相似度。</p><h3 id="嵌入矩阵-Embedding-matrix"><a href="#嵌入矩阵-Embedding-matrix" class="headerlink" title="嵌入矩阵 Embedding matrix"></a>嵌入矩阵 Embedding matrix</h3><p>当实现一个算法来学习字嵌入时，最终学习到的是一个嵌入矩阵 E，这个矩阵乘上某个词（例如位于 6257 位的 orange ）的 one-hot 向量（$o_{6257}$）得到它的词向量（$e_{6257}$），即：</p><script type="math/tex; mode=display">E·o_j=e_j</script><p>其中 E 代表 Embedding 矩阵，o 代表 one-hot 向量，e 代表 embedding 向量。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_6.png" alt=""></p><p>值得注意的是，实际中由于向量 o 大部分都是 0，所以不会真正地用 E 去对它进行矩阵乘法，而是使用特殊的函数进行查找。</p><h2 id="学习词嵌入：Word2vec-and-Glove"><a href="#学习词嵌入：Word2vec-and-Glove" class="headerlink" title="学习词嵌入：Word2vec and Glove"></a>学习词嵌入：Word2vec and Glove</h2><h3 id="“学习”词嵌入"><a href="#“学习”词嵌入" class="headerlink" title="“学习”词嵌入"></a>“学习”词嵌入</h3><p>如何通过算法学习得到词嵌入矩阵呢？</p><p>例如我们需要通过前面若干词预测最后一个词，I want a glass of orange ____. 我们先将前六个词的 one-hot 向量（10000 ）乘上随机初始化的词嵌入矩阵 E，然后将得到的所有词向量 (300) 合并为一个向量 (1800)，接着通过一层隐藏层，最后通过 softmax 输出一个 one-hot 向量 (10000)，它就是我们的预测词。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_7.png" alt=""></p><p>这个算法的一个超参数是选择固定窗的长度，上面是前六个词预测最后一个词，窗长就是六，也可以选择四个词的窗长，根据需要进行调整。</p><p>算法的参数就是 W 和 b 这些权值，使用它们进行梯度下降，使得训练集出现的可能性最大化。</p><p>这个算法为什么能学习到词嵌入？</p><p>如果训练集语料库中出现了 orange juice 和 apple juice，那么在这种样例的激励下，算法就会学习到 orange 和 apple 具有非常相似的词嵌入，这样才能更好地拟合训练集。</p><h4 id="构造-上下文-目标词-组合"><a href="#构造-上下文-目标词-组合" class="headerlink" title="构造 上下文/目标词 组合"></a>构造 上下文/目标词 组合</h4><p>对于一句话 I want a glass of orange <strong>juice</strong> to go along with my cereal. </p><p>假如 juice 是我们需要预测的目标词，那么有许多种构造上下文来预测它的方法：</p><ul><li>后面四个词</li><li>左边四个右边四个</li><li>最后一个词</li><li>附近一个词</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_8.png" alt=""></p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><h4 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h4><p>下面是一种训练词向量的 skip-gram 模型，skip-gram 指的是我们先取一个词作为 context word，然后在固定窗的长度内的另一个词作为目标词 target word，然后我们就可以建立起一个监督学习问题了。如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_9.png" alt=""></p><p>来自博客 <a href="https://blog.csdn.net/wangyangzhizhou/article/details/77530479" target="_blank" rel="noopener">seaboat</a> 的一张图片：<img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_20.png" alt=""></p><h4 id="model-details"><a href="#model-details" class="headerlink" title="model details"></a>model details</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_10.png" alt=""></p><p>$o_c \rightarrow 乘上E \rightarrow e_c \rightarrow softmax(W·e_c+b) \rightarrow \hat y$</p><p>其中 $W = \begin{bmatrix}<br>\theta_1^T\\<br>\theta_2^T\\<br>…\\<br>\theta_{10000}^T<br>\end{bmatrix}$ </p><h4 id="用-softmax-进行分类的问题"><a href="#用-softmax-进行分类的问题" class="headerlink" title="用 softmax 进行分类的问题"></a>用 softmax 进行分类的问题</h4><script type="math/tex; mode=display">\hat y _t = p ( t | c ) = \frac { e ^ { \theta _ { t } ^ { T } e _ { c } } } { \sum _ { j = 1 } ^ { 10,000 } e ^ { \theta _ { j } ^ { T } e _ { c } } }</script><p>其中 $\theta_j$ 是 softmax 层的参数 W 的第 j 行，$\theta_t$ 是 W 中与输出有关的那一行。</p><p>softmax 层存在的问题是：我们需要进行 10000 次加法，这会非常非常慢！！解决的办法是使用一种“分级 softmax”分类器 (hierarchical softmax classifier)。</p><h4 id="如何选择-context-c"><a href="#如何选择-context-c" class="headerlink" title="如何选择 context c"></a>如何选择 context c</h4><p>我们必须先选择好 context c，再在窗内随机选择 target t，那么如何选择 c？</p><p>一种办法是随机均匀地在文本中采样，但是问题是这样会采样进许多被频繁使用的词，比如 of the a an 等等，导致我们浪费时间在这些常用词的词嵌入上，所以我们需要在采样的时候适当地剔除这些词，实现常用词和不常用词的平衡。</p><h3 id="负采样算法-negative-sampling"><a href="#负采样算法-negative-sampling" class="headerlink" title="负采样算法 negative sampling"></a>负采样算法 negative sampling</h3><p>首先定义一个新的监督学习问题，与上面 skip-gram 类似，先取一个词作为 context word，然后在固定窗的长度内随机挑选另一个词作为目标词 target word，如果选到了相关的词语例如 orange juice，那么这一个词语对的 y 值就是 1（正样本），否则就是 0（负样本），负样本的个数为 k.</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_11.png" alt=""></p><h4 id="如何选取-k？"><a href="#如何选取-k？" class="headerlink" title="如何选取 k？"></a>如何选取 k？</h4><ul><li>对于小数据集：k = 5～20</li><li>对于大数据集：k = 2～5</li></ul><h4 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h4><p>我们知道上面的 Word2Vec 算法步骤为：</p><p>$o_c \rightarrow E \rightarrow e_c \rightarrow softmax(W·e_c+b) \rightarrow one-hot向量\hat y$</p><p>其中 $W = \begin{bmatrix}<br>\theta_1^T\\<br>\theta_2^T\\<br>…\\<br>\theta_{10000}^T<br>\end{bmatrix}$ ，$\hat y _t = p ( t | c ) = \frac { e ^ { \theta _ { t } ^ { T } e _ { c } } } { \sum _ { j = 1 } ^ { 10,000 } e ^ { \theta _ { j } ^ { T } e _ { c } } }$ </p><p>而负采样算法去掉 softmax 的步骤，改为对 $e_c$ 进行 10000 次逻辑回归：</p><script type="math/tex; mode=display">P(y=1|c,t)=sigmoid(\theta_t^Te_c)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_12.png" alt=""></p><p>在实际中，我们只训练其中的 k+1 个逻辑回归模型，即训练 1 个正样本和 k 个负样本的逻辑回归模型，这样使得我们不用计算 10000 元分类的 softmax 层，大大减少计算量。</p><h4 id="如何抽取负样本？"><a href="#如何抽取负样本？" class="headerlink" title="如何抽取负样本？"></a>如何抽取负样本？</h4><p>算法作者 Mikolov 认为，最好的方法是介于根据经验分布的采样和均匀分布的采样之间，他们所做的是根据词频的 3/4 次幂来抽样。</p><script type="math/tex; mode=display">P(w_i)=\frac{f^{\frac{3}{4}}(w_i)}{\sum^{10000}_{j=1}f^{\frac{3}{4}}(w_j)}</script><p>其中 $f^(w_i)$ 表示观察到的英语中某个词的词频。</p><h3 id="GloVe-词向量-global-vector"><a href="#GloVe-词向量-global-vector" class="headerlink" title="GloVe 词向量 (global vector )"></a>GloVe 词向量 (global vector )</h3><p>首先我们有一个目标词 i，还有一个上下文词 j，那么用 $X_{ij}$ 表示 i 出现在 j 的上下文的次数，一般来说，$X_{ij}=X_{ji}$ ，也就是说它表示 i 和 j 共同出现在一个上下文的次数，或者说<strong>彼此接近的频繁程度</strong>，可以遍历语料库数出来。</p><p>GloVe 算法就是实现以下的优化问题：</p><script type="math/tex; mode=display">minimize \sum\limits_{i=1}^{10000}\sum\limits_{j=1}^{10000} f(X_{ij})(\theta_i^T e_{j} + b_i+b'_j -logX_{ij})^2</script><p>其中：</p><ul><li>10000 是词汇表的个数，遍历整个词汇表</li><li>$\theta、b_i、b’_j$ 都是待学习的参数</li><li>为了防止出现 $log0$ 无意义的情况，加上权重值 $f(x_{ij})$，当 $X_{ij}=0$ 时，$f(x_{ij})=0$，定义 $0log0=0$，另外权重函数可以使得对“榴莲”这种较少出现的词给出不太小的权重，给“of the an”等常见的词给出不太大的权重</li><li>一个有趣的事实是：$\theta_i $ 和 $e_j$ 是对称的，也就是说它们两者互为参数，起到的作用是相同的，所以我们最后学习到的参数可以是这两者的平均值，即 $e_w^{(final)}=\frac{e_w+\theta_w}{2}$ </li></ul><h2 id="词嵌入的应用"><a href="#词嵌入的应用" class="headerlink" title="词嵌入的应用"></a>词嵌入的应用</h2><h3 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h3><p>情感分类的任务是分析一段文本，告诉人们某个人是不是喜欢他们正在谈论的东西 。它是自然语言处理最重要的组成部分，有许多应用。情感分类的挑战之一是可能缺乏一个特别大的标签训练集，不过使用词嵌入后，依靠一个中等大小的标签训练集也可以构建出一个很好的情感分类器。</p><p>假设有一些顾客对一家酒店的评价和这些顾客打出的星级，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_13.png" alt=""></p><p>我们需要一个情感分类器，输入一段评价，然后猜出他给出的星级。</p><h4 id="一个简单的情感分类模型"><a href="#一个简单的情感分类模型" class="headerlink" title="一个简单的情感分类模型"></a>一个简单的情感分类模型</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_14.png" alt=""></p><p>这个算法的一个问题是它忽视了词的顺序，和上下文的联系，例如“Completelylacking in <strong>good</strong> taste, <strong>good</strong> service, and <strong>good</strong> ambience” ，句子中出现了很多 good，但是实际上这是一个一星的差评，更好的方法是使用 RNN 进行情感分类。</p><h4 id="用-RNN-进行情感分类"><a href="#用-RNN-进行情感分类" class="headerlink" title="用 RNN 进行情感分类"></a>用 RNN 进行情感分类</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_15.png" alt=""></p><h3 id="词嵌入“去偏见”-Debiasing-word-embedding"><a href="#词嵌入“去偏见”-Debiasing-word-embedding" class="headerlink" title="词嵌入“去偏见” Debiasing word embedding"></a>词嵌入“去偏见” Debiasing word embedding</h3><p>由于词嵌入的训练语料是来自人类社会，而人类社会存在许多偏见，例如男人对应程序员，女人对应家庭主妇，而这是一个不正常的存在性别歧视的刻板印象。机器学习和 AI 算法越来越受人信任，可以用来帮人做出非常重要的决策，所以我们需要尽可能确保，算法中没有性别偏见、种族偏见等。一个不好的词嵌入会反应性别、种族、年龄和性取向，而比较好的算法应该能够得出：男人对应程序员而女人也对于程序员。这是非常具有人文主义的一项技术，展示了机器学习社群对减少社会偏见的责任感。</p><p>那么如何减小词嵌入中的“偏见”呢？</p><p>假设我们有一个训练好的词向量模型如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_16.png" alt=""></p><ul><li><p>第一步：确定偏差的方向</p><p>假设词向量中只有一个维度是表示性别偏差的方向，那么 $e_{he}-e_{she}$ 、$e_{male}-e_{female}​$……然后将这些差值取平均，那么这个平均值向量就是表示偏差轴的向量，也就是偏差的方向。但是实际中使用了更复杂的 SVU 算法（一种类似于 PCA 主成份分析的算法）来确定偏差方向。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_17.png" alt=""></p></li><li><p>第二步是中立化（Neutralize）：对于没有性别定义的词例如 doctor 或者 babysitter，通过将它们映射到非偏差轴摆脱偏差，如下图所示。那么那些词需要中立化？作者训练一个分类器来研究这个问题，得出的结论是大部分的英语单词都需要中立化，除了那些有性别定义的一小部分词语。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_18.png" alt=""></p></li><li><p>第三步是平均化（equalization）：对于有性别定义的词例如 grandmother 和 grandfather 或者 boy 和 girl，我们希望这些词到非偏差轴的距离相同，即离修正后的 babysitter 这类词的距离相同，做法是移动 grandmother 和 grandfather 分别到一个点，使得到中间轴的距离相等。有许多这类词，grandmother-grandfather、 boy-girl、sorority-fraternity、girlhood-boyhood、sister-brother、niece-nephew、daughter-son 等等，这些都需要进行平均化。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.2_19.png" alt=""></p></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> Word Embedding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>用 numpy 进行快速傅立叶变换 fft</title>
      <link href="/2018/12/13/%E7%94%A8%20numpy%20%E8%BF%9B%E8%A1%8C%E5%BF%AB%E9%80%9F%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%20fft/"/>
      <url>/2018/12/13/%E7%94%A8%20numpy%20%E8%BF%9B%E8%A1%8C%E5%BF%AB%E9%80%9F%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%20fft/</url>
      <content type="html"><![CDATA[<p>工程信号处理作业，用 matlab 对一个信号进行傅立叶变换，由于从没用过 matlab，所以想看看 python 能不能做，一查果然 numpy 有 fft 的函数。</p><p>fft 就是把信号在时域的采样的 N 个实数，变成时域的 N 个复数，具体的输入输出见这个<a href="https://www.youtube.com/watch?v=z7X6jgFnB6Y" target="_blank" rel="noopener">youtube 视频</a>.</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/2018/12/13_1.png" alt=""></p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">N = <span class="number">1000</span>    <span class="comment">#采样点数</span></span><br><span class="line">Td = <span class="number">4</span>      <span class="comment">#采样总时间,采样总时间的倒数就是频谱的分辨率 df</span></span><br><span class="line">fs = N/Td   <span class="comment">#采样频率</span></span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>,Td,N)<span class="comment">#采样时域的坐标</span></span><br><span class="line">y = np.sin(<span class="number">100</span>*(x<span class="number">-2</span>))/(<span class="number">50</span>*(x<span class="number">-2</span>))<span class="comment">#需要进行傅立叶变换的函数</span></span><br><span class="line"></span><br><span class="line">freqx = np.fft.fftfreq(N,d=<span class="number">1</span>/fs)<span class="comment">#频域的坐标,采样 N 个点那么频域也有 N 个点,fftfreq 直接生成 N 个频域点的*实际*坐标</span></span><br><span class="line"><span class="comment">#fftfreqs 第二个参数 d 是采样频率的倒数，那么频谱分辨率 df=1/Td=1/(N/fs)=fs/N=1/(d*N)=1/Td</span></span><br><span class="line"></span><br><span class="line">fft_vals = np.fft.fft(y)<span class="comment">#未经处理的 fft 输出,幅度只有实际值的一半,而且未经过归一化</span></span><br><span class="line">fft_theo = <span class="number">2.0</span>*np.abs(fft_vals/N)<span class="comment">#根据公式,除以 N 归一化之后 abs 求幅度,再将幅度乘以 2 就是真实的幅度值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'oringin signal'</span>)</span><br><span class="line">plt.plot(x,y,color=<span class="string">'red'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'time(s)'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'true fft output in frequency domain'</span>)</span><br><span class="line">plt.plot(freqx, fft_theo)</span><br><span class="line">plt.xlabel(<span class="string">'frequency(Hz)'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/2018/12/13/2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/2018/12/13/3.png" alt=""></p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 傅立叶变换 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 5 week 1）</title>
      <link href="/2018/12/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c5w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/12/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c5w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h1 id="part-1-Building-your-Recurrent-Neural-Network-Step-by-Step"><a href="#part-1-Building-your-Recurrent-Neural-Network-Step-by-Step" class="headerlink" title="part 1- Building your Recurrent Neural Network - Step by Step"></a>part 1- Building your Recurrent Neural Network - Step by Step</h1><p>这是这一周编程作业的第一部分，让我们从无到有构建了 RNN 和 LSTM 的前向和反向传播函数，但是作业中给的公式出现很多问题，下面都是更正后的公式，还有一些符号的提法比较模糊，下面也做了说明。</p><p>首先引入需要的包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> rnn_utils <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h3 id="单个-RNN-前向传播"><a href="#单个-RNN-前向传播" class="headerlink" title="单个 RNN 前向传播"></a>单个 RNN 前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_1.png" alt=""></p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Wax,xt) + np.dot(Waa,a_prev) + ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya,a_prev) + by)   </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure><h3 id="总的-RNN-前向传播"><a href="#总的-RNN-前向传播" class="headerlink" title="总的 RNN 前向传播"></a>总的 RNN 前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_2.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters["Wya"]</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a" and "y" with zeros</span></span><br><span class="line">    a = np.zeros((n_a,m,T_x)) <span class="comment"># 初始化一个存放所有激活值的容器</span></span><br><span class="line">    y_pred = np.zeros((n_y,m,T_x)) <span class="comment"># 初始化一个存放所有y预测值的容器</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next </span></span><br><span class="line">    a_next = a0 <span class="comment"># 将 a_next 初始化为 a&lt;0&gt; </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Append "cache" to "caches" (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure><h3 id="单个-RNN-单元反向传播"><a href="#单个-RNN-单元反向传播" class="headerlink" title="单个 RNN 单元反向传播"></a>单个 RNN 单元反向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_3.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass for the RNN-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradient of loss with respect to next hidden state</span></span><br><span class="line"><span class="string">    cache -- python dictionary containing useful values (output of rnn_cell_forward())</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradients of input data, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from cache</span></span><br><span class="line">    (a_next, a_prev, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from parameters</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of tanh with respect to a_next </span></span><br><span class="line">    dtanh = da_next * (<span class="number">1</span> - a_next**<span class="number">2</span>) <span class="comment"># 指的是代价函数对 (Wax·x^&lt;t&gt;+Waa·a^&lt;t-1&gt;+ba) 的导数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of the loss with respect to Wax</span></span><br><span class="line">    dxt = np.dot(Wax.T, dtanh)</span><br><span class="line">    dWax = np.dot(dtanh, xt.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to Waa </span></span><br><span class="line">    da_prev = np.dot(Waa.T, dtanh)</span><br><span class="line">    dWaa = np.dot(dtanh, a_prev.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to b </span></span><br><span class="line">    dba = np.sum(dtanh, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa, <span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="总的-RNN-反向传播"><a href="#总的-RNN-反向传播" class="headerlink" title="总的 RNN 反向传播"></a>总的 RNN 反向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_4.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for a RNN over an entire sequence of input data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple containing information from the forward pass (rnn_forward)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches </span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, a0, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes </span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes </span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    dWax = np.zeros((n_a, n_x))</span><br><span class="line">    dWaa = np.zeros((n_a, n_a))</span><br><span class="line">    dba = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    da0 = np.zeros((n_a, m))</span><br><span class="line">    da_prevt = np.zeros((n_a, m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop through all the time steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (≈1 line)</span></span><br><span class="line">        <span class="comment"># 这里注意：a^&lt;t&gt; 在这里分两条路影响代价函数 J，所以 da = da[:, :, t] + da_prevt</span></span><br><span class="line">        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Retrieve derivatives from gradients (≈ 1 line)</span></span><br><span class="line">        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[<span class="string">"dxt"</span>], gradients[<span class="string">"da_prev"</span>], gradients[<span class="string">"dWax"</span>], gradients[<span class="string">"dWaa"</span>], gradients[<span class="string">"dba"</span>]</span><br><span class="line">        <span class="comment"># Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)</span></span><br><span class="line">        dx[:, :, t] = dxt</span><br><span class="line">        <span class="comment"># 注意：前向传播中所有 t 的 Wax、Waa、ba 都是相等的，但是每个 t 时间的 dWaxt、dWaa、dba 都是不相等的，</span></span><br><span class="line">        <span class="comment">#      它们都影响了代价函数 J，所以需要把每一个时间 t 求到的 dWaxt、dWaat、dbat 累加起来</span></span><br><span class="line">        dWax += dWaxt</span><br><span class="line">        dWaa += dWaat</span><br><span class="line">        dba += dbat</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) </span></span><br><span class="line">    da0 = da_prevt</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa,<span class="string">"dba"</span>: dba&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="单个-LSTM-单元的前向传播"><a href="#单个-LSTM-单元的前向传播" class="headerlink" title="单个 LSTM 单元的前向传播"></a>单个 LSTM 单元的前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_5.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span></span><br><span class="line"><span class="string">          c stands for the memory value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>]</span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wi = parameters[<span class="string">"Wi"</span>]</span><br><span class="line">    bi = parameters[<span class="string">"bi"</span>]</span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>]</span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>]</span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt 将 a_prev 和 xt 拼在一起</span></span><br><span class="line">    concat = np.zeros((n_a+n_x, m))</span><br><span class="line">    concat[: n_a, :] = a_prev</span><br><span class="line">    concat[n_a :, :] = xt</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft * c_prev + it * cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wy,a_next) + by)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure><h3 id="总的-LSTM-前向传播"><a href="#总的-LSTM-前向传播" class="headerlink" title="总的 LSTM 前向传播"></a>总的 LSTM 前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_6.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters['Wy'] </span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">'Wy'</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros </span></span><br><span class="line">    a = np.zeros((n_a,m,T_x)) <span class="comment"># 存放 a 的容器</span></span><br><span class="line">    c = np.zeros((n_a,m,T_x)) <span class="comment"># 存放 c 的容器</span></span><br><span class="line">    y = np.zeros((n_y,m,T_x)) <span class="comment"># 存放 y 的容器</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a,m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache </span></span><br><span class="line">        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a </span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y </span></span><br><span class="line">        y[:,:,t] = yt</span><br><span class="line">        <span class="comment"># Save the value of the next cell state </span></span><br><span class="line">        c[:,:,t]  = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure><h3 id="单个-LSTM-单元的反向传播"><a href="#单个-LSTM-单元的反向传播" class="headerlink" title="单个 LSTM 单元的反向传播"></a>单个 LSTM 单元的反向传播</h3><p>由于这部分给的公式发生了错误，所以结合论坛里的讨论自己进行了公式的推导和更正，可以在<a href="https://chenyichen.xyz/2018/12/01/deeplearning.ai%20%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%80%E5%91%A8%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%20lstm%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E9%83%A8%E5%88%86%E7%9A%84%E5%85%AC%E5%BC%8F%E6%9B%B4%E6%AD%A3%E5%8F%8A%E6%8E%A8%E5%AF%BC/">这篇笔记</a>里找到。</p><script type="math/tex; mode=display">\begin{split}原公式中的“d \Gamma_o^{\langle t \rangle}” &= dZ_o^{<t>}=\frac{\partial J}{\partial Z_o^{<t>}}=\frac{\partial J}{\partial(W_o[a^{<t-1>},x^{<t>}]+b_o)}  \\ &= da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*\left(1-\Gamma_o^{\langle t \rangle}\right) \end{split}\tag{7}</script><script type="math/tex; mode=display">\begin{split}原公式中的“d\widetilde{c}^{\langle t \rangle}”&=dZ_c^{<t>}=\frac{\partial J}{\partial Z_c^{<t>}}=\frac{\partial J}{\partial(W_c[a^{<t-1>},x^{<t>}]+b_c)} \\&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle}* (1-\tanh(c_{next})^2) * da_{next} \right) \Gamma_u^{\langle t \rangle} * \left(1-\left(\widetilde c^{\langle t \rangle}\right)^2\right) \end{split}\tag{8}</script><script type="math/tex; mode=display">\begin{split}原公式中的“d\Gamma_u^{\langle t \rangle}”&=dZ_u^{<t>}=\frac{\partial J}{\partial Z_u^{<t>}}=\frac{\partial J}{\partial(W_u[a^{<t-1>},x^{<t>}]+b_u)}  \\&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle}* (1-\tanh(c_{next})^2) * da_{next}\right)*\Gamma_u^{\langle t \rangle}* \widetilde{c}^{\langle t \rangle} *\left(1-\Gamma_u^{\langle t \rangle}\right)\end{split}\tag{9}</script><script type="math/tex; mode=display">\begin{split}原公式中的“d\Gamma_f^{\langle t \rangle}”&=dZ_f^{<t>}=\frac{\partial J}{\partial Z_f^{<t>}}=\frac{\partial J}{\partial(W_f[a^{<t-1>},x^{<t>}]+b_f)}  \\&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle} * (1-\tanh(c_{next})^2)  * da_{next}\right)*\Gamma_f^{\langle t \rangle}* c_{prev}*\left(1-\Gamma_f^{\langle t \rangle}\right)\end{split}\tag{10}</script><script type="math/tex; mode=display">dW_f = d\Gamma_f^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{11}</script><script type="math/tex; mode=display">dW_u = d\Gamma_u^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{12}</script><script type="math/tex; mode=display">dW_c = d\widetilde c^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{13}</script><script type="math/tex; mode=display">dW_o = d\Gamma_o^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{14}</script><script type="math/tex; mode=display">db_f=np.sum(d\Gamma_f^{\langle t \rangle},axis=1,keepdims=True)\tag{15}</script><script type="math/tex; mode=display">db_u=np.sum(d\Gamma_u^{\langle t \rangle},axis=1,keepdims=True)\tag{16}</script><script type="math/tex; mode=display">db_c=np.sum(d\Gamma_c^{\langle t \rangle},axis=1,keepdims=True)\tag{17}</script><script type="math/tex; mode=display">db_o=np.sum(d\Gamma_o^{\langle t \rangle},axis=1,keepdims=True)\tag{18}</script><script type="math/tex; mode=display">da_{prev} = W_f^T[:,:n_a] d\Gamma_f^{\langle t \rangle} + W_u^T[:,:n_a]   d\Gamma_u^{\langle t \rangle}+ W_c^T[:,:n_a] d\widetilde c^{\langle t \rangle} + W_o^T[:,:n_a] d\Gamma_o^{\langle t \rangle} \tag{19}</script><script type="math/tex; mode=display">dc_{prev} = dc_{next}*\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \tag{20}</script><script type="math/tex; mode=display">dx^{\langle t \rangle} = W_f^T[:,n_a:] d\Gamma_f^{\langle t \rangle} + W_u^T[:,n_a:]  d\Gamma_u^{\langle t \rangle}+ W_c^T[:,n_a:] d\widetilde c^{\langle t \rangle} + W_o^T[:,n_a:] d\Gamma_o^{\langle t \rangle}\tag{21}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next, dc_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the LSTM-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradients of next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    dc_next -- Gradients of next cell state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    cache -- cache storing information from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from xt's and a_next's shape </span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_a, m = a_next.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) </span></span><br><span class="line">    dot = da_next * np.tanh(c_next)*ot*(<span class="number">1</span>-ot) <span class="comment"># 也就是 dJ / d(W_o[a^&#123;&lt;t-1&gt;&#125;,x^&#123;&lt;t&gt;&#125;]+b_o),下同</span></span><br><span class="line">    dcct = (dc_next + da_next*ot*(<span class="number">1</span>-np.tanh(c_next)**<span class="number">2</span>))*it*(<span class="number">1</span>-cct**<span class="number">2</span>)</span><br><span class="line">    dit = (dc_next + da_next*ot*(<span class="number">1</span>-np.tanh(c_next)**<span class="number">2</span>))*cct*it*(<span class="number">1</span>-it) <span class="comment"># it 就是更新门 updategate</span></span><br><span class="line">    dft = (dc_next + da_next*ot*(<span class="number">1</span>-np.tanh(c_next)**<span class="number">2</span>))*c_prev*ft*(<span class="number">1</span>-ft)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute parameters related derivatives. Use equations (11)-(14) </span></span><br><span class="line">    dWf = np.dot(dft, np.vstack((a_prev,xt)).T)</span><br><span class="line">    dWi = np.dot(dit, np.vstack((a_prev,xt)).T)</span><br><span class="line">    dWc = np.dot(dcct,np.vstack((a_prev,xt)).T)</span><br><span class="line">    dWo = np.dot(dot,np.vstack((a_prev,xt)).T)</span><br><span class="line">    dbf = np.sum(dft, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dbi = np.sum(dit, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dbc = np.sum(dcct, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dbo = np.sum(dot, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). </span></span><br><span class="line">    da_prev = np.dot(parameters[<span class="string">'Wf'</span>][:,:n_a].T,dft) + np.dot(parameters[<span class="string">'Wi'</span>][:,:n_a].T,dit) + np.dot(parameters[<span class="string">'Wo'</span>][:,:n_a].T,dot) + np.dot(parameters[<span class="string">'Wc'</span>][:,:n_a].T,dcct) </span><br><span class="line">    dc_prev = da_next*ot*(<span class="number">1</span>-(np.tanh(c_next))**<span class="number">2</span>)</span><br><span class="line">    dxt = np.dot(parameters[<span class="string">'Wf'</span>][:,n_a:].T,dft) + np.dot(parameters[<span class="string">'Wi'</span>][:,n_a:].T,dit) + np.dot(parameters[<span class="string">'Wo'</span>][:,n_a:].T,dot) + np.dot(parameters[<span class="string">'Wc'</span>][:,n_a:].T,dcct)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Save gradients in dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dc_prev"</span>: dc_prev, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="总的-LSTM-反向传播"><a href="#总的-LSTM-反向传播" class="headerlink" title="总的 LSTM 反向传播"></a>总的 LSTM 反向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- cache storing information from the forward pass (lstm_forward)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient of inputs, of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches.</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes </span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes</span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    da0 = np.zeros((n_a,m))</span><br><span class="line">    da_prevt = np.zeros((n_a,m))</span><br><span class="line">    dc_prevt = np.zeros((n_a,m))</span><br><span class="line">    dWf = np.zeros((n_a, n_a+n_x)) <span class="comment"># 每个时间点 t 的 Wf 都是相等的</span></span><br><span class="line">    dWi = np.zeros((n_a,n_a+n_x))</span><br><span class="line">    dWc = np.zeros((n_a,n_a+n_x))</span><br><span class="line">    dWo = np.zeros((n_a,n_a+n_x))</span><br><span class="line">    dbf = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    dbi = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    dbc = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    dbo = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop back over the whole sequence</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute all gradients using lstm_cell_backward</span></span><br><span class="line">        gradients = lstm_cell_backward(da[:,:,t], dc_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Store or add the gradient to the parameters' previous step's gradient</span></span><br><span class="line">        dx[:,:,t] = gradients[<span class="string">'dxt'</span>]</span><br><span class="line">        <span class="comment"># 注意：前向传播中所有 t 的 Wf、Wi、Wo、Wc、ba、bi、bo、bc 都是相等的，</span></span><br><span class="line">        <span class="comment">#      但是每个 t 时间的 dWf、dWi、dWo、dWc、dba、dbi、dbo、dbc 都是不相等的，</span></span><br><span class="line">        <span class="comment">#      它们都影响了代价函数 J，</span></span><br><span class="line">        <span class="comment">#      所以需要把每一个时间 t 求到的 dWf、dWi、dWo、dWc、dba、dbi、dbo、dbc 累加起来</span></span><br><span class="line">        dWf += gradients[<span class="string">'dWf'</span>]</span><br><span class="line">        dWi += gradients[<span class="string">'dWi'</span>]</span><br><span class="line">        dWc += gradients[<span class="string">'dWc'</span>]</span><br><span class="line">        dWo += gradients[<span class="string">'dWo'</span>]</span><br><span class="line">        dbf += gradients[<span class="string">'dbf'</span>]</span><br><span class="line">        dbi += gradients[<span class="string">'dbi'</span>]</span><br><span class="line">        dbc += gradients[<span class="string">'dbc'</span>]</span><br><span class="line">        dbo += gradients[<span class="string">'dbo'</span>]</span><br><span class="line">    <span class="comment"># Set the first activation's gradient to the backpropagated gradient da_prev.</span></span><br><span class="line">    da0 = gradients[<span class="string">'da_prev'</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h1 id="Part-2-Character-level-language-model-Dinosaurus-land"><a href="#Part-2-Character-level-language-model-Dinosaurus-land" class="headerlink" title="Part 2-Character level language model - Dinosaurus land"></a>Part 2-Character level language model - Dinosaurus land</h1><p>创造一个字符级的语言模型，通过一堆恐龙名字进行训练，用来生成新的恐龙名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><h3 id="数据集加载和预处理"><a href="#数据集加载和预处理" class="headerlink" title="数据集加载和预处理"></a>数据集加载和预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = open(<span class="string">'dinos.txt'</span>, <span class="string">'r'</span>).read()</span><br><span class="line">data= data.lower()</span><br><span class="line">chars = list(set(data))<span class="comment"># 去除重复元素</span></span><br><span class="line">data_size, vocab_size = len(data), len(chars)</span><br><span class="line">print(<span class="string">'There are %d total characters and %d unique characters in your data.'</span> % (data_size, vocab_size))</span><br></pre></td></tr></table></figure><p>output：There are 19909 total characters and 27 unique characters in your data.</p><p>由于是字符级别的，所以一共有 26 个英文字母加上结束符 \n = 27 个字符，将这些字符一一对应存入字典中。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(sorted(chars)) &#125; <span class="comment"># sorted 进行排序</span></span><br><span class="line">ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(sorted(chars)) &#125;</span><br><span class="line">print(ix_to_char)</span><br></pre></td></tr></table></figure><p>output：{0: ‘\n’, 1: ‘a’, 2: ‘b’, 3: ‘c’, 4: ‘d’, 5: ‘e’, 6: ‘f’, 7: ‘g’, 8: ‘h’, 9: ‘i’, 10: ‘j’, 11: ‘k’, 12: ‘l’, 13: ‘m’, 14: ‘n’, 15: ‘o’, 16: ‘p’, 17: ‘q’, 18: ‘r’, 19: ‘s’, 20: ‘t’, 21: ‘u’, 22: ‘v’, 23: ‘w’, 24: ‘x’, 25: ‘y’, 26: ‘z’}</p><h3 id="模型概况"><a href="#模型概况" class="headerlink" title="模型概况"></a>模型概况</h3><ul><li>初始化参数</li><li>优化循环<ul><li>前向传播计算 loss</li><li>反向传播计算 loss 对每个参数的梯度</li><li>进行梯度剪枝 gradient clipping，防止梯度爆炸</li><li>更新参数</li></ul></li><li>返回学习参数</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_2.png" alt=""></p><h2 id="建立模型组块"><a href="#建立模型组块" class="headerlink" title="建立模型组块"></a>建立模型组块</h2><p>两个需要建立的：Gradient clipping 和 采样 sampling</p><h3 id="梯度剪枝-gradient-clipping"><a href="#梯度剪枝-gradient-clipping" class="headerlink" title="梯度剪枝 gradient clipping"></a>梯度剪枝 gradient clipping</h3><p>反向传播得到梯度后，在更新参数之前，进行一步梯度剪枝的操作，用以确保不会发生梯度爆炸，也就是把梯度限制在某个区间内，比如 [-10,10]，大于 10 的梯度取 10，小于 -10 的梯度取 -10.</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_7.png" alt=""></p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients, maxValue)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Clips the gradients' values between minimum and maximum.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    gradients -- a dictionary containing the gradients "dWaa", "dWax", "dWya", "db", "dby"</span></span><br><span class="line"><span class="string">    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    gradients -- a dictionary with the clipped gradients.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    dWaa, dWax, dWya, db, dby = gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWax'</span>], gradients[<span class="string">'dWya'</span>], gradients[<span class="string">'db'</span>], gradients[<span class="string">'dby'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)</span></span><br><span class="line">    <span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax, dWaa, dWya, db, dby]:</span><br><span class="line">        np.minimum(gradient,maxValue,out=gradient)</span><br><span class="line">        np.maximum(gradient,-maxValue,out=gradient)<span class="comment"># 注意！！gradient 在这里只是一个临时变量，我们需要把得到的值 out 输出给它</span></span><br><span class="line"></span><br><span class="line">    gradients = &#123;<span class="string">"dWaa"</span>: dWaa, <span class="string">"dWax"</span>: dWax, <span class="string">"dWya"</span>: dWya, <span class="string">"db"</span>: db, <span class="string">"dby"</span>: dby&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><h3 id="采样-sampling"><a href="#采样-sampling" class="headerlink" title="采样 sampling"></a>采样 sampling</h3><p>模型训练好之后，如何生成文字呢？如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_8.png" alt=""></p><ul><li><p>先输入一个假的输入 $x^{<1>} = 0，a^{<0>} = 0$</0></1></p></li><li><p>按照公式计算 $\hat y^{<1>}$ 和 $a^{<1>}$</1></1></p></li><li><p>进行采样：实际上 $\hat y^{<1>}$ 是 $x^{<2>}$ 的预测值，包含了取不同值的概率，我们根据不同的概率进行随机的取值，例如取索引值 3 的概率为 0.16，那么 $x^{<2>}$ 就有 16% 的概率被赋值为 3</2></2></1></p><ul><li><p>为什么不直接取 $\hat y^{<1>}$ 中概率值最大的赋值给 $x^{<2>}$？论坛看到一个解释：</2></1></p><p>Sampling produces variety. Say you have a list of items with probability of occurrence “car” (30%) “airplane” (70%). If you simply choose the argmax of the probability from the list then you always choose airplane which gets boring. If you alternatively “sample” from the list then 70% of the time you get airplane and 30% car. This is really important in longer sentences; if you choose the argmax you end up with the same sentence all the time (exception when multiple words have exactly same probability 50:50), but sampling gives you an interesting mix of sentences from the same list.</p></li></ul></li><li><p>将采样得到的 $x^{<n+1>}$ 索引值进行 one-hot 编码赋值给 x 以输入下一个循环。重复第一步直到 y 输出一个结束符 “\n” </n+1></p></li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters, char_to_ix)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Sample a sequence of characters according to a sequence of probability distributions output of the RNN</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. </span></span><br><span class="line"><span class="string">    char_to_ix -- python dictionary mapping each character to an index.</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    indices -- a list of length n containing the indices of the sampled characters.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters and relevant shapes from "parameters" dictionary</span></span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    vocab_size = by.shape[<span class="number">0</span>]</span><br><span class="line">    n_a = Waa.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Create the one-hot vector x for the first character (initializing the sequence generation).</span></span><br><span class="line">    x = np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Step 1': Initialize a_prev as zeros </span></span><br><span class="line">    a_prev = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)</span></span><br><span class="line">    indices = []<span class="comment"># 存放每一步生成的索引值</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Idx is a flag to detect a newline character, we initialize it to -1</span></span><br><span class="line">    idx = <span class="number">-1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over time-steps t. At each time-step, sample a character from a probability distribution and append </span></span><br><span class="line">    <span class="comment"># its index to "indices". We'll stop if we reach 50 characters (which should be very unlikely with a well </span></span><br><span class="line">    <span class="comment"># trained model), which helps debugging and prevents entering an infinite loop. </span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    newline_character = char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (idx != newline_character <span class="keyword">and</span> counter != <span class="number">50</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2: Forward propagate x using the equations (1), (2) and (3)</span></span><br><span class="line">        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)</span><br><span class="line">        z = np.dot(Wya,a)+by</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3: Sample the index of a character within the vocabulary from the probability distribution y</span></span><br><span class="line">        idx = np.random.choice(vocab_size, p=y.ravel())<span class="comment"># 按照概率进行随机取值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append the index to "indices"</span></span><br><span class="line">        indices.append(idx)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 4: Overwrite the input character as the one corresponding to the sampled index.</span></span><br><span class="line">        x = np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">        x[idx] = <span class="number">1</span><span class="comment"># one-hot 编码</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update "a_prev" to be "a"</span></span><br><span class="line">        a_prev = a</span><br><span class="line">        </span><br><span class="line">        counter +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (counter == <span class="number">50</span>): <span class="comment"># 限制生成的单词字长不超过 50，防止无限循环</span></span><br><span class="line">        indices.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> indices</span><br></pre></td></tr></table></figure><h2 id="建立语言模型"><a href="#建立语言模型" class="headerlink" title="建立语言模型"></a>建立语言模型</h2><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>一些辅助函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth</span><span class="params">(loss, cur_loss)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> loss * <span class="number">0.999</span> + cur_loss * <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_sample</span><span class="params">(sample_ix, ix_to_char)</span>:</span></span><br><span class="line">    txt = <span class="string">''</span>.join(ix_to_char[ix] <span class="keyword">for</span> ix <span class="keyword">in</span> sample_ix)</span><br><span class="line">    txt = txt[<span class="number">0</span>].upper() + txt[<span class="number">1</span>:]  <span class="comment"># capitalize first character </span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'%s'</span> % (txt, ), end=<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_initial_loss</span><span class="params">(vocab_size, seq_length)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -np.log(<span class="number">1.0</span>/vocab_size)*seq_length</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_a, n_x, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initialize parameters with small random values</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        b --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    Wax = np.random.randn(n_a, n_x)*<span class="number">0.01</span> <span class="comment"># input to hidden</span></span><br><span class="line">    Waa = np.random.randn(n_a, n_a)*<span class="number">0.01</span> <span class="comment"># hidden to hidden</span></span><br><span class="line">    Wya = np.random.randn(n_y, n_a)*<span class="number">0.01</span> <span class="comment"># hidden to output</span></span><br><span class="line">    b = np.zeros((n_a, <span class="number">1</span>)) <span class="comment"># hidden bias</span></span><br><span class="line">    by = np.zeros((n_y, <span class="number">1</span>)) <span class="comment"># output bias</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"Wax"</span>: Wax, <span class="string">"Waa"</span>: Waa, <span class="string">"Wya"</span>: Wya, <span class="string">"b"</span>: b,<span class="string">"by"</span>: by&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(parameters, a_prev, x)</span>:</span></span><br><span class="line">    </span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) <span class="comment"># hidden state</span></span><br><span class="line">    p_t = softmax(np.dot(Wya, a_next) + by) <span class="comment"># unnormalized log probabilities for next chars # probabilities for next chars </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, p_t</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dy, gradients, parameters, x, a, a_prev)</span>:</span></span><br><span class="line">    </span><br><span class="line">    gradients[<span class="string">'dWya'</span>] += np.dot(dy, a.T)</span><br><span class="line">    gradients[<span class="string">'dby'</span>] += dy</span><br><span class="line">    da = np.dot(parameters[<span class="string">'Wya'</span>].T, dy) + gradients[<span class="string">'da_next'</span>] <span class="comment"># backprop into h</span></span><br><span class="line">    daraw = (<span class="number">1</span> - a * a) * da <span class="comment"># backprop through tanh nonlinearity</span></span><br><span class="line">    gradients[<span class="string">'db'</span>] += daraw</span><br><span class="line">    gradients[<span class="string">'dWax'</span>] += np.dot(daraw, x.T)</span><br><span class="line">    gradients[<span class="string">'dWaa'</span>] += np.dot(daraw, a_prev.T)</span><br><span class="line">    gradients[<span class="string">'da_next'</span>] = np.dot(parameters[<span class="string">'Waa'</span>].T, daraw)</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, gradients, lr)</span>:</span></span><br><span class="line"></span><br><span class="line">    parameters[<span class="string">'Wax'</span>] += -lr * gradients[<span class="string">'dWax'</span>]</span><br><span class="line">    parameters[<span class="string">'Waa'</span>] += -lr * gradients[<span class="string">'dWaa'</span>]</span><br><span class="line">    parameters[<span class="string">'Wya'</span>] += -lr * gradients[<span class="string">'dWya'</span>]</span><br><span class="line">    parameters[<span class="string">'b'</span>]  += -lr * gradients[<span class="string">'db'</span>]</span><br><span class="line">    parameters[<span class="string">'by'</span>]  += -lr * gradients[<span class="string">'dby'</span>]</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(X, Y, a0, parameters, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize x, a and y_hat as empty dictionaries</span></span><br><span class="line">    x, a, y_hat = &#123;&#125;, &#123;&#125;, &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    a[<span class="number">-1</span>] = np.copy(a0)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize your loss to 0</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set x[t] to be the one-hot vector representation of the t'th character in X.</span></span><br><span class="line">        <span class="comment"># if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. </span></span><br><span class="line">        x[t] = np.zeros((vocab_size,<span class="number">1</span>)) </span><br><span class="line">        <span class="keyword">if</span> (X[t] != <span class="keyword">None</span>):</span><br><span class="line">            x[t][X[t]] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Run one step forward of the RNN</span></span><br><span class="line">        a[t], y_hat[t] = rnn_step_forward(parameters, a[t<span class="number">-1</span>], x[t])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the loss by substracting the cross-entropy term of this time-step from it.</span></span><br><span class="line">        loss -= np.log(y_hat[t][Y[t],<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">    cache = (y_hat, a, x)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> loss, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(X, Y, parameters, cache)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize gradients as an empty dictionary</span></span><br><span class="line">    gradients = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve from cache and parameters</span></span><br><span class="line">    (y_hat, a, x) = cache</span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># each one should be initialized to zeros of the same dimension as its corresponding parameter</span></span><br><span class="line">    gradients[<span class="string">'dWax'</span>], gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWya'</span>] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)</span><br><span class="line">    gradients[<span class="string">'db'</span>], gradients[<span class="string">'dby'</span>] = np.zeros_like(b), np.zeros_like(by)</span><br><span class="line">    gradients[<span class="string">'da_next'</span>] = np.zeros_like(a[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagate through time</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(len(X))):</span><br><span class="line">        dy = np.copy(y_hat[t])</span><br><span class="line">        dy[Y[t]] -= <span class="number">1</span></span><br><span class="line">        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t<span class="number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients, a</span><br></pre></td></tr></table></figure><p>建立模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Execute one step of the optimization to train the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.</span></span><br><span class="line"><span class="string">    Y -- list of integers, exactly the same as X but shifted one index to the left.</span></span><br><span class="line"><span class="string">    a_prev -- previous hidden state.</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        b --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- value of the loss function (cross-entropy)</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        db -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dby -- Gradients of output bias vector, of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Forward propagate through time (≈1 line)</span></span><br><span class="line">    loss, cache = rnn_forward(X, Y, a_prev, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagate through time (≈1 line)</span></span><br><span class="line">    gradients, a = rnn_backward(X, Y, parameters, cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Clip your gradients between -5 (min) and 5 (max) (≈1 line)</span></span><br><span class="line">    gradients = clip(gradients, <span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update parameters (≈1 line)</span></span><br><span class="line">    parameters = update_parameters(parameters, gradients, learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradients, a[len(X)<span class="number">-1</span>]</span><br></pre></td></tr></table></figure><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data, ix_to_char, char_to_ix, num_iterations = <span class="number">35000</span>, n_a = <span class="number">50</span>, dino_names = <span class="number">7</span>, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Trains the model and generates dinosaur names. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    data -- text corpus</span></span><br><span class="line"><span class="string">    ix_to_char -- dictionary that maps the index to a character</span></span><br><span class="line"><span class="string">    char_to_ix -- dictionary that maps a character to an index</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to train the model for</span></span><br><span class="line"><span class="string">    n_a -- number of units of the RNN cell</span></span><br><span class="line"><span class="string">    dino_names -- number of dinosaur names you want to sample at each iteration. </span></span><br><span class="line"><span class="string">    vocab_size -- number of unique characters found in the text, size of the vocabulary</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- learned parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve n_x and n_y from vocab_size</span></span><br><span class="line">    n_x, n_y = vocab_size, vocab_size</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(n_a, n_x, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize loss (this is required because we want to smooth our loss, don't worry about it)</span></span><br><span class="line">    loss = get_initial_loss(vocab_size, dino_names)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build list of all dinosaur names (training examples).</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"dinos.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        examples = f.readlines()</span><br><span class="line">    examples = [x.lower().strip() <span class="keyword">for</span> x <span class="keyword">in</span> examples]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle list of all dinosaur names</span></span><br><span class="line">    np.random.shuffle(examples)<span class="comment"># 将样本洗牌</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the hidden state of your LSTM</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 假设 n 个恐龙名字样本</span></span><br><span class="line">        <span class="comment"># 样本1（循环1）-&gt; 样本2（循环2）-&gt;……-&gt; 样本n（循环n）-&gt;样本1（循环n+1）-&gt; ……-&gt;样本n（循环2n）-&gt;……</span></span><br><span class="line">        <span class="comment"># Use the hint above to define one training example (X,Y) (≈ 2 lines)</span></span><br><span class="line">        index = j % len(examples)<span class="comment"># 目前的循环次数取余样本数</span></span><br><span class="line">        X = [<span class="keyword">None</span>] + [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]] <span class="comment"># [None] 指第一个X&lt;0&gt;,它生成hatY&lt;1&gt;，即X&lt;n&gt;生成hatY&lt;n+1&gt;</span></span><br><span class="line">        Y = X[<span class="number">1</span>:] + [char_to_ix[<span class="string">"\n"</span>]] <span class="comment"># Y 的真实值，与 X 相同，只是没有第一个Y&lt;0&gt;，并且最后加上结束符，比X向右移动了一位</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters</span></span><br><span class="line">        <span class="comment"># Choose a learning rate of 0.01</span></span><br><span class="line">        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Use a latency trick to keep the loss smooth. It happens here to accelerate the training.</span></span><br><span class="line">        loss = smooth(loss, curr_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Every 2000 Iteration, generate "n" characters thanks to sample() to check if the model is learning properly</span></span><br><span class="line">        <span class="keyword">if</span> j % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">            print(<span class="string">'Iteration: %d, Loss: %f'</span> % (j, loss) + <span class="string">'\n'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># The number of dinosaur names to print</span></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> range(dino_names):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Sample indices and print them</span></span><br><span class="line">                sampled_indices = sample(parameters, char_to_ix, seed)</span><br><span class="line">                print_sample(sampled_indices, ix_to_char)</span><br><span class="line">      </span><br><span class="line">            print(<span class="string">'\n'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(data, ix_to_char, char_to_ix)</span><br></pre></td></tr></table></figure><p>……</p><p>Iteration: 34000, Loss: 22.396744</p><p>Mavptokekus<br>Ilabaisaurus<br>Itosaurus<br>Macaesaurus<br>Yrosaurus<br>Eiaeosaurus<br>Trodon</p><h1 id="Part-3-Improvise-a-Jazz-Solo-with-an-LSTM-Network"><a href="#Part-3-Improvise-a-Jazz-Solo-with-an-LSTM-Network" class="headerlink" title="Part 3-Improvise a Jazz Solo with an LSTM Network"></a>Part 3-Improvise a Jazz Solo with an LSTM Network</h1><p>这次作业是在 Keras 使用 LSTM 进行爵士音乐的生成。</p><p>参考 Ji-Sung Kim, 2016, <a href="https://github.com/jisungk/deepjazz" target="_blank" rel="noopener">deepjazz</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> IPython</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> music21 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> grammar <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> qa <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> preprocess <span class="keyword">import</span> * </span><br><span class="line"><span class="keyword">from</span> music_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> data_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model, Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br></pre></td></tr></table></figure><h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h2><h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><p>先不管具体的音乐理论和细节，直接加载需要的数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X, Y, n_values, indices_values = load_music_utils()</span><br><span class="line">print(<span class="string">'shape of X:'</span>, X.shape)</span><br><span class="line">print(<span class="string">'number of training examples:'</span>, X.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Tx (length of sequence):'</span>, X.shape[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'total # of unique values:'</span>, n_values)</span><br><span class="line">print(<span class="string">'Shape of Y:'</span>, Y.shape)</span><br></pre></td></tr></table></figure><p>shape of X: (60, 30, 78)<br>number of training examples: 60<br>Tx (length of sequence): 30<br>total # of unique values: 78<br>Shape of Y: (30, 60, 78)</p><ul><li>X：(m, T_x, 78) 其中 m 是样本数，T_x 是音乐片段的时间，78 是 one-hot 向量的长度</li><li>Y：(T_y, m, 78) 为了便于 feed 给 LSTM 层，将其维度变一下</li><li>n_value：把某个时间的音乐变成 one-hot 向量的长度</li><li>indices_values：78 个索引值的字典</li></ul><h3 id="模型概况-1"><a href="#模型概况-1" class="headerlink" title="模型概况"></a>模型概况</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_6.png" alt=""></p><p>每个训练用的序列样本长度可以是不同的，但是这里为了方便向量化，将所有用以训练的样本统一长度为 30 s.</p><h2 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h2><p>首先确定模型的超参数，我们必须先确定隐藏层的单元数。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_5.png" alt=""></p><p>看上面这幅图，所谓隐藏层单元数，就是 $W[a^{<t-1>},x^{<t>}]$ 的输出的维度，用 n_a 表示，这里设为 64.</t></t-1></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_a = <span class="number">64</span></span><br></pre></td></tr></table></figure><p>如果我们建立的序列模型是在测试时也给出所有序列，例如输入一个序列，预测一个标签，那么可以简单地使用 Keras 的内置函数进行模型构建，但是对于生成模型来说，我们每次只能知道序列前一个的值，而不知道整个序列，无法直接全部输入模型进行预测，只能一个时间点生成一次，然后让  $x^{\langle t\rangle} = y^{\langle t-1 \rangle}$ 继续生成下一个，而内置的 LSTM 模型的逻辑是，一次将所有时间的序列值丢进去然后得到一个输出。</p><p>所以在下面的函数中使用 for 循环调用 LSTM 层 T_x 次，由于每个时间点的 LSTM 层的参数都是一样的，所以我们定义一个<strong>全局的</strong> 层 对象，每个 for 循环都调用这个对象，实现参数共享。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reshapor = Reshape((<span class="number">1</span>, <span class="number">78</span>))                        <span class="comment"># Used in Step 2.B of djmodel(), below</span></span><br><span class="line">LSTM_cell = LSTM(n_a, return_state = <span class="keyword">True</span>)<span class="comment"># return_state 返回 c         # Used in Step 2.C</span></span><br><span class="line">densor = Dense(n_values, activation=<span class="string">'softmax'</span>)     <span class="comment"># Used in Step 2.D</span></span><br></pre></td></tr></table></figure><p>建立模型的步骤是：</p><ul><li><p>建立空列表 outputs 储存每个时间点的输出 $\hat y$ </p></li><li><p>for t in 1,…, T_x：</p><ul><li>从输入选择第 t 个时间点的序列值<ul><li>使用 <code>x = Lambda(lambda x: X[:,t,:])(X)</code>，输出维度是 (?, 78)</li></ul></li><li>调用 reshapor 把 (?,78)-&gt;(?,1,78)，便于输入 LSTM 层</li><li>调用 LSTM_cell 得到输出的 a 和 c<ul><li><code>a, _, c = LSTM_cell(input_x, initial_state=[previous hidden state, previous cell state])</code></li></ul></li><li>将隐藏层的激活值 a 通过 softmax 进行输出</li><li>把输出存进 outputs</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">djmodel</span><span class="params">(Tx, n_a, n_values)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the model</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Tx -- length of the sequence in a corpus</span></span><br><span class="line"><span class="string">    n_a -- the number of activations used in our model</span></span><br><span class="line"><span class="string">    n_values -- number of unique values in the music data </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a keras model with the </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input of your model with a shape </span></span><br><span class="line">    X = Input(shape=(Tx, n_values)) <span class="comment"># Keras 中 shape 不需要显式地指出样本数 m，第一个位置默认是样本数，打印 X 的形状为 (?, 30, 78)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define s0, initial hidden state for the decoder LSTM</span></span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">'a0'</span>)<span class="comment">#shape 其实是(?,n_a,)</span></span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">'c0'</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Create empty list to append the outputs while you iterate (≈1 line)</span></span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Tx):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.A: select the "t"th time step vector from X. </span></span><br><span class="line">        x = Lambda(<span class="keyword">lambda</span> x: X[:,t,:])(X)<span class="comment"># Lambda 层是对 X 整个进行运算，得到一个结果，所以直接返回 X[:,t,:]</span></span><br><span class="line">        <span class="comment"># Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)</span></span><br><span class="line">        x = reshapor(x) <span class="comment"># (?,78)-&gt;(?,1,78)</span></span><br><span class="line">        <span class="comment"># Step 2.C: Perform one step of the LSTM_cell</span></span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        <span class="comment"># Step 2.D: Apply densor to the hidden state output of LSTM_Cell</span></span><br><span class="line">        out = densor(a)</span><br><span class="line">        <span class="comment"># Step 2.E: add the output to "outputs"</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 3: Create model instance</span></span><br><span class="line">    model = Model(inputs=[X,a0,c0], outputs=outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>建立模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = djmodel(Tx = <span class="number">30</span> , n_a = <span class="number">64</span>, n_values = <span class="number">78</span>)</span><br></pre></td></tr></table></figure><p>编译模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">opt = Adam(lr=<span class="number">0.01</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, decay=<span class="number">0.01</span>)<span class="comment">#定义优化器</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=opt, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">60</span></span><br><span class="line">a0 = np.zeros((m, n_a))</span><br><span class="line">c0 = np.zeros((m, n_a))</span><br><span class="line"></span><br><span class="line">model.fit([X, a0, c0], list(Y), epochs=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">输出：</span><br><span class="line">......</span><br><span class="line">Epoch 100/100</span><br><span class="line">60/60 [==============================] - 0s - loss: 5.9232 - dense_1_loss_1: 3.7652 - dense_1_loss_2: 1.1579 - dense_1_loss_3: 0.3157 - dense_1_loss_4: 0.0825 - dense_1_loss_5: 0.0493 - dense_1_loss_6: 0.0368 - dense_1_loss_7: 0.0311 - dense_1_loss_8: 0.0285 - dense_1_loss_9: 0.0278 - dense_1_loss_10: 0.0222 - dense_1_loss_11: 0.0213 - dense_1_loss_12: 0.0205 - dense_1_loss_13: 0.0194 - dense_1_loss_14: 0.0199 - dense_1_loss_15: 0.0210 - dense_1_loss_16: 0.0206 - dense_1_loss_17: 0.0192 - dense_1_loss_18: 0.0213 - dense_1_loss_19: 0.0212 - dense_1_loss_20: 0.0219 - dense_1_loss_21: 0.0220 - dense_1_loss_22: 0.0202 - dense_1_loss_23: 0.0217 - dense_1_loss_24: 0.0211 - dense_1_loss_25: 0.0227 - dense_1_loss_26: 0.0202 - dense_1_loss_27: 0.0225 - dense_1_loss_28: 0.0245 - dense_1_loss_29: 0.0252 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.6833 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00</span><br></pre></td></tr></table></figure><h2 id="生成音乐"><a href="#生成音乐" class="headerlink" title="生成音乐"></a>生成音乐</h2><p>到目前为止，我们模型的几个层 reshapor、LSTM_cell、densor 中的参数已经学习完毕，存在该对象中，现在我们使用这些参数训练好的层对象来构建一个新的生成模型。</p><p>与生成恐龙名字相同，我们仍需要 sampling 操作，因此增加一个将 softmax 的输出变成 one-hot 的层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">music_inference_model</span><span class="params">(LSTM_cell, densor, n_values = <span class="number">78</span>, n_a = <span class="number">64</span>, Ty = <span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uses the trained "LSTM_cell" and "densor" from model() to generate a sequence of values.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    LSTM_cell -- the trained "LSTM_cell" from model(), Keras layer object</span></span><br><span class="line"><span class="string">    densor -- the trained "densor" from model(), Keras layer object</span></span><br><span class="line"><span class="string">    n_values -- integer, umber of unique values</span></span><br><span class="line"><span class="string">    n_a -- number of units in the LSTM_cell</span></span><br><span class="line"><span class="string">    Ty -- integer, number of time steps to generate</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    inference_model -- Keras model instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input of your model with a shape </span></span><br><span class="line">    x0 = Input(shape=(<span class="number">1</span>, n_values))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define s0, initial hidden state for the decoder LSTM</span></span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">'a0'</span>)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">'c0'</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line">    x = x0</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Create an empty list of "outputs" to later store your predicted values (≈1 line)</span></span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Loop over Ty and generate a value at every time step</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Ty):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.A: Perform one step of LSTM_cell (≈1 line)</span></span><br><span class="line">        a, _, c = LSTM_cell(x,initial_state=[a,c])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)</span></span><br><span class="line">        out = densor(a)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2.C: Append the prediction "out" to "outputs". out.shape = (None, 78) (≈1 line)</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.D: Select the next value according to "out", and set "x" to be the one-hot representation of the</span></span><br><span class="line">        <span class="comment">#           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided </span></span><br><span class="line">        <span class="comment">#           the line of code you need to do this. </span></span><br><span class="line">        x = Lambda(one_hot)(out)<span class="comment"># 下一次的输入等于上一次的输出变成 one-hot 向量</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 3: Create model instance with the correct "inputs" and "outputs" (≈1 line)</span></span><br><span class="line">    inference_model = Model(inputs=[x0,a0,c0], outputs = outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inference_model</span><br></pre></td></tr></table></figure><p>调用模型进行预测的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_and_sample</span><span class="params">(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, </span></span></span><br><span class="line"><span class="function"><span class="params">                       c_initializer = c_initializer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Predicts the next value of values using the inference model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    inference_model -- Keras model instance for inference time</span></span><br><span class="line"><span class="string">    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation</span></span><br><span class="line"><span class="string">    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell</span></span><br><span class="line"><span class="string">    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated</span></span><br><span class="line"><span class="string">    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.</span></span><br><span class="line">    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])</span><br><span class="line">    <span class="comment"># Step 2: Convert "pred" into an np.array() of indices with the maximum probabilities</span></span><br><span class="line">    indices = np.argmax(pred,axis=<span class="number">-1</span>)<span class="comment"># 将 softmax 输出中概率值最大的定为输出索引值</span></span><br><span class="line">    <span class="comment"># Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, )</span></span><br><span class="line">    results = to_categorical(indices, num_classes=n_values)<span class="comment"># 将索引值再变成 one-hot 向量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results, indices</span><br></pre></td></tr></table></figure><p>看看结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)</span><br><span class="line">print(<span class="string">"np.argmax(results[12]) ="</span>, np.argmax(results[<span class="number">12</span>]))</span><br><span class="line">print(<span class="string">"np.argmax(results[17]) ="</span>, np.argmax(results[<span class="number">17</span>]))</span><br><span class="line">print(<span class="string">"list(indices[12:18]) ="</span>, list(indices[<span class="number">12</span>:<span class="number">18</span>]))</span><br></pre></td></tr></table></figure><p>np.argmax(results[12]) = 70<br>np.argmax(results[17]) = 55<br>list(indices[12:18]) = [array([70]), array([71]), array([43]), array([2]), array([10]), array([55])]</p><h3 id="生成音乐-1"><a href="#生成音乐-1" class="headerlink" title="生成音乐"></a>生成音乐</h3><p>得到的输出必须经过很好的后处理才能确保听起来好听，输出音乐的质量很大地取决于后处理的质量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out_stream = generate_music(inference_model)</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> rnn </tag>
            
            <tag> lstm </tag>
            
            <tag> 梯度剪枝 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>deeplearning.ai 第五课第一周编程作业第一部分 lstm 反向传播部分的公式更正及推导</title>
      <link href="/2018/12/01/deeplearning.ai%20%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%80%E5%91%A8%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%20lstm%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E9%83%A8%E5%88%86%E7%9A%84%E5%85%AC%E5%BC%8F%E6%9B%B4%E6%AD%A3%E5%8F%8A%E6%8E%A8%E5%AF%BC/"/>
      <url>/2018/12/01/deeplearning.ai%20%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%80%E5%91%A8%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%20lstm%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E9%83%A8%E5%88%86%E7%9A%84%E5%85%AC%E5%BC%8F%E6%9B%B4%E6%AD%A3%E5%8F%8A%E6%8E%A8%E5%AF%BC/</url>
      <content type="html"><![CDATA[<p>第五课第一周第一部分的编程作业直接构建了 rnn 和 lstm 的前向传播和方向传播函数，但是这次作业实在做得坎坷无比，还以为是自己的问题，上论坛一看，没想到大家都在抱怨这一次的作业各种出问题，公式给的莫名其妙，得到的结果老是对不上答案，在 lstm 的反向传播部分甚至连公式都给错了，这里做一个更正，并给出自己的手写推导。</p><a id="more"></a><h2 id="更正后的公式"><a href="#更正后的公式" class="headerlink" title="更正后的公式"></a>更正后的公式</h2><script type="math/tex; mode=display">\begin{split}原公式中的“d \Gamma_o^{\langle t \rangle}” &= dZ_o^{<t>}=\frac{\partial J}{\partial Z_o^{<t>}}=\frac{\partial J}{\partial(W_o[a^{<t-1>},x^{<t>}]+b_o)}  \\ &= da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*\left(1-\Gamma_o^{\langle t \rangle}\right) \end{split}\tag{7}</script><script type="math/tex; mode=display">\begin{split}原公式中的“d\widetilde{c}^{\langle t \rangle}”&=dZ_c^{<t>}=\frac{\partial J}{\partial Z_c^{<t>}}=\frac{\partial J}{\partial(W_c[a^{<t-1>},x^{<t>}]+b_c)} \\&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle}* (1-\tanh(c_{next})^2) * da_{next} \right) \Gamma_u^{\langle t \rangle} * \left(1-\left(\widetilde c^{\langle t \rangle}\right)^2\right) \end{split}\tag{8}</script><script type="math/tex; mode=display">\begin{split}原公式中的“d\Gamma_u^{\langle t \rangle}”&=dZ_u^{<t>}=\frac{\partial J}{\partial Z_u^{<t>}}=\frac{\partial J}{\partial(W_u[a^{<t-1>},x^{<t>}]+b_u)}  \\&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle}* (1-\tanh(c_{next})^2) * da_{next}\right)*\Gamma_u^{\langle t \rangle}* \widetilde{c}^{\langle t \rangle} *\left(1-\Gamma_u^{\langle t \rangle}\right)\end{split}\tag{9}</script><script type="math/tex; mode=display">\begin{split}原公式中的“d\Gamma_f^{\langle t \rangle}”&=dZ_f^{<t>}=\frac{\partial J}{\partial Z_f^{<t>}}=\frac{\partial J}{\partial(W_f[a^{<t-1>},x^{<t>}]+b_f)}  \\&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle} * (1-\tanh(c_{next})^2)  * da_{next}\right)*\Gamma_f^{\langle t \rangle}* c_{prev}*\left(1-\Gamma_f^{\langle t \rangle}\right)\end{split}\tag{10}</script><script type="math/tex; mode=display">dW_f = d\Gamma_f^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{11}</script><script type="math/tex; mode=display">dW_u = d\Gamma_u^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{12}</script><script type="math/tex; mode=display">dW_c = d\widetilde c^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{13}</script><script type="math/tex; mode=display">dW_o = d\Gamma_o^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{14}</script><script type="math/tex; mode=display">db_f=np.sum(d\Gamma_f^{\langle t \rangle},axis=1,keepdims=True)\tag{15}</script><script type="math/tex; mode=display">db_u=np.sum(d\Gamma_u^{\langle t \rangle},axis=1,keepdims=True)\tag{16}</script><script type="math/tex; mode=display">db_c=np.sum(d\Gamma_c^{\langle t \rangle},axis=1,keepdims=True)\tag{17}</script><script type="math/tex; mode=display">db_o=np.sum(d\Gamma_o^{\langle t \rangle},axis=1,keepdims=True)\tag{18}</script><script type="math/tex; mode=display">da_{prev} = W_f^T[:,:n_a] d\Gamma_f^{\langle t \rangle} + W_u^T[:,:n_a]   d\Gamma_u^{\langle t \rangle}+ W_c^T[:,:n_a] d\widetilde c^{\langle t \rangle} + W_o^T[:,:n_a] d\Gamma_o^{\langle t \rangle} \tag{19}</script><script type="math/tex; mode=display">dc_{prev} = dc_{next}*\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \tag{20}</script><script type="math/tex; mode=display">dx^{\langle t \rangle} = W_f^T[:,n_a:] d\Gamma_f^{\langle t \rangle} + W_u^T[:,n_a:]  d\Gamma_u^{\langle t \rangle}+ W_c^T[:,n_a:] d\widetilde c^{\langle t \rangle} + W_o^T[:,n_a:] d\Gamma_o^{\langle t \rangle}\tag{21}</script><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>lstm 前向传播为：</p><script type="math/tex; mode=display">\begin{array} { l } { \tilde { c } ^ { < t > } = \tanh \left( W _ { c } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { c } \right) } \\ 更新门 \ \ { \Gamma _ { u } = \sigma \left( W _ { u } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { u } \right) } \\遗忘门 \ \  { \Gamma _ { f } = \sigma \left( W _ { f } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { f } \right) } \\ 输出门 \ \ { \Gamma _ { o } = \sigma \left( W _ { o } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { o } \right) } \\ { c ^ { < t > } = \Gamma _ { u } * \tilde { c } ^ { < t > } + \Gamma _ { f } * c ^ { < t - 1 > } } \\ { a ^ { < t > } = \Gamma _ { o } * \tanh c ^ { < t > } } \end{array}</script><p>首先易知：</p><script type="math/tex; mode=display">f(x)=sigmoid(x)=\sigma(x) \rightarrow f'(x) = f(x)(1-f(x))\\g(x)=tanh(x) \rightarrow g'(x)=1-g^2(x)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/2018/12/1/no.1.jpg" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/2018/12/1/no.2.jpg" alt=""></p><blockquote><p>* 表示逐元素相乘，<em>·</em> 表示内积</p></blockquote>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 5 week 1）—— 循环神经网络 RNN</title>
      <link href="/2018/11/08/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c5w1/"/>
      <url>/2018/11/08/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c5w1/</url>
      <content type="html"><![CDATA[<p>在“序列化模型”这门课的第一周，我们会学习到循环神经网络，这种模型对时间数据表现得非常好，它有几个变种，包括 LSTM，GRU 和 双向 RNN。</p><h1 id="序列化模型"><a href="#序列化模型" class="headerlink" title="序列化模型"></a>序列化模型</h1><h2 id="序列化模型的例子"><a href="#序列化模型的例子" class="headerlink" title="序列化模型的例子"></a>序列化模型的例子</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_1.png" alt=""></p><a id="more"></a><h2 id="标注"><a href="#标注" class="headerlink" title="标注"></a>标注</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_2.png" alt=""></p><p>假如我们输入一句话 x，如果要找出其中的人名，那么输出标签对应人名的位置取 1.</p><ul><li>$T_x,T_y$ 表示这个序列的长度</li><li>上标 $(i)$ 表示第 i 个样本，上标 <code>&lt;t&gt;</code>表示这个序列的第 t 个位置  </li></ul><p>为了表示输入句子 x，我们需要建立一个字典，在这个例子里用的是词汇量为 10,000 的字典，对于现代的自然语言处理应用而言, 这种规模属于非常小的了，对于商业级的应用，字典规模一般为 3 到 5 万词汇，10 万级词汇的字典也比较常见。构建这个字典的一个方法就是去训练集中查找，找到出现频率最高的 1 万个词，另一种方式是查找一些网上的字典，将其中包含的英语中最常见的 1 万词作为拟构建的字典。</p><p>然后对输入句子进行 <strong>one-hot</strong> 编码，句子里的任意一个词 t，设为 $x^{<t>}$， 都将表示为一个 one-hot 向量，one-hot 的意思指只有一位为 1 其余位全是 0。例如  $x^{<1>}$ 代表的单词 Harry， 就被表示为一个向量：向量的其余位全为 0，除了在第 4075 位上有一个 1 ，因为 Harry 这个单词就在词汇表中的第 4075 位上。</1></t></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_3.png" alt=""></p><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="为什么不能用标准神经网络"><a href="#为什么不能用标准神经网络" class="headerlink" title="为什么不能用标准神经网络"></a>为什么不能用标准神经网络</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_4.png" alt=""></p><p>之所以不能用上图中的标准神经网络，是因为有一些问题：</p><ul><li>不同样本的输入输出长度可能不同</li><li>无法共享从句子不同地方学习到的特征</li></ul><h2 id="什么是循环神经网络-Recurrent-Neural-Network"><a href="#什么是循环神经网络-Recurrent-Neural-Network" class="headerlink" title="什么是循环神经网络 Recurrent  Neural Network"></a>什么是循环神经网络 Recurrent  Neural Network</h2><p>读取第一个词语 $x^{<1>}$，输入一个神经网络，形成第一个神经网络的隐藏层，然后得到第一个词语的输出 $\hat y^{<1>}$，第一个神经网络的激活值 $a^{<1>}$，乘上一个参数后继续加到下一个词语的神经网络中供其使用，然后再继续往下传递，于是每一步都可以使用到先前的信息进行预测。</1></1></1></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_5.png" alt=""></p><p>这个模型的一个缺点就是： 在序列中对某一时间的预测仅使用之前的输入， 而不使用序列中之后的信息，具体来说，当我们预测 y3 时，它不会使用关于词语 x4, x5, x6 等的信息。使用双向 RNN 可以解决这个问题。</p><h2 id="RNN-的前向传播"><a href="#RNN-的前向传播" class="headerlink" title="RNN 的前向传播"></a>RNN 的前向传播</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_6.png" alt=""></p><script type="math/tex; mode=display">a^{<0>}= \vec 0\\a ^ { < 1 > } = g_1 \left( W _ { a a } a ^ { < 1 > } + W _ { a x } x ^ { < 1 > } + b _ { a } \right) \leftarrow tanh/relu\\\hat { y } ^ { < 1 > } = g_2 \left( W _ { y a } a ^ { < 1 > } + b _ { y } \right) \leftarrow sigmoid \\a ^ { < t > } = g \left( W _ { a a } a ^ { < t > } + W _ { a x } x ^ { < t > } + b _ { a } \right)\\\hat { y } ^ { < t > } = g \left( W _ { y a } a ^ { < t > } + b _ { y } \right)</script><h3 id="前向传播简化"><a href="#前向传播简化" class="headerlink" title="前向传播简化"></a>前向传播简化</h3><script type="math/tex; mode=display">a^{<t>}=g(W_a[a^{<t-1>}, x^{<t>}]+b_a)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_7.png" alt=""></p><h2 id="RNN-的反向传播"><a href="#RNN-的反向传播" class="headerlink" title="RNN 的反向传播"></a>RNN 的反向传播</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_8.png" alt=""></p><p>红线是反向传播的路径，绿线是前向传播的路径，一个很酷的名字，基于时间的反向传播。</p><h2 id="更多-RNN-的类型"><a href="#更多-RNN-的类型" class="headerlink" title="更多 RNN 的类型"></a>更多 RNN 的类型</h2><p>目前为止，我们学习的都是输入序列和输出序列模型长度相同的模型，但是它们很多时候是不同的。</p><ul><li>one-to-one：标准神经网络</li><li>many-to-many<ul><li>$T_x=T_y$: 比如输入一句话输出其中人名</li><li>$T_x \not= T_y$：比如机器翻译，输入法语，输出英语</li></ul></li><li>many-to-one：例如情感分析，输入一句话输，出情感值</li><li>one-to-many：例如音乐生成，输入一个和弦，输出一首歌的音符序列</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_11.png" alt=""></p><h1 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h1><h2 id="什么是语言模型"><a href="#什么是语言模型" class="headerlink" title="什么是语言模型"></a>什么是语言模型</h2><p>给定任何句子序列 $y^{<1>},y^{<2>},…,y^{<t_y>}$，它都能给出这个特定句子的概率 $P(y^{<1>},y^{<2>},…,y^{<t_y>})$，也就是说你在任何一个时空听到一句话，它是 $y^{<1>},y^{<2>},…,y^{<t_y>}$的概率，例如下面这个语音识别的例子。</t_y></2></1></t_y></2></1></t_y></2></1></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_12.png" alt=""></p><h2 id="如何用-RNN-构建语言模型"><a href="#如何用-RNN-构建语言模型" class="headerlink" title="如何用 RNN 构建语言模型"></a>如何用 RNN 构建语言模型</h2><ul><li>训练集：一个很大的语料库 corpus</li><li>首先将一句话标记化 Tokenize<ul><li>每一个单词被转化为一个 one-hot 向量</li><li>句子末尾可以加一个 <eos> (end of sentence) 标记表示这是句子的末尾</eos></li><li>对于不在语料库中的单词，使用一个统一标记 \<unk> (unknown)</unk></li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_13.png" alt=""></p><h2 id="采样新序列"><a href="#采样新序列" class="headerlink" title="采样新序列"></a>采样新序列</h2><p>我们训练语言模型的时候使用下面的结构：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_14.png" alt=""></p><p>但是在采样的时候，或者说要生成一个随机选择的句子，我们直接将前一层的预测值输入下一层作为下一层的输入。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_15.png" alt=""></p><p>目前我们建立的是词一级的语言模型，</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_16.png" alt=""></p><p>我们还可以建立字符级的语言模型，找出句子里所有可能出现的字符，并用来定义词汇表。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_17.png" alt=""></p><p>于是序列变成训练数据的每个字符而不是每个单词，这样做不用担心出现字符表中未出现的字符，但是最终会有更长的序列，需要更多计算资源。</p><h1 id="更多的-RNN-类型"><a href="#更多的-RNN-类型" class="headerlink" title="更多的 RNN 类型"></a>更多的 RNN 类型</h1><h2 id="RNN-中的梯度消失问题"><a href="#RNN-中的梯度消失问题" class="headerlink" title="RNN 中的梯度消失问题"></a>RNN 中的梯度消失问题</h2><p>有两句话：</p><ul><li>The <strong>cat</strong> which already ate  a bunch of food that was delicious <strong>was</strong> full.</li><li>The <strong>cats</strong> which already ate  a bunch of food that was delicious <strong>were</strong> full.</li></ul><p>cat 搭配 was，cats 搭配 were，但是他们之间隔的非常远，具有很长的依赖关系，对于 RNN 来说很难处理这种长依赖关系，为什么？</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_18.png" alt=""></p><p>与深度网络类似，较深的 RNN 也会产生<strong>梯度消失</strong>的问题，导致那么从输出 y 得到的梯度很难反向传播去影响前期层的权重，所以很难让句子前方的文字与后面的文字产生依赖关系。这导致了 RNN 的附近效应，意味着序列某个位置的的值主要受它附近的值的影响，很难做到通过各种方式反向传播到序列的开始，从而去修改神经网络在序列前期做的计算，所以这是 RNN 算法的一个缺点。</p><p>RNN 也可能产生梯度爆炸的问题，导致参数变得很大，一个解决方案是 gradient clipping，也就是如果梯度大于某些临界值，重新缩放某些梯度向量，使其不那么大。</p><p>下面是一些解决 RNN 梯度消失的方法。</p><h2 id="门控回归单元-Gated-Recurrent-Unit-GRU"><a href="#门控回归单元-Gated-Recurrent-Unit-GRU" class="headerlink" title="门控回归单元 Gated Recurrent Unit (GRU)"></a>门控回归单元 Gated Recurrent Unit (GRU)</h2><p>我们首先用一个图解释普通的 RNN 单元是如何工作：</p><script type="math/tex; mode=display">a ^ { < t > } = g \left( W _ { a } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { a } \right)\\g()=tanh()</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_19.png" alt=""></p><h3 id="简化版-GRU-原理"><a href="#简化版-GRU-原理" class="headerlink" title="简化版 GRU 原理"></a>简化版 GRU 原理</h3><p>仍然是为了解决类似 The <strong>cat</strong> which already ate  a bunch of food that was delicious <strong>was</strong> full. 这种句子前后<strong>长期依赖</strong>的问题，我们引入一个新的变量 c = memory cell，即记忆细胞，GRU 中主要的计算公式为：</p><script type="math/tex; mode=display">c^{<t>}=a^{<t>}\\\begin{array} { l }  { \tilde { c } ^ { < t > } = \tanh \left( W _ { c } \left[ c ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { c } \right) } \\{ \Gamma _ { u } = \sigma \left( W _ { u } \left[ c ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { u } \right) } \\{ c ^ { < t > } = \Gamma _ { u } * \tilde { c } ^ { < t > } + \left( 1 - \Gamma _ { u } \right) * c ^ { < t - 1 > } } \end{array}</script><ul><li>$c^{<t>}$ 是记忆细胞的值，等于某个时间的激活值</t></li><li>$ \tilde { c } ^ { &lt; t &gt; }$ 是用以取代 $c^{<t>}$ 的候选值，我们可以理解为现在这个 t 时间点的信息</t></li><li>$\Gamma _ { u }$ 是一个“门 (gate)”，由于是 sigmoid 函数的输出，所以是一个介于 0～1 之间的值，但是大部分值都非常接近 1 或者非常接近 0，u 代表 update，实现一个类似筛选器的作用，它决定了我们是否用 $ \tilde { c } ^ { &lt; t &gt; }$ 取代 $c^{<t-1>}$。</t-1></li><li>* 表示逐元素相乘</li><li>${ c ^ { &lt; t &gt; } = \Gamma _ { u } <em> \tilde { c } ^ { &lt; t &gt; } + \left( 1 - \Gamma _ { u } \right) </em> c ^ { &lt; t - 1 &gt; } } $ 这是一种类似于滑动平均的公式，$ \tilde { c } ^ { &lt; t &gt; }$ 可以理解为现在这个 t 时间点的信息，当门值 $\Gamma _u$ 为 1 时，我们将过去积累的信息 $c^{<t-1>}$ 丢弃，更新为现在的信息 $ \tilde { c } ^ { &lt; t &gt; }$，当门值 $\Gamma _u$ 为 0 时，这个单元将记住过去积累的信息。而 $\Gamma _u$ 的值到底是 1 还是 0，就由模型自己学习。</t-1></li></ul><p>分析：cat 这个词是第三人称单数，对后面谓语有影响，所以到这个词时，门值 $\Gamma _ { u }=1$，根据最后一个式子，$c^{<t>}=\tilde c^{<t>}$，即在此处我们的记忆细胞的信息发生了一次更新，好，它现在记住了这里有个第三人称单数的 cat，接着到 which、already、ate 等单词时，他们的门值 $\Gamma _ { u }=0$，$c^{<t>}= c^{<t-1>}$，也就是说，这些词对谓语 was 不存在影响，记忆细胞的值不需要更新，仍旧等于上一个旧值。也就是说 cat 这个词的信息可以一直往后产生影响。</t-1></t></t></t></p><p>用一个相似的图来表示 GRU 的原理：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_20.png" alt=""></p><h3 id="完全的-GRU-单元"><a href="#完全的-GRU-单元" class="headerlink" title="完全的 GRU 单元"></a>完全的 GRU 单元</h3><script type="math/tex; mode=display">\begin{array} { l }  { \tilde { c } ^ { < t > } = \tanh \left( W _ { c } \left[\Gamma_r* c ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { c } \right) } \\{ \Gamma _ { u } = \sigma \left( W _ { u } \left[ c ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { u } \right) } \\{ \Gamma _ { r } = \sigma \left( W _ { r } \left[ c ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { r } \right) } \\{ c ^ { < t > } = \Gamma _ { u } * \tilde { c } ^ { < t > } + \left( 1 - \Gamma _ { u } \right) * c ^ { < t - 1 > } } \end{array}</script><ul><li>新增加的门 $\Gamma_r$ (r 代表 relevant)，说明了 $c^{<t-1>}$ 对于计算候选值 $ \tilde { c } ^ { &lt; t &gt; }$ 有多大的<strong>相关性</strong></t-1></li></ul><h2 id="长短时记忆网络-Long-Short-Term-Memory-LSTM"><a href="#长短时记忆网络-Long-Short-Term-Memory-LSTM" class="headerlink" title="长短时记忆网络 Long Short Term Memory (LSTM)"></a>长短时记忆网络 Long Short Term Memory (LSTM)</h2><p>LSTM 与 GRU 相比具有三个“门”—— 更新门、遗忘门和输出门。</p><script type="math/tex; mode=display">\begin{array} { l } { \tilde { c } ^ { < t > } = \tanh \left( W _ { c } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { c } \right) } \\ 更新门 \ \ { \Gamma _ { u } = \sigma \left( W _ { u } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { u } \right) } \\遗忘门 \ \  { \Gamma _ { f } = \sigma \left( W _ { f } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { f } \right) } \\ 输出门 \ \ { \Gamma _ { o } = \sigma \left( W _ { o } \left[ a ^ { < t - 1 > } , x ^ { < t > } \right] + b _ { o } \right) } \\ { c ^ { < t > } = \Gamma _ { u } * \tilde { c } ^ { < t > } + \Gamma _ { f } * c ^ { < t - 1 > } } \\ { a ^ { < t > } = \Gamma _ { o } * \tanh c ^ { < t > } } \end{array}</script><p>一个长短时记忆单元：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_21.png" alt=""></p><p>我们可以将几个 LSTM 单元用以下的方式结合起来：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_22.png" alt=""></p><p>过去时间的信息 $c^{<t>}$ 在红线这条独立的路径中向前传播，所以之前的信息能维持很长时间。</t></p><p>LSTM 有一个变体“窥空连接 (peephole connection)”，即 $c^{<t-1>}$ 会影响门控的数值：</t-1></p><script type="math/tex; mode=display">{ \Gamma _ { u } = \sigma \left( W _ { u } \left[ a ^ { < t - 1 > } , x ^ { < t > } , c^{<t-1>} \right] + b _ { u } \right) } \\{ \Gamma _ { f } = \sigma \left( W _ { f } \left[ a ^ { < t - 1 > } , x ^ { < t > } , c^{<t-1>} \right] + b _ { f } \right) } \\{ \Gamma _ { o } = \sigma \left( W _ { o } \left[ a ^ { < t - 1 > } , x ^ { < t > }, c^{<t-1>} \right] + b _ { o } \right) }</script><h3 id="GRU-和-LSTM-的选择"><a href="#GRU-和-LSTM-的选择" class="headerlink" title="GRU 和 LSTM 的选择"></a>GRU 和 LSTM 的选择</h3><p>什么时候你应该使用 LSTM，什么时候使用 GRU？</p><p>在深度学习的历史中，LSTM 要远远早于 GRU，GRU 是一个相对近期的发明，用来作为复杂的 LSTM 模型的简化。在不同的问题上不同的算法各有千秋，所以，不存在一个普适的优秀算法 。GRU 的优点是其模型的简单性，因此更适用于构建较大的网络，它只有两个门控，从计算角度看，它的效率更高，可扩展性有利于构筑较大的模型；但是 LSTM 更加的强大和灵活，因为它具有三个门控。</p><p>LSTM 是经过历史检验的方法，因此，如果要选取一个，大多数人会把 LSTM 作为默认第一个去尝试的方法。但在过去几年 GRU 的势头越来越猛， 越来越多的团队同时也用 GRU，因为其简单而且效果可以和 LSTM 比拟，可以更容易的将其扩展到更大的问题。</p><h2 id="双向-RNN-BIdirectional-RNN-BRNN"><a href="#双向-RNN-BIdirectional-RNN-BRNN" class="headerlink" title="双向 RNN BIdirectional RNN (BRNN)"></a>双向 RNN BIdirectional RNN (BRNN)</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_23.png" alt=""></p><p>为了在某个时间点的预测获得序列前部分和后部分的信息，例如为了知道 teddy 到底是指人名还是“泰迪熊”，只凭借该词前面的信息是无法知道的，必须利用该词后面的信息。</p><p>BRNN 的结构如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_24.png" alt=""></p><p>为了预测 y3 的值，我们会利用到所有的信息，如下图的黄色线条显示了信息的流动过程：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_25.png" alt=""></p><p>图中的每个方块既可以是 GRU 单元，也可以是 LSTM 单元。</p><p>双向 RNN 的缺点是需要整个数据序列，然后才能在任何地方进行预测。例如, 如果要构建语音识别系统，使用 BRNN 需要等待人停止说话，得到整短话，才可以实际处理它。但对于许多自然语言处理应用，可以得到整个句子，标准 BRNN 算法实际上是非常有效的。</p><h2 id="深度-RNN"><a href="#深度-RNN" class="headerlink" title="深度 RNN"></a>深度 RNN</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_27.png" alt=""></p><p>$a^{[t]}$ 表示第 t 层，这一层所有的参数都是相同的 $W_a^{[t]},b_a^{[t]}$.</p><p>由于存在时间这个维度，就算很少的层数网络都变得非常大，所以 RNN 模型的层数都不会很深。</p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> rnn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 4）</title>
      <link href="/2018/11/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w4%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/11/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w4%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h2 id="神经风格转换"><a href="#神经风格转换" class="headerlink" title="神经风格转换"></a>神经风格转换</h2><p>先引入需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> nst_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><a id="more"></a><p>使用一个预先训练好的 19 层的 VGG-19 网络进行迁移学习，下面加载预训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = load_vgg_model(<span class="string">"pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><ul><li><p>load_vgg_model() 函数为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CONFIG</span>:</span></span><br><span class="line">    IMAGE_WIDTH = <span class="number">400</span></span><br><span class="line">    IMAGE_HEIGHT = <span class="number">300</span></span><br><span class="line">    COLOR_CHANNELS = <span class="number">3</span></span><br><span class="line">    NOISE_RATIO = <span class="number">0.6</span></span><br><span class="line">    MEANS = np.array([<span class="number">123.68</span>, <span class="number">116.779</span>, <span class="number">103.939</span>]).reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>)) </span><br><span class="line">    VGG_MODEL = <span class="string">'pretrained-model/imagenet-vgg-verydeep-19.mat'</span> <span class="comment"># Pick the VGG 19-layer model by from the paper "Very Deep Convolutional Networks for Large-Scale Image Recognition".</span></span><br><span class="line">    STYLE_IMAGE = <span class="string">'images/stone_style.jpg'</span> <span class="comment"># Style image to use.</span></span><br><span class="line">    CONTENT_IMAGE = <span class="string">'images/content300.jpg'</span> <span class="comment"># Content image to use.</span></span><br><span class="line">    OUTPUT_DIR = <span class="string">'output/'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_vgg_model</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns a model for the purpose of 'painting' the picture.</span></span><br><span class="line"><span class="string">    Takes only the convolution layer weights and wrap using the TensorFlow</span></span><br><span class="line"><span class="string">    Conv2d, Relu and AveragePooling layer. VGG actually uses maxpool but</span></span><br><span class="line"><span class="string">    the paper indicates that using AveragePooling yields better results.</span></span><br><span class="line"><span class="string">    The last few fully connected layers are not used.</span></span><br><span class="line"><span class="string">    Here is the detailed configuration of the VGG model:</span></span><br><span class="line"><span class="string">        0 is conv1_1 (3, 3, 3, 64)</span></span><br><span class="line"><span class="string">        1 is relu</span></span><br><span class="line"><span class="string">        2 is conv1_2 (3, 3, 64, 64)</span></span><br><span class="line"><span class="string">        3 is relu    </span></span><br><span class="line"><span class="string">        4 is maxpool</span></span><br><span class="line"><span class="string">        5 is conv2_1 (3, 3, 64, 128)</span></span><br><span class="line"><span class="string">        6 is relu</span></span><br><span class="line"><span class="string">        7 is conv2_2 (3, 3, 128, 128)</span></span><br><span class="line"><span class="string">        8 is relu</span></span><br><span class="line"><span class="string">        9 is maxpool</span></span><br><span class="line"><span class="string">        10 is conv3_1 (3, 3, 128, 256)</span></span><br><span class="line"><span class="string">        11 is relu</span></span><br><span class="line"><span class="string">        12 is conv3_2 (3, 3, 256, 256)</span></span><br><span class="line"><span class="string">        13 is relu</span></span><br><span class="line"><span class="string">        14 is conv3_3 (3, 3, 256, 256)</span></span><br><span class="line"><span class="string">        15 is relu</span></span><br><span class="line"><span class="string">        16 is conv3_4 (3, 3, 256, 256)</span></span><br><span class="line"><span class="string">        17 is relu</span></span><br><span class="line"><span class="string">        18 is maxpool</span></span><br><span class="line"><span class="string">        19 is conv4_1 (3, 3, 256, 512)</span></span><br><span class="line"><span class="string">        20 is relu</span></span><br><span class="line"><span class="string">        21 is conv4_2 (3, 3, 512, 512)</span></span><br><span class="line"><span class="string">        22 is relu</span></span><br><span class="line"><span class="string">        23 is conv4_3 (3, 3, 512, 512)</span></span><br><span class="line"><span class="string">        24 is relu</span></span><br><span class="line"><span class="string">        25 is conv4_4 (3, 3, 512, 512)</span></span><br><span class="line"><span class="string">        26 is relu</span></span><br><span class="line"><span class="string">        27 is maxpool</span></span><br><span class="line"><span class="string">        28 is conv5_1 (3, 3, 512, 512)</span></span><br><span class="line"><span class="string">        29 is relu</span></span><br><span class="line"><span class="string">        30 is conv5_2 (3, 3, 512, 512)</span></span><br><span class="line"><span class="string">        31 is relu</span></span><br><span class="line"><span class="string">        32 is conv5_3 (3, 3, 512, 512)</span></span><br><span class="line"><span class="string">        33 is relu</span></span><br><span class="line"><span class="string">        34 is conv5_4 (3, 3, 512, 512)</span></span><br><span class="line"><span class="string">        35 is relu</span></span><br><span class="line"><span class="string">        36 is maxpool</span></span><br><span class="line"><span class="string">        37 is fullyconnected (7, 7, 512, 4096)</span></span><br><span class="line"><span class="string">        38 is relu</span></span><br><span class="line"><span class="string">        39 is fullyconnected (1, 1, 4096, 4096)</span></span><br><span class="line"><span class="string">        40 is relu</span></span><br><span class="line"><span class="string">        41 is fullyconnected (1, 1, 4096, 1000)</span></span><br><span class="line"><span class="string">        42 is softmax</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    vgg = scipy.io.loadmat(path)</span><br><span class="line"></span><br><span class="line">    vgg_layers = vgg[<span class="string">'layers'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_weights</span><span class="params">(layer, expected_layer_name)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return the weights and bias from the VGG model for a given layer.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        wb = vgg_layers[<span class="number">0</span>][layer][<span class="number">0</span>][<span class="number">0</span>][<span class="number">2</span>]</span><br><span class="line">        W = wb[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        b = wb[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">        layer_name = vgg_layers[<span class="number">0</span>][layer][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">assert</span> layer_name == expected_layer_name</span><br><span class="line">        <span class="keyword">return</span> W, b</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> W, b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_relu</span><span class="params">(conv2d_layer)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return the RELU function wrapped over a TensorFlow layer. Expects a</span></span><br><span class="line"><span class="string">        Conv2d layer input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.relu(conv2d_layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_conv2d</span><span class="params">(prev_layer, layer, layer_name)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return the Conv2D layer using the weights, biases from the VGG</span></span><br><span class="line"><span class="string">        model at 'layer'.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        W, b = _weights(layer, layer_name)</span><br><span class="line">        W = tf.constant(W)</span><br><span class="line">        b = tf.constant(np.reshape(b, (b.size)))</span><br><span class="line">        <span class="keyword">return</span> tf.nn.conv2d(prev_layer, filter=W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>) + b</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_conv2d_relu</span><span class="params">(prev_layer, layer, layer_name)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return the Conv2D + RELU layer using the weights, biases from the VGG</span></span><br><span class="line"><span class="string">        model at 'layer'.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> _relu(_conv2d(prev_layer, layer, layer_name))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_avgpool</span><span class="params">(prev_layer)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Return the AveragePooling layer.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> tf.nn.avg_pool(prev_layer, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Constructs the graph model.</span></span><br><span class="line">    graph = &#123;&#125;</span><br><span class="line">    <span class="comment"># 此处将输入作为 variable，使得优化器知道更新其参数</span></span><br><span class="line">    graph[<span class="string">'input'</span>]   = tf.Variable(np.zeros((<span class="number">1</span>, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)), dtype = <span class="string">'float32'</span>)</span><br><span class="line">    graph[<span class="string">'conv1_1'</span>]  = _conv2d_relu(graph[<span class="string">'input'</span>], <span class="number">0</span>, <span class="string">'conv1_1'</span>)</span><br><span class="line">    graph[<span class="string">'conv1_2'</span>]  = _conv2d_relu(graph[<span class="string">'conv1_1'</span>], <span class="number">2</span>, <span class="string">'conv1_2'</span>)</span><br><span class="line">    graph[<span class="string">'avgpool1'</span>] = _avgpool(graph[<span class="string">'conv1_2'</span>])</span><br><span class="line">    graph[<span class="string">'conv2_1'</span>]  = _conv2d_relu(graph[<span class="string">'avgpool1'</span>], <span class="number">5</span>, <span class="string">'conv2_1'</span>)</span><br><span class="line">    graph[<span class="string">'conv2_2'</span>]  = _conv2d_relu(graph[<span class="string">'conv2_1'</span>], <span class="number">7</span>, <span class="string">'conv2_2'</span>)</span><br><span class="line">    graph[<span class="string">'avgpool2'</span>] = _avgpool(graph[<span class="string">'conv2_2'</span>])</span><br><span class="line">    graph[<span class="string">'conv3_1'</span>]  = _conv2d_relu(graph[<span class="string">'avgpool2'</span>], <span class="number">10</span>, <span class="string">'conv3_1'</span>)</span><br><span class="line">    graph[<span class="string">'conv3_2'</span>]  = _conv2d_relu(graph[<span class="string">'conv3_1'</span>], <span class="number">12</span>, <span class="string">'conv3_2'</span>)</span><br><span class="line">    graph[<span class="string">'conv3_3'</span>]  = _conv2d_relu(graph[<span class="string">'conv3_2'</span>], <span class="number">14</span>, <span class="string">'conv3_3'</span>)</span><br><span class="line">    graph[<span class="string">'conv3_4'</span>]  = _conv2d_relu(graph[<span class="string">'conv3_3'</span>], <span class="number">16</span>, <span class="string">'conv3_4'</span>)</span><br><span class="line">    graph[<span class="string">'avgpool3'</span>] = _avgpool(graph[<span class="string">'conv3_4'</span>])</span><br><span class="line">    graph[<span class="string">'conv4_1'</span>]  = _conv2d_relu(graph[<span class="string">'avgpool3'</span>], <span class="number">19</span>, <span class="string">'conv4_1'</span>)</span><br><span class="line">    graph[<span class="string">'conv4_2'</span>]  = _conv2d_relu(graph[<span class="string">'conv4_1'</span>], <span class="number">21</span>, <span class="string">'conv4_2'</span>)</span><br><span class="line">    graph[<span class="string">'conv4_3'</span>]  = _conv2d_relu(graph[<span class="string">'conv4_2'</span>], <span class="number">23</span>, <span class="string">'conv4_3'</span>)</span><br><span class="line">    graph[<span class="string">'conv4_4'</span>]  = _conv2d_relu(graph[<span class="string">'conv4_3'</span>], <span class="number">25</span>, <span class="string">'conv4_4'</span>)</span><br><span class="line">    graph[<span class="string">'avgpool4'</span>] = _avgpool(graph[<span class="string">'conv4_4'</span>])</span><br><span class="line">    graph[<span class="string">'conv5_1'</span>]  = _conv2d_relu(graph[<span class="string">'avgpool4'</span>], <span class="number">28</span>, <span class="string">'conv5_1'</span>)</span><br><span class="line">    graph[<span class="string">'conv5_2'</span>]  = _conv2d_relu(graph[<span class="string">'conv5_1'</span>], <span class="number">30</span>, <span class="string">'conv5_2'</span>)</span><br><span class="line">    graph[<span class="string">'conv5_3'</span>]  = _conv2d_relu(graph[<span class="string">'conv5_2'</span>], <span class="number">32</span>, <span class="string">'conv5_3'</span>)</span><br><span class="line">    graph[<span class="string">'conv5_4'</span>]  = _conv2d_relu(graph[<span class="string">'conv5_3'</span>], <span class="number">34</span>, <span class="string">'conv5_4'</span>)</span><br><span class="line">    graph[<span class="string">'avgpool5'</span>] = _avgpool(graph[<span class="string">'conv5_4'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> graph</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.4_1.png" alt=""></p><p>模型被我们存进了一个 dict，dict 的 key 是每一层的名字，value 是这一层的值。</p><p>我们可以对某一层进行赋值，例如对输入进行赋值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model[<span class="string">"input"</span>].assign(image)</span><br></pre></td></tr></table></figure><p>如果想得到某一层的值，我们可以：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(model[<span class="string">"conv4_2"</span>])</span><br></pre></td></tr></table></figure><h3 id="内容损失函数"><a href="#内容损失函数" class="headerlink" title="内容损失函数"></a>内容损失函数</h3><p>内容损失函数是内容图片前向传播的某一层的激活值 a_C 和生成图片前向传播的某一层的激活值 a_G 之间的差距，一般来说我们选取中间层的激活值更能代表图片的内容。</p><script type="math/tex; mode=display">J_{content}(C,G) =  \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2\tag{1}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span><span class="params">(a_C, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the content cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_content -- scalar that you compute using equation 1 above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G (≈1 line)</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the cost with tensorflow (≈1 line)</span></span><br><span class="line">    J_content = (<span class="number">1</span> / (<span class="number">4</span> * n_W * n_H * n_C)) * tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled)))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_content</span><br></pre></td></tr></table></figure><h3 id="风格代价函数"><a href="#风格代价函数" class="headerlink" title="风格代价函数"></a>风格代价函数</h3><h4 id="风格矩阵"><a href="#风格矩阵" class="headerlink" title="风格矩阵"></a>风格矩阵</h4><p>风格矩阵代表了某张图片的风格，用某一层的激活值的格拉姆矩阵表示，也就是这一层各个通道之间的相关性，具体的步骤为：先将某一层激活值展开成二维矩阵，风格矩阵就是这个二维矩阵叉乘它的转置矩阵。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.4_2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.4_3.png" alt=""></p><p>风格矩阵的斜对角，是某个通道自己和自己的相关性，表示某个通道的活跃程度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 风格矩阵生成</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(A)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    A -- matrix of shape (n_C, n_H*n_W)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    GA -- Gram matrix of A, of shape (n_C, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    GA = tf.matmul(A, A, transpose_b=<span class="keyword">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> GA</span><br></pre></td></tr></table></figure><h4 id="风格代价函数-1"><a href="#风格代价函数-1" class="headerlink" title="风格代价函数"></a>风格代价函数</h4><p>对于某一层而言，这一层的风格代价函数是，生成图片在这一层的风格矩阵和风格图片在这一层的风格矩阵的“距离”。</p><script type="math/tex; mode=display">J_{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2\tag{2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 某一层的风格代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span><span class="params">(a_S, a_G)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S </span></span><br><span class="line"><span class="string">    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from a_G</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape the images to have them of shape (n_C, n_H*n_W)</span></span><br><span class="line">    a_S = tf.transpose(tf.reshape(a_S, [n_H*n_W, n_C]))<span class="comment"># tf.reshape 是从最后一个维度开始取数，(n_H, n_W, n_C) 最后一个维度是通道数，也就是说先把第一行第一列的通道数取出</span></span><br><span class="line">    a_G = tf.transpose(tf.reshape(a_G, [n_H*n_W, n_C]))<span class="comment"># 然后按行排列，如果这一行满了 n_C 个则换行，接着取第一行第二列的通道数，接着按行排列，依次类推，所以只能reshape陈</span></span><br><span class="line">                                                       <span class="comment"># 成(n_H*n_W, n_C)形状的向量，然后进行一次转置操作变成 (n_C, n_W*n_H)  </span></span><br><span class="line">    <span class="comment"># Computing gram_matrices for both images S and G </span></span><br><span class="line">    GS = gram_matrix(a_S)</span><br><span class="line">    GG = gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Computing the loss </span></span><br><span class="line">    J_style_layer = (<span class="number">1</span> / (<span class="number">4</span> * (n_C ** <span class="number">2</span>) * (n_H*n_W) ** <span class="number">2</span>)) * tf.reduce_sum(tf.square(tf.subtract(GS,GG)))</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> J_style_layer</span><br></pre></td></tr></table></figure><p>最后总的风格代价函数是每一层的风格代价值用不同的权重组合起来，权重存放在一个字典里：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">STYLE_LAYERS = [</span><br><span class="line">    (<span class="string">'conv1_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv2_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv3_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv4_1'</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">'conv5_1'</span>, <span class="number">0.2</span>)]</span><br></pre></td></tr></table></figure><p>加上权重之后总的风格代价函数为：</p><script type="math/tex; mode=display">J_{style}(S,G) = \sum_{l} \lambda^{[l]} J^{[l]}_{style}(S,G)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_style_cost</span><span class="params">(model, STYLE_LAYERS)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the overall style cost from several chosen layers</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    model -- our tensorflow model</span></span><br><span class="line"><span class="string">    STYLE_LAYERS -- A python list containing:</span></span><br><span class="line"><span class="string">                        - the names of the layers we would like to extract style from</span></span><br><span class="line"><span class="string">                        - a coefficient for each of them</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    J_style -- tensor representing a scalar value, style cost defined above by equation (2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the overall style cost</span></span><br><span class="line">    J_style = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> layer_name, coeff <span class="keyword">in</span> STYLE_LAYERS:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Select the output tensor of the currently selected layer</span></span><br><span class="line">        out = model[layer_name]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out</span></span><br><span class="line">        a_S = sess.run(out)<span class="comment"># 先对 a_S 进行赋值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set a_G to be the hidden layer activation from same layer. Here, a_G references model[layer_name] </span></span><br><span class="line">        <span class="comment"># and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that</span></span><br><span class="line">        <span class="comment"># when we run the session, this will be the activations drawn from the appropriate layer, with G as input.</span></span><br><span class="line">        a_G = out <span class="comment"># 由于 a_G 是最后 assign 进模型，我们先不赋值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute style_cost for the current layer</span></span><br><span class="line">        J_style_layer = compute_layer_style_cost(a_S, a_G)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add coeff * J_style_layer of this layer to overall style cost</span></span><br><span class="line">        J_style += coeff * J_style_layer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> J_style</span><br></pre></td></tr></table></figure><h4 id="总的代价函数"><a href="#总的代价函数" class="headerlink" title="总的代价函数"></a>总的代价函数</h4><script type="math/tex; mode=display">J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span><span class="params">(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the total cost function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    J_content -- content cost coded above</span></span><br><span class="line"><span class="string">    J_style -- style cost coded above</span></span><br><span class="line"><span class="string">    alpha -- hyperparameter weighting the importance of the content cost</span></span><br><span class="line"><span class="string">    beta -- hyperparameter weighting the importance of the style cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- total cost as defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    J = alpha * J_content + beta * J_style</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><h3 id="进行训练"><a href="#进行训练" class="headerlink" title="进行训练"></a>进行训练</h3><p>步骤为：</p><ul><li>创建对话</li><li>加载内容图片 C</li><li>加载风格图片 S</li><li>随机初始化需要生成的图片 G</li><li>加载预训练模型 VGG</li><li>建立 tensorflow 图：<ul><li>将内容图片通过 VGG 模型计算内容代价函数</li><li>将风格图片通过 VGG 模型计算风格代价函数</li><li>计算总代价函数</li><li>定义优化器和学习率</li></ul></li><li>初始化计算图用很大的迭代数运行，每一步都更新一次生成图片 G</li></ul><p>创建对话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重设计算图</span></span><br><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启交互式对话，在没有指定会话对象时也会运行变量，不用 with tf.Session() as sess 这种语句来指明默认会话，它自己就是默认会话，更方便</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure><p>加载内容图片 C 并进行 reshape 和 归一化处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">content_image = scipy.misc.imread(<span class="string">"images/1.jpg"</span>)</span><br><span class="line">content_image = reshape_and_normalize_image(content_image)</span><br></pre></td></tr></table></figure><ul><li><p>归一化函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reshape_and_normalize_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Reshape and normalize the input image (content or style)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Reshape image to mach expected input of VGG16</span></span><br><span class="line">    image = np.reshape(image, ((<span class="number">1</span>,) + image.shape))<span class="comment"># (1,)+(400,300,3)=(1,400,300,3)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Substract the mean to match the expected input of VGG16</span></span><br><span class="line">    image = image - CONFIG.MEANS</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure></li></ul><p>加载风格图片 S 并进行归一化处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">style_image = scipy.misc.imread(<span class="string">"images/2.jpg"</span>)</span><br><span class="line">style_image = reshape_and_normalize_image(style_image)</span><br></pre></td></tr></table></figure><p>现在用噪音初始化生成图片 G，虽然每个像素是随机的噪声，但还是与内容图片 C 相关，噪声和 C 以一定的权重叠加，使得在更新 G 的像素时能更快地匹配到内容图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">generated_image = generate_noise_image(content_image)</span><br><span class="line">imshow(generated_image[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><ul><li><p>生成噪声的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_noise_image</span><span class="params">(content_image, noise_ratio = CONFIG.NOISE_RATIO)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Generates a noisy image by adding random noise to the content_image</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Generate a random noise_image</span></span><br><span class="line">    noise_image = np.random.uniform(<span class="number">-20</span>, <span class="number">20</span>, (<span class="number">1</span>, CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.COLOR_CHANNELS)).astype(<span class="string">'float32'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set the input_image to be a weighted average of the content_image and a noise_image</span></span><br><span class="line">    input_image = noise_image * noise_ratio + content_image * (<span class="number">1</span> - noise_ratio)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> input_image</span><br></pre></td></tr></table></figure></li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.4_4.png" alt=""></p><p>加载预训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = load_vgg_model(<span class="string">"pretrained-model/imagenet-vgg-verydeep-19.mat"</span>)</span><br></pre></td></tr></table></figure><p>构建计算图：</p><ol><li><p>选取某一层计算内容代价函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assign the content image to be the input of the VGG model.  </span></span><br><span class="line">sess.run(model[<span class="string">'input'</span>].assign(content_image))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select the output tensor of layer conv4_2</span></span><br><span class="line">out = model[<span class="string">'conv4_2'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a_C to be the hidden layer activation from the layer we have selected</span></span><br><span class="line">a_C = sess.run(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] </span></span><br><span class="line"><span class="comment"># and isn't evaluated yet. Later in the code, we'll assign the image G as the model input, so that</span></span><br><span class="line"><span class="comment"># when we run the session, this will be the activations drawn from the appropriate layer, with G as input.</span></span><br><span class="line">a_G = out</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the content cost</span></span><br><span class="line">J_content = compute_content_cost(a_C, a_G)</span><br></pre></td></tr></table></figure></li><li><p>计算风格代价函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Assign the input of the model to be the "style" image </span></span><br><span class="line">sess.run(model[<span class="string">'input'</span>].assign(style_image))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the style cost</span></span><br><span class="line">J_style = compute_style_cost(model, STYLE_LAYERS)</span><br></pre></td></tr></table></figure></li><li><p>计算总的代价函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 总代价函数</span></span><br><span class="line">J = total_cost(J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">80</span>)</span><br></pre></td></tr></table></figure></li><li><p>定义优化器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define optimizer</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define train_step，选择优化对象，总代价函数 J</span></span><br><span class="line">train_step = optimizer.minimize(J)</span><br></pre></td></tr></table></figure></li></ol><p>下面进行训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span><span class="params">(sess, input_image, num_iterations = <span class="number">200</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化全局变量</span></span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输入图片 G 是一个变量，使用 assign 对其进行赋值</span></span><br><span class="line">    sess.run(model[<span class="string">'input'</span>].assign(input_image))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 运行优化器，更新图片 G 的参数</span></span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获得生成图片的值</span></span><br><span class="line">        generated_image = sess.run(model[<span class="string">'input'</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print every 20 iteration.</span></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            Jt, Jc, Js = sess.run([J, J_content, J_style])</span><br><span class="line">            print(<span class="string">"Iteration "</span> + str(i) + <span class="string">" :"</span>)</span><br><span class="line">            print(<span class="string">"total cost = "</span> + str(Jt))</span><br><span class="line">            print(<span class="string">"content cost = "</span> + str(Jc))</span><br><span class="line">            print(<span class="string">"style cost = "</span> + str(Js))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># save current generated image in the "/output" directory</span></span><br><span class="line">            save_image(<span class="string">"output/"</span> + str(i) + <span class="string">".png"</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># save last generated image</span></span><br><span class="line">    save_image(<span class="string">'output/generated_image.jpg'</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_image</span><br></pre></td></tr></table></figure><ul><li><p>其中保存图片的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_image</span><span class="params">(path, image)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Un-normalize the image so that it looks good 去归一化</span></span><br><span class="line">    image = image + CONFIG.MEANS</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Clip and Save the image</span></span><br><span class="line">    image = np.clip(image[<span class="number">0</span>], <span class="number">0</span>, <span class="number">255</span>).astype(<span class="string">'uint8'</span>)<span class="comment"># 这一句将元素的值限制在 0~255 之间</span></span><br><span class="line">    scipy.misc.imsave(path, image)</span><br></pre></td></tr></table></figure></li></ul><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_nn(sess, generated_image)</span><br><span class="line">result_image = scipy.misc.imread(<span class="string">"output/generated_image.jpg"</span>)</span><br><span class="line">imshow(result_image)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.4_5.png" alt=""></p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>What you should remember:</p><ul><li>Neural Style Transfer is an algorithm that given a content image C and a style image S can generate an artistic image</li><li>It uses representations (hidden layer activations) based on a pretrained ConvNet. </li><li>The content cost function is computed using one hidden layer’s activations.</li><li>The style cost function for one layer is computed using the Gram matrix of that layer’s activations. The overall style cost function is obtained using several hidden layers.</li><li>Optimizing the total cost function results in synthesizing new images. </li></ul><h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><ul><li>使用三重损失函数</li><li>使用预训练模型来对人脸图片进行编码</li><li>使用这些编码来实现人脸验证和人脸识别</li></ul><p>照例先引入需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, ZeroPadding2D, Activation, Input, concatenate</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers.normalization <span class="keyword">import</span> BatchNormalization</span><br><span class="line"><span class="keyword">from</span> keras.layers.pooling <span class="keyword">import</span> MaxPooling2D, AveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.layers.merge <span class="keyword">import</span> Concatenate</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Lambda, Flatten, Dense</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">from</span> keras.engine.topology <span class="keyword">import</span> Layer</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_first'</span>)<span class="comment"># 将图片格式设置为通道数在前</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> fr_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> inception_blocks_v2 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold=np.nan)</span><br></pre></td></tr></table></figure><h3 id="三重损失"><a href="#三重损失" class="headerlink" title="三重损失"></a>三重损失</h3><p>由于我们采用预训练模型，不需要对三重损失进行优化，但是还是有必要知道如何实现。</p><script type="math/tex; mode=display">\mathcal{J} = \sum^{m}_{i=1} \large[ \small \mid \mid f(A^{(i)}) - f(P^{(i)}) \mid \mid_2^2 - \mid \mid f(A^{(i)}) - f(N^{(i)}) \mid \mid_2^2+ \alpha \large ] \small_+</script><ul><li>A 为锚照片，P 为正例，N 为反例，$\alpha$ 是一个裕度</li><li>f() 表示照片经过模型的输出向量，也就是编码</li><li>$[ \ \  ]_+$表示取和 0 相比的较大值</li></ul><p>为了能在 keras 模型的编译环节使用，我们使用 keras 中损失函数的格式进行自定义 loss 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triplet_loss</span><span class="params">(y_true, y_pred, alpha = <span class="number">0.2</span>)</span>:</span> </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the triplet loss as defined by formula (3)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    y_true -- y_true 其实没有用，但是在 Keras 中要定义一个损失函数就必须按照这个格式</span></span><br><span class="line"><span class="string">    y_pred -- python list containing three objects:</span></span><br><span class="line"><span class="string">            anchor -- the encodings for the anchor images, of shape (None, 128)</span></span><br><span class="line"><span class="string">            positive -- the encodings for the positive images, of shape (None, 128)</span></span><br><span class="line"><span class="string">            negative -- the encodings for the negative images, of shape (None, 128)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- real number, value of the loss</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    anchor, positive, negative = y_pred[<span class="number">0</span>], y_pred[<span class="number">1</span>], y_pred[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1</span></span><br><span class="line">    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis = <span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1</span></span><br><span class="line">    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis = <span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># Step 3: subtract the two previous distances and add alpha.</span></span><br><span class="line">    basic_loss = pos_dist - neg_dist + alpha</span><br><span class="line">    <span class="comment"># Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.</span></span><br><span class="line">    loss = tf.reduce_sum(tf.maximum(basic_loss, <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><h3 id="加载预训练模型"><a href="#加载预训练模型" class="headerlink" title="加载预训练模型"></a>加载预训练模型</h3><p>我们使用别人训练好的一个 Inception 网络来对图片进行编码。</p><ul><li>输入形状为 $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$ 的图片</li><li>输出形状为 $(m, 128)$ 的图片编码</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FRmodel = faceRecoModel(input_shape=(<span class="number">3</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br></pre></td></tr></table></figure><p>编译模型并加载权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FRmodel.compile(optimizer = <span class="string">'adam'</span>, loss = triplet_loss, metrics = [<span class="string">'accuracy'</span>])</span><br><span class="line">load_weights_from_FaceNet(FRmodel)</span><br></pre></td></tr></table></figure><p>具体的函数细节见 <a href="https://github.com/iwantooxxoox" target="_blank" rel="noopener">iwantooxxoox</a> 的 github。</p><h3 id="人脸识别和验证"><a href="#人脸识别和验证" class="headerlink" title="人脸识别和验证"></a>人脸识别和验证</h3><p>首先我们要建立一个数据库，里面存放了所有的需要识别或验证的人的照片通过神经网络的编码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">database = &#123;&#125;</span><br><span class="line">database[<span class="string">"danielle"</span>] = img_to_encoding(<span class="string">"images/danielle.png"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"younes"</span>] = img_to_encoding(<span class="string">"images/younes.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"tian"</span>] = img_to_encoding(<span class="string">"images/tian.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"andrew"</span>] = img_to_encoding(<span class="string">"images/andrew.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"kian"</span>] = img_to_encoding(<span class="string">"images/kian.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"dan"</span>] = img_to_encoding(<span class="string">"images/dan.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"sebastiano"</span>] = img_to_encoding(<span class="string">"images/sebastiano.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"bertrand"</span>] = img_to_encoding(<span class="string">"images/bertrand.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"kevin"</span>] = img_to_encoding(<span class="string">"images/kevin.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"felix"</span>] = img_to_encoding(<span class="string">"images/felix.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"benoit"</span>] = img_to_encoding(<span class="string">"images/benoit.jpg"</span>, FRmodel)</span><br><span class="line">database[<span class="string">"arnaud"</span>] = img_to_encoding(<span class="string">"images/arnaud.jpg"</span>, FRmodel)</span><br></pre></td></tr></table></figure><p>其中获取编码的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img_to_encoding</span><span class="params">(image_path, model)</span>:</span></span><br><span class="line">    img1 = cv2.imread(image_path, <span class="number">1</span>)</span><br><span class="line">    img = img1[...,::<span class="number">-1</span>]</span><br><span class="line">    img = np.around(np.transpose(img, (<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>))/<span class="number">255.0</span>, decimals=<span class="number">12</span>)</span><br><span class="line">    x_train = np.array([img])</span><br><span class="line">    embedding = model.predict_on_batch(x_train)</span><br><span class="line">    <span class="keyword">return</span> embedding</span><br></pre></td></tr></table></figure><h4 id="人脸检验"><a href="#人脸检验" class="headerlink" title="人脸检验"></a>人脸检验</h4><p>所谓人脸检验就是在识别时提供人脸照片和 ID 号，用来验证是不是本人，1 对 1 的问题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verify</span><span class="params">(image_path, identity, database, model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function that verifies if the person on the "image_path" image is "identity".</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    image_path -- path to an image</span></span><br><span class="line"><span class="string">    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.</span></span><br><span class="line"><span class="string">    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).</span></span><br><span class="line"><span class="string">    model -- your Inception model instance in Keras</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dist -- distance between the image_path and the image of "identity" in the database.</span></span><br><span class="line"><span class="string">    door_open -- True, if the door should open. False otherwise.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. </span></span><br><span class="line">    encoding = img_to_encoding(image_path, model)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute distance with identity's image </span></span><br><span class="line">    dist = np.linalg.norm(encoding - database[identity])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Open the door if dist &lt; 0.7, else don't open</span></span><br><span class="line">    <span class="keyword">if</span> dist &lt; <span class="number">0.7</span>:</span><br><span class="line">        print(<span class="string">"It's "</span> + str(identity) + <span class="string">", welcome home!"</span>)</span><br><span class="line">        door_open = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"It's not "</span> + str(identity) + <span class="string">", please go away"</span>)</span><br><span class="line">        door_open = <span class="keyword">False</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> dist, door_open</span><br></pre></td></tr></table></figure><h4 id="人脸识别-1"><a href="#人脸识别-1" class="headerlink" title="人脸识别"></a>人脸识别</h4><p>人脸识别不再提供 ID 号，只提供人脸照片，然后与数据库里的编码进行比对，找出与之距离最小的，如果距离小于某个阈值，则我们认定他是数据库中的人。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">who_is_it</span><span class="params">(image_path, database, model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements face recognition for the happy house by finding who is the person on the image_path image.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    image_path -- path to an image</span></span><br><span class="line"><span class="string">    database -- database containing image encodings along with the name of the person on the image</span></span><br><span class="line"><span class="string">    model -- your Inception model instance in Keras</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    min_dist -- the minimum distance between image_path encoding and the encodings from the database</span></span><br><span class="line"><span class="string">    identity -- string, the name prediction for the person on image_path</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">## Step 1: Compute the target "encoding" for the image. Use img_to_encoding() see example above.</span></span><br><span class="line">    encoding = img_to_encoding(image_path, model)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## Step 2: Find the closest encoding，排序算法</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "min_dist" to a large value, say 100 </span></span><br><span class="line">    min_dist = <span class="number">100</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over the database dictionary's names and encodings.</span></span><br><span class="line">    <span class="keyword">for</span> (name, db_enc) <span class="keyword">in</span> database.items(): <span class="comment"># 如果要迭代 key 和 value 必须加 .item() </span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute L2 distance between the target "encoding" and the current "emb" from the database.</span></span><br><span class="line">        dist = np.linalg.norm(encoding - db_enc)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If this distance is less than the min_dist, then set min_dist to dist, and identity to name.</span></span><br><span class="line">        <span class="keyword">if</span> dist &lt; min_dist:</span><br><span class="line">            min_dist = dist</span><br><span class="line">            identity = name</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> min_dist &gt; <span class="number">0.7</span>:</span><br><span class="line">        print(<span class="string">"Not in the database."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"it's "</span> + str(identity) + <span class="string">", the distance is "</span> + str(min_dist))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> min_dist, identity</span><br></pre></td></tr></table></figure><h3 id="算法的改进"><a href="#算法的改进" class="headerlink" title="算法的改进"></a>算法的改进</h3><ul><li>Put more images of each person (under different lighting conditions, taken on different days, etc.) into the database. Then given a new image, compare the new face to multiple pictures of the person. This would increae accuracy.</li><li>Crop the images to just contain the face, and less of the “border” region around the face. This preprocessing removes some of the irrelevant pixels around the face, and also makes the algorithm more robust.</li></ul><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><ul><li>Face verification solves an easier 1:1 matching problem; face recognition addresses a harder 1:K matching problem. </li><li>The triplet loss is an effective loss function for training a neural network to learn an encoding of a face image.</li><li>The same encoding can be used for verification and recognition. Measuring distances between two images’ encodings allows you to determine whether they are pictures of the same person. </li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
            <tag> 人脸识别 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 4 week 4）—— 人脸识别</title>
      <link href="/2018/11/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w4/"/>
      <url>/2018/11/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w4/</url>
      <content type="html"><![CDATA[<p>这一周我们将学习到如何利用 CNN 进行图片风格转换和人脸识别。</p><h2 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h2><h3 id="什么是人脸识别或人脸校验"><a href="#什么是人脸识别或人脸校验" class="headerlink" title="什么是人脸识别或人脸校验"></a>什么是人脸识别或人脸校验</h3><ul><li>人脸校验<ul><li>输出图片，姓名或身份号</li><li>输出是否输入图片是对应的那个人</li><li>这是一个 1：1 问题</li></ul></li><li>人脸识别<ul><li>有 K 个人的数据库</li><li>输入图片</li><li>输出这个人的 ID 号，如果它是这 K 个人之一的话，如果不是，输出“未识别”</li><li>这是一个 1：K 问题</li></ul></li></ul><a id="more"></a><h3 id="单样本学习-one-shot-learning"><a href="#单样本学习-one-shot-learning" class="headerlink" title="单样本学习 one-shot learning"></a>单样本学习 one-shot learning</h3><p>单样本学习指的是用<strong>一张</strong>样本照片进行学习，然后再次识别出这个人。由于只有一个样本，所以如果直接输入到 CNN 进行训练，效果非常不好。</p><p>正确的做法是学习出一个“相似度”函数 <code>d(img1, img2)</code>，它等于两个图片的不同程度，如果 $d(img1, img2) \le \tau$，那么这两张图片就是一个人，如果反之，这不是一个人。</p><h3 id="孪生网络-Siamese-network"><a href="#孪生网络-Siamese-network" class="headerlink" title="孪生网络 Siamese network"></a>孪生网络 Siamese network</h3><p>[Taigman et. al., 2014. DeepFace closing the gap to human level performance] </p><p>假设我们有两张图片 $x^{(1)}, x^{(2)}$，我们将它们通过一个卷积网络，输出两个编码向量 $f(x^{(1)})$ 和 $f(x^{(2)})$，训练这个网络，使得我们确信这个编码是对这两张图片的良好表达，这个相似度函数就是两张照片的编码的差的范数。</p><script type="math/tex; mode=display">d(x^{(1)},x^{(2)})=||f(x^{(1)})-f(x^{(2)})||^2_2</script><p>这个概念被称作孪生网络。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_1.png" alt=""></p><p>如何训练？</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_2.png" alt=""></p><ul><li>神经网络的参数定义了图片的一个编码 $f(x^{(i)})$</li><li>学习参数使得：<ul><li>如果 $x^{(i)}$ 和 $x^{(j)}$ 是同一个人，那么 $||f(x^{(1)})-f(x^{(2)})||^2_2$ 就很小</li><li>如果 $x^{(i)}$ 和 $x^{(j)}$ 是不同的人，那么 $||f(x^{(1)})-f(x^{(2)})||^2_2$ 就很大</li></ul></li></ul><h3 id="三元损失函数-Triple-Loss"><a href="#三元损失函数-Triple-Loss" class="headerlink" title="三元损失函数 Triple Loss"></a>三元损失函数 Triple Loss</h3><h4 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h4><p>首先我们有一个“锚 Anchor”照片 A，如果另一张照片和这个锚照片是同一个人，着称之为“正例 Positive” P，反之称为“反例 Negative” N，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_3.png" alt=""></p><p>我们期望的是 A 和 P 的差距 d(A, P) 要小于 A 和 N 的差距 d(A, N)，即：</p><script type="math/tex; mode=display">d(A, P) \le d(A,N) \\||f(A)-f(P)||^2 \le ||f(A)-f(N)||^2\\||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 \le 0</script><p>但是为了防止神经网络把 f(A) 和 f(P) 和 f(N) 训练得特别接近，例如当它们都为 0 时，上式仍然满足，为了防止神经网络输出退化解，增大 d(A, P) 和 d(A, N) 之间的差距，我们增加一个超参数 margin $\alpha$，式子变成：</p><script type="math/tex; mode=display">d(A, P) + \alpha \le d(A,N) \\||f(A)-f(P)||^2 + \alpha  \le ||f(A)-f(N)||^2 \\||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha \le 0</script><p>当 d(A, P) = 0.5 时，加上 $\alpha$ = 0.2，则 d(A, N) 至少为 0.7，使得 d(A, P) 远小于 d(A, N)。</p><h4 id="三元损失函数定义"><a href="#三元损失函数定义" class="headerlink" title="三元损失函数定义"></a>三元损失函数定义</h4><p>给定三个图片 A，P，N：</p><script type="math/tex; mode=display">L(A,P,N)=max(||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + \alpha, \  0 )</script><p>总的代价函数为：</p><script type="math/tex; mode=display">J = \sum\limits ^m_{i=1}L(A^{(i)},P^{(i)},N^{(i)})</script><p>我们必须保证每个人不止一张照片，假设训练集为 1000 个人的 10000 张照片，我们需要拿这 10000 张照片去生成上式的三元组。</p><h4 id="如何选择三元组-A-P-N"><a href="#如何选择三元组-A-P-N" class="headerlink" title="如何选择三元组 A, P, N"></a>如何选择三元组 A, P, N</h4><p>在训练时，如果你随意选择 A，P，N，那么 $d(A, P) + \alpha \le d(A,N)$ 这个式子会非常容易满足，因为如果随便挑的 d(A, P) 会有很大几率远小于 d(A, N)，这样导致神经网络无法从中学到东西。</p><p>我们需要做的是挑选那种<strong>很难训练</strong>的三元组，使得你挑的 A, P, N 会让 d(A,P) 很靠近 d(A,N)，这样使得算法在学习的时候要花更多的力气尝试让右边这一项增大，左边这项减小，也就是使得：</p><script type="math/tex; mode=display">d(A, P)  \approx  d(A, N)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_4.png" alt=""></p><p>更多三元组选择方法见这篇论文。</p><h3 id="人脸识别的二元分类方法"><a href="#人脸识别的二元分类方法" class="headerlink" title="人脸识别的二元分类方法"></a>人脸识别的二元分类方法</h3><p>除了三元损失函数，我们还有其他的定义损失函数的方法。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_5.png" alt=""></p><script type="math/tex; mode=display">\hat y = \sigma (\sum\limits ^{128} _{k=1}w_i|f(x^{(i)})_k-f(x^{(j)})_k|+b)</script><p>还有一种变种 $\chi ^2$ 相似度：</p><script type="math/tex; mode=display">\hat y = \sigma (\sum\limits ^{128} _{k=1}w_i \frac{(f(x^{(i)})_k-f(x^{(j)})_k)^2}{f(x^{(i)})_k+f(x^{(j)})_k}  +b)</script><p>一个小技巧是我们不必每次都计算数据库中的图片的编码值，我们可以先进行预计算，在进行识别时直接跟这些预计算的编码值进行比较，然后预测结果 y.</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_6.png" alt=""></p><h2 id="神经风格转化-Neural-style-transfer"><a href="#神经风格转化-Neural-style-transfer" class="headerlink" title="神经风格转化 Neural style transfer"></a>神经风格转化 Neural style transfer</h2><h3 id="什么是神经风格转化"><a href="#什么是神经风格转化" class="headerlink" title="什么是神经风格转化"></a>什么是神经风格转化</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_7.png" alt=""></p><h3 id="深度卷积网络的可视化"><a href="#深度卷积网络的可视化" class="headerlink" title="深度卷积网络的可视化"></a>深度卷积网络的可视化</h3><p>假设我们正在训练这样一个网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_8.png" alt=""></p><p>为了知道每一层到底在计算什么，并将其可视化，我们可以在这层挑出一个神经元，找到使得这个神经元激活值最大的九个图像小块。在更深的层，隐藏单元能看到更大一部分图像，即更大图像对这些层的输出有影响，我们将每一层可视化，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_9.png" alt=""></p><p>我们可以看到，第一层在检测一些边缘特征，随着层数的加深，检测的对象越来越复杂。</p><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>[Gatys et al., 2015. A neural algorithm of artistic style. Images on slide generated by Justin Johnson] </p><p>问题的形式是：我们有一张“内容 content 图片” C，一张“风格 style 图片” S，得到一个“生成 generated 图像” G。我们需要一个代价函数 J(G) 来评价某个图像生成的质量有多好，用梯度下降使得 G 的损失最小，从而生成想要的图像。</p><p>代价函数分为两部分： 内容代价函数，表示生成图像和内容图像之间的相似程度，以及风格代价函数，表示生成图像和风格图像之间的相似程度，再加上两个系数表示风格代价和内容代价之间的比重。</p><script type="math/tex; mode=display">J(G)= \alpha J_{content}(C,G)+\beta J_{style}(S,G)</script><p><strong>如何生成 G</strong>：</p><ul><li>将 G 随机初始化<ul><li>G : 100×100×3</li></ul></li><li>使用梯度下降最小化 J(G) <ul><li>$G:=G- \frac{\partial J(G)}{\partial G}$ 不同于更新权重，我们通过梯度下降更新 G 的像素值来最小化 J(G)</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_10.png" alt=""></p><h3 id="内容代价函数"><a href="#内容代价函数" class="headerlink" title="内容代价函数"></a>内容代价函数</h3><script type="math/tex; mode=display">J(G)= \alpha J_{content}(C,G)+\beta J_{style}(S,G)</script><ul><li>假设使用 $l$ 个隐藏层来计算内容代价函数</li><li>使用预训练的卷积网络，比如 VGG 网络</li><li>分别将内容图片 C 和生成图片 G 输入这个网络，假设 $a ^ { [ l ] ( C ) }$ 和 $a ^ { [ l ] ( G ) }$ 分别是第 $l$ 层的内容图像 C 和生成图片 G 的<strong>激活值</strong></li><li>如果 $a ^ { [ l ] ( C ) }$ 和 $a ^ { [ l ] ( G ) }$ 很接近，那么两张图片就有相似的内容，所以内容代价函数为：</li></ul><script type="math/tex; mode=display">J_{content}(C,G) = \frac{1}{2}||a ^ { [ l ] ( C ) } - a ^ { [ l ] ( G ) }||^2</script><h3 id="风格代价函数"><a href="#风格代价函数" class="headerlink" title="风格代价函数"></a>风格代价函数</h3><h4 id="如何定义一张图片的“风格”"><a href="#如何定义一张图片的“风格”" class="headerlink" title="如何定义一张图片的“风格”"></a>如何定义一张图片的“风格”</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_11.png" alt=""></p><p>假设我们使用方框框起来的那一层的激活值来测量“风格”，我们把“风格”定义为这一层的激活值的不同通道之间的相关性。</p><h4 id="关于图片风格的解释"><a href="#关于图片风格的解释" class="headerlink" title="关于图片风格的解释"></a>关于图片风格的解释</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_12.png" alt=""></p><p>假设红色通道表示红框框起来的特征（那些竖条纹），黄色通道表示黄框框起来的特征（即那些橘红色的图块），那么，红色通道和黄色通道的相关性，可以告诉你在图片中这种竖条纹和橘色图块是否经常同时出现，这是“风格”的一种衡量方式。</p><h4 id="风格矩阵"><a href="#风格矩阵" class="headerlink" title="风格矩阵"></a>风格矩阵</h4><p>我们需要过给定的图像计算出风格矩阵，风格矩阵会记录上一张幻灯片中我们提到的所有相关性。</p><ul><li>$a^{[l]}_{i,j,k}$ 是第 $l$ 层在（i, j, k）这个位置的激活值，i 是高度，j 是宽度，k 是通道</li><li>$G^{[l]}$ 是形状为 $n^{[l]}_c×n^{[l]}_c$ 的风格矩阵，$n^{[l]}_c$ 是第 l 层的通道数</li></ul><script type="math/tex; mode=display">风格图片的风格矩阵 \ G^{[l](S)}_{kk'}=\sum\limits^{n^{[l]}_H}_{i=1}\sum\limits^{n^{[l]}_W}_{j=1}a^{[l](S)}_{i,j,k}a^{[l](S)}_{i,j,k'}</script><script type="math/tex; mode=display">生成图片的风格矩阵 \ G^{[l](G)}_{kk'}=\sum\limits^{n^{[l]}_H}_{i=1}\sum\limits^{n^{[l]}_W}_{j=1}a^{[l](G)}_{i,j,k}a^{[l](G)}_{i,j,k'}</script><ul><li>kk’ 表示第 k 个通道和第 k‘ 个通道的相关性，在风格矩阵的第 k 行第 k’ 列</li><li>线性代数中 G 矩阵被称为格拉姆矩阵 gram matrix</li></ul><h4 id="风格代价函数-1"><a href="#风格代价函数-1" class="headerlink" title="风格代价函数"></a>风格代价函数</h4><script type="math/tex; mode=display">J _ { \text { style } } ^ { [ l ] } ( S , G ) =\frac { 1 } { \left( 2 n _ { H } ^ { [ l ] } n _ { W } ^ { [ l ] } n _ { C } ^ { [ l ] } \right) ^ { 2 } }||G^{[l](S)}-G^{[l](G)}||^2_F \\ = \frac { 1 } { \left( 2 n _ { H } ^ { [ l ] } n _ { W } ^ { [ l ] } n _ { C } ^ { [ l ] } \right) ^ { 2 } } \sum _ { k } \sum _ { k ^ { \prime } } \left( G _ { k k ^ { \prime } } ^ { [ l ] ( S ) } - G _ { k k ^ { \prime } } ^ { [ l ] ( G ) } \right)^2</script><ul><li>$\frac { 1 } { \left( 2 n _ { H } ^ { [ l ] } n _ { W } ^ { [ l ] } n _ { C } ^ { [ l ] } \right) ^ { 2 } }$ 是不重要的标准化系数</li></ul><p>如果我们需要将不同层的风格代价函数都考虑进去，那么式子变成：</p><script type="math/tex; mode=display">J _ { \text { style } } ( S , G ) = \sum\limits _l \lambda^{[l]}J _ { \text { style } } ^ { [ l ] } ( S , G )</script><ul><li>$\lambda$ 为一个可调整的超参数</li></ul><h2 id="1D-和-3D-卷积"><a href="#1D-和-3D-卷积" class="headerlink" title="1D 和 3D 卷积"></a>1D 和 3D 卷积</h2><h3 id="2D-到-1D-的推广"><a href="#2D-到-1D-的推广" class="headerlink" title="2D 到 1D 的推广"></a>2D 到 1D 的推广</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_13.png" alt=""></p><h3 id="2D-到-3D-的推广"><a href="#2D-到-3D-的推广" class="headerlink" title="2D 到 3D 的推广"></a>2D 到 3D 的推广</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.4_14.png" alt=""></p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
            <tag> 人脸识别 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 3）</title>
      <link href="/2018/10/25/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w3%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/10/25/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w3%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<p>这次编程作业介绍了一个极其牛逼的算法——yolo 算法，用来检测马路上的车辆和路标等，这次作业主要介绍了 yolo 算法的数据后处理部分，即从输出挑选出正确预测方框的过程。</p><a id="more"></a><p>先把需要的包引入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K  <span class="comment"># K 相当于 keras.backend.K</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Lambda, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model, Model</span><br><span class="line"><span class="keyword">from</span> yolo_utils <span class="keyword">import</span> read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes</span><br><span class="line"><span class="keyword">from</span> yad2k.models.keras_yolo <span class="keyword">import</span> yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>我们在车顶放置摄像头拍摄图片以实现自动驾驶系统，搜集所有的图片进文件夹然后通过在车的周围画框获得标签值。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3_1.png" alt=""></p><p>由于 YOLO 算法需要非常大的算力去计算，所以采用预训练好的模型来使用。</p><p>这个模型是用微软的 coco 图像数据集进行预训练的，该数据集一共有八十种物体类别，所以类别向量有 80 个数。</p><h2 id="YOLO-算法"><a href="#YOLO-算法" class="headerlink" title="YOLO 算法"></a>YOLO 算法</h2><p>详细的算法介绍见上一篇博客。</p><h3 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h3><ul><li>输入：（m, 608, 608, 3）的图像</li><li>输出：（m, 19, 19, 5, 85）的标签<ul><li>19×19 的网格</li><li>5 个锚框</li><li>一个锚框包含：1 个 Pc 值 + 4 个坐标 + 80 个类别</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3_2.png" alt=""></p><p>对于每个格子的每个锚框，我们计算这个锚框最可能包含某一类物体的“分数”，分数越高，则这个锚框包含某一类物体的概率也最大。“分数” 的计算方法如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3_3.png" alt=""></p><p>每个网格有五个锚框，一个锚框可以预测出一个方框，总共模型可以预测出 19×19×5 = 1805 个方框出来，从里面挑一些具有高概率的方框如下图所示，可是方框还是很多，需要进一步地筛选。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3_4.png" alt=""></p><h3 id="用“分数”门槛进行筛选"><a href="#用“分数”门槛进行筛选" class="headerlink" title="用“分数”门槛进行筛选"></a>用“分数”门槛进行筛选</h3><p>首先将“分数”不满足某个阈值的方框都去掉，也就是说这些方框预测出里面有某一类物体的概率太小了。</p><p>下面实现筛选函数。</p><p>最后的标签向量是一个形状为（19, 19, 5, 85）的向量，将其分为三个向量：</p><ul><li><code>box_confidence</code>:包含每个网格的每个锚框的 $p_c$ 值的向量，形状为（19, 19, 5, 1）</li><li><code>boxes</code>: 包含每个网格每个锚框的四个坐标的向量，形状为（19, 19, 5, 4）</li><li><code>box_class_probs</code>: 包含每个网格每个锚框的 80 个类的概率值，形状为（19, 19, 5, 80）</li></ul><p>用到的函数介绍：</p><ul><li><code>keras.backend.argmax(x, axis=-1)</code>：得到 x 的最大值的索引值，axis = -1 表示找最小的维度里的最大值，-1 是反方向第一个的意思</li><li><code>keras.backend.max(x, axis=None, keepdims=False)</code>: 返回 x 某个维度的最大值</li><li><code>tf.boolean_mask(tensor, mask, name=&#39;boolean_mask&#39;, axis=None)</code>: 用只有 ture 和 false 两种值的向量 mask 筛选 tensor。mask 的维度 K 必须小于 tensor 的维度 N，最后的返回值的维度是 N-K+1。 假如 tensor 形状为（a, b, c, d），那么 mask 的形状可以是 （a, b, c, d），此时返回的是 4-4+1=1 维向量，也可以是（a, b, c），此时返回的是后面 2 维向量，依次类推。</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 筛选掉分数较低的方框</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_filter_boxes</span><span class="params">(box_confidence, boxes, box_class_probs, threshold = <span class="number">.6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Filters YOLO boxes by thresholding on object and class confidence.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    box_confidence -- tensor of shape (19, 19, 5, 1)</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (19, 19, 5, 4)</span></span><br><span class="line"><span class="string">    box_class_probs -- tensor of shape (19, 19, 5, 80)</span></span><br><span class="line"><span class="string">    threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    scores -- tensor of shape (None,), containing the class probability score for selected boxes</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (None, 4), containing (b_x, b_y, b_h, b_w) coordinates of selected boxes</span></span><br><span class="line"><span class="string">    classes -- tensor of shape (None,), containing the index of the class detected by the selected boxes</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: "None" is here because you don't know the exact number of selected boxes, as it depends on the threshold. </span></span><br><span class="line"><span class="string">    For example, the actual output size of scores would be (10,) if there are 10 boxes.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Compute box scores</span></span><br><span class="line">    box_scores = box_confidence * box_class_probs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Find the box_classes thanks to the max box_scores, keep track of the corresponding score</span></span><br><span class="line">    box_classes = K.argmax(box_scores, axis = <span class="number">-1</span>) <span class="comment"># axis = -1 指的是从反向取值，在这里相当于 axis = 3</span></span><br><span class="line">    box_class_scores = K.max(box_scores, axis = <span class="number">-1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: Create a filtering mask based on "box_class_scores" by using "threshold". The mask should have the</span></span><br><span class="line">    <span class="comment"># same dimension as box_class_scores, and be True for the boxes you want to keep (with probability &gt;= threshold)</span></span><br><span class="line">    filtering_mask = box_class_scores &gt;= threshold</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Step 4: Apply the mask to scores, boxes and classes</span></span><br><span class="line">    scores = tf.boolean_mask(box_class_scores, filtering_mask) <span class="comment"># tf.boolean_mask：如果第一个参数维度为 N，那么第二个mask 参数维度K必须小于等于N，返回的值维度为 N-K+1</span></span><br><span class="line">    boxes = tf.boolean_mask(boxes, filtering_mask)             <span class="comment">#  所以这里用 19*19*5 的 mask 筛选 19*19*5 的向量，最后返回的维度是 1，即（？，），？是因为不知道多少box被选出来</span></span><br><span class="line">    classes = tf.boolean_mask(box_classes, filtering_mask)     <span class="comment">#  所有选出来的 box 被展开在一个一维向量里</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> scores, boxes, classes</span><br></pre></td></tr></table></figure><p>Keras 是基于 tensorflow 的，所以我们用如下方法进行测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test_a:</span><br><span class="line">    box_confidence = tf.random_normal([<span class="number">19</span>, <span class="number">19</span>, <span class="number">5</span>, <span class="number">1</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>, seed = <span class="number">1</span>)</span><br><span class="line">    boxes = tf.random_normal([<span class="number">19</span>, <span class="number">19</span>, <span class="number">5</span>, <span class="number">4</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>, seed = <span class="number">1</span>)</span><br><span class="line">    box_class_probs = tf.random_normal([<span class="number">19</span>, <span class="number">19</span>, <span class="number">5</span>, <span class="number">80</span>], mean=<span class="number">1</span>, stddev=<span class="number">4</span>, seed = <span class="number">1</span>)</span><br><span class="line">    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = <span class="number">0.5</span>)</span><br><span class="line">    print(<span class="string">"scores[2] = "</span> + str(scores[<span class="number">2</span>].eval()))</span><br><span class="line">    print(<span class="string">"boxes[2] = "</span> + str(boxes[<span class="number">2</span>].eval()))</span><br><span class="line">    print(<span class="string">"classes[2] = "</span> + str(classes[<span class="number">2</span>].eval()))</span><br><span class="line">    print(<span class="string">"scores.shape = "</span> + str(scores.shape))</span><br><span class="line">    print(<span class="string">"boxes.shape = "</span> + str(boxes.shape))</span><br><span class="line">    print(<span class="string">"classes.shape = "</span> + str(classes.shape))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3_5.png" alt=""></p><p>可以看到，去掉分数较低的方框后，剩余的方框都被统一展开成一个 1 维向量。其中 ？ 表示，无法确定多少个方框会被筛选掉，所以用 ？ 代替。</p><h3 id="非最大值抑制"><a href="#非最大值抑制" class="headerlink" title="非最大值抑制"></a>非最大值抑制</h3><p>就算筛掉很多低概率方框，最后还是会剩下很多重复检测的方框，即一个车被好几个不同的框认出来，但是其中只有一个是最符合的，使用非最大值抑制将这些多余的去掉，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3_6.png" alt=""></p><h4 id="交并比-IoU-如何实现"><a href="#交并比-IoU-如何实现" class="headerlink" title="交并比 IoU 如何实现"></a>交并比 IoU 如何实现</h4><p>非最大值抑制中要用到一个很重要的函数，交并比，确定两个方框的重合程度，重合程度越高即交并比越高，两个方框越可能是在检测同一个物体。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3_7.png" alt=""></p><p>提示：</p><ul><li>假设输入的是两方框两个角的坐标 (x1, y1, x2, y2)，而不是中心点和宽高</li><li>矩形的面积这样计算： (y2 - y1) × (x2 - x1)</li><li>为了计算交集的面积，必须获得交集的坐标 (xi1, yi1, xi2, yi2)，如何确定：<ul><li>xi1 = maximum of the x1 coordinates of the two boxes</li><li>yi1 = maximum of the y1 coordinates of the two boxes</li><li>xi2 = minimum of the x2 coordinates of the two boxes</li><li>yi2 = minimum of the y2 coordinates of the two boxes</li><li>要确保交集区域是正的，否则它就是 0，使用 <code>max(height, 0)</code> and  <code>max(width, 0)</code></li></ul></li><li>并集面积如何确定：两个方框面积相加再减去交集部分</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iou</span><span class="params">(box1, box2)</span>:</span></span><br><span class="line">    <span class="string">"""Implement the intersection over union (IoU) between box1 and box2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    box1 -- first box, list object with coordinates (x1, y1, x2, y2)</span></span><br><span class="line"><span class="string">    box2 -- second box, list object with coordinates (x1, y1, x2, y2)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area.</span></span><br><span class="line">    xi1 = max(box1[<span class="number">0</span>], box2[<span class="number">0</span>])</span><br><span class="line">    yi1 = max(box1[<span class="number">1</span>], box2[<span class="number">1</span>])</span><br><span class="line">    xi2 = min(box1[<span class="number">2</span>], box2[<span class="number">2</span>])</span><br><span class="line">    yi2 = min(box1[<span class="number">3</span>], box2[<span class="number">3</span>])</span><br><span class="line">    inter_area = (xi2 - xi1) * (yi2 - yi1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)</span></span><br><span class="line">    box1_area = (box1[<span class="number">2</span>] - box1[<span class="number">0</span>]) * (box1[<span class="number">3</span>] - box1[<span class="number">1</span>])</span><br><span class="line">    box2_area = (box2[<span class="number">2</span>] - box2[<span class="number">0</span>]) * (box2[<span class="number">3</span>] - box2[<span class="number">1</span>])</span><br><span class="line">    union_area = box1_area + box2_area - inter_area</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute the IoU</span></span><br><span class="line">    iou = inter_area / union_area</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure><h4 id="非最大值抑制如何实现"><a href="#非最大值抑制如何实现" class="headerlink" title="非最大值抑制如何实现"></a>非最大值抑制如何实现</h4><p>我们在第一步筛选出了一系列的方框，我们在这些方框中：</p><ul><li>选择出分数最高的</li><li>计算它和其他所有的方框的交并比，去掉其中和它交并比大于某个阈值 iou_threshold 的，因为这种方框我们认为重合率太高，是在检测同一个物体</li><li>将刚刚选出的分数最高的排出在外，在剩下的方框中重复上述操作，直到最后没有分数比现在选择的方框更低的了。</li></ul><p>需要用到的函数：</p><ul><li><code>tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold=0.5, score_threshold=float(&#39;-inf&#39;), name=None)</code>: boxes 是形状为 (方框总数量, 4) 的二维数组，scores 是对应的方框的“分数”的一维向量，用它们进行非最大值抑制，输出最后得到的方框的索引值，是一个一维向量。</li><li><code>tf.keras.backend.gather(reference, indices)</code>: 用索引值 indices 把向量 reference 中对应索引值的向量取出来</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 非最大值抑制</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_non_max_suppression</span><span class="params">(scores, boxes, classes, max_boxes = <span class="number">10</span>, iou_threshold = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Applies Non-max suppression (NMS) to set of boxes</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    scores -- tensor of shape (None,), output of yolo_filter_boxes()</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (None, 4), output of yolo_filter_boxes() that have been scaled to the image size (see later)</span></span><br><span class="line"><span class="string">    classes -- tensor of shape (None,), output of yolo_filter_boxes()</span></span><br><span class="line"><span class="string">    max_boxes -- integer, maximum number of predicted boxes you'd like</span></span><br><span class="line"><span class="string">    iou_threshold -- real value, "intersection over union" threshold used for NMS filtering</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    scores -- tensor of shape (, None), predicted score for each box</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (4, None), predicted box coordinates</span></span><br><span class="line"><span class="string">    classes -- tensor of shape (, None), predicted class for each box</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: The "None" dimension of the output tensors has obviously to be less than max_boxes. Note also that this</span></span><br><span class="line"><span class="string">    function will transpose the shapes of scores, boxes, classes. This is made for convenience.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    max_boxes_tensor = K.variable(max_boxes, dtype=<span class="string">'int32'</span>)     <span class="comment"># tensor to be used in tf.image.non_max_suppression()</span></span><br><span class="line">    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) <span class="comment"># initialize variable max_boxes_tensor</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep</span></span><br><span class="line">    nms_indices = tf.image.non_max_suppression(boxes, scores, max_boxes, iou_threshold) <span class="comment"># 返回一个一维的索引值向量，形状为 (?,) ，表示那些最后被留下来的方框的索引值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use K.gather() to select only nms_indices from scores, boxes and classes</span></span><br><span class="line">    scores = K.gather(scores, nms_indices)  <span class="comment"># 根据索引值取出对应的方框的值</span></span><br><span class="line">    boxes = K.gather(boxes, nms_indices)</span><br><span class="line">    classes = K.gather(classes, nms_indices)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> scores, boxes, classes</span><br></pre></td></tr></table></figure><h3 id="yolo-算法过滤函数"><a href="#yolo-算法过滤函数" class="headerlink" title="yolo 算法过滤函数"></a>yolo 算法过滤函数</h3><p>下面我们将所有的函数合并起来，获得最终的需要的结果。</p><p>两个实现的细节：</p><ul><li><p>由于 <code>tf.image.non_max_suppression</code> 中的参数 boxes 的坐标是 (x1, y1, x2, y2)，而我们输出的坐标是 (x, y, w, h)，所以需要对坐标进行转换，使用 <code>yolo_boxes_to_corners(box_xy, box_wh)</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_boxes_to_corners</span><span class="params">(box_xy, box_wh)</span>:</span></span><br><span class="line">    <span class="string">"""Convert YOLO box predictions to bounding box corners."""</span></span><br><span class="line">    box_mins = box_xy - (box_wh / <span class="number">2.</span>)</span><br><span class="line">    box_maxes = box_xy + (box_wh / <span class="number">2.</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> K.concatenate([</span><br><span class="line">        box_mins[..., <span class="number">1</span>:<span class="number">2</span>],  <span class="comment"># y_min</span></span><br><span class="line">        box_mins[..., <span class="number">0</span>:<span class="number">1</span>],  <span class="comment"># x_min</span></span><br><span class="line">        box_maxes[..., <span class="number">1</span>:<span class="number">2</span>],  <span class="comment"># y_max</span></span><br><span class="line">        box_maxes[..., <span class="number">0</span>:<span class="number">1</span>]  <span class="comment"># x_max</span></span><br><span class="line">    ])</span><br></pre></td></tr></table></figure></li><li><p>由于 yolo 算法是在 608×608 的图像上训练的，如果我们在 720×1280 的图片上进行测试，那么方框就不匹配这个图片，我们需要一个 <code>scale_boxes(boxes, image_shape)</code>来让方框可以适配新的图片形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scale_boxes</span><span class="params">(boxes, image_shape)</span>:</span></span><br><span class="line">    <span class="string">""" Scales the predicted boxes in order to be drawable on the image"""</span></span><br><span class="line">    height = image_shape[<span class="number">0</span>]</span><br><span class="line">    width = image_shape[<span class="number">1</span>]</span><br><span class="line">    image_dims = K.stack([height, width, height, width])</span><br><span class="line">    image_dims = K.reshape(image_dims, [<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">    boxes = boxes * image_dims</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br></pre></td></tr></table></figure></li></ul><p>过滤函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_eval</span><span class="params">(yolo_outputs, image_shape = <span class="params">(<span class="number">720.</span>, <span class="number">1280.</span>)</span>, max_boxes=<span class="number">10</span>, score_threshold=<span class="number">.6</span>, iou_threshold=<span class="number">.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts the output of YOLO encoding (a lot of boxes) to your predicted boxes along with their scores, box coordinates and classes.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yolo_outputs -- output of the encoding model (for image_shape of (608, 608, 3)), contains 4 tensors:</span></span><br><span class="line"><span class="string">                    box_confidence: tensor of shape (None, 19, 19, 5, 1)</span></span><br><span class="line"><span class="string">                    box_xy: tensor of shape (None, 19, 19, 5, 2)</span></span><br><span class="line"><span class="string">                    box_wh: tensor of shape (None, 19, 19, 5, 2)</span></span><br><span class="line"><span class="string">                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)</span></span><br><span class="line"><span class="string">    image_shape -- tensor of shape (2,) containing the input shape, in this notebook we use (608., 608.) (has to be float32 dtype)</span></span><br><span class="line"><span class="string">    max_boxes -- integer, maximum number of predicted boxes you'd like</span></span><br><span class="line"><span class="string">    score_threshold -- real value, if [ highest class probability score &lt; threshold], then get rid of the corresponding box</span></span><br><span class="line"><span class="string">    iou_threshold -- real value, "intersection over union" threshold used for NMS filtering</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    scores -- tensor of shape (None, ), predicted score for each box</span></span><br><span class="line"><span class="string">    boxes -- tensor of shape (None, 4), predicted box coordinates</span></span><br><span class="line"><span class="string">    classes -- tensor of shape (None,), predicted class for each box</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve outputs of the YOLO model</span></span><br><span class="line">    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert boxes to be ready for filtering functions </span></span><br><span class="line">    boxes = yolo_boxes_to_corners(box_xy, box_wh)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use one of the functions you've implemented to perform Score-filtering with a threshold of score_threshold (≈1 line)</span></span><br><span class="line">    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Scale boxes back to original image shape.</span></span><br><span class="line">    boxes = scale_boxes(boxes, image_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use one of the functions you've implemented to perform Non-max suppression with a threshold of iou_threshold (≈1 line)</span></span><br><span class="line">    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> scores, boxes, classes</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>Input image (608, 608, 3)</li><li>The input image goes through a CNN, resulting in a (19,19,5,85) dimensional output. </li><li>After flattening the last two dimensions, the output is a volume of shape (19, 19, 425):<ul><li>Each cell in a 19x19 grid over the input image gives 425 numbers. </li><li>425 = 5 x 85 because each cell contains predictions for 5 boxes, corresponding to 5 anchor boxes, as seen in lecture. </li><li>85 = 5 + 80 where 5 is because $(p_c, b_x, b_y, b_h, b_w)$ has 5 numbers, and and 80 is the number of classes we’d like to detect</li></ul></li><li>You then select only few boxes based on:<ul><li>Score-thresholding: throw away boxes that have detected a class with a score less than the threshold</li><li>Non-max suppression: Compute the Intersection over Union and avoid selecting overlapping boxes</li></ul></li><li>This gives you YOLO’s final output. </li></ul><h2 id="测试-yolo-算法"><a href="#测试-yolo-算法" class="headerlink" title="测试 yolo 算法"></a>测试 yolo 算法</h2><p>首先开始一个会话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess = K.get_session()</span><br></pre></td></tr></table></figure><p>定义类别，锚框和图片尺寸：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class_names = read_classes(<span class="string">"model_data/coco_classes.txt"</span>)</span><br><span class="line">anchors = read_anchors(<span class="string">"model_data/yolo_anchors.txt"</span>)</span><br><span class="line">image_shape = (<span class="number">720.</span>, <span class="number">1280.</span>)</span><br></pre></td></tr></table></figure><p>加载预训练模型：</p><p>由于训练 yolo 算法要花费很多时间，我们加载一个训练好权重的预训练模型，被放在 “yolo.h5” 当中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo_model = load_model(<span class="string">"model_data/yolo.h5"</span>)</span><br></pre></td></tr></table></figure><p>查看模型概况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo_model.summary()</span><br></pre></td></tr></table></figure><p>将输出变成我们能用的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">yolo_head</span><span class="params">(feats, anchors, num_classes)</span>:</span></span><br><span class="line">    <span class="string">"""Convert final layer features to bounding box parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    feats : tensor</span></span><br><span class="line"><span class="string">        Final convolutional layer features.</span></span><br><span class="line"><span class="string">    anchors : array-like</span></span><br><span class="line"><span class="string">        Anchor box widths and heights.</span></span><br><span class="line"><span class="string">    num_classes : int</span></span><br><span class="line"><span class="string">        Number of target classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    box_xy : tensor</span></span><br><span class="line"><span class="string">        x, y box predictions adjusted by spatial location in conv layer.</span></span><br><span class="line"><span class="string">    box_wh : tensor</span></span><br><span class="line"><span class="string">        w, h box predictions adjusted by anchors and conv spatial resolution.</span></span><br><span class="line"><span class="string">    box_conf : tensor</span></span><br><span class="line"><span class="string">        Probability estimate for whether each box contains any object.</span></span><br><span class="line"><span class="string">    box_class_pred : tensor</span></span><br><span class="line"><span class="string">        Probability distribution estimate for each box over class labels.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    num_anchors = len(anchors)</span><br><span class="line">    <span class="comment"># Reshape to batch, height, width, num_anchors, box_params.</span></span><br><span class="line">    anchors_tensor = K.reshape(K.variable(anchors), [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, num_anchors, <span class="number">2</span>])</span><br><span class="line">    <span class="comment"># Static implementation for fixed models.</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Remove or add option for static implementation.</span></span><br><span class="line">    <span class="comment"># _, conv_height, conv_width, _ = K.int_shape(feats)</span></span><br><span class="line">    <span class="comment"># conv_dims = K.variable([conv_width, conv_height])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Dynamic implementation of conv dims for fully convolutional model.</span></span><br><span class="line">    conv_dims = K.shape(feats)[<span class="number">1</span>:<span class="number">3</span>]  <span class="comment"># assuming channels last</span></span><br><span class="line">    <span class="comment"># In YOLO the height index is the inner most iteration.</span></span><br><span class="line">    conv_height_index = K.arange(<span class="number">0</span>, stop=conv_dims[<span class="number">0</span>])</span><br><span class="line">    conv_width_index = K.arange(<span class="number">0</span>, stop=conv_dims[<span class="number">1</span>])</span><br><span class="line">    conv_height_index = K.tile(conv_height_index, [conv_dims[<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Repeat_elements and tf.split doesn't support dynamic splits.</span></span><br><span class="line">    <span class="comment"># conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)</span></span><br><span class="line">    conv_width_index = K.tile(K.expand_dims(conv_width_index, <span class="number">0</span>), [conv_dims[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">    conv_width_index = K.flatten(K.transpose(conv_width_index))</span><br><span class="line">    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))</span><br><span class="line">    conv_index = K.reshape(conv_index, [<span class="number">1</span>, conv_dims[<span class="number">0</span>], conv_dims[<span class="number">1</span>], <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">    conv_index = K.cast(conv_index, K.dtype(feats))</span><br><span class="line">    </span><br><span class="line">    feats = K.reshape(feats, [<span class="number">-1</span>, conv_dims[<span class="number">0</span>], conv_dims[<span class="number">1</span>], num_anchors, num_classes + <span class="number">5</span>])</span><br><span class="line">    conv_dims = K.cast(K.reshape(conv_dims, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]), K.dtype(feats))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Static generation of conv_index:</span></span><br><span class="line">    <span class="comment"># conv_index = np.array([_ for _ in np.ndindex(conv_width, conv_height)])</span></span><br><span class="line">    <span class="comment"># conv_index = conv_index[:, [1, 0]]  # swap columns for YOLO ordering.</span></span><br><span class="line">    <span class="comment"># conv_index = K.variable(</span></span><br><span class="line">    <span class="comment">#     conv_index.reshape(1, conv_height, conv_width, 1, 2))</span></span><br><span class="line">    <span class="comment"># feats = Reshape(</span></span><br><span class="line">    <span class="comment">#     (conv_dims[0], conv_dims[1], num_anchors, num_classes + 5))(feats)</span></span><br><span class="line"></span><br><span class="line">    box_confidence = K.sigmoid(feats[..., <span class="number">4</span>:<span class="number">5</span>])</span><br><span class="line">    box_xy = K.sigmoid(feats[..., :<span class="number">2</span>])</span><br><span class="line">    box_wh = K.exp(feats[..., <span class="number">2</span>:<span class="number">4</span>])</span><br><span class="line">    box_class_probs = K.softmax(feats[..., <span class="number">5</span>:])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Adjust preditions to each spatial grid point and anchor size.</span></span><br><span class="line">    <span class="comment"># Note: YOLO iterates over height index before width index.</span></span><br><span class="line">    box_xy = (box_xy + conv_index) / conv_dims</span><br><span class="line">    box_wh = box_wh * anchors_tensor / conv_dims</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> box_confidence, box_xy, box_wh, box_class_probs</span><br></pre></td></tr></table></figure><p>过滤方框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores, boxes, classes = yolo_eval(yolo_outputs, image_shape)</span><br></pre></td></tr></table></figure><h3 id="在一张图片上运行计算图"><a href="#在一张图片上运行计算图" class="headerlink" title="在一张图片上运行计算图"></a>在一张图片上运行计算图</h3><p>计算图为：</p><p>yolo_model.input —&gt; yolo_model —&gt; yolo_modeloutput —&gt; yolo_head —&gt; yolo_outputs —&gt; yolo_eval —&gt; scores, boxes, classes</p><p>输入图片需要进行预处理以符合输入格式（608, 608）:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image, image_data = preprocess_image(<span class="string">"images/"</span> + image_file, model_image_size = (<span class="number">608</span>, <span class="number">608</span>))</span><br></pre></td></tr></table></figure><p>image 是用来画框的图片，image_data 是转化为数组的图片。</p><p>我们将上述计算图放入 predicet():</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(sess, image_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Runs the graph stored in "sess" to predict boxes for "image_file". Prints and plots the preditions.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    sess -- your tensorflow/Keras session containing the YOLO graph</span></span><br><span class="line"><span class="string">    image_file -- name of an image stored in the "images" folder.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    out_scores -- tensor of shape (None, ), scores of the predicted boxes</span></span><br><span class="line"><span class="string">    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes</span></span><br><span class="line"><span class="string">    out_classes -- tensor of shape (None, ), class index of the predicted boxes</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: "None" actually represents the number of predicted boxes, it varies between 0 and max_boxes. </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Preprocess your image</span></span><br><span class="line">    image, image_data = preprocess_image(<span class="string">"images/"</span> + image_file, model_image_size = (<span class="number">608</span>, <span class="number">608</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run the session with the correct tensors and choose the correct placeholders in the feed_dict.</span></span><br><span class="line">    <span class="comment"># You'll need to use feed_dict=&#123;yolo_model.input: ... , K.learning_phase(): 0&#125;)</span></span><br><span class="line">    out_scores, out_boxes, out_classes = sess.run([scores, boxes, classes], feed_dict=&#123;yolo_model.input:image_data, K.learning_phase(): <span class="number">0</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print predictions info</span></span><br><span class="line">    print(<span class="string">'Found &#123;&#125; boxes for &#123;&#125;'</span>.format(len(out_boxes), image_file))</span><br><span class="line">    <span class="comment"># Generate colors for drawing bounding boxes.</span></span><br><span class="line">    colors = generate_colors(class_names)</span><br><span class="line">    <span class="comment"># Draw bounding boxes on the image file</span></span><br><span class="line">    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)</span><br><span class="line">    <span class="comment"># Save the predicted bounding box on the image</span></span><br><span class="line">    image.save(os.path.join(<span class="string">"out"</span>, image_file), quality=<span class="number">90</span>)</span><br><span class="line">    <span class="comment"># Display the results in the notebook</span></span><br><span class="line">    output_image = scipy.misc.imread(os.path.join(<span class="string">"out"</span>, image_file))</span><br><span class="line">    imshow(output_image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> out_scores, out_boxes, out_classes</span><br></pre></td></tr></table></figure><p>用自己的图片进行预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out_scores, out_boxes, out_classes = predict(sess, <span class="string">"1.jpg"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3_8.png" alt=""></p><p>结论：</p><ul><li>YOLO is a state-of-the-art object detection model that is fast and accurate</li><li>It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume. </li><li>The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.</li><li>You filter through all the boxes using non-max suppression. Specifically: <ul><li>Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes</li><li>Intersection over Union (IoU) thresholding to eliminate overlapping boxes</li></ul></li><li>Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise. </li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
            <tag> YOLO 算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 4 week 3）—— 物体检测</title>
      <link href="/2018/10/23/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w3/"/>
      <url>/2018/10/23/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w3/</url>
      <content type="html"><![CDATA[<p>本周主要学习如何通过 CNN 来进行物体检测。</p><h2 id="物体定位（单个物体）"><a href="#物体定位（单个物体）" class="headerlink" title="物体定位（单个物体）"></a>物体定位（单个物体）</h2><h3 id="什么是物体定位"><a href="#什么是物体定位" class="headerlink" title="什么是物体定位"></a>什么是物体定位</h3><ul><li>图像分类：图片中是否有车</li><li>物体定位：不仅要识别出是否有车（单个对象），还要在车的周围生成一个边框，即分类+定位</li><li>物体检测：找到一张图片里的<strong>多个</strong>对象并定位它们</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_1.png" alt=""></p><a id="more"></a><h3 id="如何定位"><a href="#如何定位" class="headerlink" title="如何定位"></a>如何定位</h3><p>假设我们需要找出图片中是否有以下四类对象：</p><ul><li>行人</li><li>车</li><li>摩托车</li><li>无物体</li></ul><p>我们最后用 softmax 输出四个分类，不仅如此，为了找出它们的位置，还需要输出以下四个值：</p><ul><li>$b_x$ ：方框中心的横坐标</li><li>$b_y$：方框中心的纵坐标</li><li>$b_h$：方框的高度</li><li>$b_w$：方框的宽度</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_2.png" alt=""></p><p>最后目标标签为：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_3.png" alt=""></p><ul><li>Pc 指的是图片中存在该单个对象的概率，若有则值为 1，若没有则值为 0</li><li>C1、C2、C3 当物体为某个类时，对应的 C 值取 1，其他取 0</li><li>当 Pc 为 0 时，也就是图片中不存在物体时，剩下的 ？ 值表示我们并不关心这些值是多少</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>假设使用平方误差，那么：</p><script type="math/tex; mode=display">L(\hat y ,y)=\left\{\begin{matrix}(\hat y_1-y_1)^2+(\hat y_2-y_2)^2+...+(\hat y_8-y_8)^2 & if \ \ y_1 =1\\  (\hat y_1-y_1)^2& if \ \ y _1=0\end{matrix}\right.</script><h2 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h2><p>当我们需要找出图片中的一些特征点时，我们可以在标签中增加这些点的坐标。</p><p>例如我们需要识别出下面这张脸的五官的轮廓，我们可以定义一些特征点，产生含有这些特征点坐标的训练集，然后我们将图片丢进神经网络，最后输出若干个值。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_4.png" alt=""></p><h2 id="物体检测（多个物体）"><a href="#物体检测（多个物体）" class="headerlink" title="物体检测（多个物体）"></a>物体检测（多个物体）</h2><h3 id="滑动窗口检测法-Sliding-Windows"><a href="#滑动窗口检测法-Sliding-Windows" class="headerlink" title="滑动窗口检测法 Sliding Windows"></a>滑动窗口检测法 Sliding Windows</h3><p>拍一张照片然后裁剪掉其他不是汽车的部分，得到一张汽车剧中并几乎占全部画面的紧密裁剪的汽车图像，使用许多紧密裁剪的汽车图像作为训练集，训练一个神经网络判断是车。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_5.png" alt=""></p><p>在一张可能包含汽车的大图像中，先选择一个窗口尺寸，然后只读取这个小窗口内的图像，输入到上面训练的卷积网络中，做一个预测，滑动窗口检测指的是将小窗口按步长移动到下一个位置，再重复上述操作，再移动窗口，重复上述操作，直到滑动窗口遍历了图像中的所有位置。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_6.png" alt=""></p><p>然后我们可以取不同大小的窗口重复上述操作，只要图像某处中有一辆车，那么就会有某个窗口会圈住一辆车然后检测出来。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_7.png" alt=""></p><p>缺点：</p><ul><li>当你使用较大的窗口移动步长，算法表现会下降</li><li>当你使用较小的步长，计算成本则会非常高</li></ul><h3 id="滑动窗口检测的卷积实现"><a href="#滑动窗口检测的卷积实现" class="headerlink" title="滑动窗口检测的卷积实现"></a>滑动窗口检测的卷积实现</h3><p>如果每个小窗口的图像都通过这么一个卷积网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_8.png" alt=""></p><p>我们将全连接层改造成以下的结构，实现参数共享，减少计算量：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_9.png" alt=""></p><p>具体做法是：</p><p>我们将待检测的大图像直接输入到我们改造后的（本来用来预测窗口图像的）卷积网络中，最后的输出就是我们对应的每个滑动窗口的结果，而不是一个窗口一个窗口滑动，截取图像然后输入。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_10.png" alt=""></p><p>由于直接将整个图像输入到训练好的网络，在同一时间将所有的滑动窗口实现了前向传播，实现了参数的共享，减少了计算的成本。</p><p>缺点：由于窗口只能是正方形，所以边界框的位置不准确。</p><h2 id="边界框预测"><a href="#边界框预测" class="headerlink" title="边界框预测"></a>边界框预测</h2><h3 id="YOLO-算法介绍"><a href="#YOLO-算法介绍" class="headerlink" title="YOLO 算法介绍"></a>YOLO 算法介绍</h3><p>为了更好地检测出物体的真实边界，可以使用 YOLO 算法，是 you only look once 的缩写。</p><p>有一张图片，首先用 9×9 的网络划分（当然实际使用中可以使用更加精细的），然后将开始学到的<strong>物体定位</strong>算法运用在这每一个网格中。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_12.png" alt=""></p><p>那么根据每个网格是否有物体，如果有，那么边界框坐标是多少，是什么类别，得到每个网格的标签值 y 为：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_13.png" alt=""></p><blockquote><p>值得注意的是，每个物体就算跨越了多个网格，我们也只能将其划分到它<strong>中心点</strong>所在的网格里。</p></blockquote><p>所以最后将这些标签堆叠起来形成一个维度为 3×3×8 的标签（8 指的是每个网格的 8 个标签值）。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_14.png" alt=""></p><p>我们要做的是输入训练集进入一个卷积网络，我们通过调整网络超参数使得最后输出一个 3×3×8 的向量，这就与我们 3×3×8 的标签形成对应，构造代价函数。经过训练之后，我们随便输入一张图像，就可以知道在网格每个位置是否存在物体，对应的边界框坐标是什么。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_15.png" alt=""></p><h3 id="如何确定边界框的参数"><a href="#如何确定边界框的参数" class="headerlink" title="如何确定边界框的参数"></a>如何确定边界框的参数</h3><p>一种方法是令网格的左上角为（0，0），右下角为（1，1），那么中心点的坐标由它相对（0，0）的位置决定，一定介于 0～1 之间；而方框的长跟宽根据它和网格宽度的比例决定，可以超过 1.</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_16.png" alt=""></p><p>例如上面黄色中心点的坐标约为（0.4，0.3，0.9，0.5）。</p><h2 id="如何评价物体检测算法"><a href="#如何评价物体检测算法" class="headerlink" title="如何评价物体检测算法"></a>如何评价物体检测算法</h2><p>为了知道物体检测算法的性能好坏，可以使用一个叫做<strong>“交并比 (Intersection Over Union)”</strong>的函数.</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_18.png" alt=""></p><p>假设红框是实际的边界，紫框是算法预测的边界，黄色部分是两者的交集，绿色部分是两者的并集，那么：</p><p>交并比 IoU = 交集的面积 / 并集的面积。</p><p>判别：一般来说，如果 IoU 大于等于 <strong>0.5</strong>，便可认为定位正确。也可以定义更严格的值。</p><h2 id="非最大值抑制-non-max-suppression"><a href="#非最大值抑制-non-max-suppression" class="headerlink" title="非最大值抑制 non-max suppression"></a>非最大值抑制 non-max suppression</h2><p>如下图所示，一个 19×19 的网格将图片划分，运行 YOLO 算法，可能对于一辆汽车，有许多网格都认为该汽车的中心点在它那里，即这辆汽车的范围内的许多网格都认为自己检测到了汽车了，发生重复检测的问题。抑制非最大值可以确保算法对一个物体只检测一次，而不是检测许多次。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_19.png" alt=""></p><p>如果一辆车检测了许多次，就会在其周围产生许多方框，如下图所示，每个方框上的数字表示该网格认为自己检测到了物体的概率：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_20.png" alt=""></p><p>抑制非最大值的具体的做法为（<strong>假设只是检测单一物体</strong>）：</p><ul><li>每个网格的输出为：$\begin{bmatrix}<br>p_c\\<br>b_x\\<br>b_y\\<br>b_h\\<br>b_w\\<br>\end{bmatrix}$ </li><li>丢弃所有低概率网格，例如 $p_c \leqslant 0.6$ （或其他概率）的网格</li><li>在剩下的网格中<ul><li>挑选出具有最大 $p_c$ 值的网格</li><li>丢弃那些与刚刚选出的网格的方框的 IoU 值大于 0.5 的网格（如果 IoU 大于某个阈值，这说明这两个方框重合度太高，便认为它们识别的是同一个物体）</li><li>在剩下的网格中重复上述操作</li></ul></li></ul><p>如果检测是多种物体，比如汽车、行人、摩托车，那么正确的做法是：各自独立，进行三次“抑制非最大值”。</p><p>实际中的使用方法见下方的“非最大值抑制”。</p><h2 id="锚框法-anchor-boxes"><a href="#锚框法-anchor-boxes" class="headerlink" title="锚框法 anchor boxes"></a>锚框法 anchor boxes</h2><p>目前看过的物件检测算法，有一个问题是每一个网格只能侦测一个物件，如果一个网格想要侦测多个物体，可以使用“锚框法 anchor boxes”。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_21.png" alt=""></p><p>对于上面这张图，我们发现人和汽车的中心点几乎重叠在一起，在一个格子里，如果输出 $y = \left[ \begin{array} { c } { p _ { c } } \\ { b _ { x } } \\ { b _ { y } } \\ { b _ { w } } \\ { b _ { w } } \\ { c _ { 1 } } \\ { c _ { 2 } \\ c_3 } \end{array} \right]$，那么它无法同时输出两个物体的方框，只能二选一，我们可以在这个网格画出两个锚框（也可能更多）框住这两个物体，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_22.png" alt=""></p><p>然后将输出的标签改为：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_23.png" alt=""></p><p>用锚框 1 来框住行人，锚框 2 来框住汽车。</p><p>在使用锚框前：训练图片中的每个物体被分给一个包含了它中心点的格子。输出维度 3×3×8。</p><p>使用锚框后：训练图片中的每个物体被分给（1）包含了它中心点的格子（2）这个格子里据有更高 IoU 的锚框。输出维度为 3×3×2×8。</p><p>举个例子：</p><p> 标签值 $y=\left[ \begin{array} { l } { p _ { c } } \\ { b _ { x } } \\ { b _ { y } } \\ { b _ { h } } \\ { b _ { w } } \\ { c_ { 1 } } \\ { c _ { 2 } }\\ { c _ { 3 } } \\ { p _ { c } } \\ { b _ { x } } \\ { b _ { y } } \\ { b _ { h } } \\ { b _ { w } } \\ { c _ { 1 } } \\ { c _ { 2 } } \\ { c _ { 3 } }  \end{array} \right]$，假如某个网格有两个物体，则为 $ \left[ \begin{array} { l } { 1 } \\ { b _ { x } } \\ { b _ { y } } \\ { b _ { h } } \\ { b _ { w } } \\ { 1 } \\ { 0}\\ { 0 } \\ { 1 } \\ { b _ { x } } \\ { b _ { y } } \\ { b _ { h } } \\ { b _ { w } } \\ { 0 } \\ { 1 } \\ { 0 }  \end{array} \right] $，如果只有车，则为 $ \left[ \begin{array} { l } { 0 } \\ { ？ } \\ { ？} \\ { ？ } \\ {  ？ } \\ { ？} \\ { ？}\\ { ？ } \\ { 1 } \\ { b _ { x } } \\ { b _ { y } } \\ { b _ { h } } \\ { b _ { w } } \\ { 0 } \\ { 1 } \\ { 0 }  \end{array} \right] $  </p><p>实际上，在一个网格处理两个物体的情况很少发生，尤其是当网格为 19×19 时。</p><h2 id="YOLO-物体检测算法"><a href="#YOLO-物体检测算法" class="headerlink" title="YOLO 物体检测算法"></a>YOLO 物体检测算法</h2><p>综合上面所有要素，来组成目前最先进的目标检测算法——YOLO 算法。</p><h3 id="构建训练集并训练"><a href="#构建训练集并训练" class="headerlink" title="构建训练集并训练"></a>构建训练集并训练</h3><ul><li>三个分类类别：行人、汽车、摩托车</li><li>两个锚框</li><li>3×3 的网格划分（实际中一般是 19×19）</li><li>遍历每张图片的 9 个网格，得到标签向量，维度是 3×3×2×8</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_24.png" alt=""></p><h3 id="进行预测"><a href="#进行预测" class="headerlink" title="进行预测"></a>进行预测</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_25.png" alt=""></p><h3 id="非最大值抑制"><a href="#非最大值抑制" class="headerlink" title="非最大值抑制"></a>非最大值抑制</h3><ul><li><p>每个网格都会得到两个预测的边界框（因为有两个锚框）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_26.png" alt=""></p></li><li><p>计算每个方框的“分数”，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_29.png" alt=""></p></li><li><p>去掉所有分数低于某个阈值的方框</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_27.png" alt=""></p></li><li><p>在剩下的方框中，对于分类的每个类别（行人、汽车、摩托车等），单独对每个预测的类别执行非最大值抑制</p><ul><li>挑选出<strong>分数最大的</strong>方框</li><li>丢弃那些与刚刚分数最大方框的 IoU 值大于 0.5（或其他阈值） 的网格<ul><li>如果 IoU 大于某个阈值，这说明这两个方框重合度太高，便认为它们识别的是同一个物体</li></ul></li><li>在<strong>除了</strong>分数最大方框的剩下的方框中，找出分数最大的，重复上述操作</li><li>直到没有更低分数的方框存在，停止迭代，得到最终的预测框</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.3_28.png" alt=""></p></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 2）</title>
      <link href="/2018/10/19/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/10/19/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h2 id="Keras-教程"><a href="#Keras-教程" class="headerlink" title="Keras 教程"></a>Keras 教程</h2><ul><li>Keras 是一个高级神经网络框架，用 Python 写成，在 TensorFlow 和 CNTK 等一些更低级的框架上运行，拥有比 tensorflow 更高的抽象</li><li>Keras 能够快速搭建和试验不同的模型</li><li>Keras 比低级框架限制更多，无法实现一些非常复杂的模型，但是对一些普通模型运行很好</li></ul><p>这里用 Keras 实现一个识别图片中的人是否开心的算法。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_1.png" alt=""></p><a id="more"></a><p>先引入我们需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> kt_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p>加载数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">Y_train = Y_train_orig.T</span><br><span class="line">Y_test = Y_test_orig.T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_3.png" alt=""></p><p>在 Keras 中训练测试模型有四个步骤：</p><ul><li><p>建立模型</p><ul><li><p>自己定义一个 model() 函数进行前向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HappyModel</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the HappyModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- 输入图片的维度（不包括图片的数量！）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero-Padding: pads the border of X_input with zeroes</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CONV -&gt; BN -&gt; RELU Block applied to X</span></span><br><span class="line">    X = Conv2D(<span class="number">32</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">1</span>, <span class="number">1</span>), name = <span class="string">'conv0'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn0'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MAXPOOL</span></span><br><span class="line">    X = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'max_pool'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FLATTEN X (means convert it to a vector) + FULLYCONNECTED</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>, name=<span class="string">'fc'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model. This creates your Keras model instance, you'll use this instance to train/test the model.</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'HappyModel'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel = HappyModel(X_train.shape[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure></li></ul></li><li><p>编译模型</p><ul><li><p>model.compile(optimizer = “…”, loss = “…”, metrics = [“accuracy”])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.compile(optimizer = <span class="string">'Adam'</span>, loss = <span class="string">'binary_crossentropy'</span>, metrics = [<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></li></ul></li><li><p>在训练数据上训练模型</p><ul><li><p>model.fit(x = …, y = …, epochs = …, batch_size = …)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.fit(x = X_train, y = Y_train, epochs = <span class="number">40</span>, batch_size = <span class="number">16</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_4.png" alt=""></p></li></ul></li><li><p>在测试数据上测试模型</p><ul><li><p>model.evaluate(x = …, y = …)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">preds = happyModel.evaluate(x = X_test, y = Y_test)</span><br><span class="line">print(preds)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_5.png" alt=""></p></li></ul></li></ul><p>最后可以达到 98% 的训练集精确度，90% 的训练集精确度。</p><p>我们可以用自己的图片进行测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">'images/45.jpg'</span><span class="comment"># 用自己的图片路径</span></span><br><span class="line"></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">imshow(img)</span><br><span class="line"></span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line"></span><br><span class="line">print(happyModel.predict(x))</span><br><span class="line"><span class="keyword">if</span> happyModel.predict(x) == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">'这个人不开心哦 :('</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'这个人在笑哦 :)'</span>)</span><br></pre></td></tr></table></figure><p>我们还可以获得模型的概况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.summary()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_6.png" alt=""></p><p>还可以将模型流程图打印出来并保存为位图文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(happyModel, to_file=<span class="string">'HappyModel.png'</span>)</span><br><span class="line">SVG(model_to_dot(happyModel).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_7.png" alt=""></p><h2 id="用-Keras-创建-ResNet-残差网络"><a href="#用-Keras-创建-ResNet-残差网络" class="headerlink" title="用 Keras 创建 ResNet 残差网络"></a>用 Keras 创建 ResNet 残差网络</h2><p>这节会用残差网络创建一个非常深的卷积网络。</p><ul><li>实现基本的残差网络组块</li><li>将这些组块合在一起实现一个最先进的神经网络图像分类器</li></ul><h3 id="包"><a href="#包" class="headerlink" title="包"></a>包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> resnets_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line">K.set_learning_phase(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="非常深的神经网络的问题所在"><a href="#非常深的神经网络的问题所在" class="headerlink" title="非常深的神经网络的问题所在"></a>非常深的神经网络的问题所在</h3><ul><li>优点：很深的网络能学习到非常多层次的抽象特征，从边缘到复杂特征。</li><li>缺点：训练时会产生梯度下降，使得训练速度极慢。</li></ul><h3 id="残差网络的基本-block-组块"><a href="#残差网络的基本-block-组块" class="headerlink" title="残差网络的基本 block 组块"></a>残差网络的基本 block 组块</h3><p>有两种主要的组块在残差网络中，取决于输入/输出维度是否相同。</p><ol><li><p>The identity block （输入/输出维度相同）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_8.png" alt=""></p><p>First component of main path: </p><ul><li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is “same” and its name should be <code>conv_name_base + &#39;2b&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2c&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li></ul><p>Final step: </p><ul><li>The shortcut and the input are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The identity block 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X, f, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Second component of main path </span></span><br><span class="line">    X = Conv2D(filters = F2, kernel_size = (f,f), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'same'</span>, name = conv_name_base + <span class="string">'2b'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path </span></span><br><span class="line">    X = Conv2D(filters = F3, kernel_size = (<span class="number">1</span>,<span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2c'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2c'</span> )(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation </span></span><br><span class="line">    X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">"relu"</span>)(X)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><blockquote><p>最后的 X 加上 X_shoutcut 应该使用 Keras 内置的 Add()([]) 函数，否则后面会报错：’Tensor’ object has no attribute ‘_keras_history’</p></blockquote></li><li><p>The convolutional block（输入/输出维度不相同）</p><p>当输入输出维度不相同时，在 shourtcut 这条路径使用一个卷积操作来使得维度相同。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_9.png" alt=""></p><p>各层的细节：</p><p>First component of main path:</p><ul><li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. </li><li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be <code>conv_name_base + &#39;2b&#39;</code>.</li><li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be <code>conv_name_base + &#39;2c&#39;</code>.</li><li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li></ul><p>Shortcut path:</p><ul><li>The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;1&#39;</code>.</li><li>The BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;1&#39;</code>. </li></ul><p>Final step: </p><ul><li>The shortcut and the main path values are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>实现的代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The convolutional block 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X, f, filters, stage, block, s = <span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the convolutional block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    s -- Integer, specifying the stride to be used</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value</span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path </span></span><br><span class="line">    X = Conv2D(F1, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path</span></span><br><span class="line">    X = Conv2D(F2, (f, f), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'same'</span>, name = conv_name_base + <span class="string">'2b'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path </span></span><br><span class="line">    X = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), name = conv_name_base + <span class="string">'2c'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### SHORTCUT PATH ####</span></span><br><span class="line">    X_shortcut = Conv2D(F3, (<span class="number">1</span>,<span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'1'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'1'</span> )(X_shortcut)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure></li></ol><h3 id="建立一个-50-层的残差网络模型"><a href="#建立一个-50-层的残差网络模型" class="headerlink" title="建立一个 50 层的残差网络模型"></a>建立一个 50 层的残差网络模型</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_10.png" alt=""></p><p>这个模型的细节为：</p><ul><li>Zero-padding pads the input with a pad of (3,3)</li><li>Stage 1:<ul><li>The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”.</li><li>BatchNorm is applied to the channels axis of the input.</li><li>MaxPooling uses a (3,3) window and a (2,2) stride.</li></ul></li><li>Stage 2:<ul><li>The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>Stage 3:<ul><li>The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”.</li></ul></li><li>Stage 4:<ul><li>The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”.</li></ul></li><li>Stage 5:<ul><li>The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [512, 512, 2048], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”.</li><li>The flatten doesn’t have any hyperparameters or name.</li><li>The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be <code>&#39;fc&#39; + str(classes)</code>.</li></ul><p>实现代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50 层的残差网络模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape = <span class="params">(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span>, classes = <span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the popular ResNet50 the following architecture:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3</span></span><br><span class="line"><span class="string">    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string">    classes -- integer, number of classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">64</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">2</span>, <span class="number">2</span>), name = <span class="string">'conv1'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X) <span class="comment">#  kernel_initializer = glorot_uniform 表示 Xavier 均匀初始化</span></span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn_conv1'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage = <span class="number">2</span>, block=<span class="string">'a'</span>, s = <span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 3 </span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], stage = <span class="number">3</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 4</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'d'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'e'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 5 </span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># AVGPOOL</span></span><br><span class="line">    X = AveragePooling2D((<span class="number">2</span>,<span class="number">2</span>), name = <span class="string">'avg_pool'</span>)(X)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># output layer</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(classes, activation=<span class="string">'softmax'</span>, name=<span class="string">'fc'</span> + str(classes), kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)<span class="comment">#  kernel_initializer = glorot_uniform 表示 Xavier 均匀初始化</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create model</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'ResNet50'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="进行训练和预测"><a href="#进行训练和预测" class="headerlink" title="进行训练和预测"></a>进行训练和预测</h3><ul><li><p>建立模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = ResNet50(input_shape = (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>), classes = <span class="number">6</span>)</span><br></pre></td></tr></table></figure></li><li><p>编译模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></li><li><p>训练模型</p><ul><li><p>载入数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert training and test labels to one hot matrices</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br></pre></td></tr></table></figure></li><li><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, Y_train, epochs = <span class="number">2</span>, batch_size = <span class="number">32</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">preds = model.evaluate(X_test, Y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></li></ul><h3 id="测试自己的图片"><a href="#测试自己的图片" class="headerlink" title="测试自己的图片"></a>测试自己的图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">'images/my_image.jpg'</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line">print(<span class="string">'Input image shape:'</span>, x.shape)</span><br><span class="line">my_image = scipy.misc.imread(img_path)</span><br><span class="line">imshow(my_image)</span><br><span class="line">print(<span class="string">"class prediction vector [p(0), p(1), p(2), p(3), p(4), p(5)] = "</span>)</span><br><span class="line">print(model.predict(x))</span><br></pre></td></tr></table></figure><h3 id="对模型进行总览"><a href="#对模型进行总览" class="headerlink" title="对模型进行总览"></a>对模型进行总览</h3><ul><li><p>表格</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure></li><li><p>位图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(model, to_file=<span class="string">'model.png'</span>)</span><br><span class="line">SVG(model_to_dot(model).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure></li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>实际中非常深的普通网络不起作用，因为梯度下降它们很难训练</li><li>跳跃连接帮忙解决跳跃连接问题，它们也使得残差网络很容易学习 identity function</li><li>有两种主要的组块：The identity block and the convolutional block</li><li>把这些组块堆叠起来可以形成很深的残差网络</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 4 week 2）—— 深度卷积网络的案例研究</title>
      <link href="/2018/10/17/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w2/"/>
      <url>/2018/10/17/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w2/</url>
      <content type="html"><![CDATA[<p>本周我们直接从研究论文里学习深度 CNN 的一些使用技巧和方法。</p><h2 id="outline"><a href="#outline" class="headerlink" title="outline"></a>outline</h2><ul><li><p>一些经典的网络模型：</p><ul><li>LeNet-5</li><li>AlexNet</li><li>VGGNet</li></ul></li><li><p>残差卷积网络 ResNet</p></li><li>初始神经网络实例</li></ul><h2 id="经典卷积网络模型"><a href="#经典卷积网络模型" class="headerlink" title="经典卷积网络模型"></a>经典卷积网络模型</h2><h3 id="LeNet－５"><a href="#LeNet－５" class="headerlink" title="　LeNet－５"></a>　LeNet－５</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_1.png" alt=""></p><ul><li>60000 个参数</li><li>从左到右 $n_H,n_W$ 减小，$n_C$ 增大</li><li>一个至今使用的模式：conv -&gt; pool -&gt; conv -&gt; pool -&gt; FC -&gt; FC -&gt; output</li><li>这篇论文中使用的是 sigmoid/tanh 函数，而不是 ReLU</li><li>由于论文较久远，吴恩达建议只读论文的第二章，快速看一下第三章</li></ul><a id="more"></a><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_3.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_4.png" alt=""></p><ul><li>与 LetNet 很相似，但是大得多，大概六千万个参数</li><li>使用 ReLU 激活函数</li><li>使用两块 GPU 训练</li><li>局部响应归一层 LRN</li></ul><h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_5.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_6.png" alt=""></p><ul><li>超过一亿三千万个参数，十分庞大的网络</li><li>结构非常统一，缺点是网络过于庞大</li></ul><p>建议先读 AlexNet 的论文，再读 VGG-16 的论文，最后读最难的 LeNet。</p><h2 id="残差网络-ResNet"><a href="#残差网络-ResNet" class="headerlink" title="残差网络 ResNet"></a>残差网络 ResNet</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_7.png" alt=""></p><p>训练网络很深时会发生梯度消失或者梯度爆炸，利用“跳跃连接” ( skip connection)，可以训练很深很深的残差网络。</p><h3 id="残差结构-residual-block"><a href="#残差结构-residual-block" class="headerlink" title="残差结构 residual block"></a>残差结构 residual block</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_8.png" alt=""></p><ul><li>绿色的叫<strong>主路径 (main  path)</strong></li><li>紫色的叫<strong>快捷路径 (short cut)</strong> 或 <strong>跳跃连接 (skip connection)</strong></li><li>最后两者合一得到的 $a^{[l+1]} = g(z^{[l+2]}+a^{[l]})$ </li></ul><h3 id="残差网络-Residual-Network"><a href="#残差网络-Residual-Network" class="headerlink" title="残差网络 Residual Network"></a>残差网络 Residual Network</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_9.png" alt=""></p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>对于一个普通网络 (plain network)，随着层数的增大，理论上训练误差应该减小，但是实际上是先减小后增大，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_10.png" alt=""></p><p>而如果使用 ResNet，即使训练一个 1000 层的网络，性能也不会先增后降。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_11.png" alt=""></p><h3 id="为什么-ResNet-有用"><a href="#为什么-ResNet-有用" class="headerlink" title="为什么 ResNet 有用"></a>为什么 ResNet 有用</h3><p>假设我们在一个网络后面加上一个残差结构，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_12.png" alt=""></p><p> 所有的激活函数使用的都是 ReLU，所以 $a \geq 0 $ </p><p>$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(w^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$</p><p>加入使用正则化使权重衰减，w 和 b 变得很小，如果没有残差结构，那么可能会发生梯度消失，但是如果有残差结构，那么就算 w 和 b 衰减到 0，$a^{[l+2]}=g(a^{[l]})=ReLU(a^{[l]})=a^{[l]}$，所以额外增加两层也不会损害神经网络的表现。</p><p>要实现跳跃连接，我们必须假设 $z^{[l+2]}$ 和 $a^{[l]}$ 是同维度，这样才能相加，所以深度残差网络里面很多  <strong>same 卷积</strong>，保证卷积之后维度不变。</p><p>但是有时候 $z^{[l+2]}$ 和 $a^{[l]}$ 维度不同，比如 256 和 128，我们可以加上一个系数 $W_S$：</p><p>$a^{[l+2]}=g(w^{[l+2]}a^{[l+1]}+b^{[l+2]}+W_sa^{[l]})$</p><p>此处$W_S$ 是一个 256×128 的矩阵。  </p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_13.png" alt=""></p><h2 id="Inception-网络"><a href="#Inception-网络" class="headerlink" title="Inception 网络"></a>Inception 网络</h2><h3 id="1-1-卷积"><a href="#1-1-卷积" class="headerlink" title="1*1 卷积"></a>1*1 卷积</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_14.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_15.png" alt=""></p><p>1×1 卷积本质上就是一个全连接神经网络，逐一作用于这 6×6=36 个不同的位置，这个小神经网络接收一行 32 个数的输入，然后输出过滤器数个输出值。这个有时被称作“网中网”。</p><p>一个实例：</p><p>我们可以用 1×1 卷积来缩小通道数：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_16.png" alt=""></p><h3 id="Inception-network-引例"><a href="#Inception-network-引例" class="headerlink" title="Inception network 引例"></a>Inception network 引例</h3><p>在设计卷积网络的某一层的时候，有可能会用到1×1的卷积层或者3×3的卷积层或者5×5的卷积层或者池化层，Inception 网络在某一层将这些全部用上，使得网络更复杂，效果也更好。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_17.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_18.png" alt=""></p><p>我们将输出进行不同的卷积操作，但是都进行 “same padding”，使得输出维度都一样，最后将所有的输出堆叠起来，这就形成了 Inception 网络的一个模块。</p><h3 id="计算成本问题"><a href="#计算成本问题" class="headerlink" title="计算成本问题"></a>计算成本问题</h3><p>我们看 5×5 的卷积核。</p><ol><li><p>像上图一样直接将 5×5 的卷积核与 28×28×192 的输入进行卷积，得到 28×28×32 的输出</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_19.png" alt=""></p><p>最后进行的乘法运算次数为 28×28×32×5×5×192 约等于 1.2亿 次</p></li><li><p>在进行 5×5 卷积之前进行一个 1×1 卷积（<strong>瓶颈层</strong>），先将通道数减到 16，再进行 5×5 卷积，得到 28×28×32 的输出</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_20.png" alt=""></p><p>最后的运算次数为 28×28×16×192×1×1 + 28×28×32×5×5×16 约为 1240万 次，是第一种方法的十分之一运算量。</p></li></ol><h3 id="Inception-模块"><a href="#Inception-模块" class="headerlink" title="Inception 模块"></a>Inception 模块</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_21.png" alt=""></p><h3 id="Inception-网络-1"><a href="#Inception-网络-1" class="headerlink" title="Inception 网络"></a>Inception 网络</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_22.png" alt=""></p><ul><li><p>红色部分是一个一个 Inception 模块</p></li><li><p>绿色的旁支是以隐藏层作为输入的 softmax 函数，防止网络太深了过拟合</p></li><li><p>这个网络是谷歌开发的 gooleNet，用来纪念 Lecun 的 LeNet</p></li><li><p>Inception 这个名字来源于《盗梦空间 (Inception)》表情包：），作者们把这个图片当成了构建更加有深度的神经网络的动机：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_23.png" alt=""></p></li></ul><h2 id="搭建卷积网络的小技巧"><a href="#搭建卷积网络的小技巧" class="headerlink" title="搭建卷积网络的小技巧"></a>搭建卷积网络的小技巧</h2><h3 id="使用开源资源"><a href="#使用开源资源" class="headerlink" title="使用开源资源"></a>使用开源资源</h3><ul><li>使用文献中的网络结构</li><li>使用开源的网络结构算法实现</li><li>使用人家预训练好的模型然后在自己的数据集上进行微调</li></ul><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>如果你想实现一个计算机视觉应用而不想从零开始训练权重，实现更快的方式通常是下载已经训练好权重的网络结构，把这个作为预训练迁移到你感兴趣的新任务上，计算机视觉的研究社区已经很擅长把许多数据库发布在网络上，如 ImageNet、MSCOCO、PASCAL 等数据库，许多计算机视觉的研究者已经在上面训练了自己的算法，有时训练要耗费好几周时间，占据许多 GPU，事实上有人已经做过这种训练，这意味着你可以下载这些开源的权重为你自己的神经网络做好的初始化开端，且可以用迁移学习来迁移知识，从这些大型公共数据库迁移知识到你自己的问题上。</p><p>假如我们要识别家里的两只猫，训练数据量不够，我们可以从网上下载一些训练好的模型，比如在有 1000 类物体的 ImageNet 数据库上训练的模型。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_24.png" alt=""></p><p>将紫色框住的部分全部“冻结住”，可训练参数为零，将这部分看成一个模块或者函数，我们需要做的是把 1000 分类的 softmax 改成 3 分类的然后训练 softmax 这部分的参数即可。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_25.png" alt=""></p><p>当你有更大的带标签数据集，你可以冻结更少的层数。只冻结前面这些层，然后训练后面这些层，（1）你可以用最后几层的权重，作为初始化开始做梯度下降，把 softmax 改成自己需要的（2）或者你也可以去掉最后几层，然后用自己的新神经元和新 softmax 输出。原则是：你数据越多，所冻结的层数可以越少，自己训练的层数可以越多。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_26.png" alt=""></p><p>最后，如果你有许多数据，你可以用该开源网络和权重来初始化整个网络然后训练。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在计算机视觉领域，更多的数据对几乎所有的计算机视觉工作都有好处，但是对于绝大多数问题，总是不能获得足够多的数据，这就需要数据增强。</p><ol><li><p>普通数据增强方式</p><p>镜像 mirroring</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_27.png" alt=""></p><p>随机裁剪 random cropping</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_28.png" alt=""></p><p>旋转 rotation</p><p>剪切 shearing</p><p>局部弯曲 local warping</p><p>……</p></li><li><p>色彩变化 color shifting</p><p>给三个颜色通道 R G B 增加不同的值，可以使用 PCA 主成份分析算法来实现。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_29.png" alt=""></p></li></ol><h4 id="数据增强线程"><a href="#数据增强线程" class="headerlink" title="数据增强线程"></a>数据增强线程</h4><p>数据存在硬盘中，在进入 CPU/GPU 训练之前使用几个 CPU 线程进行失真处理（数据增强），训练线程和失真线程可以并行处理。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_30.png" alt=""></p><h3 id="目前计算机视觉的状态"><a href="#目前计算机视觉的状态" class="headerlink" title="目前计算机视觉的状态"></a>目前计算机视觉的状态</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_31.png" alt=""></p><p>机器学习的两种知识来源：</p><ul><li>带标签的数据</li><li>手工构建的 特征/网络结构/其他组件</li></ul><p>当我们拥有大量的数据，我们只需要简单的算法和更少的手工构建部分，而当我们数据量不够时，就需要更多的手工构建部分，需要花更多时间在设计网络结构上。传统的计算机视觉论文，相当依赖复杂的手工构建，就算近些年随着数据量的增加，手工构建的数量明显减少，但还是有很多手工设计的网络架构，所以超参数选择在计算机视觉比起其他领域更加复杂。</p><h3 id="在基准数据集上做的好-赢得比赛-的技巧"><a href="#在基准数据集上做的好-赢得比赛-的技巧" class="headerlink" title="在基准数据集上做的好 / 赢得比赛 的技巧"></a>在基准数据集上做的好 / 赢得比赛 的技巧</h3><p>这些方法在比赛时可以使得结果准确度增加一些微弱的优势，但是会增加很多运算成本，所以这些东西并不会真的用在实际的服务客户的系统上。</p><ul><li><p>Ensembling 总效果</p><ul><li>独立训练几个网络然后取输出值的平均值</li></ul></li><li><p>multi-crop at test time 在测试时多重裁剪</p><ul><li><p>在测试图像的多个裁剪子图像中运行分类器然后求结果平均值</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_32.png" alt=""></p></li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 1）</title>
      <link href="/2018/10/16/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/10/16/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h1 id="cnn-辅助函数的构建"><a href="#cnn-辅助函数的构建" class="headerlink" title="cnn 辅助函数的构建"></a>cnn 辅助函数的构建</h1><p>这周的编程作业内容是使用 numpy 实现卷积层和池化层，包括前向传播和方向传播。</p><h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="作业大纲"><a href="#作业大纲" class="headerlink" title="作业大纲"></a>作业大纲</h2><ul><li>卷积函数，包括：<ul><li>零填充</li><li>卷积窗口</li><li>卷积前向传播</li><li>卷积反向传播</li></ul></li><li>池化函数，包括：<ul><li>池化前向传播</li><li>创造 mask</li><li>Distribute value</li><li>池化反向传播</li></ul></li></ul><p>注意：每一步前向传播都需要储存一些参数在 cache 中，以便方向传播时可以用</p><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>先构建两个辅助函数。</p><h3 id="零填充-Zero-Padding"><a href="#零填充-Zero-Padding" class="headerlink" title="零填充 Zero-Padding"></a>零填充 Zero-Padding</h3><p>该辅助函数的作用是在该图片周围加零，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_1.png" alt=""></p><ul><li>它使图片在通过卷积层时尺寸不会减小，对深层网络尤其有效</li><li>使我们保留边缘的重要信息</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X, pad)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    在图片的长宽方向填充 pad 宽的 0 像素值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- 代表 m 张图片的形状为 (m, n_H, n_W, n_C) 的数组</span></span><br><span class="line"><span class="string">    pad -- 整数，图片水平方向和竖直方向的填充值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.pad(填充对象, ((维度1的前方，维度1的后方),(..,..),(..,..),(..,..)), '填充方式', constant_values = (前方填充值, 后方填充值)) </span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>,<span class="number">0</span>),(pad,pad),(pad,pad),(<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>, constant_values = (<span class="number">0</span>,<span class="number">0</span>) )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br></pre></td></tr></table></figure><h3 id="单步卷积运算"><a href="#单步卷积运算" class="headerlink" title="单步卷积运算"></a>单步卷积运算</h3><ul><li>取出输入向量</li><li>在输入的每个位置使用过滤器进行卷积</li><li>输出另一个向量</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_single_step</span><span class="params">(a_slice_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将过滤器与一小片图片进行卷积操作</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_slice_prev -- 形状为 (f, f, n_C_prev) 的之前图片的一小片</span></span><br><span class="line"><span class="string">    W -- 形状为 (f, f, n_C_prev) 形状与过滤器一样的权重参数矩阵</span></span><br><span class="line"><span class="string">    b -- 形状为 (1, 1, 1) 的偏差参数矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 标量，过滤器与一小片图片卷积的结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先逐元素相乘</span></span><br><span class="line">    s = np.multiply(a_slice_prev, W)</span><br><span class="line">    <span class="comment"># 再全部相加</span></span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    <span class="comment"># 加上偏差 b</span></span><br><span class="line">    Z = Z + float(b) <span class="comment"># b 为矩阵，先用 float() 转为标量再相加</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><h3 id="卷积神经网络——前向传播"><a href="#卷积神经网络——前向传播" class="headerlink" title="卷积神经网络——前向传播"></a>卷积神经网络——前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_2.png" alt=""></p><p>切片方法：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_3.png" alt=""></p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_prev, W, b, hparameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    卷积函数的前向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 形状为 (m, n_H_prev, n_W_prev, n_C_prev)，前一层的激活值</span></span><br><span class="line"><span class="string">    W -- 形状为 (f, f, n_C_prev, n_C)，权重矩阵</span></span><br><span class="line"><span class="string">    b -- 形为 (1, 1, 1, n_C)，偏差矩阵</span></span><br><span class="line"><span class="string">    hparameters -- 包含步长 "stride" 和填充 "pad" 的字典</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 形为 (m, n_H, n_W, n_C) 的卷积输出</span></span><br><span class="line"><span class="string">    cache -- 反向传播中要用到的缓存值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 A_prev 的 shape 中获取维度信息  </span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 W 的 shape 中获取维度信息</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = np.shape(W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取步长和填充信息</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用公式计算输出的维度信息，int() 可用于向下取整</span></span><br><span class="line">    n_H = int((n_H_prev+<span class="number">2</span>*pad-f)/stride) + <span class="number">1</span></span><br><span class="line">    n_W = int((n_W_prev+<span class="number">2</span>*pad-f)/stride) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用 0 初始化输出矩阵</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输入进行填充</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># 对 m 个训练图像的循环</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i,:,:,:]             <span class="comment"># 选出第 i 个图像</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                         <span class="comment"># 对输出向量高度方向的循环</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                     <span class="comment"># 对输出向量宽度方向的循环</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):                 <span class="comment"># 对输出向量通道数（过滤器个数）的循环</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到循环到 (i,h,w,c) 时候对应的图像“小片”</span></span><br><span class="line">                    vert_start = h*stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w*stride </span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 进行切片操作</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line">                    <span class="comment"># 将对应的图像小片和过滤器进行卷积得到 (i,h,w,c) 处的值</span></span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])</span><br><span class="line">                                          </span><br><span class="line">    <span class="comment"># 确保输出维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将一些信息储存在缓存中以便反向传播可以用</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化层减小了输入的高度和宽度，帮助减少计算量，使得特征检测器在输入中的位置更加不变。</p><ul><li>max pooling</li><li>average pooling</li></ul><p>池化层没有参数学习，只需要确定超参数，例如滤波器的大小。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_prev, hparameters, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    池化层的前向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "f" and "stride"</span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出输入的维度信息</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出过滤器的参数 </span></span><br><span class="line">    f = hparameters[<span class="string">"f"</span>]</span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义输出的维度</span></span><br><span class="line">    n_H = int(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = int(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化输出矩阵</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))              </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                           <span class="comment"># 对所有训练样本循环</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                     <span class="comment"># 对高度方向循环</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                 <span class="comment"># 对宽度方向循环</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range (n_C):            <span class="comment"># 对输出通道数的循环</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到切片索引值</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 对输入进行切片</span></span><br><span class="line">                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 进行池化操作</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.max(a_prev_slice)</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.average(a_prev_slice)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缓存</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保证输出维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h2 id="cnn-中的反向传播"><a href="#cnn-中的反向传播" class="headerlink" title="cnn 中的反向传播"></a>cnn 中的反向传播</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>这部分是可选项，在课程中也没有给详细的推导过程和解释，具体的解释可以参考以下几个网站：</p><ul><li><a href="https://grzegorzgwardys.wordpress.com/2016/04/22/8/" target="_blank" rel="noopener">参考1</a></li><li><a href="http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/" target="_blank" rel="noopener">参考2</a></li><li><a href="https://www.cnblogs.com/pinard/p/6494810.html" target="_blank" rel="noopener">参考3</a></li></ul><p>在参考 1 中我们可以知道，$dA^{[l-1]}$ 就是将 $dZ^{[l]}$ 与翻转 180 度的过滤器矩阵进行卷积的结果，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_4.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_5.jpg" alt=""></p><p>这正是下面式子所表示的，+= 就是将四个叠加起来，问题是下面的式子中的 W 并没有旋转 180 度……这点一直无法解释……</p><script type="math/tex; mode=display">d A^{[l-1]} + = \sum _ { h = 0 } ^ { n _ { l I } } \sum _ { w = 0 } ^ { n _ { W } } W _ { c } \times d Z ^{[l]} _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">d W _ { c } + = \sum _ { h = 0 } ^ { n _ { H } } \sum _ { w = 0 } ^ { n _ { W } } a _ { s l i c e } \times d Z _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">d b = \sum _ { h } \sum _ { w } d Z _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db[:,:,:,c] += dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward(), output of conv_forward()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),</span></span><br><span class="line"><span class="string">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    dW -- gradient of the cost with respect to the weights of the conv layer (W)</span></span><br><span class="line"><span class="string">          numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    db -- gradient of the cost with respect to the biases of the conv layer (b)</span></span><br><span class="line"><span class="string">          numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (A_prev, W, b, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = np.shape(W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters"</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from dZ's shape</span></span><br><span class="line">    (m, n_H, n_W, n_C) = np.shape(dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev, dW, db with the correct shapes</span></span><br><span class="line">    dA_prev = np.zeros(np.shape(A_prev))                           </span><br><span class="line">    dW = np.zeros(np.shape(W))   </span><br><span class="line">    db = np.zeros(np.shape(b))   </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pad A_prev and dA_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    dA_prev_pad = zero_pad(dA_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select ith training example from A_prev_pad and dA_prev_pad</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i,:,:,:]</span><br><span class="line">        da_prev_pad = dA_prev_pad[i,:,:,:]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice"</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the slice from a_prev_pad</span></span><br><span class="line">                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Update gradients for the window and the filter's parameters using the code formulas given above</span></span><br><span class="line">                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br><span class="line">                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br><span class="line">                    db[:,:,:,c] += dZ[i, h, w, c]</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span></span><br><span class="line">        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h3 id="池化层-1"><a href="#池化层-1" class="headerlink" title="池化层"></a>池化层</h3><p>虽然池化层没有参数，但是还是需要将梯度反向传播到池化层的上一层，以便反向传播能继续下去。</p><h4 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max pooling"></a>max pooling</h4><p>对于 max pooling 而言，只有原来的最大值才对最终的代价函数有影响，所以我们只需要计算代价函数对这个最大值的梯度即可，其他的置为零，首先创造一个蒙板函数：</p><script type="math/tex; mode=display">X = \left[ \begin{array} { l l } { 1 } & { 3 } \\ { 4 } & { 2 } \end{array} \right] \quad \rightarrow \quad M = \left[ \begin{array} { l l } { 0 } & { 0 } \\ { 1 } & { 0 } \end{array} \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a mask from an input matrix x, to identify the max entry of x.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Array of shape (f, f)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    mask = (x == np.max(x)) <span class="comment"># x 中等于 np.max(x) 的都为真，其他为假</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><h4 id="average-pooling"><a href="#average-pooling" class="headerlink" title="average pooling"></a>average pooling</h4><p>由于 average pooling 中过滤器中每个值都对最终结果有影响，所以这每个值的梯度都是下一层梯度的平均一份，因为它们每个数对最终代价函数的贡献都是一样的，所以我们将一个梯度分散为若干个相等的梯度：</p><script type="math/tex; mode=display">d Z = 1 \quad \rightarrow \quad d Z = \left[ \begin{array} { l l } { 1 / 4 } & { 1 / 4 } \\ { 1 / 4 } & { 1 / 4 } \end{array} \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span><span class="params">(dz, shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Distributes the input value in the matrix of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dz -- input scalar</span></span><br><span class="line"><span class="string">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Array of size (n_H, n_W) for which we distributed the value of dz</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shape (≈1 line)</span></span><br><span class="line">    (n_H, n_W) = shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the value to distribute on the matrix (≈1 line)</span></span><br><span class="line">    average = n_H * n_W</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a matrix where every entry is the "average" value (≈1 line)</span></span><br><span class="line">    a = dz * np.ones((n_H, n_W)) / average</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><h4 id="合并到一个函数"><a href="#合并到一个函数" class="headerlink" title="合并到一个函数"></a>合并到一个函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span><span class="params">(dA, cache, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现池化层的反向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A</span></span><br><span class="line"><span class="string">    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters </span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 cache 取出参数</span></span><br><span class="line">    (A_prev, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出超参数</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    f = hparameters[<span class="string">'f'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出 dA_prev 和 dA 的维度信息</span></span><br><span class="line">    m, n_H_prev, n_W_prev, n_C_prev = np.shape(A_prev)</span><br><span class="line">    m, n_H, n_W, n_C = np.shape(dA)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 dA_prev 初始化</span></span><br><span class="line">    dA_prev = np.zeros(np.shape(A_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                        <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对每个训练样例单独操作</span></span><br><span class="line">        a_prev = A_prev[i, :,:,:]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop on the vertical axis</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop on the horizontal axis</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels (depth)</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到切片索引值</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride </span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 用两种方式计算反向传播</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 进行切片</span></span><br><span class="line">                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                        <span class="comment"># 创造目前切片的蒙板，使得该切片中最大值置 1，其他置 0</span></span><br><span class="line">                        mask = create_mask_from_window(a_prev_slice)</span><br><span class="line">                        <span class="comment"># 将 dA[i,h,w,c] 这个位置的值乘上蒙板得到 [i,h,w,c] 这个位置反向传播的结果，最后根据链式法则将所有支路相加，也就是 +=</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i,h,w,c]</span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 首先得到 [i,h,w,c] 这个特定位置的梯度值 dA[i,h,w,c]</span></span><br><span class="line">                        da = dA[i,h,w,c]</span><br><span class="line">                        <span class="comment"># 过滤器形状</span></span><br><span class="line">                        shape = (f,f)</span><br><span class="line">                        <span class="comment"># 将 dA[i,h,w,c] 这个值分散得到该位置反向传播结果，根据链式法则将所有位置的结果相加</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)</span><br><span class="line">                        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev</span><br></pre></td></tr></table></figure><h1 id="cnn-的应用"><a href="#cnn-的应用" class="headerlink" title="cnn 的应用"></a>cnn 的应用</h1><p>这部分使用 tensorflow 来构建一个分类器。</p><h2 id="包的引入和数据集"><a href="#包的引入和数据集" class="headerlink" title="　包的引入和数据集"></a>　包的引入和数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> cnn_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">＃ 加载数据集</span><br><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>仍然是识别手势的数据集：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_6.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片示例</span></span><br><span class="line">index = <span class="number">6</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(Y_train_orig[:, index])))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_7.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集信息</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br><span class="line">conv_layers = &#123;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_8.png" alt=""></p><h2 id="创建占位符"><a href="#创建占位符" class="headerlink" title="创建占位符"></a>创建占位符</h2><p>首先需要创建输入数据的占位符，以便在运行 sess 时可以喂数据进去。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建占位符</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_H0, n_W0, n_C0, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_H0 -- scalar, height of an input image</span></span><br><span class="line"><span class="string">    n_W0 -- scalar, width of an input image</span></span><br><span class="line"><span class="string">    n_C0 -- scalar, number of channels of the input</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype "float"</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype "float"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    X = tf.placeholder(tf.float32, shape = [<span class="keyword">None</span>,n_H0, n_W0, n_C0 ]) <span class="comment"># 第一个参数是数据类型，第二个是占位符形状</span></span><br><span class="line">    Y = tf.placeholder(tf.float32, shape = [<span class="keyword">None</span>, n_y ])</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><h2 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h2><ul><li>初始化使用 W = tf.get_variable(“W”, [1,2,3,4], initializer = …)</li><li>初始化器使用 tf.contrib.layers.xavier_initializer(seed = 0)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [4, 4, 3, 8]</span></span><br><span class="line"><span class="string">                        W2 : [2, 2, 8, 16]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, W2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                              <span class="comment"># 这句不用管，确保我们的输出和教程一样</span></span><br><span class="line"></span><br><span class="line">    W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>)) </span><br><span class="line">    W2 = tf.get_variable(<span class="string">"W2"</span>, [<span class="number">2</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">16</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>))</span><br><span class="line">   </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>卷积层 （步长 1，same 填充） -&gt; RELU 激活 -&gt; maxpool 池化（8×8过滤器，8×8步长，same 填充） -&gt; 卷积层（步长 1，same 填充）-&gt; RELU 激活 -&gt; maxpool 池化（4×4 过滤器，4×4 步长）-&gt;拍扁 -&gt; 全连接层（输出结点 6 个，不需要调用 softmax，因为在 tensorflow 中，softmax 和代价函数被整合进一个函数中）</p><p>使用的函数为：</p><ul><li>卷积层：<code>tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = &#39;SAME&#39;)</code>  <ul><li>X 为输入，W1 为过滤器，strides 必须为 [1,s,s,1]，s 为步长，padding 类型为 same </li></ul></li><li>maxpool 池化层：<code>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = &#39;SAME&#39;)</code> <ul><li>f 为过滤器尺寸</li></ul></li><li>relu：<code>tf.nn.relu(Z1)</code> <ul><li>Z1 可以是任意形状</li></ul></li><li>拍扁：<code>tf.contrib.layers.flatten(P)</code><ul><li>返回一个 [batch_size,k] 的张量，也就是说会保留样本个数那个维度</li></ul></li><li>全连接层：<code>tf.contrib.layers.fully_connected(F, num_outputs)</code> <ul><li>num_outputs 为输出层结点个数</li><li>注意：tensorflow 会自动帮我们初始化全连接层的参数并在训练模型的时候自动训练，所以不用初始参数</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "W2"</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># CONV2D: stride of 1, padding 'SAME'</span></span><br><span class="line">    Z1 = tf.nn.conv2d(X, W1, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A1 = tf.nn.relu(Z1)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 8x8, sride 8, padding 'SAME'</span></span><br><span class="line">    P1 = tf.nn.max_pool(A1, ksize = [<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># CONV2D: filters W2, stride 1, padding 'SAME'</span></span><br><span class="line">    Z2 = tf.nn.conv2d(P1, W2, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A2 = tf.nn.relu(Z2)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 4x4, stride 4, padding 'SAME'</span></span><br><span class="line">    P2 = tf.nn.max_pool(A2, ksize = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># FLATTEN</span></span><br><span class="line">    P2 = tf.contrib.layers.flatten(P2)</span><br><span class="line">    <span class="comment"># FULLY-CONNECTED without non-linear activation function (not not call softmax).</span></span><br><span class="line">    <span class="comment"># 6 neurons in output layer. Hint: one of the arguments should be "activation_fn=None" </span></span><br><span class="line">    Z3 = tf.contrib.layers.fully_connected(P2, <span class="number">6</span>, activation_fn = <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><h2 id="计算代价函数"><a href="#计算代价函数" class="headerlink" title="计算代价函数"></a>计算代价函数</h2><ul><li>计算所有样例的损失函数：<code>tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y)</code> <ul><li>返回所有样例的损失函数的一个向量</li></ul></li><li>计算损失函数均值（代价函数）：<code>tf.reduce_mean()</code>  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="总模型"><a href="#总模型" class="headerlink" title="总模型"></a>总模型</h2><ul><li>创造占位符</li><li>初始化参数（全连接层不要）</li><li>前向传播</li><li>计算损失</li><li>创建优化器</li><li>小批量梯度下降</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 总模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.009</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          num_epochs = <span class="number">100</span>, minibatch_size = <span class="number">64</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer ConvNet in Tensorflow:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    train_accuracy -- real number, accuracy on the train set (X_train)</span></span><br><span class="line"><span class="string">    test_accuracy -- real number, testing accuracy on the test set (X_test)</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    ops.reset_default_graph()<span class="comment"># 重置默认的计算图</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)       <span class="comment"># 不用管，保持结果一致 (tensorflow seed)</span></span><br><span class="line">    seed = <span class="number">3</span>                    <span class="comment"># 不用管，保持结果一致 (numpy seed)</span></span><br><span class="line">    (m, n_H0, n_W0, n_C0) = X_train.shape             </span><br><span class="line">    n_y = Y_train.shape[<span class="number">1</span>]                            </span><br><span class="line">    costs = []                  <span class="comment"># 记录代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据输入数据形状创建占位符</span></span><br><span class="line">    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播: 在 tensorflow 计算图中构建前向传播</span></span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 代价函数计算: 往计算图中增加代价函数</span></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播: 定义优化器. Use an AdamOptimizer that minimizes the cost.</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化全局参数</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 开始会话 sess 计算计算图</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 运行全局初始化</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 进行训练循环</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">            minibatch_cost = <span class="number">0.</span></span><br><span class="line">            num_minibatches = int(m / minibatch_size) <span class="comment"># 计算小批量的个数</span></span><br><span class="line">            seed = seed + <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)<span class="comment"># 划分小批量</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 选择小批量</span></span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line">                <span class="comment"># 运行会话以执行 optimizer and the cost, the feedict should contain a minibatch for (X,Y).</span></span><br><span class="line">                _ , temp_cost= sess.run([optimizer,cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)               </span><br><span class="line">                </span><br><span class="line">                minibatch_cost += temp_cost / num_minibatches</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每五次迭代打印一次 cost</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, minibatch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(minibatch_cost)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># plot the cost</span></span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the correct predictions</span></span><br><span class="line">        predict_op = tf.argmax(Z3, <span class="number">1</span>)<span class="comment"># 找到 Z3 中最大值的索引号，1 为按行取</span></span><br><span class="line">        correct_prediction = tf.equal(predict_op, tf.argmax(Y, <span class="number">1</span>))<span class="comment"># 相等则为 1 否则为 0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate accuracy on the test set</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">        print(accuracy)</span><br><span class="line">        train_accuracy = accuracy.eval(&#123;X: X_train, Y: Y_train&#125;)<span class="comment"># accuracy.eval() 相当于 sess.run(accuracy)</span></span><br><span class="line">        test_accuracy = accuracy.eval(&#123;X: X_test, Y: Y_test&#125;)</span><br><span class="line">        print(<span class="string">"Train Accuracy:"</span>, train_accuracy)</span><br><span class="line">        print(<span class="string">"Test Accuracy:"</span>, test_accuracy)                              </span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> train_accuracy, test_accuracy, parameters</span><br></pre></td></tr></table></figure><h2 id="Tensorflow-使用感想"><a href="#Tensorflow-使用感想" class="headerlink" title="Tensorflow 使用感想"></a>Tensorflow 使用感想</h2><p>在 tensorflow 中，计算图中的任何值都不会被计算出来，除非你使用 <code>sess.run(feed_dict)</code>或者 <code>tensor.eval(feed_dict)</code>，在求值的时候，从要求的值往前推，把这一条线上所有需要的 placeholder 找出来然后填入 feed_dict。</p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 4 week 1）—— 卷积神经网络基础</title>
      <link href="/2018/09/28/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w1/"/>
      <url>/2018/09/28/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w1/</url>
      <content type="html"><![CDATA[<h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><h3 id="计算机视觉的问题"><a href="#计算机视觉的问题" class="headerlink" title="计算机视觉的问题"></a>计算机视觉的问题</h3><ul><li><p>图像分类：例如猫分类器</p></li><li><p>物体检测：例如自动驾驶不仅需要识别出图片中是否有车，还要计算出这张图片中汽车的位置</p></li><li><p>神经风格转换：如下图</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_1.png" alt=""></p></li></ul><a id="more"></a><h3 id="计算机视觉的挑战"><a href="#计算机视觉的挑战" class="headerlink" title="计算机视觉的挑战"></a>计算机视觉的挑战</h3><p>图片的像素可以任意大，输入维度可以任意大，会导致：</p><ul><li>很难获得足够数据避免过拟合</li><li>对计算量和内存的需求很大</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_2.png" alt=""></p><h2 id="卷积运算-Convolutional-operation"><a href="#卷积运算-Convolutional-operation" class="headerlink" title="卷积运算 (Convolutional operation)"></a>卷积运算 (Convolutional operation)</h2><h3 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h3><p>假设要检测下图的垂直边缘和水平边缘：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_3.png" alt=""></p><p>检测垂直边缘可以使用如下方法：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_4.png" alt=""></p><ul><li>左边是某个图像</li><li>中间是一个 3×3 的<strong>过滤器 (fliter)</strong>，是一个矩阵，有时也称为<strong>核 (kernel)</strong> </li><li><strong>*</strong> 符号表示<strong>卷积运算</strong> </li><li>将蓝框中的值逐元素乘以过滤器矩阵的值然后相加得到第一个结果 -5，下面的以此类推</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_5.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_6.png" alt=""></p><p>中间的过滤器就是一个垂直边缘检测器，那么它是如何检测边缘的？</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_7.png" alt=""></p><ul><li>边缘左侧像素值大，更明亮，右侧像素值小，更暗，最后得到的结果是更亮的区域在正中间，与检测出的垂直边缘相对应</li><li>检测出的边缘看起来很厚，是因为使用 6×6 的小图像，如果使用大图像就不会发生这种比例失调</li><li>过滤器告诉我们一个信息：某个垂直边缘是 3×3 的区域，左边有亮像素，右边有暗像素，而不在意中间有什么，这种区域被认为有垂直边缘</li></ul><h3 id="更多边缘检测例子"><a href="#更多边缘检测例子" class="headerlink" title="更多边缘检测例子"></a>更多边缘检测例子</h3><p>还有许多种精心设计的过滤器，比如水平边缘过滤器：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_8.png" alt=""></p><p>还有 sobel filter：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_9.png" alt=""></p><p>还有 scharr filter：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_10.png" alt=""></p><p>随着深度学习的发展我们发现，如果你想要检测一些复杂图片的边界，可能并不需要计算机视觉的研究人员挑选出这 9 个矩阵元素。你可以把矩阵里的这 9 个元素当做参数，通过反向传播来学习得到他们的数值，它能学到比前述这些人为定义的过滤器更加善于捕捉你的数据的统计学特征的过滤器。除了垂直和水平边界，同样能够学习去检测 45 度的边界 70 度 或 73 度，无论什么角度。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_11.png" alt=""></p><h3 id="填充-paddling"><a href="#填充-paddling" class="headerlink" title="填充 (paddling)"></a>填充 (paddling)</h3><h4 id="卷积运算的一些问题"><a href="#卷积运算的一些问题" class="headerlink" title="卷积运算的一些问题"></a>卷积运算的一些问题</h4><p>如果我们用 3×3 的过滤器去卷积 6×6 的图像，最后得到的是一个 4×4 的图像，这样会出现两个问题：</p><ul><li>输出的图像逐渐缩小，如果网络的层数很深，那么许多层之后会得到一个非常小的图片</li><li>角落上的像素值只会在输出中被使用一次，而中间的像素会重叠许多次，导致丢失了图片靠近边界上的信息</li></ul><h4 id="怎麽办"><a href="#怎麽办" class="headerlink" title="怎麽办"></a>怎麽办</h4><p>为了消除这两个问题，可以在卷积前用一个额外的边缘 (border) 填充图片，例如我们可以在 6×6 的图片边缘用一个 1 像素大小的额外边缘，那么这个图片就变成 8×8，和 3×3 的过滤器卷积之后得到一个 6×6 的图片，和原来尺寸一样，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_12.png" alt=""></p><ul><li>填充的边缘厚度为 p，上图中 p = 1</li></ul><h4 id="到底填充多少？——两种卷积"><a href="#到底填充多少？——两种卷积" class="headerlink" title="到底填充多少？——两种卷积"></a>到底填充多少？——两种卷积</h4><ol><li><p>valid 卷积 —— 没有填充</p><script type="math/tex; mode=display">n \times n \quad * \quad f \times f \rightarrow (n-f+1) \times (n-f+1)\\6 \times 6 \quad * \quad 3 \times 3 \rightarrow 4 \times 4</script></li><li><p>same 卷积 —— 填充后使得输入大小等于输出大小</p><script type="math/tex; mode=display">(n+2p) \times (n+2p) \quad * \quad f \times f \rightarrow (n+2p-f+1) \times (n+2p-f+1)\\\Rightarrow \ \ n+2p-f+1=n\\\Rightarrow \ \ p=\frac{f-1}{2}</script><ul><li>其中 p 为填充的厚度，f 为过滤器的边长</li><li>当 f 为奇数时，p 按照上面公式取值即可以使得输入输出相同</li><li>一般实际中 f 都是使用奇数，有如下两个原因：<ul><li>如果 f 是偶数，则需要一些不对称填充</li><li>奇数过滤器可以有个中心点，称之为中心像素，可以描绘过滤器的位置</li></ul></li></ul></li></ol><h3 id="带步长的-strided-卷积"><a href="#带步长的-strided-卷积" class="headerlink" title="带步长的 (strided) 卷积"></a>带步长的 (strided) 卷积</h3><p>在卷积时过滤器移动的步长不为 1 称为带步长的卷积。如果步长 stride = 2 结果如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_13.png" alt=""></p><p>加入我们有：</p><ul><li>n×n 的图像</li><li>f×f 的过滤器</li><li>填充 padding 为 p</li><li>步长 strides 为 s </li></ul><p>如果将两者卷积，输出为：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_14.png" alt=""></p><ul><li>其中 [ ] 为向下取整，也就是小于它的最大整数，[z] = floor(z)</li></ul><h3 id="一个提法的纠正"><a href="#一个提法的纠正" class="headerlink" title="一个提法的纠正"></a>一个提法的纠正</h3><p>实际上，在正式数学课本中，卷积操作前还需要一个额外的“翻转”操作，即将原来的过滤器先水平翻转再竖直翻转，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_15.png" alt=""></p><p>而深度学习中的“卷积”操作，将翻转操作取消了，数学上应该叫“交叉相关”，但是大多数深度学习文献都叫它卷积操作。</p><h3 id="对三维立方体的卷积"><a href="#对三维立方体的卷积" class="headerlink" title="对三维立方体的卷积"></a>对三维立方体的卷积</h3><p>假如我们的图像是 RGB 图像，那么它不是一个二维的灰度图像，而是一个 6×6×3 的三维图像，其中 3 为图像的<strong>通道数</strong>，这种图像如何卷积呢？</p><h4 id="单过滤器"><a href="#单过滤器" class="headerlink" title="单过滤器"></a>单过滤器</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_17.png" alt=""></p><ul><li>3 为图像的通道数，图像的通道数应该和过滤器的通道数一致</li><li>将每一层的过滤器的 9 个数字与对应通道的图像像素值相乘，最后把这所有的 27 个数字相加得到第一个输出，其他的以此类推</li></ul><h4 id="多过滤器"><a href="#多过滤器" class="headerlink" title="多过滤器"></a>多过滤器</h4><p>如果我们希望检测多个边缘，那么可以使用多个过滤器。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_18.png" alt=""></p><p>总结：</p><script type="math/tex; mode=display">n \times n \times n_c \quad * \quad f \times f \times n_c \ \rightarrow (n-f+1) \times (n-f+1) \times n_c'</script><ul><li><p>其中 $n_c$ 为图像通道数，图像通道数应该和过滤器一致</p><ul><li>有时候 $n_c$ 称之为三维立方体的深度</li></ul></li><li><p>其中 $n_c’$ 为过滤器的数量，即检测的特征数量</p></li></ul><p>现在我们不仅可以直接处理拥有 3 个通道的 RGB 图片，而且更重要的是，可以检测两个特征，比如垂直、水平边缘，或者 10 个，128 个，甚至几百个不同的特征。</p><h2 id="单层卷积神经网络"><a href="#单层卷积神经网络" class="headerlink" title="单层卷积神经网络"></a>单层卷积神经网络</h2><h3 id="引例-1"><a href="#引例-1" class="headerlink" title="引例"></a>引例</h3><p>下面是一个卷积层的计算过程。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_20.png" alt=""></p><ul><li>假设输入图像为一个 6×6×3 的 $a^{[0]}$，那么输出为 4×4×2 的 $a^{[1]}$ </li><li>其中过滤器相当于参数 $w^{[1]}$，第二个蓝框里相当于 $z^{[1]}$，经过激活函数 Relu 之后变为 $a^{[1]}$</li></ul><h3 id="一层卷积层的参数"><a href="#一层卷积层的参数" class="headerlink" title="一层卷积层的参数"></a>一层卷积层的参数</h3><p>假设你有 10 个 3×3×3 的过滤器在一层神经网络中，那么这一层有多少个参数？</p><p>parameters = ( 3×3×3 +1 ) × 10 = 280 个</p><p>这是一个很好的特性：不管输入的图像有多大，比方 1000 x 1000 或者 5000 x 5000，这里的参数个数不变. 依然是 280 个。因此 用这 10 个过滤器来检测一个图片的不同的特征，比方垂直边缘线，水平边缘线或者其他不同的特征，不管图片多大，所用的参数个数都是一样的。</p><h3 id="标记符号总结"><a href="#标记符号总结" class="headerlink" title="标记符号总结"></a>标记符号总结</h3><p>如果第 $l$ 层是一个卷积层：</p><ul><li>过滤器尺寸：$f^{[l]}$</li><li>填充厚度：$p^{[l]}$</li><li>卷积步长：$s^{[l]}$</li><li>过滤器数量：$n_c^{[l]}$</li><li>输入：$n_H^{[l-1]} \times n_W^{[l-1]} \times n_c^{[l-1]}$ <ul><li>其中 H 代表高度，W 代表宽度，C 代表通道 channel</li></ul></li><li>输出：$n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$ <ul><li>$n_H^{[l]}=[\frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]$，$n_W^{[l]}=[\frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]$ <ul><li>其中 $[ \ ]$ 为向下取整</li></ul></li></ul></li><li>每个过滤器为：$f^{[l]} \times f^{[l]} \times n_c^{[l-1]}$ (注意不是 $n_c^{[l]}$) </li><li>这一层的激活值：$a^{[l]} \rightarrow n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$<ul><li>向量化激活值：$ A^{[l]} \rightarrow m \times n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$ </li></ul></li><li>权重：$f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$<ul><li>其中 $n_c^{[l]}$ 为 $l$ 层的过滤器数量</li></ul></li><li>偏差：$1 \times 1 \times 1 \times n_c^{[l]}$ </li></ul><h2 id="深层卷积神经网络"><a href="#深层卷积神经网络" class="headerlink" title="深层卷积神经网络"></a>深层卷积神经网络</h2><h3 id="引例-2"><a href="#引例-2" class="headerlink" title="引例"></a>引例</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_21.png" alt=""></p><ul><li>最后我们得到一个 7×7×40 的激活值，一共 1960 个数展开为一个列向量，然后输入到一个逻辑回归或者 softmax 单元得到预测值</li></ul><h3 id="卷积网络中的层类型"><a href="#卷积网络中的层类型" class="headerlink" title="卷积网络中的层类型"></a>卷积网络中的层类型</h3><ul><li>卷积层 Convolution Layers (简称 Conv)</li><li>池化层 Pooling Layers (简称 Pool)</li><li>全连接层 Fully connected Layers (简称 FC)</li></ul><h3 id="池化层-Pooling-Layers"><a href="#池化层-Pooling-Layers" class="headerlink" title="池化层 (Pooling Layers)"></a>池化层 (Pooling Layers)</h3><ul><li><p>Max pooling 最大值采样</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_22.png" alt=""></p><ul><li>Max pooling 和卷积操作基本一样，只是最后输出的是滤波器范围内的最大值</li><li>如果你把这个 4x4 的区域看作某个特征的集合，那么一个大的数字就意味着它或许检测到了一个特定的特征，所以左侧上方的四分之一区域有这样的特征，它或许是一个垂直的边沿 亦或一个更高或更弱，但是右侧上方的四分之一区域没有这个特征。所以 max pooling 做的是检测到所有地方的特征，四个特征中的一个被保留在 max pooling 的输出中。</li></ul></li><li><p>Average pooling 平均值采样</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/img/dl_4.1_23.png" alt=""></p><ul><li>平均值采样输出的是滤波器范围内所有值的平均值</li><li>不如 max pooling 常用</li></ul></li></ul><p>总结：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_24.png" alt=""></p><p>池化层没有任何参数要学习！是确定的函数！</p><h3 id="识别手写数字的例子"><a href="#识别手写数字的例子" class="headerlink" title="识别手写数字的例子"></a>识别手写数字的例子</h3><p>下面是一个用来识别手写数字的神经网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_25.png" alt=""></p><ul><li>conv 是 卷积层，pool 是池化层，FC 是全连接层</li><li>由于池化层没有权重和参数，所以不把它算作一个独立的层，而是视 conv1 和 pool1 为 layer 1，但是在某些文献中把卷积层和池化层视为两个独立的层</li></ul><p>总结：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_26.png" alt=""></p><ul><li>池化层没有参数</li><li>卷积层趋向于拥有越来越少的参数</li></ul><h2 id="为什么卷积如此有用"><a href="#为什么卷积如此有用" class="headerlink" title="为什么卷积如此有用"></a>为什么卷积如此有用</h2><ul><li>参数共享： 在特征检测器中（例如垂直边缘检测）对于图像的一部分是有用的，那么对于另一部分可能也是有用的。</li><li>连接的稀疏性：在每一层中，每个输出值只依靠一小部分输出值。也就是说每个输出值都只依赖于滤波器覆盖到的那几个数字，而剩下的像素值对它没有影响。</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>坎坷开学路</title>
      <link href="/2018/09/26/%E5%9D%8E%E5%9D%B7%E9%80%89%E5%AF%BC%E5%B8%88/"/>
      <url>/2018/09/26/%E5%9D%8E%E5%9D%B7%E9%80%89%E5%AF%BC%E5%B8%88/</url>
      <content type="html"><![CDATA[<p>今天是9月26号，在哈工大深圳校区的荔园写下这篇日记，距离开学已经过去了十天左右的时间，本来以为可以开开心心入学，没想到碰到了许多倒霉事儿：</p><a id="more"></a><ul><li>首先是开学延迟十五天，这导致了后面许多 schedule 非常紧张，开学第三天就选课上课。</li><li>开学前寄来了四个行李包裹，其中一个死活找不到，最后在另外一个快递点找到，寄来的台式电脑里面的独立显卡死活找不到，还以为是快递员偷了，结果是人家帮我另外放在一个盒子里了。</li><li>开学第二天就遭遇所谓“建国以来第三大的台风”——“山竹”，导致各种活动取消，不过这个台风也不过如此～</li><li>the last but not the least :），最倒霉的就是选导师太晚导致最后被学院分了一个没人选的导师：）</li></ul><p>选导师这件事情有必要特别来记录一下，仔细想来确实反映了自己许多缺点。</p><p>其实找导师我行动得并不晚，网上很多人说初试成绩出来后就要联系，于是考研初试成绩一出我就根据网上说的给好几个发了邮件，但都被委婉拒绝。在群里得知机械专业开学有个所谓的“双选会”，在这之前老师都不会理你，于是我就跟大多数同学一样，暑假两个月把选导师的事情抛在脑后。</p><p>其实每个学院那么多导师，在同学的口碑中都有个排序，有些导师方向好人品好自然就很抢手，临近开学，偶然得知许多同学已经被一些抢手的导师收了，这个时候我有点慌了，但还是信心满满，给最热门的一个导师 J 发了邮件，他第二天就回我了，让我给他发封简历，但是之后再也没了下文，我还傻乎乎地等待这个 J 回信，觉得还有希望，准备提前几天去找 J，并自信地认为一定能成功，从而错过了给其他导师发邮件的黄金时间。</p><p>提前两天去的学校，还以为自己去的很早：），其实有些人提前一个月就来了。去找了那个导师 J，他说他不确定能不能收我而且只有一个名额了，我这才知道他让我给他发简历只是说说而已：），为了保险，又去找了其他几个比较热门的导师，结果一个个都不在办公室，在这抢导师最激烈的几天，我还想着，发短信打电话会不会不礼貌啊，发邮件会不会看不到啊，会不会有更好的选择啊，也许 J 老师后面会增加名额啊…… 犹犹豫豫的，于是这段时间又被我错过了。</p><p>到了双选会那天，其实大部分热门的导师已经没有名额了，只是上台介绍一下自己的项目走个过场，但是还是有蛮多导师有名额，可以第二天去面试，这个时候我又犹豫了，因为我对他们的研究方向不太感兴趣（殊不知在机械学院根本没有搞 AI 的老师，要想转行搞 AI 只能靠自己，我的那种犹豫根本没意义）。当时让我印象最深的就是学院的首席学术顾问 clarence 教授，加拿大的两院院士，学术水平非常高，当时室友让我选他，我又担心自己语言问题，跟他无法交流，又纠结了很久。双选会结束当晚，所有的导师办公室都排起了面试的长队，我又怂了，觉得自己竞争不过别人，于是随便找了一个没人选的导师 L 签了双选表（之所以选他还因为当时找他时，他拉着我谈了<strong>两个小时</strong>，说自己如何如何尊重学生，我一冲动就签了）。第二天我才发现，所有人都叫我别选他，说学长口中的口碑不好，但是也说不出哪里不好……这个时候我心里有点发怵，他口碑这么差，是不是该换个人？</p><p>又过了一天，我室友告诉我他一个朋友选了 clarence 教授，教授还给了她一个项目介绍书，我看了看，大概是用神经网络识别流水线上的苹果进行分拣之类的，也就是神经网络在实际中的应用，这不就是我想做的项目吗！于是我下定决心换导师，给教授发了邮件，表明想做这个项目，教授很快就回了我，让我发封英文简历，我赶紧急匆匆地做了个英文简历发给他，然后他让我待会儿去他办公室找他，我心想有戏了！P.S.教授的回信，开头必加 “dear student:”，在许多连邮件都不回的导师中真是一股清流。</p><p>来到他办公室，他给我介绍了另一个项目：模拟油液管道泄漏，拍照片，提取特征，使用 cnn 训练数据然后用来判断现实中管道是否泄漏，我表示非常愿意做这个项目！但是教授说，现在有另外两个人也对这个项目感兴趣，他向学院申请是否能将名额拓展至三个，如果不能就给我做，让我先回去考虑一下，晚上给他答复。我心想这不就成了嘛！于是开开心心上课去了：）。</p><p>上完第一节课到了三点半，我心想要从 L 导师那里把双选表拿回来啊！给自己做了很久思想工作才敢去找他了，跟他说自己看上了另外一个导师的项目，想换导师，他挽留了我几次我都拒绝了，场面极其尴尬，但最后还是把表还给我了，走前还数落我确定了导师还背后联系别的导师……我心想管他呢，反正已经找好下家了：）。</p><p>拿到表准备去找 clarence 教授签字，突然看到手机上来自教授的两封未读邮件，看完内容我顿时石化……他让我在三点之前给他发封邮件确认是否愿意做这个项目，如果没回就给另外两个学生做了，而我一下午都没看手机：），飞奔到教授的办公室，他告诉我没办法了，他已经上报学院了，他没办法再要我了。</p><p>走出他的办公室，我开始联系其他导师，一个又一个地打电话，一个又一个办公室地跑，最后的结果是：所有的导师名额已经满了，我只能灰溜溜地回去找 L 导师……这时我走在学校的广场上，看着高耸的主楼，茫然无措，像个无助的孩子，感觉我这些天的所有折腾，既狼狈又滑稽，一次又一次错过所有找导师的机会，现在连之前根本不想去的导师都去不了咯，极度的绝望让我突然生出了退学的想法，想一切重新来过……</p><p>总之最后我还是回到了被我得罪了的 L 导师那里，四个没找到导师的同学被强制分到了他名下，也就是说没一个学生主动选他……我还向 L 导师道了个歉，希望以后不要被穿小鞋……</p><p>现在看来，这件事可能没我当时想象的那么严重，但是的确反映了我许多毛病：</p><ul><li><p>完美主义太严重</p><ul><li>这导致了选导师的时候拖延，总想在了解所有导师情况之后再选。</li></ul></li><li><p>好高骛远，过分自信</p><ul><li><p>总想找到最好的导师，希望他的研究方向和自己的完美契合，还希望人品好，学术水平高，而且觉得自己一定会被接收，但事实恰恰相反，自己跟大神相比相差太远，根本没有找到好导师的砝码。</p></li><li><p>也许以后找工作也是一样，只要自己够厉害了，不怕没人需要。</p></li></ul></li><li><p>做事情想太多，太纠结，不自信，缺乏勇气</p><ul><li>本来在双选会第二天就可以去找 calrence 教授，但是总纠结自己到底够不够格当他学生，语言交流有没有问题，以后如果要去加拿大怎么办等等问题，等到自己鼓起勇气的时候已经晚了。</li></ul></li><li><p>有时候太冲动</p><ul><li>不应该在当时直接签了 L 导师，也不应该在下家还没确定好就贸然把表拿回来。</li></ul></li><li><p>人云亦云，容易被他人意见左右</p><ul><li>群里有谁说哪个老师好，就跟着一窝蜂找他，大家说哪个老师不好，自己也开始先入为主。</li></ul></li><li><p>太极端，情绪化，把事情想的太严重</p><ul><li>其实很多事情没我想的那么严重，当我知道自己只能回到L导师那里之后，甚至萌生了退学重来的想法，其实现在看来，都是小事儿，但是当时居然情绪波动那么大。</li></ul></li></ul><p>通过这件事情反省一下自己，希望自己能学到一些教训。</p>]]></content>
      
      <categories>
          
          <category> 个人随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
            <tag> 研究生生活 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 3 week 2）—— 机器学习策略（二）</title>
      <link href="/2018/09/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c3w2/"/>
      <url>/2018/09/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c3w2/</url>
      <content type="html"><![CDATA[<h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p>如果你想得到一个训练算法来做人类可以做的任务，但是训练的算法还没有达到人类的效果，你就需要手动地检查算法中的错误，来知道你下一步该做什么，这个过程叫做错误分析。</p><h3 id="如何进行错误分析"><a href="#如何进行错误分析" class="headerlink" title="如何进行错误分析"></a>如何进行错误分析</h3><p>假设训练一个分类猫的算法，准确率为 90%，错误率为 10%，我们对分错的样本进行检查，发现分错的样本中有一些把狗认成了猫，这时是否应该将重心转移到使分类器对狗的表现更好？</p><p>错误分析：</p><ul><li>拿到约 <strong>100</strong> 张分类错误的开发集图片并进行手动检测<ul><li>一般不需要检测上万张图片，几百张就足够了！</li></ul></li><li>输出错误的图片里多少张狗</li></ul><p>假设只有 5 张，那么在错误图片中只有 5% 的狗，如果在狗的问题上花了大量的时间，那么就算是最好的情况，也<strong>最多</strong>只是把错误率从 10% 降到 9.5%，所以并不值得。但是假设 100 张错误图片里有 50 张狗的图片，那么可以确定很值得将时间花在狗身上，因为错误率<strong>最多</strong>可能从 10% 降到 5%.</p><a id="more"></a><p>有时候我们可以并行考虑好几个 idea 是否值得付出努力。</p><p>假设有下面几个提高猫咪分类器性能的 idea：</p><ul><li>提高对狗的识别</li><li>提高对其他猫科动物的识别</li><li>提高模型对模糊图像的性能</li></ul><p>如何评估这几个方案，可以列一张表，记录每个分类错误的图片分属于哪一个错误类别，然后计算百分比：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_1.png" alt=""></p><p>通过上面的分析我们可以知道现在应该专注于提高对模糊的图片（43%）或者猫科动物图片（61%）的准确度，但仅仅是一个参考，因为这取决于获得这些数据<strong>容不容易</strong>，假如要获得猫科动物的图片需要大量的时间，那么这一方面就可以先放放。</p><h3 id="清除错误标记（标签）"><a href="#清除错误标记（标签）" class="headerlink" title="清除错误标记（标签）"></a>清除错误标记（标签）</h3><p>当你检查数据集时发现一些输出标签 Y 标记错误，需不需要花时间修正这些标签。</p><h4 id="对于训练集"><a href="#对于训练集" class="headerlink" title="对于训练集"></a>对于训练集</h4><ul><li>深度学习算法在训练集上对于一些随机错误非常稳健，也就是说基本不需要管训练集上的一些随机的错误标记</li><li>但是注意算法对系统误差比较敏感，如果标记员一直把白色的狗标记为猫，那么训练出来的分类器也会学着这么做</li></ul><h4 id="对于开发-测试集"><a href="#对于开发-测试集" class="headerlink" title="对于开发/测试集"></a>对于开发/测试集</h4><p>推荐的方法是在错误分析时增加一列“因为标签错误而分类错误”的统计：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_2.png" alt=""></p><p>如上如图，加入标签错误是 6%，那么我们值得花时间去纠正这 6% 的样本的标签吗？</p><p>假设算法的总错误率是 10% 或 2%，考虑这两种情况下标签错误对评估的影响：</p><div class="table-container"><table><thead><tr><th>算法的总错误率</th><th>10%</th><th>2%</th></tr></thead><tbody><tr><td>由标签错误引起的错误率</td><td>0.6%</td><td>0.6%</td></tr><tr><td>由其他因素引起的错误率</td><td>9.4%</td><td>1.4%</td></tr></tbody></table></div><p>当总错误率为 10% 时，并不值得花时间去纠正这些错误的标签，因为即使纠正了也只提高 0.6% 的准确度，但是当总错误率为 2% 时，如果我们不纠正标签，那么他们引起的错误就占了一大半，这时纠正开发集中的标签错误就很有必要了。</p><h3 id="纠正开发集-测试集的原则"><a href="#纠正开发集-测试集的原则" class="headerlink" title="纠正开发集/测试集的原则"></a>纠正开发集/测试集的原则</h3><ul><li>对开发集和测试集进行一样的处理，确保他们来自相同的分布，当你纠正开发集中的一些问题，将这个过程也运用到测试集中</li><li>考虑检查你算法预测正确的和预测错误的样本</li><li>训练集 和 开发/测试集可以服从稍微不同的分布</li></ul><h3 id="建立第一个模型的建议"><a href="#建立第一个模型的建议" class="headerlink" title="建立第一个模型的建议"></a>建立第一个模型的建议</h3><ul><li>设立开发/测试集和评估指标</li><li><strong>快速地建立一个不那么复杂的初始模型</strong></li><li>使用偏差/方差分析/错误分析进行模型的迭代</li></ul><h2 id="训练集-和-开发-测试集-的不匹配"><a href="#训练集-和-开发-测试集-的不匹配" class="headerlink" title="训练集 和 开发/测试集 的不匹配"></a>训练集 和 开发/测试集 的不匹配</h2><p>深度学习算法都希望有大量的训练数据，这导致很多团队将能找到的任何数据都塞进训练集，只为有更多的训练数据，即使很多这种数据来自于与开发集和测试集不同的分布。因此在深度学习时代，越来越多的团队正在使用的，训练数据并非来自与开发集和测试集相同的分布。</p><h3 id="在不同的分布上进行训练和测试"><a href="#在不同的分布上进行训练和测试" class="headerlink" title="在不同的分布上进行训练和测试"></a>在不同的分布上进行训练和测试</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_3.png" alt=""></p><p>要训练一个猫咪分类器，目前手上有两种数据，一种是从网上爬下来的十分精美的图片 200000 个，另一种是用户上传的质量很差的图片 10000 张，这两种数据的分布差异非常大，但是我们更关心的是右边用户上传的数据，是我们的“靶子”，现在有两种数据集分配方案。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_4.png" alt=""></p><p>第一种是将所有图片混合洗牌，其中 205000 张作为训练集，剩下的 5000 张在开发/测试集之间平分。这种分法有一个很大的缺点！按照这种分法，开发集和测试集中按照比例可以算出用户上传的照片只有约 119 张，大部分都是网上爬的图片，并没有很好地反映我们所关心的数据（用户的图片），也就是说靶子放错了！</p><p>而正确的第二种分法是：将所有网上爬的 200000 张图片<strong>加上用户的 5000 张作为训练集</strong>，而剩下的用户的 5000 张平分为开发集和测试集，这样一来，你的开发集就瞄准了正确的目标。但缺点就是使得训练集分布和开发集分布有些不同，但是长期来说性能更好。</p><p>总结：</p><ul><li>在某些情况下，可以允许训练集和开发/测试集来自不同的分布，以可以获得更大的训练集</li><li>但是注意：训练集中<strong>也需要</strong>包含足够的开发集分布数据，来避免数据不匹配问题！</li></ul><h3 id="数据分布不匹配时的偏差-方差分析方法"><a href="#数据分布不匹配时的偏差-方差分析方法" class="headerlink" title="数据分布不匹配时的偏差/方差分析方法"></a>数据分布不匹配时的偏差/方差分析方法</h3><p> 当你的训练集、开发集、测试集来自不同的分布时，偏差和方差的分析方法也会相应变化。</p><h4 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h4><p>对于一个猫分类器，假设人类误差是 0，且训练集和开发集数据来自不同分布，训练集误差为 1%，开发集误差为 10%，那么它们之间 9% 的差距到底是因为 (1)模型泛化能力不足导致方差太大，还是因为 (2)训练集数据分布不同？</p><h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p>为了找出原因，我们定义一个新的数据集：训练-开发集 (training-dev set).</p><p>训练-开发集：将所有训练集数据洗牌，取出一小块作为训练-开发集，它的分布与训练集相同。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_5.png" alt=""></p><blockquote><p>分布相同：</p><ul><li>训练集 $\leftrightarrow$ 训练-开发集</li><li>开发集 $\leftrightarrow$ 测试集</li></ul></blockquote><p>划分好之后，计算出模型在如下各个数据集上的误差如下表：</p><div class="table-container"><table><thead><tr><th style="text-align:left">例子</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th></tr></thead><tbody><tr><td style="text-align:left">训练集误差</td><td style="text-align:center">1%</td><td style="text-align:center">1%</td><td style="text-align:center">10%</td><td style="text-align:center">10%</td></tr><tr><td style="text-align:left">训练-开发集误差</td><td style="text-align:center">9%</td><td style="text-align:center">1.5%</td><td style="text-align:center">11%</td><td style="text-align:center">11%</td></tr><tr><td style="text-align:left">开发集误差</td><td style="text-align:center">10%</td><td style="text-align:center">10%</td><td style="text-align:center">12%</td><td style="text-align:center">20%</td></tr></tbody></table></div><ul><li>例子1：训练集误差 1%，训练-开发集误差 9%，两者同分布都差距这么大，说明模型泛化能力不好，属于高方差问题。</li><li>例子2：在同样是在没训练过的数据集上预测，与训练集分布一样的训练-开发集误差很小，而与训练集分布不同的开发集（你所关心的）却误差很大，算法未在你所关心的分布上训练的很好，这就是<strong>数据不匹配</strong>问题 (mismatch problem)。</li><li>例子3：人类误差 0，训练集误差却高达 10%，这是明显是高偏差问题。</li><li>例子4：10% 说明高偏差；11%～20% 说明数据不匹配程度相当大。</li></ul><h4 id="train-sets-和-dev-test-sets-不匹配问题的通用的解决方案"><a href="#train-sets-和-dev-test-sets-不匹配问题的通用的解决方案" class="headerlink" title="train sets 和 dev/test sets 不匹配问题的通用的解决方案"></a>train sets 和 dev/test sets 不匹配问题的通用的解决方案</h4><p>一个模型可能的误差类型：</p><ul><li><p><strong>人类误差</strong> (训练集分布)  $&lt;\overset{\text{反映了不同分布的识别难度大小}}{===========}&gt;$ 人类误差 (开发集分布) </p><p>$\Updownarrow$ 反映可避免偏差</p></li><li><p><strong>训练集误差</strong> (训练集分布)</p><p> $\Updownarrow$ 反映方差情况</p></li><li><p><strong>训练-开发集误差</strong> (训练集分布)</p><p> $\Updownarrow$ 反映（训练集和开发集）数据不匹配情况</p></li><li><p><strong>开发集误差</strong> (开发/测试集分布)</p><p> $\Updownarrow$ 反映算法对开发集的过拟合情况，因为如果这个差距太大，则也许将神经网络</p><p> $\Updownarrow$ 调的太偏向开发集了，此时需要一个更大的开发集</p></li><li><p><strong>测试集误差</strong> (开发/测试集分布)</p></li></ul><h3 id="如何改善数据不匹配问题"><a href="#如何改善数据不匹配问题" class="headerlink" title="如何改善数据不匹配问题"></a>如何改善数据不匹配问题</h3><ul><li>人工分析误差，试着理解训练集与开发/测试集之间的差异<ul><li>举个例子，在识别车载语音识别时，可能开发集中很多车辆噪音，而训练集没有，这是由实际情况决定的</li></ul></li><li>想方设法使得训练集与之更相似：可以收集更多与开发/测试集相似的数据加入训练集，或者进行人工合成数据加入训练集，只要合成的数据让人类觉得很真实，那么就可以骗过算法<ul><li>比如模拟车载噪音数据，将训练集中干净的音频加上一段噪音合成出噪音下的说话声。但是注意：一小段<strong>重复</strong>用来合成的汽车噪音会使得学习算法对这段声音产生过拟合</li><li>再比如无人驾驶中的汽车识别，可以通过三维建模画出我们需要的汽车样子。但是注意：建模所用的汽车模型非常有限，加入只有 20 种，那么神经网络很可能对这 20 种汽车产生过拟合</li></ul></li></ul><h2 id="迁移学习-Transfer-learning"><a href="#迁移学习-Transfer-learning" class="headerlink" title="迁移学习 (Transfer learning)"></a>迁移学习 (Transfer learning)</h2><h3 id="什么叫迁移学习"><a href="#什么叫迁移学习" class="headerlink" title="什么叫迁移学习"></a>什么叫迁移学习</h3><p>深度学习中最有力的方法之一，是有时你可以把在一个任务中神经网络学习到的东西，应用到另一个任务中去。比如，你可以让神经网络学习去识别物体，比如猫，然后用学习到的（一部分）知识来帮助你更好地识别X射线的结果。这就是所谓的迁移学习。</p><h3 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h3><p>假如我们我们用猫狗的图片训练了一个如下的图片识别的神经网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_6.png" alt=""></p><p>如果我们想把这个模型应用到另一个场景，比如 x 光片的诊断，实现方法是移除这个神经网络的最后一层和其相关的权重，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_7.png" alt=""></p><p>要实现迁移学习，要做的是：</p><ul><li>将数据集换成 x 光诊断的图片</li><li>随机初始化新的最后一层的权重 W 和 偏差 b</li><li>重新训练该网络<ul><li>如果数据量小：可以只训练最后一层的参数，保留其他的参数</li><li>如果数据量大：可以训练所有层</li></ul></li></ul><p><strong>预训练 (pre training)</strong>：我们把猫狗图片训练的模型用到其他地方，那么用猫狗图片对模型的训练就称之为预训练。预训练对模型的权重进行了预初始化，其他的训练是在预初始化权重的基础上继续训练。</p><p><strong>微调 (fine tuning)</strong>：在预训练的基础上继续训练，比如上面的再用 x 光诊断图片继续训练，对权重进行更新，称之为“微调”。</p><p>有时候你也可以将最后一层移除后增加一些新的层，然后对新建的几层重新训练：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_8.png" alt=""></p><h3 id="为什么迁移学习有效？"><a href="#为什么迁移学习有效？" class="headerlink" title="为什么迁移学习有效？"></a>为什么迁移学习有效？</h3><p>是因为从从大规模的图像识别数据集中学习到的边界检测，曲线检测，明暗对象检测等低层次的信息，或许能够帮助你的学习算法更好地去进行放射扫描结果的诊断。它会在图像的组织结构和自然特性中学习到很多信息，其中一些信息会非常的有帮助。所以当神经网络学会了图像识别意味着它可能学习到了以下信息：关于不同图片的点，线，曲面等等信息，在不同图片中看起来是什么样子的。或许关于一个物体对象的很小的细节，都能够帮助你在放射信息诊断的神经网络中，学习得更快一些或者减少学习需要的数据。</p><h3 id="什么时候采用迁移学习？"><a href="#什么时候采用迁移学习？" class="headerlink" title="什么时候采用迁移学习？"></a>什么时候采用迁移学习？</h3><ul><li><p>任务 A 有着比任务 B 多得多的数据，可以从 A 迁移到 B</p><ul><li><p>例如，假设你在一个图像识别任务中拥有一百万个样本，这就意味着大量数据中的低层次特征信息或者大量的有帮助的特征信息在神经网络的前几层被学习到了。但是对于放射扫描结果的诊断任务，或许仅仅是100个 X 光扫描样本。所以你从图像识别中学习到的大量的信息可以被用于迁移并且这些信息会有效地帮助你处理好放射诊疗。</p></li><li><p>总结：只能大数据模型迁移到小数据模型，而不能小数据模型迁移到大数据模型。</p></li></ul></li><li><p>任务 A 和任务 B 有着相同的输入</p><ul><li>例如识别猫狗和诊断 X 光输入的都是图片</li></ul></li><li><p>当你认为任务 A 中的低层次特征会帮助到任务 B </p><ul><li>例如你判断两个任务有某种可以互通的特征，比如两者都是识别物体或都是语音识别</li></ul></li></ul><h2 id="多任务学习-multi-task-learning"><a href="#多任务学习-multi-task-learning" class="headerlink" title="多任务学习 (multi-task learning)"></a>多任务学习 (multi-task learning)</h2><p>在多任务学习中，几个任务一起进行，让神经网络同时做几件事情。</p><h3 id="如何实现-1"><a href="#如何实现-1" class="headerlink" title="如何实现"></a>如何实现</h3><p>在自动驾驶时拍下一张照片需要识别出照片中是否存在一些物体，如果有就置 1，否则为 0 ：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_9.png" alt=""></p><div class="table-container"><table><thead><tr><th style="text-align:center">行人</th><th style="text-align:center">0</th></tr></thead><tbody><tr><td style="text-align:center">汽车</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">交通牌</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">信号灯</td><td style="text-align:center">0</td></tr></tbody></table></div><p>于是输出标签变为（4，m）的矩阵：</p><script type="math/tex; mode=display">Y=\begin{bmatrix} |&  |&  & | & \\  y^{(1)}& y^{(2)} &,... , & y^{(m)} & \\  |& | &  & | & \end{bmatrix}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_10.png" alt=""></p><p>搭建上图中的网络进行训练，最后一层有四个单元，表示识别的四种物体的概率，那么代价函数可以表示为：</p><script type="math/tex; mode=display">J=\frac{1}{m} \sum\limits^{m}_{i=1}\sum\limits^{4}_{j=1}L(\hat y^{(i)}_j,y^{(i)}_j)</script><ul><li><p>有时候 4 个标签会有缺失（下图中的 “？”），你仍然可以进行训练，只需要省略缺失项， $\sum\limits^{4}_{j=1}$ 只对有标签的值进行求和即可</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_11.png" alt=""></p></li><li><p>该方法相当于将四个独立的神经网络合在一起，但是效果更好</p></li></ul><h3 id="什么时候适用多任务学习"><a href="#什么时候适用多任务学习" class="headerlink" title="什么时候适用多任务学习"></a>什么时候适用多任务学习</h3><ul><li>你要训练的任务共享一些低层次的特征<ul><li>例如行人、交通灯、汽车、交通牌这些物体都是道路特征</li></ul></li><li>非硬性：多任务中每个任务的数据量相当相似<ul><li>如果你集中在某一任务上，其他任务比这单一任务有多得多的数据</li></ul></li><li>如果你可以训练一个足够大的网络来做好所有的任务<ul><li>如果神经网络不够大，与单独训练每个任务相比，多任务训练会损害准确率</li></ul></li></ul><h2 id="端到端深度学习-end-to-end-deep-learning"><a href="#端到端深度学习-end-to-end-deep-learning" class="headerlink" title="端到端深度学习 (end-to-end deep learning)"></a>端到端深度学习 (end-to-end deep learning)</h2><h3 id="什么是“端到端深度学习”"><a href="#什么是“端到端深度学习”" class="headerlink" title="什么是“端到端深度学习”"></a>什么是“端到端深度学习”</h3><p>一个由许多阶段组成的学习系统，输入 x，输出 y，如果我们将其中的许多阶段用单个神经网络进行替代，直接建立从输入端 x 到输出端 y 的映射，就叫做“端到端深度学习”。</p><h3 id="一些例子"><a href="#一些例子" class="headerlink" title="一些例子"></a>一些例子</h3><ul><li><p>例子 1</p><p>加入要建立一个语音识别系统，那么一般思路为：</p><p>输入音频 x $\xrightarrow{MFCC算法}$ 提取低级特征 $\xrightarrow{ML算法}$ 找到音素 $\xrightarrow{组合}$ 单词  $\xrightarrow{剪辑}$ 输出文字 y                    </p><p>而端到端深度学习的实现方式为直接建立输入语音 x 和文字脚本 y 的映射：</p><p>输入音频 x   $\xrightarrow{ \quad \quad\quad\quad\quad\quad\quad 端到端深度学习 \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}$ 输出文字 y   </p></li><li><p>例子 2</p><p>机器翻译系统，传统机器学习思路：</p><p>英语 $\rightarrow$ 文本分析 $\rightarrow$ 提取特征 $\rightarrow$ … $\rightarrow$ 中文</p><p>端到端机器学习使用大量的英文和中文对应文本的数据集进行直接映射：</p><p>英文$\xrightarrow{ \quad \quad\quad\quad\quad 端到端深度学习 \quad\quad\quad}$ 中文 </p></li></ul><h3 id="end-to-end-DL-的优缺点"><a href="#end-to-end-DL-的优缺点" class="headerlink" title="end-to-end DL 的优缺点"></a>end-to-end DL 的优缺点</h3><ol><li>优点<ul><li>端到端数据学习让数据“说话”，从 X $\rightarrow$ Y 的映射中更好地学习到数据内在的统计学特征，而不是被迫去反映人的先见<ul><li>例如传统机器学习利用到语言中“音素”的概念，但是这个概念是是一个人造的产物，无法确定让算法使用“音素”是不是有利的</li></ul></li><li>需要更少的人工设计组件，简化工作流程</li></ul></li><li>缺点<ul><li>为了得到 X 到 Y 的映射，需要非常大量的 (X $\rightarrow$Y) 数据</li><li>它排除了一些具有潜在用途的手工设计组件<ul><li>如果数据量不够，那么一个精心手工设计的系统实际上可以向算法中注入人类关于这个问题的知识，是很有帮助的</li></ul></li></ul></li></ol><h3 id="什么时候需要运用端到端深度学习"><a href="#什么时候需要运用端到端深度学习" class="headerlink" title="什么时候需要运用端到端深度学习"></a>什么时候需要运用端到端深度学习</h3><p>关键问题：你是否有充足的数据去学习出具有能够映射 X 到 Y 所需复杂度的方程？</p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 3 week 1）—— 机器学习策略（一）</title>
      <link href="/2018/09/08/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c3w1/"/>
      <url>/2018/09/08/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c3w1/</url>
      <content type="html"><![CDATA[<p>现在来到 coursera 的 deeplearning.ai 课程的第三课，这门课程叫 Structuring Machine Learning Projects 结构化机器学习项目，将会学习到构建机器学习项目的一些策略和基本套路，防止南辕北辙。</p><h2 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化 (Orthogonalization)"></a>正交化 (Orthogonalization)</h2><h3 id="何为正交化"><a href="#何为正交化" class="headerlink" title="何为正交化"></a>何为正交化</h3><p>如同老式电视机一样，一个旋钮控制一个确定的功能，如屏幕横向伸长或纵向伸长，而不会一个旋钮控制两个属性。</p><h3 id="ML-的正交化"><a href="#ML-的正交化" class="headerlink" title="ML 的正交化"></a>ML 的正交化</h3><p>我们希望某个旋钮能单独让下面的某个步骤运行良好而不影响其他，这就叫正交化。</p><ul><li>首先训练集要拟合的很好<ul><li>否则减小偏差</li></ul></li><li>如果训练集运行良好，则希望开发集运行良好<ul><li>否则减小方差</li></ul></li><li>如果在训练集和开发集运行良好，则希望在测试集运行良好<ul><li>否则采用更大的开发集</li></ul></li><li>最后希望在真实世界表现良好<ul><li>否则调整开发集或者改变代价函数</li></ul></li></ul><a id="more"></a><p>当你发现某个步骤表现不好的时候，找到一个特定的旋钮进行调节，从而解决问题。</p><h2 id="设置目标"><a href="#设置目标" class="headerlink" title="设置目标"></a>设置目标</h2><h3 id="设置单一化的评估指标"><a href="#设置单一化的评估指标" class="headerlink" title="设置单一化的评估指标"></a>设置单一化的评估指标</h3><ul><li>精确率 precision：模型识别出是猫的集合中，有多少百分比真正是猫</li><li>召回率 recall：对于全部是猫的图片有多少能正确识别出来</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_1.png" alt=""></p><p>A、B 分类器分别有两个指标 P 和 R，各有长处，不好判断，于是可以计算它们的调和平均数，变成一个单一指标 F1 score，公式为：</p><script type="math/tex; mode=display">F1=\frac{2}{\frac{1}{P}+\frac{1}{R}}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_2.png" alt=""></p><p>再比如我们知道各个算法在不同国家的错误率，我们求这些错误率的平均值，得到一个单一指标，从而知道 C 算法性能最好。</p><h3 id="优化指标和满足指标"><a href="#优化指标和满足指标" class="headerlink" title="优化指标和满足指标"></a>优化指标和满足指标</h3><p>优化指标 optimizing metric：指这个指标越大（越小）越好，需要它表现得<strong>尽可能</strong>好，一般一个问题只需要一个优化指标，其他的是满足指标。</p><p>满足指标 satisficing metric：不需要尽可能好，只需要到达某个门槛即可，只要到达了这个门槛就不关心它的值到底是多少。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_3.png" alt=""></p><p>上图中的精确度指标是优化指标，运行时间是满足指标，假设我们设置门槛为 100ms，则首先淘汰 C 分类器，然后由于 B 的优化指标更好，我们需要精确度越大越好，所以最好的分类器是 B，也就是说我们要选出运行时间在 100ms 以内且精确度最大的分类器。</p><h3 id="开发集和测试集的设置"><a href="#开发集和测试集的设置" class="headerlink" title="开发集和测试集的设置"></a>开发集和测试集的设置</h3><h4 id="指导原则"><a href="#指导原则" class="headerlink" title="指导原则"></a>指导原则</h4><ul><li>开发集给我们树立了一个靶子，选择的开发集和测试集要能够反映出将来需要预测的数据和你认为重要的数据</li><li>开发集和测试集应该来自相同的分布</li><li>训练集和开发集可以来自不同的分布</li></ul><p>设定开发集和评估方法就像放置一个目标，然后团队通过各种尝试接近这个目标，使得模型在这个开发集和评估方法上做得更好。如果开发集和测试集来自不同的地方，当你的团队花了几个月靠拢开发集，最后用测试集做测试时，会发现表现并不好。就像让你的团队用数个月的时间瞄准一个目标，但是数个月后你说：“等等，我要测试一下，我要换个目标！”</p><p>假设现在有来自世界八个地区的数据，如果将其中四个地区的数据作为 dev sets，另外四个地区的数据作为 test sets，是完全错误的，会导致开发集和测试集的数据分布不同，正确的做法应该是将所有地区的数据混合再分成开发集和测试集，保证数据分布相同。</p><p>总结：找到将来你需要的预测的数据，同时放入开发集和测试集中，瞄准你需要的目标。</p><h4 id="数据划分比例"><a href="#数据划分比例" class="headerlink" title="数据划分比例"></a>数据划分比例</h4><ul><li>几百到上万个样本：训练集/测试集=70/30；训练集/开发集/测试集=60/20/20</li><li>上百万个样本：训练集/开发集/测试集=98/1/1<ul><li>假如一百万个样本，拿一万当开发集，一万当测试集即可</li></ul></li><li>测试集有时候可以用开发集代替，但是有测试集会更安心，而且需要足够大才能有足够的自信来评估整个模型表现</li></ul><h3 id="什么时候重新定义指标"><a href="#什么时候重新定义指标" class="headerlink" title="什么时候重新定义指标"></a>什么时候重新定义指标</h3><ol><li><p>当你的算法由于某些特殊情况影响了用户体验或者说用户有特殊的需要时</p><p>例如我们设置指标为：开发集上猫咪分类的错误率越低越好。算法 A 错误率：3%；算法 B 错误率：5%；根据我们的原有指标来看，明显算法 A 更好。但是现在算法 A 会在推荐猫咪图片时，错误地推荐一些色情图片，这对用户来说是无法忍受的，而 B 算法就不会，所以这种情况下，我们的评估指标发生了变化，B 算法更符合我们的需要。</p><p>那么如何修改评价指标？</p><script type="math/tex; mode=display">Error=\frac{1}{\sum\limits_iw^{(i)}}\frac{1}{m_{dev}}\sum\limits_{i=1}^{m_{dev}}w^{(i)}I\{y_{predict}^{(i)} \neq y^{(i)} \}\\w^{(i)}= \left\{\begin{aligned}1 && 如果x^{(i)}不是色情图片\\10 &  & 如果x^{(i)}是色情图片\end{aligned}\right.</script><ul><li>$m_{dev}$ 为开发集的个数</li><li>$y_{predict}^{(i)}$ 为第 i 个样本的预测值，取值为 1 或 0</li><li>$I\{y_{predict}^{(i)} \neq y^{(i)} \}$ 如果括号里为真，则值为 1，即错误数加 1</li><li>如果某个图片是色情图片，则权重 w=10， 将会给它产生更大的误差值，也就是说假如一个图片是色情图片，则它相当于十个错误图片，会大大提高错误率</li><li>$\frac{1}{\sum\limits_iw^{(i)}}$ 是归一化操作，使得 error 介于 0～1 之间</li></ul></li><li><p>实际情况发生了变化而当前的算法无法满足实际需要时</p><p>例如你用网上找到十分精美的图片用作训练集和开发集，来开发猫咪识别算法，但是用户使用时上传各种各样的图片，可能取景不好，可能猫没照全，可能图像模糊，也就是说实际情况发生了改变，我们的目标也发生了改变，这个时候你需要从实际情况获取更多的数据，开始修改你的指标或者开发/测试集了，让它们实际中做得更好。</p></li></ol><h3 id="机器学习的两个正交化步骤"><a href="#机器学习的两个正交化步骤" class="headerlink" title="机器学习的两个正交化步骤"></a>机器学习的两个正交化步骤</h3><ul><li>立靶子：讨论如何定义一个指标来评估分类器</li><li>射靶子：如何很好地满足这个指标</li></ul><h2 id="与人类表现相比较"><a href="#与人类表现相比较" class="headerlink" title="与人类表现相比较"></a>与人类表现相比较</h2><h3 id="为什么要和人类表现相比较"><a href="#为什么要和人类表现相比较" class="headerlink" title="为什么要和人类表现相比较"></a>为什么要和人类表现相比较</h3><ul><li>机器学习算法快速发展，慢慢在某些领域变得和人类相比有竞争力</li><li>在某些领域，机器学习效率比人类更高</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_4.png" alt=""></p><p>绿线指<strong>贝叶斯误差 (Bayers error)</strong>，它指从 x 映射到 y 的最好的理论函数，永远无法超越，不管是人类还是机器。</p><p>当机器的表现超越人类之后进展放慢，原因有：</p><ul><li>人类的误差在许多任务中与贝叶斯误差相差不远，超越人类之后没有那么大改善空间</li><li>当表现超越人类之后，就没有什么工具来进行提高了</li></ul><p>当机器的表现不如人类，我们可以用以下工具来进行提高：</p><ul><li>从人类得到标签进行学习</li><li>人类可以对偏差进行分析，改进算法</li><li>可以得到更好的偏差/方差分析，即得到一个改进的标准</li></ul><h3 id="用人类标准评估偏差"><a href="#用人类标准评估偏差" class="headerlink" title="用人类标准评估偏差"></a>用人类标准评估偏差</h3><p>方法：用人类的误差的最低值作为贝叶斯误差的一个估计值。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_5.png" alt=""></p><p>第一个例子训练集误差跟人类误差差距较大，故专注于减小偏差，第二个例子开发集误差跟训练集误差差距更大，所以专注于减小方差。吴恩达把训练集上的误差和人类误差之间的差值称之为“可回避的偏差”，也就是偏差的下限。所以左边的例子可回避偏差为 7%，右边的为 0.5%，左边的偏差可提高空间很大。</p><h3 id="更好地定义“人类表现”"><a href="#更好地定义“人类表现”" class="headerlink" title="更好地定义“人类表现”"></a>更好地定义“人类表现”</h3><p>假设看一张 x 光片，不同的人类误差如下：</p><ul><li>普通人：3%</li><li>普通医生：1%</li><li>有经验的医生：0.7%</li><li>一个有经验的医生团队讨论：0.5% $\rightarrow$ 作为贝叶斯误差的估计值</li></ul><p>那么如何定义人类表现呢？搞清楚你的目的：</p><ul><li>如果是为了超过一个普通人的水平，那么将人类表现定为 1%</li><li>如果目的是作为贝叶斯误差的代替，那么定义为 0.5% 更好</li></ul><p>举个例子：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_6.png" alt=""></p><p>第一种情况，无论人类表现定义为哪种，可避免偏差都很大，所以专注于减小偏差，第种，无论人类表现定义为哪种，都专注于减小方差，第三种情况由于训练集误差只有 0.7%，所以人类表现的最好定义是 0.5%。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_7.png" alt=""></p><p>总之：人类误差（贝叶斯误差的估计值）和训练集误差之间的的差距代表了可避免的偏差，训练集误差和开发集误差之间的差距代表了方差。</p><h3 id="ML-明显超过人类的领域"><a href="#ML-明显超过人类的领域" class="headerlink" title="ML 明显超过人类的领域"></a>ML 明显超过人类的领域</h3><ul><li>在线广告</li><li>产品推荐</li><li>物流（预测运输时间）</li><li>贷款批准</li><li>某些医学领域</li><li>某些语音识别</li><li>某些图像识别</li></ul><p>人类更擅长的是自然感知任务，包括视觉，语音识别，自然语言处理等；机器更适合处理结构化数据相关任务。</p><h2 id="如何提高模型表现"><a href="#如何提高模型表现" class="headerlink" title="如何提高模型表现"></a>如何提高模型表现</h2><ul><li>如何减小可避免偏差<ul><li>训练更大的模型</li><li>训练更久/更高的优化算法<ul><li>动量算法、RMSprop、Adam</li></ul></li><li>改变神经网络结构/超参数搜寻<ul><li>使用 RNN/CNN</li></ul></li></ul></li><li>如何减小方差<ul><li>更多数据</li><li>正则化<ul><li>L2、dropout、数据集增强</li></ul></li><li>改变神经网络架构/超参数搜寻</li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 2 week 3）—— 调参方法、批量归一化和深度学习框架</title>
      <link href="/2018/09/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w3/"/>
      <url>/2018/09/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w3/</url>
      <content type="html"><![CDATA[<p>本周主要学习超参数的系统调整，批量归一化和深度学习框架的相关知识。</p><h2 id="超参数的调整"><a href="#超参数的调整" class="headerlink" title="超参数的调整"></a>超参数的调整</h2><h3 id="调参的优先级"><a href="#调参的优先级" class="headerlink" title="调参的优先级"></a>调参的优先级</h3><ul><li>第一优先级：学习率 $\alpha$ </li><li>第二优先级：动量算法参数 $\beta$ (默认值0.9)，最小批的个数 mini-batch size，每层的隐藏单元数 hidden units</li><li>第三优先级：神经网络的层数 layers，学习率衰减 learning rate decay</li><li>几乎不需要调优：Adam 算法中的 $\beta_1,\beta_2,\varepsilon$，几乎每次只用默认值 $0.9,0.999,10^{-8}$    </li></ul><h3 id="调参的注意事项"><a href="#调参的注意事项" class="headerlink" title="调参的注意事项"></a>调参的注意事项</h3><ol><li><p>随机取样</p><p>当需要调整多个超参数时，尽量在网格中随机取样（下图右），而不是规则抽样（下图左）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_1.png" alt=""></p><a id="more"></a><p>   由于你不知道两个参数哪个更加重要，需要更多尝试。我们先假设超参数 1 更加重要：如果是左边的规则取样，那么即使取样 25 个点，实际上只尝试到了 5 个参数 1 的值，如果像右边那样随机取样，那么就可以尝试到 25 个参数 1 的值，而超参数 2 的值对结果影响并不大，所以随机取样更加合理。</p><p>   如果同时要选择三个超参数，那么就在三维空间中随机取样。</p></li><li><p>区域定位的抽样方案</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_2.png" alt=""></p><p>假设在上图的超参数样本空间中发现右下角那三个打圈的点表现更好，那么接下来对这些点所在的区域进行限定，然后在这个区域进行密度更高的随机抽样，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_3.png" alt=""></p></li><li><p>采用合适的尺度</p><p>假设现在需要调整学习率 $\alpha$，我们认为它的是一个 0.0001～1 之间的一个值，在数轴上画出它的范围：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_4.png" alt=""></p><p>如果我们在这个之间随机取样，如下图，那么将会有 90% 的样本落在 0.1～1 之间，只有 10% 的样本落在 0.001～0.1 之间，这明显是不合理的。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_5.png" alt=""></p><p>正确的方法应该是使用“对数尺度”而不是“线性尺度”来进行取样，即将数轴变成对数数轴，分成 0.0001、0.001、0.01、0.1、1 这几个部分，在每个部分之间进行均匀取样。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_6.png" alt=""></p><p>假设我们要在线性尺度的 a，b 两点之间取样， 那么就需要在对数尺度 $log_{10}^a$ 和 $log_{10}^b$ 之间进行均匀取样得到 $r$ ，那么对应的超参数就是 $10^r$.</p><p>假设现在我们要取样的是动量算法参数 $\beta$，它的合适范围在 0.9～0.999，我们可以先取样 $1-\beta$，它对应的范围在 0.1~0.001。使用上述方法取样得到 r，则最后的超参数为 $\beta=1-10^{r}$ .</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_7.png" alt=""></p><p>随着 $\beta$ 趋近于 1 ，其结果对于 $\beta$ 的改变非常敏感，例如 $\beta$ 从0.9 变成 0.9005 这没什么大不了，结果几乎不变，但是如果 $\beta$ 从 0.999 变成 0.9995，它将会对你正在运行的算法产生巨大的影响，即：在 $\beta$ 趋近于 1 的时候，你能得到更高效的样本分布，搜索超参数时更有效率。</p></li></ol><h3 id="两者超参数搜索策略"><a href="#两者超参数搜索策略" class="headerlink" title="两者超参数搜索策略"></a>两者超参数搜索策略</h3><ol><li><p>熊猫模式</p><p>精心照料某个单一的模型，通常你需要处理一个非常庞大的数据集，但没有充足的计算资源，比如没有很多CPU 没有很多GPU，那你只能一次训练一个或者非常少量的模型，这种情况下 即使枯燥的训练你也要盯着，每天都在照看你的模型，尝试微调参数，就像一个母亲一样每天精心照料着你的模型，即使是在好几天甚至几周的训练过程中都不离不弃，照看着模型，观察性能曲线，耐心地微调学习率，这通常发生在你没有足够的计算资源同时训练几个模型的情况下。就像熊猫产子一样，数量稀少，经常一次一个，但是会投入全部精力，确保孩子平安无事。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_8.png" alt=""></p></li><li><p>鱼子酱模式</p><p>并行训练许多个模型，这种情况下你可能设置一些超参数，然后让模型自己运行一天或几天，与此同时，你可能会使用完全不同的超参数，开始运行另一个不同的模型，同时，你可能开始训练第三个模型，你会同时运行许多不同的模型，用这样的方式，你就可以尝试不同的超参数设置。这可以让超参数选择变得简单，只要找一个最终结果最好的就行了。这更像鱼类的行为，为了简单易懂，称之为鱼子酱策略，有许多鱼在交配季节能产下一亿枚卵，然后无需投入太多精力去照看某个鱼卵，只希望其中一个或者一部分能够存活。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_9.png" alt=""></p></li></ol><p>如何挑选合适的模式？取决于你有多少计算资源，如果你有足够的计算机来并行训练很多模型，那不用考虑别的，采用鱼子酱模式就行了，尝试大量不同的超参数，看看结果如何。但在某些应用领域，例如在线广告设置以及计算机视觉识别，都有海量的数据和大量的模型需要去训练，而同时训练大量模型是极其困难的事情，这时只能采用熊猫模式。</p><h2 id="批量归一化-Batch-Normalization"><a href="#批量归一化-Batch-Normalization" class="headerlink" title="批量归一化 (Batch Normalization)"></a>批量归一化 (Batch Normalization)</h2><p>批量归一化可以使得你的超参数搜索变得简单，即神经网络对于超参数的选择不再那么敏感，让你更容易训练非常深的网络。</p><h3 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h3><p>之前在第二周第一课中提到对训练集的归一化，先求训练集的均值，再求训练集的方差，将训练集每个数据减去均值再处以标准差就对其进行了归一化操作。可以让代价函数从一个扁圆变成一个正圆形，提高梯度下降的效率。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_10.png" alt=""></p><p>上述只适用于没有隐藏层的逻辑回归，如果是一个层数更多的模型呢？每一层的输出又是下一层的输入，那么我们就需要将每一层的输入都进行归一化操作，否则该层以后的训练都不会很有效率。对任何一个隐藏层的输入进行归一化操作就叫做批量归一化 BN。</p><p>对于输入是在激活函数之前做归一化还是在之后做归一化存在争议，一般来说：<strong>对线性值 z 进行归一化，也就是在激活之前进行归一化</strong>。</p><h3 id="批量归一化的实现"><a href="#批量归一化的实现" class="headerlink" title="批量归一化的实现"></a>批量归一化的实现</h3><p>下面是在神经网络某一层的实现过程：</p><p>$\mu = \frac{1}{m} \sum\limits _i z^{(i)} \leftarrow 求数据的均值\\ \sigma^2 = \frac{1}{m}\sum\limits _i (z^{(i)-\mu^{}})^2 \leftarrow 求数据的方差\\ z_{norm}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}} \leftarrow 将数据归一化为正态分布\\ \tilde{z}^{(i)}=\gamma z_{norm}^{(i)}+\beta \leftarrow 变换尺度，使得数据有可控的方差和均值$  </p><ul><li>其中 $\gamma,\beta$  是模型的参数，可以从反向传播中学习，使用梯度下降来更新它们</li><li>$\varepsilon$ 是一个很小的数防止分母为零</li></ul><p>在整个神经网络上如何实现？以下面的神经网络为例。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_11.png" alt=""></p><p>对于每个小批次，都做如下的操作：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_12.png" alt=""></p><p>有效参数：$W^{[l]},\gamma^{[2]},\beta^{[l]}$ </p><p>之所以参数没有 b，是因为在进行归一化时，参数 b 在均值中被减掉，所以永远是零，于是我们的前向传播算法变为</p><p>$z^{[l]}=W^{[l]}a^{[l-1]} \\ z_{norm}^{[l]}=\frac{z^{[l]}-\mu^{[l]}}{\sqrt{ {\sigma^2}^{[l]}+\varepsilon}} \\ \tilde{z}^{[l]}=\gamma^{[l]} z_{norm}^{[l]}+\beta^{[l]}$ </p><p>其中参数 $z^{[l]},z_{norm}^{[l]},\tilde{z}^{[l]},\gamma^{[l]},\beta^{[l]}$ 的维度都是 $(n^{[l]},1)$ </p><p>批量归一化的小批量梯度下降的实现过程：</p><p>$for \quad t=1…num_minibatch:\\ \quad \quad 计算第 t 批次的前向传播\\ \quad \quad \quad \quad 在隐藏层中进行归一化，用 \tilde{z}^{[l]}代替z^{[l]} \\ \quad \quad  用反向传播计算 dW^{[l]},d\beta^{[l]},d\gamma^{[l]} (没有 db^{[l]})\\ \quad \quad 更新参数 W^{[l]}:=W^{[l]}-\alpha dW^{[l]} \\ \quad \quad \quad \quad \quad \quad \beta^{[l]}:=\beta^{[l]}-\alpha d\beta^{[l]} \\ \quad \quad \quad \quad \quad \quad \gamma ^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]}$ </p><h3 id="为什么-BN-如此有效"><a href="#为什么-BN-如此有效" class="headerlink" title="为什么 BN 如此有效"></a>为什么 BN 如此有效</h3><h4 id="协变量"><a href="#协变量" class="headerlink" title="协变量"></a>协变量</h4><p>首先要提到协变量 (Covariate)，什么是协变量？它与自变量是相对的。</p><p>自变量：指研究者主动操纵，而引起因变量发生变化的因素或条件，因此自变量被看作是因变量的原因。在机器学习中，训练的模型可以称之为自变量。 </p><p>协变量：在实验的设计中，协变量是一个独立变量(解释变量)，不为实验者所操纵，但仍影响响应。在机器学习中，模型的输入变量是协变量。</p><h4 id="协变量偏移-Covariate-Shift"><a href="#协变量偏移-Covariate-Shift" class="headerlink" title="协变量偏移 (Covariate Shift)"></a>协变量偏移 (Covariate Shift)</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_13.png" alt=""></p><p>假设我们用左边黑猫的图片训练了一个逻辑回归模型，则训练的模型是自变量，而输入的数据是协变量。因为黑猫和黄猫的颜色深浅明显是不同的，输入数据分布发生了改变，也就是说发生了协变量偏移 (Covariate Shift)，所以我们不能指望这个模型可以适用于右边的猫的图片。</p><p>即使两个完全一样的模型，如果输入变量的分布不同，得到的结果也会有很大差异，所以机器学习算法都要求输入变量在训练集和测试集上的分布是相似的。如果训练了一个模型，然而输入 x 的分布发生了变化，那么我们就得重新训练模型。</p><h4 id="神经网络中的内部协变量偏移-Internal-Covariate-Shift"><a href="#神经网络中的内部协变量偏移-Internal-Covariate-Shift" class="headerlink" title="神经网络中的内部协变量偏移 (Internal Covariate Shift)"></a>神经网络中的内部协变量偏移 (Internal Covariate Shift)</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_14.png" alt=""></p><p>在上面这个神经网络中，第二层的激活值可以当作这一层后面的神经网络的输入，从而影响第三第四第五层的参数的值，那么它就是这个神经网络的内部协变量，但是它的值并不是一成不变的，它又是前面的神经网络的参数所决定，这些参数不停地更新，导致第二层的值不停在改变，从而发生了内部协变量偏移 (Internal Covariate Shift)。</p><h4 id="BN-的作用"><a href="#BN-的作用" class="headerlink" title="BN 的作用"></a>BN 的作用</h4><p>批量归一化将每个隐藏单元的输入进行归一化，减少了隐藏单元值分布的不稳定性，虽然这些隐藏结点的值会发生变化，但是 BN 算法确保无论它怎么变化，它的均值和方差将保持不变，所以限制了先前层中参数的更新对后面的网络的输入的影响，使后层神经网络有更加稳固的基础，从而提高整个网络的学习速度。</p><p>在 BN 中，由于每个小批次都被那一批次的均值和方差归一化，因为我们在该 min-batch 上计算了均值和方差，而不是在整个数据集上计算，所以该均值和方差包含有噪声，所以某个 $\tilde z$ 也会有噪声，就像 dropout 给隐藏层增加噪声一样，所以 BN 会有轻微的正则化效果，但是它不是一个正则化方法。</p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><h3 id="多分类问题概况"><a href="#多分类问题概况" class="headerlink" title="多分类问题概况"></a>多分类问题概况</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_15.png" alt=""></p><p>假设我们要将图片分为四类，则不同的类别用不同的数字代表，比如 1 是小鸡，2 是小猫，3 是小狗，0 是其他动物，则这是一个多分类问题，分类的类别用 C 表示，在这里 C = 4.</p><p>解决这类问题，只需要将神经网络的输出层单元数变成 4，即 $n^{[L]}=4=C$，第一个单元表示图片是小鸡的概率，第二个单元表示图片是小猫的概率，第三个单元表示图片是小狗的概率，第四个单元表示图片是其他的概率，输出向量 $\hat y$ 的维度为 (4,1).</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_16.png" alt=""></p><h3 id="softmax-层的前向传播"><a href="#softmax-层的前向传播" class="headerlink" title="softmax 层的前向传播"></a>softmax 层的前向传播</h3><p>第 L 层为输出层，这一层也叫 softmax 层，这一层的运算如下：</p><p>$z^{[L]}=W^{[L]}a^{[l-1]}+b^{[L]}  \\  激活函数：\\ t=e^{(z^{[L]})} \\ \hat y = a^{[L]}=\frac{t}{\sum\limits _{i=1}^4 t_i},a^{[L]}_i=\frac{t_i}{\sum\limits _{i=1}^4 t_i}$  </p><ul><li>$z^{[L]},t,\hat y$ 的维度都是 (4,1) 向量</li><li>i 代表第 L 的第 i 个结点</li></ul><p>把上面的运算当成一个激活函数：</p><script type="math/tex; mode=display">a^{[L]}=g^{[L]}(z^{[L]})\\g^{[L]}=softmax()</script><ul><li>softmax() 输入一个向量，输出一个尺寸一样的向量</li></ul><p>下面是在逻辑回归中将输入通过 softmax 的线性分类结果：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_18.png" alt=""></p><p>那么 softmax 的实质是什么？</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_19.png" alt=""></p><p>实际上 softmax 将一组向量映射成了它们的概率向量，这组向量中某个值越大，它通过 softmax 映射的概率值就越大。与 softmax 对应的是 hardmax，它也是将输入向量映射为另一个向量，但它是直接找出输入向量的最大值并将它置为 1，其他置为 0，非常简单粗暴，而 softmax 的映射就更加平和。同时 softmax 将逻辑回归中的二分类推广到 C分类，也就是多分类，当 C=2 的时候，softmax 就简化为逻辑回归。</p><h3 id="softmax-的代价函数"><a href="#softmax-的代价函数" class="headerlink" title="softmax 的代价函数"></a>softmax 的代价函数</h3><p>假设某个样本的标签值 $y=\begin{bmatrix} 0\\1\\0\\0 \end{bmatrix}$，预测值 $\hat y=\begin{bmatrix} 0.3\\0.2\\0.1\\0.4 \end{bmatrix}$，用下列公式计算单个样本代价函数：</p><script type="math/tex; mode=display">L(\hat y,y)=-\sum\limits_{j=1}^4 y_jlog\hat y_j</script><p>所有样本的总代价函数为：</p><script type="math/tex; mode=display">J(W^{[1]},b^{[1]},...)=\frac{1}{m}\sum\limits_{i=1}^m L(\hat y,y)</script><ul><li>实际上 $y,\hat y$ 应该是 $Y,\hat Y$ ，是一个维度为 （C，m）的矩阵</li></ul><p>简单的证明：因为 $y_2=1,y_1=y_3=y_4=0$，则 $L(\hat y,y)=-\sum\limits_{j=1}^4 y_jlog\hat y_j=-y_2log \hat y_2=-log \hat y_2$，假设代价函数越小，则 $log \hat y_2$ 越大，则 $\hat y_2$ 越大，则图片的标签值为 2 的概率越大，而这张图片的实际标签值就是 2。于是我们建立逻辑链条：代价函数越小 $\rightarrow$ 预测这张图片标签值是 2 的概率 $\hat y_2$ 越大 $\rightarrow$ 图片真实标签值就是 2 $\rightarrow$ 越符合实际情况 $\rightarrow$ 模型预测越准确 </p><h3 id="softmax-的反向传播"><a href="#softmax-的反向传播" class="headerlink" title="softmax 的反向传播"></a>softmax 的反向传播</h3><p>初始化公式为：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial z^{[L]}}=dz^{[L]}=\hat y -y</script><ul><li>其中 $dz^{[L]},\hat y,y$ 都是 （C，1）的向量</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 超参数 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 2）</title>
      <link href="/2018/09/04/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c2w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/09/04/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c2w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<p>本次作业实现了几种梯度下降算法，比较了它们的不同。</p><h2 id="普通梯度下降-BGD"><a href="#普通梯度下降-BGD" class="headerlink" title="　普通梯度下降 (BGD)"></a>　普通梯度下降 (BGD)</h2><p>所谓普通梯度下降就是一次处理所有的 m 个样本，也叫批量梯度下降 (Batch Gradient Descent)，公式为：</p><script type="math/tex; mode=display">W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\\b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_gd</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    普通梯度下降</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters to be updated:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients to update each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 参数的个数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] =parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="随机梯度下降-Stochastic-Gradient-Descent-SGD"><a href="#随机梯度下降-Stochastic-Gradient-Descent-SGD" class="headerlink" title="随机梯度下降 Stochastic Gradient Descent (SGD)"></a>随机梯度下降 Stochastic Gradient Descent (SGD)</h2><p>SGD 一个只处理一个样本进行梯度下降，速度比 GD 快，但是在朝着最小值行进的过程中会发生震荡，而 GD 是平滑地稳步向最小值走。GD 是考虑周全再行动，每一步都朝着全局最优走，所以很慢，而 SGD 是走一步看一步，并不是每一步的迭代都朝着全局最优的方向走，噪声很多，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_1.png" alt=""></p><p>SGD 算法中有三个 for 循环：</p><ul><li>迭代次数的循环</li><li>所有 m 个训练样例的循环</li><li>神经网络所有层的循环</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):<span class="comment"># 遍历所有的 m 个训练样本</span></span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure><h2 id="小批量梯度下降-Mini-Batch-Gradient-descent-MBGD"><a href="#小批量梯度下降-Mini-Batch-Gradient-descent-MBGD" class="headerlink" title="小批量梯度下降 Mini-Batch Gradient descent (MBGD)"></a>小批量梯度下降 Mini-Batch Gradient descent (MBGD)</h2><p>由于 BGD 和 SGD 是两个极端，如果一次既不处理一个样本，也不处理所有样本，而处理介于两者之间的样本数，即一次处理整个训练样本的一小批，学习速度会更快。</p><p>MBGD 比 SGD 的震荡更小：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/l_cdoe_2.2_2.png" alt=""></p><h3 id="划分-mini-batch-的步骤"><a href="#划分-mini-batch-的步骤" class="headerlink" title="划分 mini -batch 的步骤"></a>划分 mini -batch 的步骤</h3><p>一共有两步：</p><ul><li><p>洗牌 (shuffle)：将训练集 （X，Y）进行随机的洗牌，打乱每一列的顺序，确保所有的样本会被随机分成不同的小批次。注意 X 和 Y 需要进行一样的洗牌操作，一一对应，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_3.png" alt=""></p></li><li><p>划分 (Partition)：将训练集划分为每个大小为 mini_batch_size 的小批次。注意总样本数不一定总是能被 mini_batch_size 整除，所以最后一个小批次比 mini_batch_size 要小，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_4.png" alt=""></p></li></ul><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size = <span class="number">64</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将总样本随机划分为许多小批次</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- 存放划分好的(mini_batch_X, mini_batch_Y)的 list</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]<span class="comment"># 总样本数</span></span><br><span class="line">    mini_batches = []<span class="comment"># 存放划分好的最小批</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 第一步：洗牌</span></span><br><span class="line">    permutation = list(np.random.permutation(m))   <span class="comment"># 生成 [0，1，2，...,m-1] 的数组并随机打乱</span></span><br><span class="line">    shuffled_X = X[:, permutation]<span class="comment"># 将打乱的数组作为列标签，使得 X 的每列被随机打乱</span></span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))<span class="comment"># 同上</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二步：划分</span></span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># math.floor() 取整函数计算完整的 mini_batch 数目</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):<span class="comment"># 注意是从 0 开始，所以下面都是 k+1</span></span><br><span class="line">        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+<span class="number">1</span>)* mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+<span class="number">1</span>)* mini_batch_size]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)<span class="comment"># 组合成一个 tuple</span></span><br><span class="line">        mini_batches.append(mini_batch)<span class="comment"># 将划分好的 tuple 放入 list 中</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 单独处理最后一个数量小于 mini_batch_size 的批次</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size:]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size:]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure><h2 id="动量梯度下降"><a href="#动量梯度下降" class="headerlink" title="动量梯度下降"></a>动量梯度下降</h2><p>由于小批量梯度下降也会产生震荡问题，所以我们采用动量算法，考虑之前几次迭代的梯度来减小震荡。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_5.png" alt=""></p><p>蓝色线是每一次梯度的方向，但是每次下降不沿着梯度方向走，而是沿着红线走，它是前几次梯度方向的加权平均值 v ——称之为“速度”。</p><script type="math/tex; mode=display">\begin{cases}v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \\W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}\end{cases}\\\begin{cases}v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \\b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}} \end{cases}</script><h3 id="初始化-v"><a href="#初始化-v" class="headerlink" title="初始化 v"></a>初始化 v</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_velocity</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes the velocity as a python dictionary with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity.</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 神经网络层数</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))   <span class="comment"># v_dW 和 dW 的维度相同</span></span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))   <span class="comment"># v_db 和 db 的维度相同</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用动量算法更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- 初始化过的 v</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># 神经网络层数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 v</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure><h2 id="Adam-算法"><a href="#Adam-算法" class="headerlink" title="Adam 算法"></a>Adam 算法</h2><p>Adam 是训练神经网络最有效的算法之一，结合了动量算法和 RMSprop 算法的优点。</p><p>一共三步：</p><ul><li>计算之前梯度的指数加权平均 v，然后其计算偏差修正值 v_correct</li><li>计算之前梯度的平方的指数加权平均 s，然后计算其偏差修正值 s_correct</li><li>用上面的值更新参数</li></ul><p>公式：</p><script type="math/tex; mode=display">\begin{cases}v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \\v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}}} + \varepsilon}\end{cases}</script><h3 id="初始化-v-和-s"><a href="#初始化-v-和-s" class="headerlink" title="初始化 v 和 s"></a>初始化 v 和 s</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span> :</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   初始化 v 和 s，他们的维度都和对应的梯度相同</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters["W" + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters["b" + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class="line"><span class="string">                    v["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class="line"><span class="string">                    s["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    s["db" + str(l)] = ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> </span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))</span><br><span class="line">        s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))</span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure><h3 id="用-v-和-s-更新参数"><a href="#用-v-和-s-更新参数" class="headerlink" title="用 v 和 s 更新参数"></a>用 v 和 s 更新参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate = <span class="number">0.01</span>,beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用 Adam 算法更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有参数的字典</span></span><br><span class="line"><span class="string">    grads -- 包含所有参数梯度的字典</span></span><br><span class="line"><span class="string">    v -- 初始化过的 v</span></span><br><span class="line"><span class="string">    s -- 初始化过的 s</span></span><br><span class="line"><span class="string">    t -- 当前的批次</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class="line"><span class="string">    epsilon -- 防止分母为零的超参数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 参数个数              </span></span><br><span class="line">    v_corrected = &#123;&#125;<span class="comment"># 用来存放 v_correct</span></span><br><span class="line">    s_corrected = &#123;&#125;<span class="comment"># 用来存放 s_correct</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 Adam 算法</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment"># 计算 v 值</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算偏差修正过的 v 值</span></span><br><span class="line">        v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span> - beta1 ** t)<span class="comment"># beta ** t 表示 beta 的 t 次幂 </span></span><br><span class="line">        v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span> - beta1 ** t)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 计算 s 值</span></span><br><span class="line">        s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta2) * np.square(grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)])<span class="comment"># np.square() 表示逐元素平方</span></span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta2) * np.square(grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算偏差修正过的 s 值</span></span><br><span class="line">        s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line">        s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * ( v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (np.sqrt(s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]) + epsilon))<span class="comment"># np.sqrt() 为逐元素开平方</span></span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * ( v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (np.sqrt(s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]) + epsilon))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure><h2 id="比较几种优化算法的区别"><a href="#比较几种优化算法的区别" class="headerlink" title="比较几种优化算法的区别"></a>比较几种优化算法的区别</h2><ul><li>使用普通的批量梯度下降，调用函数：<ul><li>update_parameters_with_gd()</li></ul></li><li>使用动量算法梯度下降，调用函数：<ul><li>initialize_velocity() 和 update_parameters_with_momentum()</li></ul></li><li>使用 Adam 梯度下降，调用函数：<ul><li>initialize_adam() 和 update_parameters_with_adam()</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, layers_dims, optimizer, learning_rate = <span class="number">0.0007</span>, mini_batch_size = <span class="number">64</span>, beta = <span class="number">0.9</span>,beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>, epsilon = <span class="number">1e-8</span>, num_epochs = <span class="number">10000</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    可以使用不同优化函数的 3 层神经网络</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- python list, containing the size of each layer</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    mini_batch_size -- the size of a mini batch</span></span><br><span class="line"><span class="string">    beta -- Momentum hyperparameter</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(layers_dims)             </span><br><span class="line">    costs = []                       </span><br><span class="line">    t = <span class="number">0</span>                           </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化优化函数</span></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># 普通梯度下降没有初始化要求</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">        v = initialize_velocity(parameters)<span class="comment"># 初始化动量算法</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)<span class="comment"># 初始化 Adam 算法</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化算法循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        </span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size)<span class="comment"># 划分最小批</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不同批次的循环</span></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 从 minibatchs 中取出当前最小批</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 前向传播</span></span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算代价函数</span></span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数，三种方式</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)<span class="comment"># 批量（普通）梯度下降更新参数</span></span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)<span class="comment"># 动量梯度下降更新参数</span></span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># 当前的批次，Adam 算法与 t 有关</span></span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每一千步打印一次代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 代价函数画图</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'epochs (per 100)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="使用-MBGD"><a href="#使用-MBGD" class="headerlink" title="使用 MBGD"></a>使用 MBGD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"gd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Gradient Descent optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_6.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_7.png" alt=""></p><h3 id="使用动量算法"><a href="#使用动量算法" class="headerlink" title="使用动量算法"></a>使用动量算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">"momentum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Momentum optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_8.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_9.png" alt=""></p><h3 id="使用-Adam-算法"><a href="#使用-Adam-算法" class="headerlink" title="使用 Adam 算法"></a>使用 Adam 算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Adam optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_10.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_11.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_12.png" alt=""></p><ul><li>动量算法经常会有帮助，但是在小学习率和简单的的数据集中它的影响几乎可以忽略。</li><li>代价函数的震荡是因为某些小批次的数据噪声更多</li><li>Adam 算法比动量算法和 MBGD 表现精确度更高，但是如果给足够的时间，三个算法都会得到很好的精确度，但是这说明 Adam 算法运行更快</li><li>Adam 算法的优点<ul><li>虽然比动量算法和 MBGD 所需的内存更多，但是相对来说所需内存还是较少</li><li>几乎不需要怎么调整它的超参数 $\beta_1,\beta_2,\varepsilon$ ，直接使用默认值，就能得到很好的结果</li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 优化算法 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 2 week 2）—— 优化算法的改进</title>
      <link href="/2018/09/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w2/"/>
      <url>/2018/09/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w2/</url>
      <content type="html"><![CDATA[<p>本周主要学习不同的参数优化方式，提高模型的训练速度。</p><h2 id="小批量梯度下降算法-mini-batch-gradient-descent"><a href="#小批量梯度下降算法-mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降算法 (mini-batch gradient descent)"></a>小批量梯度下降算法 (mini-batch gradient descent)</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><p>向量化可以有效率地同时计算 m 个样本，但是当样本数为几百万时，速度依然会很慢，每一次迭代都必须先处理几百万的数据集才能往前一步，所以我们可以使用这个算法进行加速。</p><p>首先我们将训练集划分为一个一个微小的训练集，也就是小批量训练集 (mini-batch)，比如每一个微小训练集只有 1000 个样本：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_2.png" alt=""></p><a id="more"></a><p>我们把第 t 个批次的数据用上标 {t} 表示，例如 $X^{\{t\}},Y^{\{t\}}$ </p><p>$for \quad t = 1,…,5000: (每一步用 (X^{\{t\}},Y^{\{t\}}) 做一次梯度下降)\\ \quad X^{\{t\}} 为输入的前向传播 (1000 个样例)\\ \quad 计算代价函数 J^{\{t\}}\\ \quad 反向传播计算梯度 (只用  X^{\{t\}} 和 Y^{\{t\}})\\ \quad 更新参数 $   </p><p>以上是对训练集的一次迭代，可以得到 5000 次梯度逼近，继续使用另一个 for 循环进行多次迭代。</p><h3 id="理解算法"><a href="#理解算法" class="headerlink" title="理解算法"></a>理解算法</h3><p>小批量梯度下降 (mini-batch gradient descent) 和批量梯度下降 (batch gradient descent) 中代价函数的变化如下图：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_3.png" alt=""></p><p>梯度下降有三种类型，根据每个批次的样本数不同划分：</p><ul><li>若 minibatch 尺寸 = m，称之为：批量梯度下降 (BGD) ( X{t} , Y{t} ) = ( X , Y ) </li><li>若 minibatch 尺寸 = 1，称之为：随机梯度下降 (SGD)  ( X{t}, Y{t} ) = ( X{1}, Y{1} ), ( X{2}, Y{2} ) ,…, ( X{m}, Y{m} ) </li><li>若 minibatch 尺寸介于 1 到 m 之间：小批量梯度下降 (MBGD)</li></ul><p>在实际中一般采用第三种，因为第一种和第二种都有缺点：</p><ul><li>随机梯度下降失去了使用向量加速的机会，而且会发生震荡，但是速度比批量梯度下降快</li><li>批量梯度下降的缺点是如果训练集太大，在每次的迭代上要花费太长的时间</li><li>小批量梯度下降，则既可以利用到向量化，又可以不用等待整个训练集都遍历一遍就可以进行梯度下降，而且震荡要比随机梯度下降要小，是最好的方案</li></ul><p>下图展示了不同梯度下降的过程，蓝线是批量梯度下降，绿线是小批量梯度下降，紫线是随机梯度下降。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_4.png" alt=""></p><h3 id="如何选择-mini-batch-的-size"><a href="#如何选择-mini-batch-的-size" class="headerlink" title="如何选择 mini-batch 的 size"></a>如何选择 mini-batch 的 size</h3><p>最小批包含的样本数是一个可调的超参数，用如下原则来确定：</p><ul><li>如果训练集很小（m &lt; 2000）：直接使用批量梯度下降</li><li>如果训练集很大：一般选择 <strong>64 ～ 512</strong> 作为每个批次的大小，由于计算机内存的布局和访问方式，把它设为 2 的幂数代码运行会更快，故一般选择 <strong>64，126，256，512</strong> 这几个值</li><li>确保 minibatch 的 X{t} 和 Y{t} 能够放进 CPU 和 GPU 的内存</li></ul><h2 id="指数加权平均-Exponentially-weighted-averages"><a href="#指数加权平均-Exponentially-weighted-averages" class="headerlink" title="指数加权平均 (Exponentially weighted averages)"></a>指数加权平均 (Exponentially weighted averages)</h2><h3 id="什么是指数加权平均"><a href="#什么是指数加权平均" class="headerlink" title="什么是指数加权平均"></a>什么是指数加权平均</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_5.png" alt=""></p><p>上图表示的是伦敦一年 365 天气温的散点图，第 t 天的气温值为 $\theta_t$，给出如下公式计算<strong>指数加权平均</strong> (统计学中称为“指数加权滑动平均”)：</p><script type="math/tex; mode=display">v_t=\beta v_{t-1}+(1-\beta)\theta_t</script><p>其中 $v_t$ 可以近似认为是前 $\frac{1}{1-\beta}$ 天的气温平均值，例如：</p><ul><li>若 $\beta = 0.9$ 则 $v_t$ 可认为计算的是前 10 天的平均气温值，如红线所示</li><li>若 $\beta = 0.98$ 则 $v_t$ 可认为计算的是前 50 天的平均气温值，如绿线所示</li><li>若 $\beta = 0.5$ 则 $v_t$ 可认为计算的是前 2 天的平均气温值，如黄线所示</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_6.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_7.png" alt=""></p><h3 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_8.png" alt=""></p><h3 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h3><script type="math/tex; mode=display">v_0=0\\v_1=\beta v_0+(1-\beta)\theta_1\\v_2=\beta v_1+(1-\beta)\theta_2\\v_3=\beta v_2+(1-\beta)\theta_3\\...</script><p>伪代码：</p><p>$v_\theta=0\\repeat\{\\ \quad \quad \quad get \ next \ \theta_t \\  \quad \quad \quad v_\theta:=\beta v_\theta+(1-\beta)\theta_t\\ \quad \quad \quad \}$ </p><h3 id="小技巧：偏差修正"><a href="#小技巧：偏差修正" class="headerlink" title="小技巧：偏差修正"></a>小技巧：偏差修正</h3><p>当 $\beta=0.98$ 时，我们预计会得到绿色那条线： </p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_6.png" alt=""></p><p>但是实际上我们会得到下图紫色那条线：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_9.png" alt=""></p><p>根据公式 $v_t=\beta v_{t-1}+(1-\beta)\theta_t$ ：<br>$v_0=0\\v1=0.98v_0+0.02\theta_1=0.02\theta_1\\v_2=0.98v_1+0.02\theta_2=0.98*0.02\theta_1+0.02\theta_2$</p><p>可以看到 $v_2$ 的值将远小于 $\theta_1$ 和 $\theta_2$ ，造成估计的偏差，有一种方法可以在估算的初期将偏差修正：</p><script type="math/tex; mode=display">v_t=\frac{v_t}{1-\beta^t}</script><p>$t=2:1-\beta ^t=1-0.98^2=0.0396\\\frac{v_2}{0.0396}=\frac{0.0196 \theta_1 + 0.02\theta_2}{0.0396} \rightarrow \theta_1 和 \theta_2的加权平均数，从而消除了偏差$  </p><p>当 $t​$ 逐渐增大，$\beta^t​$ 的值将趋于 0，修正偏差基本无影响，所以绿线和紫线在后半段几乎重合。</p><h2 id="动量梯度下降算法-Gradient-descent-with-momentum"><a href="#动量梯度下降算法-Gradient-descent-with-momentum" class="headerlink" title="动量梯度下降算法 (Gradient descent with momentum)"></a>动量梯度下降算法 (Gradient descent with momentum)</h2><h3 id="普通梯度下降的困境"><a href="#普通梯度下降的困境" class="headerlink" title="普通梯度下降的困境"></a>普通梯度下降的困境</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_10.png" alt=""></p><ul><li>红点是梯度下降最小点，对于普通的梯度下降，路径如蓝线所示，朝着最小值缓慢地震荡前进，这种震荡会减慢学习的速度</li><li>不能使用太大的学习率，否则会产生超调，产生发散，无法收敛到最小点</li><li>在上下方向希望学习慢一点，因为不希望出现那些震荡，水平方向希望学习快一点，因为希望快速从左到右</li></ul><h3 id="动量梯度下降步骤"><a href="#动量梯度下降步骤" class="headerlink" title="动量梯度下降步骤"></a>动量梯度下降步骤</h3><p>$v_{dW}=0(维度和dW相同)\\v_{db}=0(维度和db相同)\\在第 t 次迭代：\\ \quad \quad 计算当前小批次的 dW，db \\ \quad \quad v_{dW}=\beta v_{dW}+(1-\beta)dW \\ \quad \quad v_{db}=\beta v_{db}+(1-\beta)db \\ \quad \quad W:=W-\alpha v_{dW},b:=b-\alpha v_{db} $    </p><ul><li>超参数 $\alpha, \beta$ ， 其中 $\beta = 0.9$ 效果最好，相当于计算前十次迭代的梯度平均值</li><li>一般不需要对 $v_{dW},v_{db}$ 进行偏差修正 </li><li>另一个版本为 $v_{dW}=\beta v_{dW}+dW,v_{db}=\beta v_{db}+db$ 也是正确的，但吴恩达不推荐</li></ul><h3 id="算法解释"><a href="#算法解释" class="headerlink" title="算法解释"></a>算法解释</h3><p>解释：这些操作可以让梯度下降变得平滑，因为 $v_{dW}$ 计算的是这一次迭代之前的若干次的梯度的平均值，震荡的路径在纵轴方向是 $\nearrow \searrow \nearrow \searrow$ ，这些梯度一正一负，相互抵消，在纵轴方向的平均值趋于 0 ，减弱了震荡，而横轴方向是 $\rightarrow \rightarrow\rightarrow\rightarrow$ ，都指向了右边，所以横轴方向的平均值依然很大。</p><p>形象解释：把代价函数图像想象成一个碗，有一个球滑向碗的最低点，把导数项 dW 和 db 想象成球滚下时的加速度，而把动量项 $v_{dW},v_{db}$ 想象成球的速度。</p><p>导数项给了球一个加速度，然后球向下滚，因为有加速度，所以它滚得越来越快，因为$\beta$ 是一个略小于1的数，可以把它看作摩擦力，让球不至于无限加速下去。与梯度下降中每一步都独立于之前步骤所不同的是，现在你的球可以向下滚并获得动量，沿碗向下加速并获得动量。</p><h2 id="均方根传递梯度下降算法——RMSprop-算法-Root-Mean-Square-prop"><a href="#均方根传递梯度下降算法——RMSprop-算法-Root-Mean-Square-prop" class="headerlink" title="均方根传递梯度下降算法——RMSprop 算法 (Root Mean Square prop)"></a>均方根传递梯度下降算法——RMSprop 算法 (Root Mean Square prop)</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_11.png" alt=""></p><p>仍然是这个震荡的问题，为了便于理解，假设只有两个参数 w 和 b，我们可以看到要减小震荡，必须使得纵轴即参数 b 的学习速度很慢，而横轴即参数 w 的学习速度很快，我们仍然使用指数加权平均的知识来实现梯度下降。</p><h3 id="RMSprop-实现步骤"><a href="#RMSprop-实现步骤" class="headerlink" title="RMSprop 实现步骤"></a>RMSprop 实现步骤</h3><p>$ S_{dW}=0(维度和dW相同)\\S_{db}=0(维度和db相同)\\在第 t 次迭代：\\ \quad \quad 计算当前小批次的 dW，db \\ \quad \quad S_{dW}=\beta’ S_{dW}+(1-\beta’)(dW)^2 \\ \quad \quad S_{db}=\beta’ S_{db}+(1-\beta’)(db)^2 \\ \quad \quad W:=W-\alpha \frac{dW}{\sqrt{S_{dW}}+\varepsilon },b:=b-\alpha \frac{db}{\sqrt{S_{db}}+\varepsilon } $ </p><ul><li>$\beta’$ 是为了和动量算法中的  $\beta$  相区分</li><li>$(dW)^2$ 是逐元素平方</li><li>$\varepsilon$ 是一个非常小的数比如 $10^{-8}$ 确保分母不为零</li></ul><h3 id="算法理解"><a href="#算法理解" class="headerlink" title="算法理解"></a>算法理解</h3><p>如上图所示，我们希望纵轴参数 b 的梯度很小，学习更慢，减小震荡，希望横轴参数 w 的梯度很大，学习更快，也就是希望作为分母的 $\sqrt{S_{dW}}$ 很小，而 $\sqrt{S_{db}}$ 很大。由于纵轴方向上是震荡的，路径更加偏向 b 轴一些，所以 $(db)^2$ 会相对更大，从而$\sqrt{S_{db}}$ 会相对更大，而 $(dW)^2$ 会相对更小，从而 $\sqrt{S_{dW}}$ 相对更小。</p><h2 id="自适应矩估计梯度下降算法——Adam-算法-Adaptive-Moment-Estimation"><a href="#自适应矩估计梯度下降算法——Adam-算法-Adaptive-Moment-Estimation" class="headerlink" title="自适应矩估计梯度下降算法——Adam 算法 (Adaptive Moment Estimation)"></a>自适应矩估计梯度下降算法——Adam 算法 (Adaptive Moment Estimation)</h2><p>这是一种结合了动量算法和 RMSprop 两者优点的算法，被广泛使用且已经被证明在很多不同种类的神经网络架构中都十分有效。</p><h3 id="Adam-实现步骤"><a href="#Adam-实现步骤" class="headerlink" title="Adam 实现步骤"></a>Adam 实现步骤</h3><p>$v_{dW}=0,S_{dW}=0,v_{db}=0,S_{db}=0   \leftarrow  初始化\\ 在第 t 次迭代：\\ \quad  计算当前小批次的 dW，db \\   \quad v_{dW}=\beta_1 v_{dW}+(1-\beta_1)dW  ，v_{db}=\beta_1 v_{db}+(1-\beta_1)db \leftarrow  动量算法\\ \quad S_{dW}=\beta_2 S_{dW}+(1-\beta_2)(dW)^2，S_{db}=\beta_2 S_{db}+(1-\beta_2)(db)^2 \leftarrow RMSprop\\  \quad v_{dW}^{correct}=\frac{v_{dW}}{1-\beta_1^t}，v_{db}^{correct}=\frac{v_{db}}{1-\beta_1^t}   \leftarrow v 的偏差修正   \\ \quad  S_{dW}^{correct}=\frac{S_{dW}}{1-\beta_2^t}，S_{db}^{correct}=\frac{v_{db}}{1-\beta_2^t}     \leftarrow S 的偏差修正   \\ \quad W:=W-\alpha \frac{v_{dW}^{correct}}{\sqrt{S_{dW}^{correct}}+\varepsilon },b:=b-\alpha \frac{v_{db}^{correct}}{\sqrt{S_{db}^{correct}}+\varepsilon }   \leftarrow 更新参数$   </p><h3 id="超参数的默认值"><a href="#超参数的默认值" class="headerlink" title="超参数的默认值"></a>超参数的默认值</h3><ul><li>学习率 $\alpha$ ：尝试不同值比较效果</li><li>$\beta_1：0.9$   </li><li>$\beta_2:0.999$ </li><li>$\varepsilon:10^{-8}$    </li></ul><blockquote><p>参数为 $\beta_1$ 的梯度值的指数加权平均 $v$ 称为第一阶的矩，参数为 $\beta_2$ 的梯度平方值的指数加权平均 $S$ 被称为第二阶的矩</p></blockquote><h2 id="学习率衰减-learning-rate-decay"><a href="#学习率衰减-learning-rate-decay" class="headerlink" title="学习率衰减 (learning rate decay)"></a>学习率衰减 (learning rate decay)</h2><h3 id="为什么需要学习率衰减"><a href="#为什么需要学习率衰减" class="headerlink" title="为什么需要学习率衰减"></a>为什么需要学习率衰减</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_12.png" alt=""></p><p>假如我们采用固定步长，如蓝色路径所示，那么它会逐步向最小值靠近，但不会完全收敛到这点，所以算法会在最小值周围浮动，但是却永远不会真正收敛。</p><p>如果慢慢地衰减学习率，刚开始学习率取值较大，学习速度较快，但随着学习率的的逐渐衰减，步长也会逐渐减小，所以最后将围绕着离极值点更近的的区域摆动，不会继续漂流远离。</p><h3 id="如何实现-1"><a href="#如何实现-1" class="headerlink" title="如何实现"></a>如何实现</h3><p>1 次迭代（1 epoch）：遍历完一次数据集，完成一次梯度更新</p><p>学习率衰减就是使学习率随着迭代次数 epoch _num 单调递减，比如下面这个公式：</p><script type="math/tex; mode=display">\alpha = \frac{1}{1+decay\_rate*epoch\_num}\alpha_0</script><ul><li>decay_rate 为衰减率，是一个超参数</li><li>epoch_num 是迭代次数</li><li>$\alpha_0$ 是初始学习率，也是一个超参数</li></ul><p>代一些具体值看看学习率衰减的情况：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_13.png" alt=""></p><p>还有一些其他的学习率衰减公式，比如下面这个：</p><script type="math/tex; mode=display">\alpha = 0.95 ^{epoch\_num}\alpha_0</script><ul><li>使得学习率呈指数级下降</li></ul><p>还有：</p><script type="math/tex; mode=display">\alpha = \frac{k}{\sqrt{epoch\_num}}\alpha_0 \ \ 或者\frac{k}{\sqrt{t}}\alpha_0</script><p>还有采用离散阶梯函数的学习率衰减，比如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_14.png" alt=""></p><p>有时候人们还会使用手动的梯度衰减，但是比较少。</p><h2 id="局部最优问题-the-problem-of-local-optima"><a href="#局部最优问题-the-problem-of-local-optima" class="headerlink" title="局部最优问题 (the problem of local optima)"></a>局部最优问题 (the problem of local optima)</h2><p>深度学习早期，人们担心优化算法会陷入局部最优之中。</p><h3 id="什么是局部最优"><a href="#什么是局部最优" class="headerlink" title="什么是局部最优"></a>什么是局部最优</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_15.png" alt=""></p><p>假设上图中是参数 w1 和 w2关于代价函数的三维图像，人们担心在对这些参数进行优化的时候会优化到图中蓝点所在的位置，这些值只是在“谷底”，而不是真正的最小值，是局部最优。</p><h3 id="为什么不用担心局部最优问题"><a href="#为什么不用担心局部最优问题" class="headerlink" title="为什么不用担心局部最优问题"></a>为什么不用担心局部最优问题</h3><p>在训练一个神经网络时，代价函数中大部分梯度为零的点并不是上图中的局部最优，而是<strong>鞍点 (saddle point)</strong>，这种点周围的函数从一个方向看是凸函数，从另一个是凹函数，就像马鞍一样，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_16.png" alt=""></p><p>如果一个点要是局部最优点，那么这个点周围的函数需要在所有方向看都是凸函数或者都是凹函数，但是由于神经网络的参数有上万乃至上百万个，如果某个点要成为局部最优，那么在所有的上百万个维度中函数的方向都得是一样的凹函数或者凸函数，这件事发生的概率非常非常低。更有可能碰到的是鞍点，某些方向的曲线向上弯曲，某些方向的向下弯曲，并非所有的曲线都向上或者向下弯曲，所以在高维空间中，不用担心发生局部最优问题！</p><h3 id="停滞区问题-Plateaus"><a href="#停滞区问题-Plateaus" class="headerlink" title="停滞区问题 (Plateaus)"></a>停滞区问题 (Plateaus)</h3><p>停滞区：导数长时间接近于零的一段区域</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_17.png" alt=""></p><p>当沿着马鞍面向下移动，移动到鞍点，但鞍点不是我们需要的最小值点，需要继续往下移动。但是这个地方梯度为零或者接近于零，曲面很平，需要花费很长时间缓慢地在这个停滞区内找到这个点而不能继续往下，当左侧或右侧有随机扰动，才能继续往下走，这会让学习速度变得非常慢。</p><p>解决办法：采用 动量算法、RMSprop算法、Adam算法等可以改善这个情况。</p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络优化 </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《Beck》观后感</title>
      <link href="/2018/09/01/beck%E8%A7%82%E5%90%8E%E6%84%9F/"/>
      <url>/2018/09/01/beck%E8%A7%82%E5%90%8E%E6%84%9F/</url>
      <content type="html"><![CDATA[<p>　　我终于找到我的本命番了，它的名字叫《Beck》，为啥我知道这是本命番呢，因为这是第一部我看到最后迟迟舍不得看完的番，一共２６集，每天睡前看几集，看了有十几天，不敢一次看完，因为看完了我的梦就碎了，是的，这是一个追梦的故事，我相信任何热爱摇滚，热爱音乐，热爱吉他，渴望或者已经开始玩乐队的人，都会对这部番有着深深的感动和发自内心的共鸣。因为这部番，我拿起了好久没碰的木琴，并开始挑选我的第一把电吉他……</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_topimg.jpg" alt=""></p><a id="more"></a><p>　　我是怎么发现这部如此小众的动漫的呢，是群里一个新加的朋友告诉我的，这个群是我建的摇滚群，只有我一个人（笑），本来准备将群解散，在前段时间突然加进来一个人。我本以为和以前那些加入者一样，只是些悦耳旋律的伪摇滚迷，没想到谈论一番，此人对摇滚很有自己的理解，是我所理解的“硬核摇滚迷”，最让我惊讶的是，此人居然会变听歌边做笔记，我瞬间对他刮目相看。虽然我自己对摇滚乐理解也不深，但是感觉找到了能与之一谈的对摇滚乐有较强鉴赏能力的人。他给我看了他的四张专辑，其中两张都是名为《beck》的专辑，我以为是某个乐队名，结果他告诉我这是一部动漫的名字，并且强烈建议我去看，本来是抱着试试的心态去看，结果发现了自己的本命番哈哈。这人的昵称叫“Lucille”，就是动漫里那把传奇吉他的名字。</p><p>　　这部番是少见的乐队题材动漫，讲的是一个日本少年从只听流行乐的普通少年因为喜欢的女孩爱上摇滚开始弹吉他，加入乐队，最后乐队开始全美巡演的故事。这个少年叫小雄，在接触摇滚之前是你能想象的最普通的少年，偶然的机会碰到自己好久不见的童年伙伴小泉，她是一个喜欢摇滚的女生，有一次和她出去听乐队 live，结束后她和别人激烈地谈论着自己喜欢的乐队的 The dying breed，不听摇滚的小雄完全无法一起谈论，这让他非常低落，后来小泉给了他一张卷子DB的专辑，从此小雄打开了新世界的大门……不得不说，小雄刚开始对摇滚有点爱屋及乌的感觉，也许每个人都会因为迷恋一个人而爱上另一样东西吧。如果说小泉是带小雄进入摇滚世界的启蒙者，那么龙介就是带小雄进入乐队世界的启蒙者，他因为被龙介的狗咬伤而认识龙介，他给了小雄第一把吉他，然后发生了一段微妙而真实的小插曲。刚拿到吉他的小雄，也许是想要装逼，直接拿着吉他而不放进琴箱就上街溜达，结果一个平地摔把吉他摔断了，导致龙介暂时和他绝交，心灰意冷的小雄走在街上，好心的斋藤先生路过，决定帮他修吉他，代价是在他店里打工，没想到斋藤先生是个老摇滚迷，所以带小雄学吉他，又成了小雄吉他的启蒙者。小雄刚学吉他想要让全世界知道的心理，学几下就嫌手指疼放弃的描写，种种细节，对我们这些玩吉他的人来说真的是再亲切不过了，关于情节，实在非常的有趣且跌宕起伏，真想一口气把它们都详细地叙述出来！可惜篇幅有限，只能靠你们自己去看啦。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_3.png" alt="乐队成员"></p><p>　　这部动漫对人物的塑造和对乐队的描绘真的是入木三分。懦弱敏感而胆小的小雄，面对霸凌任由欺负，却在最后敢于面对上万的观众单独上台，乐队的主心骨主音吉他手龙介经常一副冷漠脸，可他的心里却隐藏着比谁都大的野心和热情，低调的眯眯眼鼓手阿樱（眯眯眼都是怪物hhh），贝斯技术高超而稳重的贝斯手阿平以及集正直、幽默、直率、满腔热血于一体的极富感染力的主唱千叶。龙介把这么一群人组合到一起，用他的狗的名字成立一个乐队——beck。这部动漫还有一个我印象很深的人物是斋藤先生，他四十多岁还没有结婚，独自生活经营着一家快要倒闭的纸店，他好色、猥琐、自私但是善良且忠厚，就是这个一个猥琐穷困的中年男人，居然是一个被摇滚拯救人生的摇滚老炮，家里收藏了几把吉他，虽然每天在外面跑业务低声下气，但是只要一有空，就进入摇滚的世界开始享受，他有一只鸟，只对好听的音乐欢呼，斋藤先生的梦想就是有一天能让它欢呼。我真的被这个角色感动到了，因为从他身上，我看到了一种可能性，在生活的重压下就算是一个四十岁的中年人都能享受摇滚的可能性，希望自己像他一样，不管到多老，都能享受摇滚，享受音乐。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_2.png" alt="斋藤先生"></p><p>　　这部动漫另一个有趣之处在于，虽然里面所有乐队和人都是虚构的，但是致敬了许多经典的摇滚乐队，比如刚开始提到最多的乐队The dying breed，我感觉对应现实中就是 nirvana，因为DB的吉他手艾迪的形象跟科本真的是太像了，金色长发，传奇吉他手，另外还有斋藤先生最爱的乐队rocket boy，风格完全和披头士一模一样，另外里面还经常提到滚石，齐柏林飞艇，地下丝绒等许多乐队，还有衬衫上专辑上都能看到许多经典乐队的影子，详细的可以见知乎有些大神的分析，这些细节对于摇滚迷来说真的不能再亲切了。</p><p>　　既然是音乐番，那么怎么能少的了音乐。这部番的音乐制作非常精良，我最喜欢的是小雄妙手偶得的那首《spit out》，不仅打动了动漫里音乐节的一万多人，更打动了屏幕外的我，但是最让人印象深刻的应该是剧里DB的《moon on the water》，这首歌见证了小雄的成长，刚开始觉得这首歌不好听，但是随着剧情深入，越听越有味道，尤其是月色下小雄和真帆在游泳池里的场景，让人无法忘怀，引用网易云一句评论：“真帆在泳池唱这首歌，月光皎洁，风吹过草地，水面泛起波纹，那一瞬间真的恍若天籁，又真实简单的让人动容。”到 youtube 上一查，果然许多人在翻唱这首歌，看完这部番的那天，我把这首歌听了一整天，然后花了一天学会弹，唱着歌词，满脑子都是真帆的样子。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_4.jpg" alt="moon on the water"></p><p>　　之所以迟迟不舍得看完，是因为这部动漫构建了一个梦，构建了一个普通少年的摇滚梦，它细腻的描写和真实的情感，给这个梦添上了翔实的细节，看完了，美好的梦就没了。不得不说，小雄是我所希望成为的那一种人，小雄的人生是我想过的人生，有摇滚，有朋友，有吉他，有乐队，但对我来说这些只是一个飘渺的幻影，因为我并不知道我该怎么样获得这种生活，而这部动漫给我描述另一个和我一样普通的少年如何走上这条路的过程，我将自己，也许每个热爱摇滚的青年都会这么做，带入小雄这个角色，为乐队的繁荣而开心，为乐队的解散而难受，所以这部番才能带给我如此多的感动。乐队，一个如此遥远而熟悉的名词啊，总有一天，我要像小雄一样，组一个最强的乐队！</p><p>　　不知不觉写了好多，但是我觉得我拙劣的文笔还是无法表达出我对这部作品有多喜爱。吉他在考研之后好久没碰，看完这部番，我又重新拿起了吉他，开始重新出发，谢谢你 beck，我又找回了我最初的梦。也许我不能像小雄一样碰到龙介，但是我希望成为自己的龙介。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_5.png" alt="谢谢你小雄"></p>]]></content>
      
      <categories>
          
          <category> 读书观影笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 摇滚 </tag>
            
            <tag> 音乐 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 1）</title>
      <link href="/2018/09/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c2w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/09/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c2w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>这次编程作业对比了三种不同的初始化方法的不同，三种方法分别是“零初始化”、“随机初始化”、“He 初始化”。</p><p>这是所用的数据，我们要将红点和蓝点分类：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_1.png" alt=""></p><a id="more"></a><h3 id="零初始化"><a href="#零初始化" class="headerlink" title="零初始化"></a>零初始化</h3><p>也就是将参数 w 和 b 全都初始化为 0，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l],layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>最后的代价函数随迭代次数的变化如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_2.png" alt=""></p><p>训练集的精确度为 0.5，测试集的精确度为 0.5</p><p>分类结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_3.png" alt=""></p><p>我们可以发现将所有参数初始化为零无法分类任何数据，因为无法打破对称性，这意味着每层的每个神经元都在学习一样的东西。</p><h3 id="随机初始化（为很大的值）"><a href="#随机初始化（为很大的值）" class="headerlink" title="随机初始化（为很大的值）"></a>随机初始化（为很大的值）</h3><p>将权重矩阵随机地初始化为很大的值（×10），偏差向量继续初始化为零，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)  </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*<span class="number">10</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>代价函数的变化如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_4.png" alt=""></p><p>训练集精确度为 0.83，测试集为 0.86</p><p>分类的结果为：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_5.png" alt=""></p><p>分析：</p><ul><li>代价函数开始的值很高，是因为用很大的随机值初始化权重会使得最后的激活（sigmoid）输出值 $a^{[L]}$ 非常接近 0 或者 1，代价函数公式为$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L] (i)}\right) \large{)} \small$ ，当 $a^{<a href="i">L</a>}\approx0$ 时，$log(a^{<a href="i">L</a>})=log(0)\rightarrow$ 无穷大</li><li>不好的初始化可能导致梯度消失/爆炸，这会减慢优化算法的速度</li><li>如果训练上面的的网络更长时间，可以得到更好的结果，但是用大随机值初始化会减慢优化的速度</li></ul><h3 id="He初始化"><a href="#He初始化" class="headerlink" title="He初始化"></a>He初始化</h3><p>这是用某个人名命名的初始化，与上面的随机初始化相似，只是在末尾不是乘以 10 而是 $\sqrt{\frac{2}{n^{[l-1]}}}$ ，这个推荐用来初始化包含 relu 激活函数的层，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span> </span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/layers_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>代价函数图像如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_6.png" alt=""></p><p>训练集上的精确度达到了 0.99，测试集上的精确度达到了 0.96 </p><p>分类的结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_7.png" alt=""></p><p>分析：我们可以看到 He 初始化在很少的迭代次数上就将蓝点和红点分类得很好</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>不同的初始化有不同的结果</li><li>随机初始化用来打破权重对称确保不同的隐藏层能学习到不同的东西</li><li>不要把任何值初始化得太大</li><li>对于有 relu 激活函数的网络 He 初始化非常有效</li></ul><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>本次编程作业将会学到如何在深度学习模型中运用正则化。</p><h3 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> compute_cost, predict, forward_propagation, backward_propagation, update_parameters</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y, test_X, test_Y = load_2D_dataset()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_8.png" alt=""></p><h3 id="非正则化模型"><a href="#非正则化模型" class="headerlink" title="非正则化模型"></a>非正则化模型</h3><p>将正则化系数 lambd 设为 0，将 keep_prob 设为 1，模型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, print_cost = True, lambd = <span class="number">0</span>, keep_prob = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个三层的神经网络: 线性-&gt;RELU-&gt;线性-&gt;RELU-&gt;线性-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- If True, print the cost every 10000 iterations</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learned by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            <span class="comment"># 记录代价函数值</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                        <span class="comment"># 样本总数</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="keyword">if</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 代价函数</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span>:</span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        <span class="keyword">assert</span>(lambd==<span class="number">0</span> <span class="keyword">or</span> keep_prob==<span class="number">1</span>)    <span class="comment"># 可以同时使用 L2 正则化和 dropout，但是这个例子只使用两者之一</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span> <span class="keyword">and</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        <span class="keyword">elif</span> lambd != <span class="number">0</span>:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每一万步打印代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 画出代价函数点图</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (x1,000)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>我们先不使用正则化试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the training set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p>结果是：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_9.png" alt=""></p><p>在测试集上的精确度为 0.91，而在训练集上的有 0.95，打印出分类图像看看：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_10.png" alt=""></p><p>很明显，分类器将一些训练集中的噪声学习进去了，发生了过拟合，接下来用正则化试试。</p><h3 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h3><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>代价函数为：</p><script type="math/tex; mode=display">J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)}</script><p>L2 正则化是在原本代价函数的基础上加上一个正则化项：</p><script type="math/tex; mode=display">J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost}</script><p>计算带有正则化项的代价函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算带有 L2 正则化项的代价函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing parameters of the model</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - value of the regularized loss function (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) <span class="comment"># 没加正则化项的代价函数</span></span><br><span class="line">    </span><br><span class="line">    L2_regularization_cost = (lambd/(<span class="number">2</span>*m))*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))<span class="comment"># np.square() 用来平方</span></span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>由于代价函数变了，所以反向传播计算某个参数的梯度也要加上正则化项对它的梯度：$\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   用带了正则化项的代价函数计算反向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- 前向传播缓存</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y<span class="comment"># 反向传播初始化</span></span><br><span class="line">    </span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + (lambd / m * W3)<span class="comment"># (lambd / m * W3) 是加上的正则化项梯度</span></span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">   </span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) + (lambd / m * W2)<span class="comment"># 多加了额外项</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) + (lambd / m * W1)<span class="comment"># 多加了额外项</span></span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>现在将正则化系数 lambd 设为 0.7 看看效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_11.png" alt=""></p><p>测试集精确度提高到了 0.93，打印分类图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with L2-regularization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_12.png" alt=""></p><p>可以看到噪点已经没有被学习进去。</p><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><ul><li>正则化参数 $\lambda$ 是一个可以在开发集上调整的超参数</li><li>L2 正则化让分类的边界更加平滑，但是如果正则化参数太大，则很可能导致“过平滑”，即变成一根直线，造成很大的偏差</li></ul><h4 id="L2-正则化的原理"><a href="#L2-正则化的原理" class="headerlink" title="L2 正则化的原理"></a>L2 正则化的原理</h4><p>L2 正则化依赖于一个假设，即权重更小的模型比权重更大的模型更简单，所以，通过在代价函数里<strong>惩罚</strong>权重的平方值，驱使所有的权重值变得更小，因为如果你有高权重值，那么代价函数就会变得非常大！这生成了一个更加平滑的模型，在模型中输出改变得比输入更慢。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>代价函数计算：正则化项应该加进代价函数中</li><li>反向传播：在代价函数对权重的梯度中应该加入正则化项对其的梯度</li><li>权重最后被驱使变得更小：权重衰减</li></ul><h3 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h3><h4 id="dropout-技术的原理"><a href="#dropout-技术的原理" class="headerlink" title="dropout 技术的原理"></a>dropout 技术的原理</h4><p>dropout 正则化在每次迭代中丢弃一些结点，也就是将这些结点的激活值变成零，每个结点被保留的概率是 keep_prob，被丢弃的结点在整个这次迭代过程中都不会出现。</p><p>当你丢弃某些神经元时，实际上改变了模型的结构，每一次迭代，你都在训练不同的模型，而这些模型是原有模型的子集。使用 dropout 让神经元们对某个特定的神经元的激活不再那么敏感，因为它随时可能会被丢弃。</p><h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><p>首先在前向传播中实现 dropout，假设是第 l 层，一共有如下四步：</p><ul><li><p>创造一个和 $A^{[l]}$ 形状一样的筛选矩阵，她每个元素取 1 的概率是 keep_prob</p><ul><li>先创造一个值在 0 到 1 之间的随机矩阵： D[i] = np.random.rand(A[i].shape[0],A[i].shape[1])，其中 np.random.rand() 创造的随机值范围为 (0,1)</li><li>对这个随机矩阵设置门槛，小于它的值取 1，否则取 0：D[i] &lt; keep_prob</li></ul></li><li><p>将 l 层的激活值矩阵逐元素乘以筛选矩阵，完成 dropout：A[i] = A[i] * D[i]</p></li><li>为了补偿丢弃的值，将上一步的结果处以 keep_prob：A[i] = A[i] / keep_prob</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现带 dropout 的前向传播: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (20, 2)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (20, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 20)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span></span><br><span class="line"><span class="string">    cache -- tuple, information stored for computing the backward propagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取出初始化后的参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 第一层的 dropout</span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>])<span class="comment"># 第一步 1: 初始化矩阵 D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D1 = D1 &lt; keep_prob<span class="comment"># 第二步: 把 D1 的值变成 0 或 1 (用 keep_prob 作为门槛)得到筛选矩阵</span></span><br><span class="line">    A1 = A1 * D1<span class="comment"># 第三步: 乘上筛选矩阵</span></span><br><span class="line">    A1 = A1 / keep_prob <span class="comment"># 第四步：补偿没有被丢弃的结点的值</span></span><br><span class="line">  </span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 第二层的 dropout</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])<span class="comment"># 第一步 1: 初始化矩阵 D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D2 = D2 &lt; keep_prob<span class="comment"># 第二步: 把 D1 的值变成 0 或 1 (用 keep_prob 作为门槛)得到筛选矩阵</span></span><br><span class="line">    A2 = A2 * D2<span class="comment"># 第三步: 乘上筛选矩阵</span></span><br><span class="line">    A2 = A2 / keep_prob <span class="comment"># 第四步：补偿没有被丢弃的结点的值</span></span><br><span class="line">   </span><br><span class="line">    Z3 = np.dot(W3, A2) + b3<span class="comment"># 输出层不用 dropout</span></span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)<span class="comment"># 切记将每层的筛选矩阵缓存</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure><p>然后我们在反向传播中使用 dropout，一共两步：</p><ul><li>将每层的筛选矩阵 D[i] 从缓存中取出，将 dA[i] 也乘上筛选矩阵，因为一个结点被丢弃后该结点的梯度值也归零</li><li>由于 A[i] 除以了 keep_prob，它对应的 dA[i] 也应该除以 keep_prob 来进行补偿</li></ul><p>反向传播函数代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现加了 dropout 的反向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_with_dropout()</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y<span class="comment"># 反向传播初始化</span></span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第二层的筛选</span></span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dA2 = dA2 * D2<span class="comment"># 第一步: 将 dA2 的值乘上筛选矩阵</span></span><br><span class="line">    dA2 = dA2 / keep_prob<span class="comment"># 第二步: 补偿没被丢弃的 dA2 的值</span></span><br><span class="line"></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第一层筛选</span></span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dA1 = dA1 * D1<span class="comment">#第一步: 将 dA1 的值乘上筛选矩阵</span></span><br><span class="line">    dA1 = dA1 / keep_prob<span class="comment"># 第二步: 补偿没被丢弃的 dA1 的值</span></span><br><span class="line"></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>现在运行使用了 dropout 的模式试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, keep_prob = <span class="number">0.86</span>, learning_rate = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><blockquote><p>注意：不要在测试过程中使用 dropout！！</p></blockquote><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_13.png" alt=""></p><p>测试集精度提高到了 0.95！画图看看分类的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with dropout"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_14.png" alt=""></p><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><ul><li>dropout 是一门正则化技术</li><li>只在训练时使用 dropout，不要在测试时使用</li><li>在前向和反向传播中都要同时使用 dropout </li><li>在前向传播和反向传播中都要记得进行值的补偿，即除以 keep_prob</li></ul><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>三种方式的最终结果对比如下表：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_15.png" alt=""></p><p>我们可以学到：</p><ul><li>正则化可以帮助减少过拟合</li><li>正则化可以迫使权重的值变得更小</li><li>L2 正则化和 dropout 正则化是两种非常有效的正则化技术</li></ul><h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>假设你正在搭建一个深度学习模型来检测诈骗，但是反向传播经常会有 bug，由于这是一个关键步骤，所以你的 boss 想要你的反向传播一定完全正确，所以我们在模型搭建好之后进行梯度检查。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>反向传播计算梯度 $\frac{\partial J}{\partial \theta}$, 其中 $\theta$ 代表模型的所有参数， $J$ 是前向传播的代价函数</p><p>由于前向传播很好实现，有十足的把握是正确的，而且非常确信代价函数 $J$ 百分百正确， 所以可以用 $J$ 来检验 $\frac{\partial J}{\partial \theta}$ 的正确性。</p><p>梯度的定义式如下：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}</script><ul><li>$\frac{\partial J}{\partial \theta}$ 是我们需要检验是否计算正确的梯度</li><li>我们需要计算 $J(\theta + \varepsilon)$ 和 $J(\theta - \varepsilon)$ ，因为 $J$ 是一定正确的</li></ul><p>假设我们有个三层的模型：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_16.png" alt=""></p><p>先引入所需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> gc_utils <span class="keyword">import</span> sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector</span><br></pre></td></tr></table></figure><p>然后是我们已经实现好的前向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_n</span><span class="params">(X, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation (and computes the cost) presented in Figure 3.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- training set for m examples</span></span><br><span class="line"><span class="string">    Y -- labels for m examples </span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (5, 4)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (5, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 5)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- the cost function (logistic cost for one example)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cost</span></span><br><span class="line">    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(<span class="number">1</span> - A3), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">1.</span>/m * np.sum(logprobs)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, cache</span><br></pre></td></tr></table></figure><p>实现好的反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_n</span><span class="params">(X, Y, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in figure 2.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    Y -- true "label"</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_n()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) </span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,</span><br><span class="line">                 <span class="string">"dA2"</span>: dA2, <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2,</span><br><span class="line">                 <span class="string">"dA1"</span>: dA1, <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>不确定我们刚刚实现的反向传播是否正确，所以我们写一个梯度检验的函数。</p><h3 id="多参数梯度检验的实现"><a href="#多参数梯度检验的实现" class="headerlink" title="多参数梯度检验的实现"></a>多参数梯度检验的实现</h3><p>对于下式：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}</script><p>其中 $\theta$ 不是一个标量，而是一个字典 parameters，所以我们需要先将这个字典转化为一个向量 parameters_value，方便进行取值，转化的过程如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2%2C1_17.png" alt=""></p><p>将字典转化为向量的函数 dictionary_to_vector() 和将向量转化回字典的函数 vector_to_dictionary() 和将梯度字典转化为梯度向量的函数 gradients_to_vector() 代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dictionary_to_vector</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Roll all our parameters dictionary into a single vector satisfying our specific required shape.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    keys = []</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">"W1"</span>, <span class="string">"b1"</span>, <span class="string">"W2"</span>, <span class="string">"b2"</span>, <span class="string">"W3"</span>, <span class="string">"b3"</span>]:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># flatten parameter</span></span><br><span class="line">        new_vector = np.reshape(parameters[key], (<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">        keys = keys + [key]*new_vector.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">            theta = new_vector</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            theta = np.concatenate((theta, new_vector), axis=<span class="number">0</span>)</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, keys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vector_to_dictionary</span><span class="params">(theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    parameters[<span class="string">"W1"</span>] = theta[:<span class="number">20</span>].reshape((<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    parameters[<span class="string">"b1"</span>] = theta[<span class="number">20</span>:<span class="number">25</span>].reshape((<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">    parameters[<span class="string">"W2"</span>] = theta[<span class="number">25</span>:<span class="number">40</span>].reshape((<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line">    parameters[<span class="string">"b2"</span>] = theta[<span class="number">40</span>:<span class="number">43</span>].reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">    parameters[<span class="string">"W3"</span>] = theta[<span class="number">43</span>:<span class="number">46</span>].reshape((<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    parameters[<span class="string">"b3"</span>] = theta[<span class="number">46</span>:<span class="number">47</span>].reshape((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradients_to_vector</span><span class="params">(gradients)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Roll all our gradients dictionary into a single vector satisfying our specific required shape.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">"dW1"</span>, <span class="string">"db1"</span>, <span class="string">"dW2"</span>, <span class="string">"db2"</span>, <span class="string">"dW3"</span>, <span class="string">"db3"</span>]:</span><br><span class="line">        <span class="comment"># flatten parameter</span></span><br><span class="line">        new_vector = np.reshape(gradients[key], (<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">            theta = new_vector</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            theta = np.concatenate((theta, new_vector), axis=<span class="number">0</span>)</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure><p>现在我们得到了 paremeters_values (也就是 $\theta$) 的向量，下面是进行梯度检查的步骤：</p><p>for  each i in len(paremeters_values):</p><ul><li><p>计算 $J(…,\theta[i]+\varepsilon,…)$，即 J_plus[i]：</p><ul><li>$\theta^+$ = np.copy(parameters_values)</li><li>$\theta^+[i]=\theta^+[i]+\varepsilon$  </li><li>将 $\theta^+$ 重新转换回参数字典（使用 vector_to_dictionary 函数）</li><li>用新的参数带入前向传播计算 J_plus[i]</li></ul></li><li><p>同样的方法计算$J(…,\theta[i]-\varepsilon,…)$，即 J_minus[i]</p></li><li>用梯度估算式计算梯度 $gradapprox[i]=\frac{J_plus[i]-J_minus[i]}{2\varepsilon}$ </li><li>计算估算值和实际值的差异：$ difference = \frac {| grad - gradapprox |_2}{| grad |_2 + | gradapprox |_2 } $</li></ul><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check_n</span><span class="params">(parameters, gradients, X, Y, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    检查反向传播计算的梯度是否正确</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">    gradients -- 反向传播的输出，包含了代价函数对 theta 中每个参数的梯度的实际计算值的字典，需要检验正确性</span></span><br><span class="line"><span class="string">    X -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    Y -- true "label"</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置变量</span></span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters) <span class="comment"># 将参数值字典拍扁成一个向量存，_ 表示没有用到的返回值</span></span><br><span class="line">    grad = gradients_to_vector(gradients) <span class="comment"># 将梯度字典也拍扁成一个向量</span></span><br><span class="line">    num_parameters = parameters_values.shape[<span class="number">0</span>] <span class="comment"># 参数的总个数 </span></span><br><span class="line">    J_plus = np.zeros((num_parameters, <span class="number">1</span>))<span class="comment"># 初始化</span></span><br><span class="line">    J_minus = np.zeros((num_parameters, <span class="number">1</span>))<span class="comment"># 初始化</span></span><br><span class="line">    gradapprox = np.zeros((num_parameters, <span class="number">1</span>))<span class="comment"># 初始化</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算梯度的估算值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第一步：计算 J_plus[i]</span></span><br><span class="line">        thetaplus = np.copy(parameters_values)    <span class="comment"># 1.将参数向量 copy 过来，"_" 表示函数输出两个参数但是第二个用不到</span></span><br><span class="line">        thetaplus[i][<span class="number">0</span>] =  thetaplus[i][<span class="number">0</span>] + epsilon<span class="comment"># 2.给第 i 个参数加上微小量</span></span><br><span class="line">        J_plus[i], _ =forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))<span class="comment"># 3.计算参数加上微小量之后的代价函数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第二步：计算 J_minus[i]</span></span><br><span class="line">        thetaminus = np.copy(parameters_values)<span class="comment"># 1.将参数向量 copy 过来                    </span></span><br><span class="line">        thetaminus[i][<span class="number">0</span>] = thetaminus[i][<span class="number">0</span>] - epsilon<span class="comment"># 2.给第 i 个参数减去微小量    </span></span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X,Y,vector_to_dictionary(thetaminus))   <span class="comment"># 3.计算减去微小量之后的代价函数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第三步：计算第 i 个参数的梯度估算值 gradapprox[i]</span></span><br><span class="line">        gradapprox[i] = (J_plus[i]-J_minus[i]) / np.float(<span class="number">2</span> * epsilon)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第四步：计算估算值和实际值之间的差异</span></span><br><span class="line">    numerator = np.linalg.norm(grad-gradapprox)<span class="comment"># 计算分子</span></span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)<span class="comment"># 计算分母</span></span><br><span class="line">    difference = numerator / denominator<span class="comment"># 计算差异   </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> difference &gt; <span class="number">1e-7</span>:<span class="comment"># 若差异值大于10的-7次方，则可能有错误</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[93m"</span> + <span class="string">"There is a mistake in the backward propagation! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    <span class="keyword">else</span>:<span class="comment"># 若差异值小于10的-7次方，则梯度检验通过</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[92m"</span> + <span class="string">"Your backward propagation works perfectly fine! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><p>下面运用这个函数来对我们的反向传播进行检验：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, Y, parameters = gradient_check_n_test_case()</span><br><span class="line"></span><br><span class="line">cost, cache = forward_propagation_n(X, Y, parameters)</span><br><span class="line">gradients = backward_propagation_n(X, Y, cache)</span><br><span class="line">difference = gradient_check_n(parameters, gradients, X, Y)</span><br></pre></td></tr></table></figure><p>>&gt; There is a mistake in the backward propagation! difference = 1.18904178788e-07</p><p>结果显示，梯度检验可能没有通过，反向传播中可能有错误，于是我们可以返回之前写的反向传播中进行仔细检查！</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>由于 $\frac{\partial J}{\partial \theta} \approx  \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}$ 的计算成本很高，所以梯度检验非常慢，因此不要在训练的时候进行梯度检验，只需要调用几次检验反向传播函数是否正确即可</li><li>梯度检验不能与 dropout 一起运行，可以在运行梯度检验之前先关掉 dropout，检验完后再打开</li></ul><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><ul><li>梯度检验核实了反向传播中计算的梯度和公式估计的梯度之间的接近程度</li><li>梯度检验很慢，所以不要在训练的每一次迭代中运行，只有当你想确保你的代码正确的时候才要用，然后在实际的学习过程中关掉它</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络优化 </tag>
            
            <tag> 正则化 </tag>
            
            <tag> 初始化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 2 week 1）—— 正则化等</title>
      <link href="/2018/08/29/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w1/"/>
      <url>/2018/08/29/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w1/</url>
      <content type="html"><![CDATA[<p>deeplearning.ai 的第二个课程名为 <strong>改进深度神经网络：超参数调整，正则化和优化 (Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization)</strong> ，这门课程教你使深度学习表现更好的“魔法”，而不是把神经网络当作一个黑箱，你会理解什么使得神经网络表现更好，而且能更系统地得到好的结果，你还会学到一些 tensorflow 知识。通过三周的学习，你将能够：</p><ul><li>了解构建深度学习应用程序的行业最佳实践</li><li>能够有效地使用常见的神经网络技巧，包括初始化，L2 正则化和丢失正则化，批量归一化，梯度检查</li><li>能够实现和应用各种优化算法，例如小批量梯度下降，动量，PMSprop 和 Adam，并检查它们的收敛性</li><li>了解如何设置训练/开发/测试集和分析偏差/方差</li><li>能够在 TensorFlow 上实现神经网络</li></ul><a id="more"></a><h2 id="配置你的机器学习应用"><a href="#配置你的机器学习应用" class="headerlink" title="配置你的机器学习应用"></a>配置你的机器学习应用</h2><h3 id="训练-开发-测试集-Train-Dev-Test-sets"><a href="#训练-开发-测试集-Train-Dev-Test-sets" class="headerlink" title="训练/开发/测试集 (Train/Dev/Test sets)"></a>训练/开发/测试集 (Train/Dev/Test sets)</h3><p>假设我们有一组数据，我们将这组数据分成 <strong>训练集 train sets</strong> 和 <strong>开发集 dev sets</strong>（有时称作 hold-out 交叉验证集）和 <strong>测试集 test sets</strong>。</p><p>训练集用来训练模型，开发集用来评估不同超参数下的模型哪一个在开发集上效果最好，测试集用来评估最终的训练效果。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_1.png" alt=""></p><p>它们在一组数据中的比例是：</p><ul><li>在“前机器学习时代”即小数据时代：训练集/测试集 = 70%/30%；训练集/开发集/测试集 = 60%/20%/20%，当数据为10000以下时，这是被认为最好的分法</li><li>在大数据时代：假设有一百万个样本，开发集只需要一万个即可，测试集也只需要一万个便可评估模型性能，所以此时的比例为 训练/开发/测试 = 98%/1%/1%</li></ul><p>对于训练集和测试集的数据分布不匹配的问题，例如训练集来自网上爬取的精美图片，而训练集和开发集来自用户上传的模糊图片，这种情况的经验法则是：确保<strong>开发集和测试集</strong>的数据分布相同。</p><p>当不需要无偏估计时，可以没有测试集，这个时候开发集有时被称为“测试集”。</p><h3 id="偏差-bias-和方差-variance"><a href="#偏差-bias-和方差-variance" class="headerlink" title="偏差 (bias) 和方差 (variance)"></a>偏差 (bias) 和方差 (variance)</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_2.png" alt=""></p><ul><li>第一幅图中的分类器发生了<strong>欠拟合</strong>现象，我们称这个模型为<strong>“高偏差”</strong></li><li>第三幅图发生了<strong>过拟合</strong>现象，我们称这个模型为<strong>“高方差”</strong></li></ul><p>假设训练一个识别猫的分类器，有以下四种结果：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_3.png" alt=""></p><ul><li>第一个分类器训练集误差 1%，开发集 11%，则说明训练集拟合得较好，但是模型的泛化能力不行，也就是高方差，有可能发生过拟合现象</li><li>第二个分类器训练集误差 15%，开发集 16%，说明训练集拟合得不是很好，但是开发集误差与训练集相近，说明泛化能力较好，模型具有高偏差，有可能发生欠拟合现象</li><li>第三个分类器不仅训练集误差较大，而且开发集误差与训练集误差差距很多，说明此时模型既有高偏差，又有高方差，是最差的情况</li><li>第四个分类器训练集误差小，开发集误差也很小，此时的模型是低偏差、低方差，是最好的情况</li></ul><blockquote><p>这种分析方法基于人类识别出猫的误差约为零，如果图片模糊，连真人都无法识别出来，则理想误差（贝叶斯误差）就会非常高</p></blockquote><p>用一张图片更直观地展现“高偏差，高方差”的情况：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_4.png" alt=""></p><p>一个好的分类器如虚线所示，作为直线分类器具有高偏差，但是紫色线表示的分类器不仅大部分是直线，而且在途中扭曲地绕过了两个错误的样本，这样使得它具有了高方差，所以这个分类器既具有高方差又有高偏差。</p><h3 id="机器学习基本准测"><a href="#机器学习基本准测" class="headerlink" title="机器学习基本准测"></a>机器学习基本准测</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_5.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_7.png" alt=""></p><blockquote><p>在深度学习之前的时代中我们能用的工具不是很多，我们没有太多那种能够单独减小偏差或单独减小方差而不顾此失彼的工具。但在当前这个深度学习和大数据的时代，只要你能不断扩大所训练的网络的规模或不断获得更多数据，那扩大网络几乎总是能够减小偏差而不增大方差，而获得更多数据（用恰当的方式正则化）几乎总是能够减小方差而不增大偏差。有了这两步，再加上能够选取不同的网络来训练，我们就有了能够单独削减偏差或单独削减方差而不会过多影响另一个指标的能力，这样就不需小心地平衡两者。它能够解释为何深度学习在监督学习中如此有用以及为何在深度学习中，偏差与方差的权衡要不明显得多。</p></blockquote><h2 id="神经网络的正则化-regularization"><a href="#神经网络的正则化-regularization" class="headerlink" title="神经网络的正则化 (regularization)"></a>神经网络的正则化 (regularization)</h2><p>如果你怀疑你的神经网络在数据上发生了过拟合，也就是存在高方差问题，需要首先尝试使用正则化，虽然获取更多数据也是解决高方差问题的一个很可靠的方法，但你并不是总能获取到更多的训练数据或者获取更多数据的代价太大，但使用正则化通常有助于防止过拟合并降低网络的误差。</p><h3 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h3><h4 id="什么是-L2-正则化"><a href="#什么是-L2-正则化" class="headerlink" title="什么是 L2 正则化"></a>什么是 L2 正则化</h4><p>假如考虑最简单的逻辑回归模型，我们的优化目标是找到参数 $w，b$ 使得代价函数 $J(w,b)$ 最小，其中 $w \in \mathbb{R^{n_x}} , b \in \mathbb{R},$</p><script type="math/tex; mode=display">J(w,b)= \frac {1}{m} \sum\limits_{i=1}^{m} L({\hat y}^{(i)},y^{(i)})</script><p>当发生过拟合问题时，我们对上式引入正则化项，L2 正则化则是使用 L2 范数：</p><script type="math/tex; mode=display">J(w,b)= \frac {1}{m} \sum\limits_{i=1}^{m} L({\hat y}^{(i)},y^{(i)}) + \frac{\lambda}{2m}||w||^2_2</script><ul><li>$\lambda$ 称之为<strong>正则化参数</strong>，是一个需要调优的超参数，通常在开发集上配置这个参数，尝试一系列的值找出最好的那个，lambda 是 python 保留的关键字，在编程中我们把它写为“lambd”，避免和保留关键字冲突</li><li>$||w||_2$ 为 L2 范数 (欧几里德范数)，且 $||w||_2^2=\sum\limits_{j=1}^{n_x}w_j^2=w^Tw$ </li><li>我们通常省略 b 的相关项 $\frac{\lambda}{2m}b^2$，因为 w 往往是一个非常高维的参数矢量，几乎所有的参数都集中在 w 中，而 b 只是大量参数中的一个参数，即使加上了 b 的相关项也不会起到太大作用</li><li>L1 正则化使用的是 L1 范数，正则化项为 $\frac{\lambda}{2m}\sum\limits_{j=1}^{n_x}|w_j|=\frac{\lambda}{2m}||w||_1$，使用 L1 正则化会使得模型变得“稀疏”，这意味着 w 中有很多 0，吴恩达认为 L1 并没有压缩模型的作用且 L2 正则化用的更多</li></ul><p>对于 L 层的神经网络来说，正则化项为各层范数之和：</p><script type="math/tex; mode=display">J(W^{[1]},b^{[1]},...,W^{[L]},b^{[L]})= \frac {1}{m} \sum\limits_{i=1}^{m} L({\hat y}^{(i)},y^{(i)}) + \frac{\lambda}{2m}\sum\limits_{l=1}^{L}||W^{[l]}||^2_F</script><ul><li>其中：$||W^{[l]}||^2_F=\sum\limits_{i=1}^{n^{[l-1]}}\sum\limits_{j=1}^{n^{[l]}}(W_{ij}^{[l]})^2，W:(n^{[l]},n^{[l-1]})$，即 $W$ 矩阵的每个元素的平方求和，这个矩阵的范数，称为矩阵的<strong>费罗贝尼乌斯范数</strong> (注意，这不叫矩阵的 L2 范数)，使用角标 F表示 </li><li>式子最后加的正则化项也叫<strong>“惩罚项”</strong>，用来防止权重过大</li></ul><p>那么该如何使用 L2 正则化呢，我们可以证明，在代价函数添加正则项之后，其对 $W^{[l]}$ 的偏导 $\frac{\partial J}{\partial W^{[l]}}=dW^{[l]}$ 也要添加一项：</p><script type="math/tex; mode=display">dW^{[l]}=(正则化之前的dW^{[l]}) + \frac{\lambda}{m}W^{[l]}</script><p>然后更新参数即可：</p><script type="math/tex; mode=display">W^{[l]}:=W^{[l]}-\alpha dW^{[l]}\\ \quad \quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \ \ \ =W^{[l]}-\alpha((正则化之前的dW^{[l]}) + \frac{\lambda}{m}W^{[l]})\\   \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad  =(1-\frac{\alpha\lambda}{m})W^{[l]}-\alpha(正则化之前的dW^{[l]})</script><p>我们可以看到矩阵 $W^{[l]}$前多了一个略小于1的系数，也就是说无论 $W$ 是多少，都让它变小一点，由于这个原因，L2 正则化有时被称为<strong>“权重衰减”</strong></p><h4 id="为什么-L2-正则化可以减小过拟合"><a href="#为什么-L2-正则化可以减小过拟合" class="headerlink" title="为什么 L2 正则化可以减小过拟合"></a>为什么 L2 正则化可以减小过拟合</h4><ol><li><p>例子1</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_8.png" alt=""></p><p>假如目前发生了过拟合，是第三种情况，根据正则化公式：</p><script type="math/tex; mode=display">W^{[l]}=(1-\frac{\alpha\lambda}{m})W^{[l]}-\alpha(正则化之前的dW^{[l]})</script><p>假如我们把正则化参数 $\lambda$ 调得足够大，使得 $W$ 变得很小，接近于0，这样会使得隐藏单元的影响被消除了，因为如果权重是零，那么这个单元便可有可无，那么这个大的神经网络就被简化为一个很小的神经网络，如上图左上角红框框起来的部分，这种情况与逻辑回归单元很类似 (只是深度变深)。而逻辑回归正是图中第一种高偏差的情况，于是上述操作使得我们将模型从第三种过拟合的情况变成第一种高偏差的情况！如果我们找到一个合适的中间值 $\lambda$，那么就可以将过拟合状态变成中间那种“刚刚好”的情况！</p></li><li><p>例子2</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_9.png" alt=""></p><p>对于上图的 tanh 激活函数，中间的一部分（红色的）接近于线性函数，如果 $\lambda$ 增大，则 $W^{[l]}$减小，由于$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$，则$Z^{[l]}$减小，更接近于在中间简单的线性部分取值，那么这个函数呈现相对线性，从而使神经网络只能计算一些离线性函数很近的相对简单的值，不能计算复杂的非线性函数，因此不太容易发生过拟合</p><blockquote><p>我的理解是，线性函数可以类比为某种“直脑筋”，直来直去，大大咧咧，所以容易产生高偏差，而非线性我类比为那种想得很多的人，过度思虑，所以经常会把一些噪声学进模型里，造成过度的拟合，正如太过焦虑某个事情容易走火入魔，L2 正则化大概是往这种“过度思虑”的心态里加上一些更加坦率更加直接的力量来矫正它吧 ：）</p></blockquote></li></ol><h3 id="丢弃正则化-Dropout-Regularization"><a href="#丢弃正则化-Dropout-Regularization" class="headerlink" title="丢弃正则化 (Dropout Regularization )"></a>丢弃正则化 (Dropout Regularization )</h3><h4 id="什么是丢弃正则化"><a href="#什么是丢弃正则化" class="headerlink" title="什么是丢弃正则化"></a>什么是丢弃正则化</h4><p>假设下图所示的神经网络发生过拟合：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_10.png" alt=""></p><p>使用随机失活技术来处理，首先为丢弃网络中的某个节点设置一个概率值，假设为 50%，遍历这个网络的每一层，对每一层的每一个结点作一次公平投币，使得这个结点有 50% 的几率被保留，50% 的几率被丢弃（取值为0），丢弃完这些结点后，我们得到一个小得多的网络，再进行反向传播，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_11.png" alt=""></p><h4 id="丢弃正则化的一种实现——反向随机失活-inverted-dropout"><a href="#丢弃正则化的一种实现——反向随机失活-inverted-dropout" class="headerlink" title="丢弃正则化的一种实现——反向随机失活 (inverted dropout)"></a>丢弃正则化的一种实现——反向随机失活 (inverted dropout)</h4><p>假设在神经网络的第 3 层上，即 $l=3$，首先设置一个向量 d3，它表示第 3 层的失活向量，它和 a3 的形状一样：</p><script type="math/tex; mode=display">第一步：d3=np.random.rand(a3.shape[0],a3.shape[1])<keep.prob</script><ul><li>np.random.randn() 产生的随机数介于 [0,1) 之间</li><li>keep.prob 是某个结点被保留的概率，小于它的值都取 1，反之取 0，也就是说上面这条语句产生了一个维度是 (a3.shape[0],a3.shape[1]) 的只包含 0 或者 1 的矩阵，某个元素取 1 的概率是 keep.prob，这是一个筛选矩阵</li></ul><p>接着我们用这个筛选矩阵来将 a3 随机失活：</p><script type="math/tex; mode=display">第二步：a3= np.multiply(a3,d3)</script><ul><li>np.multiply() 是逐元素相乘</li><li>如果 a3 某个元素乘到了 d3 中对应的某个刚好是 1 的元素（概率为 keep.prob），那么这个值保留，如果乘到了刚好是 0 的元素（概率为 1-keep.prob）,则这个值清零，即这个单元失活。也就是说： 每个结点都有 keep.prob 的概率被保留，1-keep.prob 的概率被失活</li></ul><p>接着我们将 a3 除以保留概率 keep.prob:</p><script type="math/tex; mode=display">第三步：a3 = a3/keep.prob</script><ul><li>假设 a3 有 50 个单元，keep.prob 为 0.8，则意味着平均会有 50×0.2=10 个单元失活清零，那么$z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}$也会减少约 20%，所以为了不减少 z4 的期望值，我们需要除以 0.8，提供大约 20% 的校正值</li></ul><p>注意：<strong>不要在预测使用随机失活算法！</strong>因为我们不想我们预测值的输出也是随机的，这样做只会给预测带来噪声！</p><h4 id="为什么-dropout-正则化有效？"><a href="#为什么-dropout-正则化有效？" class="headerlink" title="为什么 dropout 正则化有效？"></a>为什么 dropout 正则化有效？</h4><p>一个直觉：不能依赖任何一个特征，所以必须分散权重。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_12.png" alt=""></p><p>对于上面这个神经元来说，如果使用 dropout，这些输入会被随机丢弃，这就意味着，它不能依赖于任何一个特征，因为每一个都有可能被随机丢弃，或者说每一个输入都有可能随机失活，所以在特定的时候，就不愿把所有的赌注或权重只放在某一个输入上，因此这个神经元会更积极地使用这种方式对于每个输入给一个较小的权重。泛化这些权值有利于压缩这些权重的平方和（平方范数）。</p><p>就和 L2 正则化一样，使用 dropout 有助于收缩权值，防止过拟合。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_13.png" alt=""></p><h4 id="使用-dropout-的注意事项"><a href="#使用-dropout-的注意事项" class="headerlink" title="使用 dropout 的注意事项"></a>使用 dropout 的注意事项</h4><p>对于上图中的神经网络，每一层的权重矩阵维度都不一样，例如第一层是 (7,3)，第二层是 (7,7)，第三层是 (3,7)，我们可以对每一层设置不同的留存率 (keep.prob)，由于第二层有最大的 (7,7) 权值矩阵，参数最多，这一层最容易发生过拟合，于是我们可以在这一层设置最低的留存率，比如 0.5，而对于不那么担心会发生过拟合的层，例如权重矩阵较小的第三层，我们可以设为 0.7，而第四第五层我们完全不担心会发生过拟合，可以将留存率设为 1.0。另外在实际中，不对输入层进行随机失活。</p><p>最早对dropout技术的成功应用，是在计算机视觉领域，在这个领域中，输入层向量维度非常大，因为要包含每个像素点的值，几乎不可能有足够的数据，因此 dropout 在计算机视觉领域使用非常频繁，几乎已经成为一种默认了，但是除非算法已经过拟合了，所以不需考虑使用 dropout，所以相对计算机视觉领域，它在其他应用领域使用会少一些。</p><p>dropout 的一个缺点：代价函数变得不是那么明确，因为每次都有结点随机失活，所以代价函数定义不明确，也就是说你看不出它随着迭代的变化曲线，不能使用绘图的方法进行调参，这个时候可以先关闭 dropout，把留存率都设为 1，确保代价函数是单调递减的再打开 dropout。</p><h3 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h3><h4 id="数据集扩增-data-augmentation"><a href="#数据集扩增-data-augmentation" class="headerlink" title="数据集扩增 (data augmentation)"></a>数据集扩增 (data augmentation)</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_14.png" alt=""></p><p>增加数据的方法：</p><ul><li>图片水平翻转，可以使数据量翻倍</li><li>随机裁剪图片</li><li>数字的随机旋转或者变形</li></ul><p>增加数据集可以做出额外的伪训练样本，但这些额外的伪训练样本能增加的信息量不如全新的、独立的猫照片多，但因为这么做只有一些计算代价，所以这是一个获得更多数据的廉价方式，因此可以算作正则化，减少了过拟合。</p><h4 id="早终止法-early-stopping"><a href="#早终止法-early-stopping" class="headerlink" title="早终止法 (early stopping)"></a>早终止法 (early stopping)</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_15.png" alt=""></p><p>假设某个模型的代价函数随迭代次数在训练集上的变化（蓝线）和开发集误差（紫线）如上图所示，代价函数一直下降，而开发集误差先下降一段，然后开始增大，在最低点的那次迭代附近，神经网络表现最好，早终止法指的是，在这里把神经网络的训练停住，并选取这个最小开发集误差对应的参数值。</p><p>为什么有效？在迭代初期 w 很小，迭代后期 w 很大，正如 L2 正则化一样，通过停在半路，我们得到一个不大不小的 w 值，就能减小过拟合且不至于偏差太大了。</p><p>缺点：如果停止了梯度下降，意味着打断了优化代价函数 J 的过程，所以使得在降低代价函数这件事上做得就不够好，但同时你又想做到避免过拟合，也就是想用一个方法来干<strong>降低代价函数</strong>和<strong>减少过拟合</strong>这两件事情，而没有用不同方法来解决这两个问题，使问题更加复杂了。</p><p>优点：不用尝试大量的正则化参数 $\lambda$ 的值，只要运行一次梯度下降过程。</p><h2 id="问题的优化-optimization"><a href="#问题的优化-optimization" class="headerlink" title="问题的优化 (optimization )"></a>问题的优化 (optimization )</h2><h3 id="训练集的归一化-Normalization"><a href="#训练集的归一化-Normalization" class="headerlink" title="训练集的归一化 (Normalization)"></a>训练集的归一化 (Normalization)</h3><h4 id="什么是归一化"><a href="#什么是归一化" class="headerlink" title="什么是归一化"></a>什么是归一化</h4><p>假设某个训练集输入只有两个维度，$x=(x_1,x_2)^T$，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_16.png" alt=""></p><p>将输入归一化有两个步骤：</p><ol><li><p>将均值归零</p><p>$均值\mu = \frac{1}{m}\sum\limits ^m_{i=1}x^{(i)}\\x:=x- \mu $   </p><p>于是训练集变成下图的样子，使样本点分布在原点周围：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_17.png" alt=""></p><p>我们可以注意到特征 $x_1$的方差比 $x_2$的方差大，即更加分散，于是我们进行第二步。</p></li><li><p>将方差归一化</p><p>$方差\sigma^2=\frac{1}{m} \sum\limits ^m_{i=1}(x^{(i)} )^2\\x=\frac{x}{\sqrt{\sigma^2}}​$   </p><blockquote><p>注意：第一个式子本来应该是 $\sigma^2=\frac{1}{m} \sum\limits ^m_{i=1}(x^{(i)} - \mu)$，但是在第一步中 $x$ 已经减了 $\mu$，所以此处不需要</p></blockquote><p>于是训练集变成下面图里的样子：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_18.png" alt=""></p></li></ol><p>值得注意的是：当使用归一化对数据进行缩放时，<strong>训练集和测试集应该使用相同的 $\mu$ 和 $\sigma^2$ </strong>，也就是从训练集计算出来的 $\mu$ 和 $ \sigma^2$ </p><h4 id="为什么要对数据归一化"><a href="#为什么要对数据归一化" class="headerlink" title="为什么要对数据归一化"></a>为什么要对数据归一化</h4><p>假设 $x_1$ 的取值介于 1～10000，而 $x_2$ 的取值介于 0～1，那么会导致参数 w1 和 w2 的取值范围有很大不同，那么代价函数就如下图所示（两根轴用 w 和 b 代替）：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_19.png" alt=""></p><p>这个代价函数就像一个“扁平的碗”，它的等高线就是一个长长的椭圆，那么梯度下降必须采用很小的步长，经历许多步，反复辗转才能下降到最小值，而经过归一化的代价函数如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_20.png" alt=""></p><p>代价函数变成了一个更倾向正球体的图形，等高线也趋近于圆形，那么梯度下降能直接朝着最小值而去，而且采用更长的步长。</p><p>归一化适用于输入特征的尺寸非常不同的情况， 如果输入特征本来尺度就相近，那么这一步就不那么重要，不过因为归一化的步骤几乎从来没有任何害处，所以可以总是进行归一化。</p><h3 id="梯度消失和梯度爆炸-vanishing-exploding-gradients"><a href="#梯度消失和梯度爆炸-vanishing-exploding-gradients" class="headerlink" title="梯度消失和梯度爆炸 (vanishing / exploding gradients)"></a>梯度消失和梯度爆炸 (vanishing / exploding gradients)</h3><p>梯度消失或爆炸指的是，当你在训练一个深度神经网络的时候，损失函数的导数或者说斜率，有时会变得非常大或者非常小甚至是呈指数级减小的一种现象。</p><h4 id="为什么会发生梯度消失或梯度爆炸"><a href="#为什么会发生梯度消失或梯度爆炸" class="headerlink" title="为什么会发生梯度消失或梯度爆炸"></a>为什么会发生梯度消失或梯度爆炸</h4><p>有一个非常深的神经网络如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_21.png" alt=""></p><p>假设激活函数为线性函数，即 $g(z)=z$，且 $b^{[l]}=0$，那么我们可以求出最终的预测值：$\hat y=W^{[1]}W^{[2]}W^{[3]}…W^{[L-1]}W^{[L]}X$，假设每个权重矩阵只比单位矩阵大一点点，例如 $W^{[l]}=\begin{bmatrix}<br>1.5 &amp; 0\\<br>0 &amp; 1.5<br>\end{bmatrix}$，那么 $\hat y = W^{[L]}{\begin{bmatrix}<br>1.5 &amp; 0\\<br>0 &amp; 1.5<br>\end{bmatrix}}^{L-1}X$，如果 L 很大即层数很深，那么激活函数值会呈指数级增长，最后的预测值发生爆炸。反之，如果每个权重矩阵只比单位矩阵小一点点，那么激活函数值会呈指数级减少，最后的预测值可能消失。同理，反向传播中的梯度也会在层数很大时指数级增长或者减少，这样会使得梯度下降变得非常非常慢。</p><h4 id="如何改善梯度消失或爆炸——参数初始化"><a href="#如何改善梯度消失或爆炸——参数初始化" class="headerlink" title="如何改善梯度消失或爆炸——参数初始化"></a>如何改善梯度消失或爆炸——参数初始化</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_22.png" alt=""></p><p>对于一个单神经元，如果我们忽略参数 b，那么 $z=w_1x_1+w_2x_2+…+w_nx_n$，我们不想让 z 变大或变小，如果 n 越大，那么 $w_i$ 就要越小，因为 z 是 n 个 $w_ix_i$ 的和，而 x 是不变的。一个解决办法是用方差为 $\frac{k}{n}$ 的正态分布初始化参数 w，其中 k 值取决于激活函数：</p><ol><li><p>对于 tanh 函数的 <strong>“Xavier 初始化”</strong>：</p><script type="math/tex; mode=display">将随机生成的 W 值乘以\sqrt{\frac{1}{n^{[l-1]}}}\\W^{[l]}=np.random.randn(n^{[l]},n^{[l-1]})*np.sqrt(\frac{1}{n^{[l-1]}})</script><ul><li>np.sqrt() 是所有元素开平方，* 是逐元素相乘</li></ul></li><li><p>对于 ReLU 函数有 <strong>“He 初始化”</strong>：</p><script type="math/tex; mode=display">将随机生成的W值乘以\sqrt{\frac{2}{n^{[l-1]}}}\\W^{[l]}=np.random.randn(n^{[l]},n^{[l-1]})*np.sqrt(\frac{2}{n^{[l-1]}})</script></li></ol><ol><li>另一种变体：<script type="math/tex; mode=display">\sqrt {\frac{2}{n^{[l-1]}+n^{[l]}}}\\W^{[l]}=np.random.randn(n^{[l]},n^{[l-1]})*np.sqrt(\frac{2}{n^{[l-1]}+n^{[l]}})</script></li></ol><h3 id="梯度检查-gradient-cheking"><a href="#梯度检查-gradient-cheking" class="headerlink" title="梯度检查 (gradient cheking)"></a>梯度检查 (gradient cheking)</h3><p>梯度检查是检查反向传播中计算的梯度是否正确并找出错误的方法，使用梯度检查可以节省时间，调试代码，检验反向传播算法。</p><h4 id="梯度检查的公式"><a href="#梯度检查的公式" class="headerlink" title="梯度检查的公式"></a>梯度检查的公式</h4><p>我们用一个近似公式来计算代价函数对参数的梯度：</p><script type="math/tex; mode=display">f'(\theta)\approx\frac{f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2\varepsilon}</script><ul><li>其中 $\varepsilon$ 是一个很小的数</li><li>这种双向导数公式比单向更精确</li></ul><p>接下来：</p><ol><li><p>把 $W^{[1]},b^{[1]},W^{[2]},b^{[2]},…,W^{[L]},b^{[L]}$ reshape 成一个大向量 $\theta$ </p></li><li><p>把 $dW^{[1]},db^{[1]},dW^{[2]},db^{[2]},…,dW^{[L]},db^{[L]}$ reshape 成一个大向量 $d\theta$</p></li><li><p>梯度检查算法：</p><p> $for \quad each \quad  i:\\  \quad \quad  \  d\theta_{approx}{[i]}=\frac{J(\theta_1,\theta_2,…,\theta_i+\varepsilon,…)-J(\theta_1,\theta_2,…,\theta_i-\varepsilon,…)}{2\varepsilon}\\  \hspace{3.5cm} \approx d\theta[i]=\frac{\partial J}{\partial \theta_i}$</p><ul><li>其中 $d\theta_{approx}[i]$ 指的用上面的公式计算的值，$d\theta[i]$ 指的是程序计算出的值</li><li>$\theta[i]$ 指的是第 i 层的参数</li></ul></li><li><p>检查 $d\theta_{approx}[i]$ 和 $d\theta[i]$ 是否相等，用下式来评估它们之间的差距：</p><script type="math/tex; mode=display">\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||_2}<\varepsilon</script><ul><li>其中分子为两个向量的欧几里德距离，值为两个向量的每个分量的差的平方之和再开方，分母为两个向量的欧几里德长度（L1 范数）之和</li><li>若 $\varepsilon=10^{-7}$，则可认为误差很小， 若 $\varepsilon$ 在 $10^{-5}$ 量级，则可能需要再三检查这个向量的每个分量，若在 $10^{-3}$ 量级，则很可能有错误，仔细检查每个部分，总之这个数字应该非常非常小</li></ul></li></ol><h4 id="实现梯度检查的注意事项"><a href="#实现梯度检查的注意事项" class="headerlink" title="实现梯度检查的注意事项"></a>实现梯度检查的注意事项</h4><ul><li>不要在训练的时候进行梯度检查——只在 debug 阶段</li><li>如果算法没通过梯度检查，需要检查它的组成，找出漏洞，也就是如果 $d\theta_{approx}[i]$ 和 $d\theta[i]$ 差距很大，检查不同的 i 值，看看哪些 $d\theta_{approx}[i]$ 和 $d\theta[i]$ 很大</li><li>记着正则化，$J(\theta)= \frac {1}{m} \sum\limits_{i=1}^{m} L({\hat y}^{(i)},y^{(i)}) + \frac{\lambda}{2m}\sum\limits_{l=1}^{L}||W^{[l]}||^2_F$，在用公式计算时 $d\theta_{approx}$ 时，别忘了正则化项</li><li>不要在使用 dropout 时进行梯度检查，可以将 keep.prob 设为 1</li><li>反向传播算法在 w 和 b 接近 0 的时候是正确的，但是当 w 和 b 变大的时候，算法精确度有所下降，可以在进行几次训练的迭代后，再运行梯度检验</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络优化 </tag>
            
            <tag> 正则化 </tag>
            
            <tag> 初始化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 4）</title>
      <link href="/2018/08/27/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w4%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/08/27/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w4%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h1 id="第一部分：基本架构"><a href="#第一部分：基本架构" class="headerlink" title="第一部分：基本架构"></a>第一部分：基本架构</h1><p>这是深度学习专项课程第一课第四周的编程作业的第一部分，通过这一部分，可以学到：</p><ul><li>使用非线性单元比如 ReLU 来提高模型</li><li>建立一个更深的神经网络（大于一个隐藏层）</li><li>实现一个易用的神经网络类</li></ul><h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 包的引入</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline <span class="comment"># %符号为ipython的魔法函数，与画图有关，在pycharm中会报错</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h2><ul><li>初始化一个两层神经网络的参数以及一个 L 层的神经网络的参数</li><li><p>实现前向传播模块（下图中的紫色模块）</p><ul><li>实现某一层的前向传播步骤的<strong>线性部分</strong>（得到 $Z^{[l]}$）</li><li>给定激活函数（relu/sigmoid）</li><li>组合之前的两步形成一个<strong>[线性—&gt;激活]</strong>前向传播函数</li><li>重复<strong>[线性—&gt;ReLU]</strong>前向传播函数 L-1 次（对于 1 到 L-1 层），再加上<strong>[线性—&gt;sigmoid]</strong>在末尾（对于二元分类的最终层 L）。这形成了一个新的 L_model_forward 函数</li></ul></li><li><p>计算损失函数</p></li><li><p>实现反向传播模块（下图中的红色部分）</p><ul><li>计算某一层的反向传播函数步骤的<strong>线性部分</strong></li><li>计算激活函数的梯度（relu_backward/sigmoid_backward）</li><li>组合之前的两步形成一个新的<strong>[线性—&gt;激活]</strong>反向传播函数</li><li>重复<strong>[线性—&gt;relu]</strong>反向传播 L-1 次，然后加上<strong>[线性—&gt;sigmoid]</strong>反向传播在末尾。这形成了一个新的 L_model_backward 函数</li></ul></li><li><p>更新参数</p></li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1.png" alt=""></p><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><h3 id="两层神经网路初始化"><a href="#两层神经网路初始化" class="headerlink" title="两层神经网路初始化"></a>两层神经网路初始化</h3><ul><li>模型结构为：<em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</em></li><li>权重 W 采用随机初始化</li><li>偏差 b 采用初始化为零的方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单隐层神经网络的初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="L-层的神经网络的初始化"><a href="#L-层的神经网络的初始化" class="headerlink" title="L 层的神经网络的初始化"></a>L 层的神经网络的初始化</h3><p>假设输入 X 维度是（12288，209），则其他的维度如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2.png" alt=""></p><ul><li>模型结构为：<em>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</em></li><li>权重 W 采用随机初始化</li><li>偏差 b 采用初始化为零</li><li>将每层单元个数储存在 list 变量 layer_dims 里面</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L 层神经网络的初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- 包含每一层的维度数据的list数组</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">   </span><br><span class="line">    parameters = &#123;&#125;    <span class="comment"># 先建立一个空的参数dict</span></span><br><span class="line">    L = len(layer_dims)   <span class="comment"># 神经网络的总层数L </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数 W1，b1，W2，b1... 初始化</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l],layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l],<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="前向传播模块"><a href="#前向传播模块" class="headerlink" title="前向传播模块"></a>前向传播模块</h2><ul><li>线性前向传播</li><li>线性—&gt;激活前向传播，其中激活函数可以是 ReLU 或者 sigmoid</li><li>整个模型为：[线性 —&gt; relu] ×（L-1）—&gt; 线性 —&gt; sigmoid</li></ul><h3 id="线性前向传播"><a href="#线性前向传播" class="headerlink" title="线性前向传播"></a>线性前向传播</h3><ul><li>输入上一层的激活值 A_prev，这一层的参数 W，b，</li><li>输出这一层的线性值 Z 和 “线性缓存”</li><li>“线性缓存”是一个值为 (A_prev,W,b) 的 tuple</li><li>$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 之前层的激活值 (或输入数据): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- 权重矩阵: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- 偏差向量, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 这一层激活函数的输入值，也叫“前激活”参数 </span></span><br><span class="line"><span class="string">    cache -- “线性缓存”，值为 (A_prev,W,b) 的 tuple，可以更有效率地计算反向传播</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算 Z 值</span></span><br><span class="line">    Z = np.dot(W,A_prev) + b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A_prev, W, b)<span class="comment"># “线性缓存”</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h3 id="一层线性-激活前向传播"><a href="#一层线性-激活前向传播" class="headerlink" title="一层线性-激活前向传播"></a>一层线性-激活前向传播</h3><ul><li>输入上一层的激活值 A_prev，这一层的参数 W 和 b，以及激活函数名 (字符串)</li><li>输出这一层的激活值 A 和 “前向传播缓存”</li><li>“前向传播缓存” = “线性缓存”(A_prev,W,b) + “激活缓存”Z，是一个值为 ( (A_prev,W,b), Z) 的 tuple，简称“缓存”</li><li>$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一层线性-激活前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 之前层的激活值 (或输入数据): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- 权重矩阵: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- 偏差向量, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- 在这一层用到的激活函数, 用字符串储存: "sigmoid" 或 "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- 这一层的激活值 </span></span><br><span class="line"><span class="string">    cache -- 前向传播缓存，是一个包含 "linear_cache"（线性缓存，A_prev，W，b） 和 "activation_cache"（激活缓存，Z） 的 tuple：((A_prev,W,b),Z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 判断激活函数是 sigmoid 还是 relu</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev,W,b)<span class="comment"># 先得到这一层的线性值 Z 和“线性缓存” (A_prev,W,b)</span></span><br><span class="line">        A, activation_cache = sigmoid(Z)<span class="comment"># 再将 Z 激活得到 A 和“激活缓存” Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev,W,b) <span class="comment"># 同理</span></span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)<span class="comment"># 将线性缓存和激活缓存合并成一个“前向传播缓存” tuple</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><p>其中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid 函数和 relu 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(Z)</span>:</span></span><br><span class="line"></span><br><span class="line">    A = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    cache = Z <span class="comment"># 激活缓存 Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z)</span>:</span></span><br><span class="line">    </span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    cache = Z<span class="comment"># 激活缓存 Z</span></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h3 id="L-层模型的前向传播"><a href="#L-层模型的前向传播" class="headerlink" title="L 层模型的前向传播"></a>L 层模型的前向传播</h3><p>对于二元分类，先 [线性-relu激活] L-1 次，再 [线性-sigmoid激活] 一次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二元分类 L 层模型的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 初始训练集, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- 初始化函数 initialize_parameters_deep() 输出的初始化之后的参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- 最终层即 L 层的激活值</span></span><br><span class="line"><span class="string">    caches -- 包含每一层"前向传播缓存"的 list (一共有 L-1 个, 索引值为 0 到 L-1)</span></span><br><span class="line"><span class="string">              其中每一层的缓存是一个 tuple：((A,W,b),Z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 神经网络的层数，由于参数有 W 和 b，故要除以 2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 [线性 -&gt; RELU] 前向传播 L-1 次，并把每一次的缓存加到总缓存 caches 中.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span> + str(l)], parameters[<span class="string">'b'</span> + str(l)], <span class="string">'relu'</span>)<span class="comment"># 切记是 'relu'字符串 而不是 relu</span></span><br><span class="line">        caches.append(cache)<span class="comment"># 将每一次的缓存加入总缓存 caches</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 [线性 -&gt; sigmoid] 前向传播 1 次，并把这一次的缓存加到总缓存 caches 中.</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span> + str(L)], parameters[<span class="string">'b'</span> + str(L)], <span class="string">'sigmoid'</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><blockquote><p>关于缓存，在对输入进行线性运算时，生成<strong>“线性缓存”</strong>，是一个 (A_prev,W,b) 的 tuple，在对线性值进行激活运算时，生成<strong>“激活缓存”</strong>，是一个值为 Z 的数组，在进行 [线性-激活] 的前向传播时，生成的是<strong>“前向传播缓存”</strong>，简称<strong>“缓存”</strong>，是一个值为 ((A_prev,W,b),Z) 的 tuple</p></blockquote><h2 id="计算代价函数"><a href="#计算代价函数" class="headerlink" title="计算代价函数"></a>计算代价函数</h2><script type="math/tex; mode=display">cost = -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right))</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">  </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]<span class="comment"># 总样本个数</span></span><br><span class="line"></span><br><span class="line">    cost = <span class="number">-1</span>/m * ( np.dot(Y, np.log(AL).T) + np.dot((<span class="number">1</span>-Y), np.log(<span class="number">1</span>-AL).T) )</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># 确保代价函数是一个数而不是数组 (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="反向传播模块"><a href="#反向传播模块" class="headerlink" title="反向传播模块"></a>反向传播模块</h2><ul><li>线性反向传播</li><li>线性—&gt;激活反向传播，其中“激活”计算 relu 或者 sigmoid 函数的梯度</li><li>整个模型为：[线性 —&gt; relu] ×（L-1）—&gt; 线性 —&gt; sigmoid</li></ul><h3 id="线性反向传播"><a href="#线性反向传播" class="headerlink" title="线性反向传播"></a>线性反向传播</h3><p>已知 $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$ 和该层 “线性缓存”(A_prev,W,b)，求 $dW^{[l]}, db^{[l]} ,dA^{[l-1]}$ ，如下图所示</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3.png" alt=""></p><p>计算公式为：</p><script type="math/tex; mode=display">Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\\dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T}\\db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}\\dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- 代价函数对某l层线性输出 Z 的偏导 (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- 该l层的前向传播“线性缓存” tuple：(A_prev, W, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- 代价函数对前一层 l-1 层的激活值的偏导数，与 A_prev 的形状相同</span></span><br><span class="line"><span class="string">    dW -- 代价函数对当前层 l 层的权重 W 的偏导数，形状与 W 相同</span></span><br><span class="line"><span class="string">    db -- 代价函数对当前层 l 层的偏差 b 的偏导数，形状与 b 相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache<span class="comment"># 将该层“线性缓存”提取出来</span></span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]<span class="comment"># 总样本数m</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式计算 dW，db，dA_prev</span></span><br><span class="line">    dW = <span class="number">1</span>/m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h3 id="一层线性-激活反向传播"><a href="#一层线性-激活反向传播" class="headerlink" title="一层线性-激活反向传播"></a>一层线性-激活反向传播</h3><ul><li>已知 $dA^{[l]} = \frac{\partial \mathcal{L} }{\partial A^{[l]}}$ 和 $g(.)$ 和该层 “前向传播缓存”((A_prev,W,b),Z)，先求 $dZ^{[l]}$, 再求 $dW^{[l]}, db^{[l]} ,dA^{[l-1]}$ </li><li>$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一层线性-激活反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- 当前层l层的激活值梯度 </span></span><br><span class="line"><span class="string">    cache -- “前向传播缓存”tuple：((A_prev,W,b),Z)</span></span><br><span class="line"><span class="string">    activation -- 在这一层用到的激活函数, 用字符串储存: "sigmoid" 或 "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- 代价函数对前一层 l-1 层的激活值的偏导数，与 A_prev 的形状相同</span></span><br><span class="line"><span class="string">    dW -- 代价函数对当前层 l 层的权重 W 的偏导数，形状与 W 相同</span></span><br><span class="line"><span class="string">    db -- 代价函数对当前层 l 层的偏差 b 的偏导数，形状与 b 相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache<span class="comment"># 从“前向传播缓存”中提取“线性缓存”和“激活缓存”</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)<span class="comment"># 先用 dA 求得 dZ</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)  <span class="comment"># 再运行线性反向传播</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)<span class="comment">#同理</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p>其中 dA 求得 dZ 的函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现单个 relu 单元的反向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- 某l层的激活值</span></span><br><span class="line"><span class="string">    cache -- 该层的“激活缓存” Z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- 代价函数对l层的 Z 的梯度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    Z = cache<span class="comment"># 将 Z 值从“激活缓存”取出</span></span><br><span class="line">    dZ = np.array(dA, copy=<span class="keyword">True</span>) <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据 relu 函数，当 Z&lt;=0，dZ 也为零</span></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, cache)</span>:</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (<span class="number">1</span>-s)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br></pre></td></tr></table></figure><h3 id="L-层模型的反向传播"><a href="#L-层模型的反向传播" class="headerlink" title="L 层模型的反向传播"></a>L 层模型的反向传播</h3><p>对于二元分类，先 [sigmoid—&gt;线性] 一次，再 [relu—&gt;线性] L-1 次</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.4.png" alt=""></p><ul><li>反向传播初始化，即求 dAL 可以使用公式：$dAL=-\frac{Y}{AL}-\frac{1-Y}{1-AL}$ </li><li>将梯度数据存入名为 grads 的 dict 中：$grads[“dW” + str(l)] = dW^{[l]}$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L 层模型的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- 前向传播的最终激活值向量</span></span><br><span class="line"><span class="string">    Y -- 真实的“标签”向量 (包含 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- 总缓存，包含每一层的“前向传播缓存”，用 sigmoid 函数进行激活的缓存是 cache[L-1]，用 relu 函数进行激活的缓存是 cache[l] (for l in range(L-1),即 l = 0,1...,L-2)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- 包含代价函数对 A,W,b 的梯度的 dict</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;<span class="comment"># 先建立一个储存梯度的空 dict</span></span><br><span class="line">    L = len(caches)<span class="comment"># 神经网络的总层数</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]<span class="comment"># 训练集样本的个数</span></span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># 让 Y 和 AL 的形状相同</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播的初始化，即求 dAL</span></span><br><span class="line">    dAL =  - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))<span class="comment"># 逐元素相除</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第 L 层先 [sigmoid -&gt; 线性] 反向传播一次. 输入：dAL, current_cache. 输出：grads["dAL-1"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]  <span class="comment">#从总缓存 caches 中取出当前缓存 </span></span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L<span class="number">-1</span>)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">'sigmoid'</span>) <span class="comment"># 进行一次反向传播，，并把dAL-1，dWL，dbL 放入字典中</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 继续反向传播，从 l=L-2 循环到 l=0，故从 l+1 开始</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):  <span class="comment"># 将 range() 倒序</span></span><br><span class="line">        <span class="comment"># 第 l 层: [relu -&gt; 线性] 反向传播求梯度.</span></span><br><span class="line">        <span class="comment"># 输入：grads["dA" + str(l + 1)], current_cache. 输出：grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]<span class="comment"># 取出当前层的缓存</span></span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">'dA'</span> + str(L<span class="number">-1</span>)], current_cache, <span class="string">'relu'</span>)<span class="comment"># 输入的是 dAL-1</span></span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h2 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h2><script type="math/tex; mode=display">W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \\b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用梯度下降更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有初始化后的参数的字典</span></span><br><span class="line"><span class="string">    grads -- 包含所有参数的梯度的字典</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有更新过的参数的字典</span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = Wl 的值</span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = bl 的值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 神经网络的层数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用公式更新参数</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="第二部分：图片识别应用"><a href="#第二部分：图片识别应用" class="headerlink" title="第二部分：图片识别应用"></a>第二部分：图片识别应用</h1><p>这是深度学习专项课程第一课第四周的编程作业的第二部分，通过这一部分，可以学到：</p><ul><li>学习如何使用第一部分中构建的辅助函数来建立我们需要的任何结构的模型</li><li>用不同的模型结构进行实验观察每一种的表现</li><li>认识到在从头开始构建神经网络之前构建辅助函数使得任务更加容易</li></ul><h2 id="包的引入-1"><a href="#包的引入-1" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v3 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>数据集包括：</p><ul><li>m_train 个训练集，包括图片集 train_set_x_orig 和标签集 train_set_y</li><li>m_test 个测试集，包括图片集 test_set_x_orig 和标签集 test_set_y</li><li>每张图片都是<strong>方形</strong> (height = num_px, width = num_px)，有三个颜色通道，所以数组形状是 (num_px, num_px, 3)</li><li>每个图片集都要进行预处理，所以原始数据加上 <strong>_orig</strong>，但是标签集不需要预处理</li></ul><h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><h4 id="加载原始数据集"><a href="#加载原始数据集" class="headerlink" title="加载原始数据集"></a>加载原始数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(train_set_x_orig[index]) <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</span><br><span class="line"><span class="comment"># np.squeeze() 用于把一个数组的 shape 中为 1 的维度删掉，即让 train_set_y[:, index] 变为一个数</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_1.png" alt=""></p><h4 id="确定图片维度和个数以防止出错"><a href="#确定图片维度和个数以防止出错" class="headerlink" title="确定图片维度和个数以防止出错"></a>确定图片维度和个数以防止出错</h4><ul><li>训练集的个数：m_train</li><li>测试集的个数：m_test</li><li>图片（正方形）的尺寸即边长的像素数：num_px</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定维度和个数</span></span><br><span class="line"><span class="comment"># train_set_x_orig 形状为 (m_train, num_px, num_px, 3)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;Number of training examples: m_train = 209</span><br><span class="line">  Number of testing examples: m_test = 50</span><br><span class="line">  Height/Width of each image: num_px = 64</span><br><span class="line">  Each image is of size: (64, 64, 3)</span><br><span class="line">  train_set_x shape: (209, 64, 64, 3)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x shape: (50, 64, 64, 3)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br></pre></td></tr></table></figure><h4 id="重构图片数组变为标准输入矩阵"><a href="#重构图片数组变为标准输入矩阵" class="headerlink" title="重构图片数组变为标准输入矩阵"></a>重构图片数组变为标准输入矩阵</h4><p>把尺寸为 (num_px, num_px, 3) 的图片变为 shape 为 (num_px ∗ num_px ∗ 3, 1) 的向量 </p><p>把一个 shape 为 (a,b,c,d) 的矩阵变为一个 shape 为 (b∗c∗d, a) 的矩阵的技巧：<code>x_flatten = X.reshape(X.shape[0], -1).T</code></p><blockquote><p>实际上，reshape() 是按<strong>行</strong>取元素，按<strong>行</strong>放元素</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重构图片数组</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))<span class="comment">#重构后完整性检查</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;train_set_x_flatten shape: (12288, 209)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x_flatten shape: (12288, 50)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br><span class="line">  sanity check after reshaping: [17 31 56 22 33]</span><br></pre></td></tr></table></figure><h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><p>为了使得数据在一个合适的尺度上，我们需要将数据标准化，对于图片来说，由于图片的每个像素的 RGB 值介于 0 到 255 之间，所以我们可以将每个特征值除以 255，这样就能将它们标准化了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></span><br></pre></td></tr></table></figure><h2 id="一般方法"><a href="#一般方法" class="headerlink" title="一般方法"></a>一般方法</h2><ul><li>初始化参数 / 定义超参数</li><li><p>进行 num_iterations 次循环：</p><ul><li>前向传播</li><li>计算代价函数</li><li>反向传播</li><li>更新参数</li></ul></li><li><p>用训练过的参数来预测标签值</p></li></ul><h2 id="两层的神经网络构建"><a href="#两层的神经网络构建" class="headerlink" title="两层的神经网络构建"></a>两层的神经网络构建</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.5.png" alt=""></p><p>会用到的之前构建的函数有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>确定每层的单元数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># 输入向量维度</span></span><br><span class="line">n_h = <span class="number">7</span><span class="comment"># 隐藏层单元数</span></span><br><span class="line">n_y = <span class="number">1</span><span class="comment"># 输出层单元数</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><p>将构建的函数组合起来形成主函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个两层的神经网络: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 输入数据, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- 真实的标签向量 (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- 每一层的单元数 (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- 梯度下降的迭代次数</span></span><br><span class="line"><span class="string">    learning_rate -- 梯度下降学习率</span></span><br><span class="line"><span class="string">    print_cost -- 如果值为 True 则每一百步打印一次代价函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 包含更新后的参数 W1，W2，b1，b2 的字典</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []<span class="comment"># 用来记录每百步的代价函数的值</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]<span class="comment"># 样本数</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数字典中取出 W1，W2，b1，b2</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降循环 num_iterations 次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向传播，输出下一层的激活值和“前向传播缓存”</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">'relu'</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算代价函数</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播初始化，即求得 dAL</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播，获得前一层的激活值梯度和这一层的参数梯度</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">'relu'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将参数梯度都存入 grads 字典</span></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 从参数字典中取出参数并赋值以便下一次迭代</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每百步打印一次代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)<span class="comment"># 方便等下打印图表</span></span><br><span class="line">       </span><br><span class="line">    <span class="comment"># 打印出代价函数的图表</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters<span class="comment"># 返回更新过的参数</span></span><br></pre></td></tr></table></figure><p>用我们的数据集进行测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.7.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.8.png" alt=""></p><p>看看预测训练集的精确度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure><p>>&gt; Accuracy: 1.0</p><p>再看看预测测试集的精确度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p>>&gt; Accuracy: 0.72</p><p>我们可以看到比第二周的逻辑回归的 70% 的精确度要稍高，接下来我们建立一个 L 层的神经网络进行试验。</p><h2 id="L-层的神经网络"><a href="#L-层的神经网络" class="headerlink" title="L 层的神经网络"></a>L 层的神经网络</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.9.png" alt=""></p><p>可以用到的之前构建的函数有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>确定每层的单元数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>]<span class="comment"># 这是一个四层的模型</span></span><br></pre></td></tr></table></figure><p>将辅助函数组合到主函数中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个 L 层的神经网络: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 输入数据向量, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- 真实的标签向量 (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- 包含输入向量维度和每层的单元数的列表, 长度为层数 + 1</span></span><br><span class="line"><span class="string">    learning_rate -- 梯度下降学习率</span></span><br><span class="line"><span class="string">    num_iterations -- 梯度下降循环迭代次数</span></span><br><span class="line"><span class="string">    print_cost -- 若为 True，则每百步打印一次代价函数 cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 模型训练出来的参数，可用于预测标签值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    costs = []<span class="comment"># 用来记录代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数初始化</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向传播: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)  <span class="comment"># 注意在前向传播的过程中会得到总缓存 caches</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算代价函数</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)  <span class="comment"># 注意反向传播时要用上前向传播的总缓存 caches</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 每百步打印一次代价函数值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 画出代价函数图表</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>用数据集训练模型试试看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>得到的结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.10.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.11.png" alt=""></p><p>看看预测训练集的精确度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure><p>>&gt; Accuracy: 0.985645933014</p><p>看看预测测试集的精确度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p>>&gt; Accuracy: 0.8</p><p>我们可以看到 4 层神经网络的 80% 精确度比逻辑回归的 70% 和 2 层神经网络的 72% 精确度要高很多！</p><p>还可以拿自己的图片进行预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先将自己的图片加到 image 文件夹</span></span><br><span class="line">my_image = <span class="string">"my_image.jpg"</span> <span class="comment"># 将这个改成你图片的名字 </span></span><br><span class="line">my_label_y = [<span class="number">1</span>] <span class="comment"># 图片的真实标签值 (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class="line"></span><br><span class="line">fname = <span class="string">"images/"</span> + my_image</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="keyword">False</span>))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">my_image = my_image/<span class="number">255.</span></span><br><span class="line">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your L-layer model predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.13.png" alt=""></p><p>找了十张动物图片来进行测试，前五张不是猫，后五张是猫，测试结果是：第一张熊猫识别错误，第二张北极熊识别错误，第三张大象识别正确，第四张兔子识别正确，第五张非洲狮识别正确（其实对这张最没信心，因为外形和猫实在是太像了，反而识别正确了==），第六张猫识别错误，第七张猫识别错误，第八张猫识别正确，第九张猫识别正确，第十张猫识别正确，粗略看来，这十张图的正确率有六成，模型仍需改进！</p><h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><p>试着打印出分类错误的图片：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.12.png" alt=""></p><p>我们可以发现有以下特征的图片更难判断正确：</p><ul><li>猫的身体在不寻常的位置</li><li>猫的颜色与背景颜色相似</li><li>特殊的猫的颜色和种类</li><li>相机角度</li><li>图片的明亮度</li><li>大小变化（猫在图片中很大或者很小）</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 4）—— 深度神经网络</title>
      <link href="/2018/08/15/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w4/"/>
      <url>/2018/08/15/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w4/</url>
      <content type="html"><![CDATA[<p>本周主要介绍<strong>深度神经网络</strong> (Deep Neural Networks)。</p><h2 id="深度神经网络概况"><a href="#深度神经网络概况" class="headerlink" title="深度神经网络概况"></a>深度神经网络概况</h2><h3 id="什么是深度神经网络"><a href="#什么是深度神经网络" class="headerlink" title="什么是深度神经网络"></a>什么是深度神经网络</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.1.png" alt=""></p><p>所谓深浅取决于神经网络的层数，例如左上角的逻辑回归模型是一个“最浅的”神经网络，而右下角的神经网络具有五个隐藏层，可以算得上是深度神经网络。</p><a id="more"></a><h3 id="神经网络的符号含义"><a href="#神经网络的符号含义" class="headerlink" title="神经网络的符号含义"></a>神经网络的符号含义</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.2.png" alt=""></p><ul><li><strong>l</strong> —— 表示神经网络的层数</li><li>$n^{[l]}$  表示 l 层的单元数，图中 $n^{[0]}=n_x=3,n^{[1]}=5,n^{[2]}=5,n^{[3]}=3,n^{[4]}=n^{[l]}=1$</li><li>$a^{[l]}=g^{[l]}(z^{[l]})$ 表示 l 层的激活向量</li><li>$W^{[l]}$ 表示产生 l 层的权重</li><li>$b^{[l]}$ 表示产生 l 层的偏差</li></ul><h3 id="为什么我们需要“深度”"><a href="#为什么我们需要“深度”" class="headerlink" title="为什么我们需要“深度”"></a>为什么我们需要“深度”</h3><h4 id="例子1-——-面部识别"><a href="#例子1-——-面部识别" class="headerlink" title="例子1 —— 面部识别"></a>例子1 —— 面部识别</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.3.png" alt=""></p><p>神经网络的第一层可以被认为是一个边缘检测器，这一层的神经元正试图找到人脸的边缘在哪里，通过将像素分组的方法形成边缘，然后取消边缘检测，并将边缘组合在一起，形成面部的一部分，最后将面部的不同部位组合在一起我们可以识别不同的面部，我们可以将神经网络的浅层当作简单的检测函数，然后后一层将他们组合在一起，以便我们可以学习更复杂的功能</p><h4 id="例子2-——-电路理论"><a href="#例子2-——-电路理论" class="headerlink" title="例子2 —— 电路理论"></a>例子2 —— 电路理论</h4><p>用一个隐藏神经元数量较少但是具有深度的神经网络来计算某些函数时，如果我们尝试用浅层神经网络来计算同样的函数，则需要指数级的隐藏神经元来进行计算。</p><h2 id="建立深度神经网络的基本框架"><a href="#建立深度神经网络的基本框架" class="headerlink" title="建立深度神经网络的基本框架"></a>建立深度神经网络的基本框架</h2><h3 id="框架的建立过程"><a href="#框架的建立过程" class="headerlink" title="框架的建立过程"></a>框架的建立过程</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.4.png" alt=""></p><p>假设方框中的隐藏层为第 l 层：</p><p>参数：$W^{[l]},b^{[l]}$</p><p>前向传播：输入 $a^{[l-1]}$ ，输出 $a^{[l]}$，缓存 $z^{[l]}$</p><p>反向传播：输入 $da^{[l]}$ 和缓存 $z^{[l]}$，输出 $da^{[l-1]}$，$dW^{[l]}$，$db^{[l]}$</p><p>画成框图如下所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.5.png" alt=""></p><h3 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.6.png" alt=""></p><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>输入 $a^{[l-1]}$ ，输出 $a^{[l]}$，缓存 $z^{[l]}$</p><ol><li><p>非向量化</p><script type="math/tex; mode=display">z^{[l]}=W^{[l]}a^{[a-1]}+b^{[l]}\\a^{[l]}=g^{[l]}(z^{[l]})</script></li><li><p>向量化</p><script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[a-1]}+b^{[l]}\\A^{[l]}=g^{[l]}(Z^{[l]})</script></li></ol><blockquote><p>$X=A^{[0]} → A^{[1]}→A^{[2]}→…→ A^{[l]}$ </p></blockquote><h3 id="维度的确定"><a href="#维度的确定" class="headerlink" title="维度的确定"></a>维度的确定</h3><p>有如下的一个5层的神经网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.7!.png" alt=""></p><p>对于单个训练样本：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.8.png" alt=""></p><p>$z^{[l]},dz^{[l]},a^{[l]},da^{[l]}:(n^{[l]},1)\\W^{[l]},dW^{[l]}:(n^{[l]},n^{[l-1]})\\b^{[l]},db^{[l]}:(n^{[l]},1)$</p><p>对于多个训练样本：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.9.png" alt=""></p><p>$ Z^{[l]},dZ^{[l]},A^{[l]},dA^{[l]}:(n^{[l]},m)\\W^{[l]},dW^{[l]}:(n^{[l]},n^{[l-1]})\\b^{[l]},db^{[l]}:(n^{[l]},1)$</p><blockquote><p>其中 $l=0$ 时，$A^{[0]}=X=(n^{[0]},m)$</p></blockquote><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="公式-1"><a href="#公式-1" class="headerlink" title="公式"></a>公式</h3><p>输入 $da^{[l]}$ 和缓存 $z^{[l]}$，输出 $da^{[l-1]}$，$dW^{[l]}$，$db^{[l]}$</p><ol><li><p>非向量化</p><script type="math/tex; mode=display">dz^{[l]}=da^{[l]}*g^{[l]'}(z^{[l]})\\dW^{[l]}=dz^{[l]}a^{[l-1]}\\db^{[l]}=dz^{[l]}\\da^{[l-1]}=W^{[l]T}dz^{[l]}\\上式可得：da^{[l]}=W^{[l+1]T}dz^{[l+1]}\\带入第一个式子可得：dz^{[l]}=W^{[l+1]T}dz^{[l+1]}*g^{[l]'}(z^{[l]})</script></li><li><p>向量化</p><script type="math/tex; mode=display">dZ^{[l]}=dA^{[l]}*g^{[l]'}(Z^{[l]})\\dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}\\db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)\\dA^{[l-1]}=W^{[l]T}dZ^{[l]}</script></li></ol><h2 id="参数-parameters-与超参数-hyperparameters"><a href="#参数-parameters-与超参数-hyperparameters" class="headerlink" title="参数 (parameters) 与超参数 (hyperparameters)"></a>参数 (parameters) 与超参数 (hyperparameters)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>什么是参数：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},W^{[3]},b^{[3]}…$ </p><p>什么是<strong>超参数</strong>：学习率 (learning rate) $\alpha$ ，迭代次数 (iterations)，隐藏层数 (hidden layers) L，隐藏单元数 (hidden units) $n^{[1]},n^{[2]}…$ ，激活函数 (activation function) 的选择，动量，最小批大小，各种形式的正则化参数等等，由于这些参数都会影响参数 W 和 b 的最终结果，所以我们称之为<strong>超参数</strong></p><h3 id="深度学习是一个基于试验的过程"><a href="#深度学习是一个基于试验的过程" class="headerlink" title="深度学习是一个基于试验的过程"></a>深度学习是一个基于试验的过程</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.10.png" alt=""></p><h3 id="深度学习和大脑有什么关系"><a href="#深度学习和大脑有什么关系" class="headerlink" title="深度学习和大脑有什么关系"></a>深度学习和大脑有什么关系</h3><p>吴恩达：深度学习和大脑相似度并不高，但深度学习和大脑的确有某些可以对比的地方，如下图，但是人脑中的神经元是如何进行学习的还是一个迷，而且至今我们还不清楚到底人脑中有没有一个类似于反向传播或者梯度下降的算法</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.11.png" alt=""></p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 3）</title>
      <link href="/2018/08/14/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w3%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/08/14/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w3%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<p>这是深度学习专项课程第一课第三周的编程作业，通过这次编程作业，可以学到：</p><ul><li>用一个单隐层神经网络实现一个二元分类器</li><li>使用非线性的激活函数</li><li>计算交叉熵损失</li><li>实现前向和后向传播</li></ul><h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> * <span class="comment"># 提供测试例子评估函数正确性</span></span><br><span class="line"><span class="keyword">import</span> sklearn <span class="comment"># 提供简单有效的数据挖掘和数据分析工具</span></span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets <span class="comment"># 提供一些有用的函数</span></span><br><span class="line"></span><br><span class="line">%matplotlib.inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># 使得每次生成的随机数都和第一次相同</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载一个花朵形状的二分类数据集进 X 和 Y</span></span><br><span class="line">X, Y = load_planar_dataset()</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment"># load_planar_dataset 的具体代码</span></span><br><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">load_planar_dataset</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     np.random.seed(<span class="number">1</span>)</span><br><span class="line">&gt;     m = <span class="number">400</span> <span class="comment"># number of examples</span></span><br><span class="line">&gt;     N = int(m/<span class="number">2</span>) <span class="comment"># number of points per class</span></span><br><span class="line">&gt;     D = <span class="number">2</span> <span class="comment"># dimensionality</span></span><br><span class="line">&gt;     X = np.zeros((m,D)) <span class="comment"># data matrix where each row is a single example</span></span><br><span class="line">&gt;     Y = np.zeros((m,<span class="number">1</span>), dtype=<span class="string">'uint8'</span>) <span class="comment"># labels vector (0 for red, 1 for blue)</span></span><br><span class="line">&gt;     a = <span class="number">4</span> <span class="comment"># maximum ray of the flower</span></span><br><span class="line">&gt; </span><br><span class="line">&gt;     <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">&gt;         ix = range(N*j,N*(j+<span class="number">1</span>))</span><br><span class="line">&gt;         t = np.linspace(j*<span class="number">3.12</span>,(j+<span class="number">1</span>)*<span class="number">3.12</span>,N) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># theta</span></span><br><span class="line">&gt;         r = a*np.sin(<span class="number">4</span>*t) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># radius</span></span><br><span class="line">&gt;         X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]</span><br><span class="line">&gt;         Y[ix] = j</span><br><span class="line">&gt;         </span><br><span class="line">&gt;     X = X.T</span><br><span class="line">&gt;     Y = Y.T</span><br><span class="line">&gt; </span><br><span class="line">&gt;     <span class="keyword">return</span> X, Y</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.spectral) <span class="comment"># 画散点图，其中参数 c 是颜色索引值，s 是尺寸，cmap 是颜色索引方式</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1.png" alt=""></p><h3 id="确定数据维度"><a href="#确定数据维度" class="headerlink" title="　确定数据维度"></a>　确定数据维度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定数据集的尺寸</span></span><br><span class="line">shape_X = np.shape(X)</span><br><span class="line">shape_Y = np.shape(Y)</span><br><span class="line">m = X.shape[<span class="number">1</span>] <span class="comment"># 训练集尺寸</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'the shape of X is:'</span> + str(shape_X))</span><br><span class="line">print(<span class="string">'the shape of Y is:'</span> + str(shape_Y))</span><br><span class="line">print(<span class="string">'I have m = %d training examples!'</span> %(m))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.2.png" alt=""></p><h2 id="用-sklearn-库实现简单的逻辑回归"><a href="#用-sklearn-库实现简单的逻辑回归" class="headerlink" title="用 sklearn 库实现简单的逻辑回归"></a>用 sklearn 库实现简单的逻辑回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练逻辑回归分类器</span></span><br><span class="line">clf = sklearn.linear_model.logisticRegressionCV() <span class="comment"># 建立一个逻辑回归的对象 clf</span></span><br><span class="line">clf.fit(X.T, Y.T) <span class="comment"># 用数据集拟合模型，返回一个训练过的模型对象，注意数据集的尺寸为 (n_samples, n_features)，所以要进行转置</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印逻辑回归的边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x),X,Y)</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">&gt;     <span class="comment"># 设置图形最大最小值</span></span><br><span class="line">&gt;     x_min, x_max = X[<span class="number">0</span>, :].min() - <span class="number">1</span>, X[<span class="number">0</span>, :].max() + <span class="number">1</span></span><br><span class="line">&gt;     y_min, y_max = X[<span class="number">1</span>, :].min() - <span class="number">1</span>, X[<span class="number">1</span>, :].max() + <span class="number">1</span></span><br><span class="line">&gt;     h = <span class="number">0.01</span></span><br><span class="line">&gt;     <span class="comment"># np.meshgrid() 形成间隔为 h 的点阵</span></span><br><span class="line">&gt;     xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">&gt;     <span class="comment"># 预测点阵的标签值</span></span><br><span class="line">&gt;     Z = model(np.c_[xx.ravel(), yy.ravel()]) <span class="comment"># np.c_[] 按行连接两个矩阵，ravel() 把多维数组变成一维数组</span></span><br><span class="line">&gt;     Z = Z.reshape(xx.shape)</span><br><span class="line">&gt;     <span class="comment"># 生成等高线图像</span></span><br><span class="line">&gt;     plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) <span class="comment"># 生成等高线图并添加颜色</span></span><br><span class="line">&gt;     plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">&gt;     plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">&gt;     plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=y, cmap=plt.cm.Spectral)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印精确度</span></span><br><span class="line">LR_predictions = clf.predict(X.T) <span class="comment"># 用模型预测的 X 的标签值</span></span><br><span class="line">print(<span class="string">'Accuracy of logistic regression: %d '</span> % float( (np.dot(Y,LR_prediction) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.4.png" alt=""></p><p>我们可以发现使用逻辑回归进行预测的准确度非常低！</p><h2 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h2><h3 id="建立神经网络的一般方法"><a href="#建立神经网络的一般方法" class="headerlink" title="建立神经网络的一般方法"></a>建立神经网络的一般方法</h3><ul><li>定义神经网络结构，包括输入特征个数，隐藏神经元个数等</li><li>初始化模型参数</li><li>循环以下步骤：<ul><li>实现前向传播</li><li>计算代价函数</li><li>实现反向传播，得到梯度</li><li>使用梯度下降更新参数</li></ul></li><li>将所有辅助函数合并到一个主函数 nn_model() 中</li></ul><h3 id="定义神经网络结构"><a href="#定义神经网络结构" class="headerlink" title="定义神经网络结构"></a>定义神经网络结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定 n_x n_h n_y 三个变量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    </span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># 计算输入层的尺寸</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment"># 计算隐藏层的尺寸，即隐藏神经元个数</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># 计算输出层的尺寸</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>注意：</p><ul><li>确保所有参数的尺寸正确</li><li>对于权重 W 采用<strong>随机初始化</strong>的方法</li><li>对于偏差 b 采用初始化为<strong>零</strong>的方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    </span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span> <span class="comment"># 0.01 使得初始化后的参数更小，梯度下降更快，W1(n_h, n_x)</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>)) <span class="comment"># 切记是 np.zeros(()),b1(n_h,1)</span></span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span> <span class="comment"># W2(n_y, n_h)</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>)) <span class="comment"># b2(n_y,1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保参数唯独正确</span></span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数值放入字典 parameters</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>步骤：</p><ul><li>从 parameters 字典中取出每个参数</li><li>实现前向传播，计算 Z1,A1,Z2,A2 的值</li><li>把这些值放在字典 cache 中</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 把参数从 parameters 取出来</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算Z1 Z2 A1 A2 的值</span></span><br><span class="line">    Z1 = np.dot(W1,X) + b1</span><br><span class="line">    A1 = np.tanh(Z1) <span class="comment"># numpy 内置的 tanh() 函数</span></span><br><span class="line">    Z2 = np.dot(W2,A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2) <span class="comment"># sigmoid 已经 imported</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保 A2 的尺寸正确</span></span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 把求到的值放入字典 cache</span></span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><h3 id="计算代价函数"><a href="#计算代价函数" class="headerlink" title="计算代价函数"></a>计算代价函数</h3><script type="math/tex; mode=display">J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="number">1</span>-A2), (<span class="number">1</span>-Y)) <span class="comment"># np.multiply() 为逐元素相乘</span></span><br><span class="line">    cost = -(<span class="number">1</span>/m)*np.sum(logprobs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保 cost 的维度是我们需要的，比如把 [[17]] 变成 17</span></span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost,float))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.14.png" alt=""></p><ul><li>因为  $g^{[1]}(z) = tanh(z)$ ， 所以 $g^{[1]’}(z) = 1-a^2$，所以可以使用 <code>(1 - np.power(A1, 2))</code> 来计算 $g^{[1]’}(Z^{[1]})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">back_propgation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 首先将 W1 和 W2 从 parameters 里取出来,将 A1 和 A2 从 cache 里取出来</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = paramaters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = (<span class="number">1</span>/m)*np.dot(dZ2,A1.T)</span><br><span class="line">    db2 = (<span class="number">1</span>/m)*np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T,dZ2),(<span class="number">1</span>-np.power(A1,<span class="number">2</span>)))</span><br><span class="line">    dW1 = (<span class="number">1</span>/m)*np.dot(dZ1,X.T)</span><br><span class="line">    db1 = (<span class="number">1</span>/m)*np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将数据存入 grads 中</span></span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><script type="math/tex; mode=display"> \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降法更新数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 首先将参数从 parameters 中取出来</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 然后将梯度从 grads 中取出来</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数，此处只更新一次参数</span></span><br><span class="line">    W1 = W1 - learning_rate*dW1</span><br><span class="line">    b1 = b1 - learning_rate*db1</span><br><span class="line">    W2 = W2 - learning_rate*dW2</span><br><span class="line">    b2 = b2 - learning_rate*db2</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将更新后的参数放到 parameters 中</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="将以上所有辅助函数合并到主函数中"><a href="#将以上所有辅助函数合并到主函数中" class="headerlink" title="将以上所有辅助函数合并到主函数中"></a>将以上所有辅助函数合并到主函数中</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iteration = <span class="number">10000</span>, print_cost=False )</span>:</span></span><br><span class="line">    <span class="comment"># 定义模型结构</span></span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sized(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数取出</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降迭代</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        A2, cache = forword_propagation(X, paramaters)</span><br><span class="line">        <span class="comment"># 计算代价函数</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line">        <span class="comment"># 梯度下降更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate = <span class="number">1.2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每千步打印一次 cost 值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>：</span><br><span class="line">        print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="用模型进行预测"><a href="#用模型进行预测" class="headerlink" title="用模型进行预测"></a>用模型进行预测</h3><p>如果最后的预测值大于 0.5，则标签值为 1，反之标签值为 0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用前向传递函数进行预测</span></span><br><span class="line">    A2, cache = forward_propagation(X,parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>) </span><br><span class="line">    <span class="comment"># predictions = np.where(A2 &gt; 0.5, 1, 0)</span></span><br><span class="line">    <span class="comment"># numpy.where(condition, x, y)当 conditon 的某个位置的为 true 时，输出 x 的对应位置的元素，否则选择 y 对应位置的元素；</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h2 id="分析讨论"><a href="#分析讨论" class="headerlink" title="分析讨论"></a>分析讨论</h2><h3 id="当隐藏节点个数为-4-时"><a href="#当隐藏节点个数为-4-时" class="headerlink" title="当隐藏节点个数为 4 时"></a>当隐藏节点个数为 4 时</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 n_h = 4 的模型</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印出分类边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印出精确度</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))  /  float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.7.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.8.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.9.png" alt=""></p><p>我们可以看到对比逻辑回归，四个隐藏节点的神经网络模型的预测精确度非常高！</p><p>接下来我们看看不同隐藏节点对预测结果的影响。</p><h3 id="改变隐藏节点个数观察结果"><a href="#改变隐藏节点个数观察结果" class="headerlink" title="改变隐藏节点个数观察结果"></a>改变隐藏节点个数观察结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印不同隐藏节点模型的预测结果</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>] <span class="comment"># 不同隐藏节点个数</span></span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.10.png" alt=""></p><h3 id="得出结论"><a href="#得出结论" class="headerlink" title="得出结论"></a>得出结论</h3><ul><li>隐藏节点的个数越多，对数据集的拟合程度越好，直到最终发生过拟合</li><li>最好的隐藏节点个数似乎大概在 5 个左右，在这个值附近的的模型对数据集拟合得很好且没有发生过拟合</li><li>regularization 是减小大型模型（比如 n_h = 50）过拟合的一个方法，在后面会学到 </li></ul><h3 id="更换其他数据集进行试验"><a href="#更换其他数据集进行试验" class="headerlink" title="更换其他数据集进行试验"></a>更换其他数据集进行试验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line"></span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"></span><br><span class="line">dataset = <span class="string">"noisy_circles"</span> <span class="comment"># 选择数据集</span></span><br><span class="line"></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">5</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出分类边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印精确度</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.11.png" alt=""></p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 3）—— 浅层神经网络</title>
      <link href="/2018/08/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w3/"/>
      <url>/2018/08/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w3/</url>
      <content type="html"><![CDATA[<p>本周主要介绍<strong>单隐层神经网络</strong> (one hidden layer Neural Network)。</p><h2 id="神经网络概况"><a href="#神经网络概况" class="headerlink" title="神经网络概况"></a>神经网络概况</h2><h3 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h3><p>逻辑回归的计算图如下，这是一个最小的神经网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.1.png" alt=""></p><p>堆叠一系列的 $\sigma$ 单元，构建一个单隐层神经网络：</p><a id="more"></a><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.3.png" alt=""></p><h3 id="神经网络的含义"><a href="#神经网络的含义" class="headerlink" title="神经网络的含义"></a>神经网络的含义</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.4.png" alt=""></p><ul><li>第一个方框称之为 <strong>输入层 (Input layer)</strong> ，这一层的激活向量用 $a^{[0]}$ 表示，$x = a^{[0]}$，其中字母 a 表示 activate(激活值)</li><li>第二个方框称之为 <strong>隐藏层 (Hidden layer )</strong>，这一层的激活向量用 $a^{[1]}$ 表示</li><li>第三个方框称之为 <strong>输出层 (Output layer)</strong>，这一层的激活向量用 $a^{[2]}$ 表示，$\hat y = a^{[2]}$</li><li>神经网络的层数用右上角的方括号表示，输入层不算入层数</li><li>第一层的参数为 $W^{[1]} (4,3)$ 和 $b^{[1]} (4,1)$</li><li>第二层的参数为 $W^{[2]} (1,4)$ 和 $b^{[2]} (1,4)$</li><li>每个圆圈称之为 <strong>节点 (node)</strong></li></ul><h2 id="神经网络的前向传播及其向量化"><a href="#神经网络的前向传播及其向量化" class="headerlink" title="神经网络的前向传播及其向量化"></a>神经网络的前向传播及其向量化</h2><h3 id="非向量化公式"><a href="#非向量化公式" class="headerlink" title="非向量化公式"></a>非向量化公式</h3><p>在逻辑回归中一个圆圈代表两步运算，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.5.png" alt=""></p><p>在神经网络中每个圆圈也代表两步运算，将圆圈隔离出来看便是一个逻辑回归模型，如图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.6.png" alt=""></p><p>整理成如下的样子：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.7.png" alt=""></p><h3 id="单训练样本的向量化"><a href="#单训练样本的向量化" class="headerlink" title="单训练样本的向量化"></a>单训练样本的向量化</h3><p>已知：</p><script type="math/tex; mode=display">z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1, a^{[1]}_1= \sigma (z^{[1]}_1) \\z^{[1]}_2 = w^{[1]T}_2 x + b^{[1]}_2, a^{[1]}_2= \sigma (z^{[1]}_2) \\z^{[1]}_3 = w^{[1]T}_3 x + b^{[1]}_3, a^{[1]}_3= \sigma (z^{[1]}_3) \\z^{[1]}_4 = w^{[1]T}_4 x + b^{[1]}_4, a^{[1]}_4= \sigma (z^{[1]}_4) \\</script><p>推导：</p><script type="math/tex; mode=display">z^{[1]}=\begin{bmatrix}z^{[1]}_1\\ z^{[1]}_2\\ z^{[1]}_3\\ z^{[1]}_4\end{bmatrix}=\begin{bmatrix}w^{[1]T}_1x+b^{[1]}_1\\ w^{[1]T}_2x+b^{[1]}_2\\ w^{[1]T}_3x+b^{[1]}_3\\ w^{[1]T}_4x+b^{[1]}_4\end{bmatrix}=\begin{bmatrix}--& w^{[1]T}_1 &-- \\  --& w^{[1]T}_2 &-- \\ -- & w^{[1]T}_3 & --\\  --& w^{[1]T}_4 & --\end{bmatrix}_{(4,3)}\begin{bmatrix}x_1\\ x_2\\ x_3\end{bmatrix}+\begin{bmatrix}b^{[1]}_1\\ b^{[1]}_2\\ b^{[1]}_3\\ b^{[1]}_4\end{bmatrix}=W^{[1]}x + b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=\begin{bmatrix}a^{[1]}_1\\ a^{[1]}_2\\ a^{[1]}_3\\ a^{[1]}_4\end{bmatrix}=\sigma (z^{[1]})</script><p>最后得到计算神经网络前向传播的四个公式：</p><script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]} =W^{[1]}a^{[0]}+b^{[1]}\\a^{[1]}=\sigma (z^{[1]}) \\z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} \\a^{[2]}=\sigma (z^{[2]}) \\</script><blockquote><p>注意：</p><script type="math/tex; mode=display">\begin{bmatrix}--& w^{[1]T}_1 &-- \\  --& w^{[1]T}_2 &-- \\ -- & w^{[1]T}_3 & --\\  --& w^{[1]T}_4 & --\end{bmatrix}_{(4,3)}=W^{[1]} \neq W^{[1]T}</script><p>这里与逻辑回归中有所不同，参数 $W^{[1]} (4,3)$ 和 $W^{[2]} (1,4)$ 的右上角没有转置符号，所有的参数 w 都成了行向量而不是列向量，也就是说 $W^{[2]}$ 相当于逻辑回归中的 $w^T$ </p></blockquote><h3 id="多个训练样本的向量化"><a href="#多个训练样本的向量化" class="headerlink" title="多个训练样本的向量化"></a>多个训练样本的向量化</h3><ol><li>记法：</li></ol><p>对于第 i 个训练样本的预测值我们记为：$\hat y^{(i)} = a^{ [2]  (i)}$，其中 [2] 表示第二层，(i) 表示第 i 个训练样本。</p><p>为了输出所有样本的预测值，我们可以遍历所有的样本：</p><p>$for i = 1 \quad to \quad  m: \\ \quad z^{[1] (i)}=W^{[1]} x^{(i)}+b^{[1]} =W^{[1]} a^{[0] (i)}+b^{[1]} \\  \quad a^{[1] (i)}=\sigma (z^{[1] (i)}) \\ \quad z^{[2] (i)}=W^{[2]} a^{[1] (i)}+b^{[2]} \\ \quad a^{[2] (i)}=\sigma (z^{[2] (i)}) $</p><ol><li>公式：</li></ol><script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}\\A^{[1]}=\sigma (Z^{[1]})\\Z^{[2]}=W^{[2]}A^{[2]}+b^{[1]}\\A^{[2]}=\sigma (Z^{[2]})\\</script><p>其中：</p><script type="math/tex; mode=display">Z^{[1]}= \begin{bmatrix}| &|  &  &| \\  z^{[1](1)}&  z^{[1](2)} & ... &  z^{[1](m)}\\  |& | &  &| \end{bmatrix}\quadA^{[1]}= \begin{bmatrix}| &|  &  &| \\  a^{[1](1)}&  a^{[1](2)} & ... &  a^{[1](m)}\\  |& | &  &| \end{bmatrix}\quadX^{[1]}= \begin{bmatrix}| &|  &  &| \\  x^{[1](1)}&  x^{[1](2)} & ... &  x^{[1](m)}\\  |& | &  &| \end{bmatrix}\\</script><script type="math/tex; mode=display">Z^{[2]}= \begin{bmatrix}| &|  &  &| \\  z^{[2](1)}&  z^{[2](2)} & ... &  z^{[2](m)}\\  |& | &  &| \end{bmatrix}\quadA^{[2]}= \begin{bmatrix}| &|  &  &| \\  a^{[2](1)}&  a^{[2](2)} & ... &  a^{[2](m)}\\  |& | &  &| \end{bmatrix}\quadX^{[2]}= \begin{bmatrix}| &|  &  &| \\  x^{[2](1)}&  x^{[2](2)} & ... &  x^{[2](m)}\\  |& | &  &| \end{bmatrix}\\</script><blockquote><p>矩阵的<strong>横向</strong>代表不同的<strong>训练样本</strong>，<strong>纵向</strong>代表不同的<strong>隐藏节点 (hidden unit)</strong> </p></blockquote><ol><li>推导过程：</li></ol><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.8.png" alt=""></p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h3><h4 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h4><script type="math/tex; mode=display">sigmoid: g(z)=a=\frac {1}{1+e^{-z}}\\ \quad \quad \quad g'(z)=a(1-a)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.9.png" alt=""></p><ul><li>由于 tanh 函数比 sigmoid 函数更好，所以 sigmoid 函数只用在<strong>二元分类问题的输出层</strong></li></ul><h4 id="tanh-函数"><a href="#tanh-函数" class="headerlink" title="tanh 函数"></a>tanh 函数</h4><script type="math/tex; mode=display">tanh：g(z)=a=\frac {e^{z}-e^{-z}}{e^{z}+e^{-z}}\\\quad \quad \quad \quad \quad \quad \quad \quad g'(z)=1-(tanh(z))^2=1-a^2</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.10.png" alt=""></p><ul><li>也叫双曲正切函数，是一个移位后重新调整比例使输出范围在 -1 到 1 之间的 sigmoid 函数</li><li>sigmoid 函数使隐藏层输出的平均值逼近于 0.5 ，而 tanh 函数更加逼近 0，这样使得下一层的学习更加简单，所以 tanh 函数在大多数情况下都严格优于 sigmoid 函数</li><li>缺点是在 z 很大时，函数的斜率会接近 0 ，这样使得学习速率变得很慢</li></ul><h4 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="ReLU 函数"></a>ReLU 函数</h4><script type="math/tex; mode=display">ReLU:g(z)=a=max\{0,z\}\\\quad \quad \quad \quad g'(z)=\left\{\begin{matrix}0 &  &if \ z<0 \\  1&  & if \ z\geqslant 0\end{matrix}\right.</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.11.png" alt=""></p><ul><li>实际上当 z = 0 时导数无定义，但是工程上由于 z = 0 的概率无限小，所以 g’(0) 可以设为任意值</li><li>也叫线性整流函数</li><li>减少了激活函数导数趋向 0 的现象</li><li>隐藏层的激活函数的首选，用的最多</li></ul><h4 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h4><script type="math/tex; mode=display">Leaky ReLU:g(z)= a = max\{ 0.01z,z\}\\\quad \quad \quad \quad \quad \quad \quad g'(z)=\left\{\begin{matrix}0.01 &  &if \ z<0 \\  1&  & if \ z\geqslant 0\end{matrix}\right.</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.12.png" alt=""></p><ul><li>实际上当 z = 0 时导数无定义，但是工程上由于 z = 0 的概率无限小，所以 g’(0) 可以设为任意值</li><li>一般效果好于 ReLU，但是用的较少</li></ul><h3 id="为什么神经网络需要非线性激活函数"><a href="#为什么神经网络需要非线性激活函数" class="headerlink" title="为什么神经网络需要非线性激活函数"></a>为什么神经网络需要非线性激活函数</h3><p>已知公式：</p><p>$z^{[1]}=W^{[1]} x+b^{[1]} \\ a^{[1]}=g^{[1]} (z^{[1]}) \\ z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \\ a^{[2]} = g^{[2]} (z^{[2]})$</p><p>假设 $g^{[1]} 和g^{[2]}$ 是线性激活函数：$g^{[1]}=g^{[2]}=z$  </p><p>即：$a^{[1]}=z^{[1]}$，$a^{[2]}=z^{[2]}$ </p><p>可以推导出：</p><p>$a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}z^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}\\=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W’x+b’$</p><p>我们可以发现，当激活函数是线性函数时，不管神经网络有多少层，最后的输出值只是输入值的线性函数，这样还不如去除所有的隐藏层，因为我们可以看到最后的结果和逻辑回归模型一模一样，所以线性的隐藏层<strong>没有任何作用</strong>，线性函数的组合还是线性函数。</p><blockquote><p>只有一种情况会用到线性激活函数，就是例如预测房价这类回归问题的<strong>输出层</strong></p></blockquote><h2 id="神经网络的反向传播及其向量化"><a href="#神经网络的反向传播及其向量化" class="headerlink" title="神经网络的反向传播及其向量化"></a>神经网络的反向传播及其向量化</h2><h3 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h3><p>$输入: n_x=n^{[0]}, n^{[1]},n^{[2]}=1\\参数: W^{[1]} \rightarrow (n^{[1]},n^{[0]}),b^{[1]}\rightarrow (n^{[1]},1),W^{[2]} \rightarrow  (n^{[2]},n^{[1]}),b^{[2]} \rightarrow (n^{[1]},1)\\ 代价函数：J(W^{[1]},b^{[1]},W^{[2]},b^{[2]})=\frac {1}{m} \sum\limits_{i=1}^nL(\hat y,y),\hat y=a^{[1]}\\梯度下降：\\ repeat:\{ \\ 计算预测值 \hat y,i=1,2…,m\\计算 dW^{[1]}=\frac {dJ}{dW^{[1]}}, db^{[1]}=\frac {dJ}{db^{[1]}},…\\ W^{[1]}:=  W^{[1]}-\alpha dW^{[1]}\\b^{[1]}:=  b^{[1]}-\alpha db^{[1]}\\W^{[2]}:=  W^{[2]}-\alpha dW^{[2]}\\b^{[2]}:=  b^{[2]}-\alpha db^{[2]}\\ \}$</p><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.14.png" alt=""></p><ul><li>左边是单个训练样本的向量化，右边是多个训练样本的向量化</li><li>keepdims = True 是为了保持输出我们需要的维度，防止出现秩为 1 的数组</li><li>第四行的 <strong>*</strong> 号代表 <strong>逐元素相乘</strong>  </li><li>由于 W 的每个 w 都变成行向量，这点与逻辑回归不同，故第二行、第五行的 $a^{[1]T},x^T,A^{[1]T},X^T$ 都要加转置符号</li><li>只有当是二元分类时，第一行公式才成立</li><li>$dZ^{[1]} \rightarrow (n^{[1]},m),W^{[2]T}dZ^{[2]} \rightarrow (n^{[1]},m),g^{[1]’}(Z^{[1]}) \rightarrow (n^{[1]},m)$</li></ul><h3 id="向量化的推导过程"><a href="#向量化的推导过程" class="headerlink" title="向量化的推导过程"></a>向量化的推导过程</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.15.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.17.png" alt=""></p><h2 id="参数的随机初始化"><a href="#参数的随机初始化" class="headerlink" title="参数的随机初始化"></a>参数的随机初始化</h2><h3 id="为什么不能将权重-W-全初始化为零"><a href="#为什么不能将权重-W-全初始化为零" class="headerlink" title="为什么不能将权重  W 全初始化为零"></a>为什么不能将权重  W 全初始化为零</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.18.png" alt=""></p><p>对于上面的的神经网络，我们假设将 $W^{[1]}$ 和 $b^{[1]}$ 全都初始化为零，正如我们在逻辑回归中做的那样，即 $W^{[1]}=\begin{bmatrix}0 &amp;0 \\0 &amp; 0\end{bmatrix}\quad b^{[1]}=\begin{bmatrix}0\\0\end{bmatrix}$ ，导致 $a^{[1]}_1=a^{[2]}_2$ ，在反向传播时有，$dz^{[1]}_1=dz^{[2]}_2$ ，于是 $dW=\begin{bmatrix}u&amp;v \\u &amp; v\end{bmatrix}$ ，由于 $W^{[1]}=W^{[1]}-\alpha dW$，所以 $W^{[1]}=\begin{bmatrix}a&amp;b \\a &amp; b\end{bmatrix}$，我们可以看到，无论迭代多少次，无论神经网络训练多久，这两个隐藏神经元的参数始终都相同，他们始终都在做相同的运算，称这两个神经元是<strong>对称的</strong>，不能给我们带来任何帮助。</p><h3 id="参数随机初始化"><a href="#参数随机初始化" class="headerlink" title="参数随机初始化"></a>参数随机初始化</h3><script type="math/tex; mode=display">W^{[1]}=np.random.randn((2,2))*0.01\\b^{[1]}=np,zero((2,1))\\W^{[2]}=np.random.randn((1,2))*0.01\\b^{[2]}=0</script><blockquote><p>数据 <strong>0.0.1</strong> 必须比较小，因为如果 W 非常大，那么 z 也相应会非常大，那么可以发现此处对应的激活函数比如 sigmoid 或者 tanh 函数的斜率会非常小，这意味着梯度下降得非常慢，所以应该使 W 为一个较小的值</p></blockquote>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 2）</title>
      <link href="/2018/08/09/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/08/09/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h2 id="numpy-使用基础"><a href="#numpy-使用基础" class="headerlink" title="numpy 使用基础"></a>numpy 使用基础</h2><p>Numpy 是 Python 里用于科学计算的模块，由一个<a href="www.numpy.org">开源社区</a>进行维护，下面介绍一些用于神经网络搭建的函数的构建</p><h3 id="sigmoid-函数，np-exp"><a href="#sigmoid-函数，np-exp" class="headerlink" title="sigmoid 函数，np.exp( )"></a>sigmoid 函数，np.exp( )</h3><p>回忆：$sigmoid(x)=\frac {1}{1+e^{-x}}$</p><p>如果 $ x = (x_1, x_2, …, x_n)$ 是一个行向量，那么 $np.exp(x)$ 会将 $exp( )$ 函数用于 x 的每个元素，输出 $np.exp(x) = (e^{x_1}, e^{x_2}, …, e^{x_n})$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.exp的例子</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])<span class="comment">#x 为一个行向量</span></span><br><span class="line">print(np.exp(x)) <span class="comment"># 结果为 (exp(1), exp(2), exp(3))</span></span><br></pre></td></tr></table></figure><p>>> [  2.71828183   7.3890561   20.08553692]</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更多向量操作例子</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (x + <span class="number">3</span>)</span><br><span class="line">print(<span class="number">1</span>/x)</span><br></pre></td></tr></table></figure><p>>&gt;[4 5 6]</p><p>>&gt;[ 1.          0.5         0.33333333]</p><script type="math/tex; mode=display">\text{For } x \in \mathbb {R}^n \text{,     } sigmoid(x) = sigmoid\begin{pmatrix}x_1  \\x_2  \\    ...  \\    x_n  \\\end{pmatrix} = \begin{pmatrix}\frac{1}{1+e^{-x_1}}  \\\frac{1}{1+e^{-x_2}}  \\    ...  \\    \frac{1}{1+e^{-x_n}}  \\\end{pmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#用 np.exp() 代替 numpy.exp()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">sigmoid(x)</span><br></pre></td></tr></table></figure><p>>&gt;array([ 0.73105858,  0.88079708,  0.95257413])</p><h3 id="求-sigmoid-函数的导数-dZ"><a href="#求-sigmoid-函数的导数-dZ" class="headerlink" title="求 sigmoid 函数的导数 (dZ)"></a>求 sigmoid 函数的导数 (dZ)</h3><blockquote><p>注意：在前面的笔记中，吴恩达的课程视频中写的 dZ 的求法是：$dZ=A-Y$，但是在课程 ppt 和编程作业中的求法是：$dZ=\sigma(x)(1-\sigma(x))$</p></blockquote><script type="math/tex; mode=display">sigmoid\_derivative(x) = {\sigma' (x)} = \sigma(x)(1-\sigma(x))=A(1-A)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span></span><br><span class="line"><span class="string">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    s = sigmoid(x)</span><br><span class="line">    ds = s * (<span class="number">1</span> - s)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid_derivative(x) = "</span> + str(sigmoid_derivative(x)))</span><br></pre></td></tr></table></figure><p>>&gt;sigmoid_derivative(x) = [ 0.19661193  0.10499359  0.04517666]</p><h3 id="reshape-命令重构矩阵"><a href="#reshape-命令重构矩阵" class="headerlink" title="reshape 命令重构矩阵"></a>reshape 命令重构矩阵</h3><ul><li>X.shape 命令用于得到一个矩阵或向量的维度（形状）</li><li>X.reshape 命令用于把 X 重构为某个维度，例如把 shape (length,height,depth=3) 的图片变成输入，重构为 shape (length∗height∗3,1) 的向量</li></ul><p>下面的代码把一张图片变为一个向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    v = image.reshape((image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>],<span class="number">1</span>)) </span><br><span class="line">    <span class="comment">#image.shape[0] 获得图片或数组第一个维度的长度</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values</span></span><br><span class="line">image = np.array([[[ <span class="number">0.67826139</span>,  <span class="number">0.29380381</span>],</span><br><span class="line">        [ <span class="number">0.90714982</span>,  <span class="number">0.52835647</span>],</span><br><span class="line">        [ <span class="number">0.4215251</span> ,  <span class="number">0.45017551</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">0.92814219</span>,  <span class="number">0.96677647</span>],</span><br><span class="line">        [ <span class="number">0.85304703</span>,  <span class="number">0.52351845</span>],</span><br><span class="line">        [ <span class="number">0.19981397</span>,  <span class="number">0.27417313</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">0.60659855</span>,  <span class="number">0.00533165</span>],</span><br><span class="line">        [ <span class="number">0.10820313</span>,  <span class="number">0.49978937</span>],</span><br><span class="line">        [ <span class="number">0.34144279</span>,  <span class="number">0.94630077</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"image2vector(image) = "</span> + str(image2vector(image)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">image2vector(image) = [[ 0.67826139]</span><br><span class="line"> [ 0.29380381]</span><br><span class="line"> [ 0.90714982]</span><br><span class="line"> [ 0.52835647]</span><br><span class="line"> [ 0.4215251 ]</span><br><span class="line"> [ 0.45017551]</span><br><span class="line"> [ 0.92814219]</span><br><span class="line"> [ 0.96677647]</span><br><span class="line"> [ 0.85304703]</span><br><span class="line"> [ 0.52351845]</span><br><span class="line"> [ 0.19981397]</span><br><span class="line"> [ 0.27417313]</span><br><span class="line"> [ 0.60659855]</span><br><span class="line"> [ 0.00533165]</span><br><span class="line"> [ 0.10820313]</span><br><span class="line"> [ 0.49978937]</span><br><span class="line"> [ 0.34144279]</span><br><span class="line"> [ 0.94630077]]</span><br></pre></td></tr></table></figure><h3 id="输入矩阵的行向量标准化"><a href="#输入矩阵的行向量标准化" class="headerlink" title="输入矩阵的行向量标准化"></a>输入矩阵的行向量标准化</h3><p>将数据标准化会使得模型表现得更好，因为标准化之后梯度下降收敛速度更快，我们可以通过将输入矩阵 x 每一个<strong>行向量</strong>除以该行向量的模，即 $ \frac{x}{| x|} $</p><p>如果 <script type="math/tex">x = \begin{bmatrix}    0 & 3 & 4 \\    2 & 6 & 4 \\\end{bmatrix}</script> 那么 $| x| = np.linalg.norm(x, axis = 1, keepdims = True) =\begin{bmatrix}    5 \\    \sqrt{56} \\\end{bmatrix} $ 而且 $ x_normalized = \frac{x}{| x|} = \begin{bmatrix}    0 &amp; \frac{3}{5} &amp; \frac{4}{5} \\    \frac{2}{\sqrt{56}} &amp; \frac{6}{\sqrt{56}} &amp; \frac{4}{\sqrt{56}} \\\end{bmatrix}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    x_norm = np.linalg.norm(x, ord = <span class="number">2</span>,axis = <span class="number">1</span>,keepdims = <span class="keyword">True</span>)</span><br><span class="line">    <span class="comment">#计算 x 的范数或者说行向量的模，其中 ord = 2 表示范数类型为 2，即平方和的开方，axis =1 表示按行向量处理，keepdims = True 表示保持矩阵的二维特性</span></span><br><span class="line">    x = x / x_norm <span class="comment">#用 x 矩阵除以行向量的模，自动进行广播拓展</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line">print(<span class="string">"normalizeRows(x) = "</span> + str(normalizeRows(x)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; normalizeRows(x) = [[ 0.          0.6         0.8       ]</span><br><span class="line"> [ 0.13736056  0.82416338  0.54944226]]</span><br></pre></td></tr></table></figure><h3 id="广播（broadcasting）和-softmax-函数"><a href="#广播（broadcasting）和-softmax-函数" class="headerlink" title="广播（broadcasting）和 softmax 函数"></a>广播（broadcasting）和 softmax 函数</h3><p>你可以把 softmax 函数看作一个用来标准化的函数，当算法需要进行二元或者更多元分类时</p><ul><li><p>$ \text{for  } \  x \in \mathbb{R}^{1\times n} \text{,     } softmax(x) = softmax(\begin{bmatrix}  x_1  &amp;&amp;    x_2 &amp;&amp;    …  &amp;&amp;    x_n  \end{bmatrix}) = \begin{bmatrix}     \frac{e^{x_1}}{\sum_{j}e^{x_j}}  &amp;&amp;    \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &amp;&amp;    …  &amp;&amp;    \frac{e^{x_n}}{\sum_{j}e^{x_j}} \end{bmatrix} $</p></li><li><p>$\text{for a matrix } x \in \mathbb{R}^{m \times n} \text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  </p><script type="math/tex; mode=display">softmax(x) = softmax\begin{bmatrix}    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\    \vdots & \vdots & \vdots & \ddots & \vdots \\    x_{m1} & x_{m2} & x_{m3} & \dots  & x_{mn}\end{bmatrix} \\= \begin{bmatrix}    \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} & \dots  & \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\    \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} & \dots  & \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\    \vdots & \vdots & \vdots & \ddots & \vdots \\    \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} & \dots  & \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}\end{bmatrix} = \begin{pmatrix}    softmax\text{(first row of x)}  \\    softmax\text{(second row of x)} \\    ...  \\    softmax\text{(last row of x)} \\\end{pmatrix}</script></li></ul><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Calculates the softmax for each row of the input x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Your code should work for a row vector and also for matrices of shape (n, m).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n,m)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    s -- A numpy matrix equal to the softmax of x, of shape (n,m)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对 x 每个元素求 exp()</span></span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 x_exp 每行的和，使用 np.sum(..., axis = 1, keepdims = True)</span></span><br><span class="line">    x_sum = np.sum(x_exp, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用广播特性自动计算 softmax 函数</span></span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span> ,<span class="number">0</span>]])</span><br><span class="line">print(<span class="string">"softmax(x) = "</span> + str(softmax(x)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; softmax(x) = [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04</span><br><span class="line">    1.21052389e-04]</span><br><span class="line"> [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04</span><br><span class="line">    8.01252314e-04]]</span><br></pre></td></tr></table></figure><h3 id="两种损失函数-L1-和-L2"><a href="#两种损失函数-L1-和-L2" class="headerlink" title="两种损失函数 L1 和 L2"></a>两种损失函数 L1 和 L2</h3><ol><li><p>L1 损失函数定义为：</p><script type="math/tex; mode=display">\begin{align*} & L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}| \end{align*}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L1 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = np.sum(np.abs(y - yhat))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L1 = "</span> + str(L1(yhat,y)))</span><br></pre></td></tr></table></figure><p>>&gt; L1 = 1.1</p></li><li><p>L2 损失函数定义为：</p><script type="math/tex; mode=display">\begin{align*} & L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2 \end{align*}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L2 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = np.dot(y-yhat, y-yhat) <span class="comment"># 直接写成自己和自己做点积</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L2 = "</span> + str(L2(yhat,y)))</span><br></pre></td></tr></table></figure><p>>&gt; L2 = 0.43</p></li></ol><h2 id="用逻辑回归识别猫的图片"><a href="#用逻辑回归识别猫的图片" class="headerlink" title="用逻辑回归识别猫的图片"></a>用逻辑回归识别猫的图片</h2><p>实现步骤：</p><ul><li>建立一个学习算法的一般结构，包括：<ul><li>初始化参数</li><li>计算代价函数和它对参数的导数</li><li>使用梯度下降法迭代参数</li></ul></li><li>把这三个函数用正确的顺序聚合到一个模型主函数中</li></ul><h3 id="相关包"><a href="#相关包" class="headerlink" title="相关包"></a>相关包</h3><ul><li>numpy：python 科学计算基础包</li><li>h5py：与存储在 H5 文件中的数据集交互的常用的包 </li><li>matplotlib：python 中用来画图像的一个包</li><li>PIL 和 scipy：最后用来测试自己的模型使用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 方便调用</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><p>建立一个简单的判断图片是不是猫的识别算法，数据集包括：</p><ul><li>m_train 个训练集，包括图片集 train_set_x_orig 和标签集 train_set_y</li><li>m_test 个测试集，包括图片集 test_set_x_orig 和标签集 test_set_y</li><li>每张图片都是<strong>方形</strong> (height = num_px, width = num_px)，有三个颜色通道，所以数组形状是 (num_px, num_px, 3)</li><li>每个图片集都要进行预处理，所以原始数据加上 <strong>_orig</strong>，但是标签集不需要预处理</li></ul><h4 id="加载原始数据集"><a href="#加载原始数据集" class="headerlink" title="加载原始数据集"></a>加载原始数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(train_set_x_orig[index]) <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</span><br><span class="line"><span class="comment"># np.squeeze() 用于把一个数组的 shape 中为 1 的维度删掉，即让 train_set_y[:, index] 变为一个数</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_1.png" alt=""></p><h4 id="确定图片维度和个数以防止出错"><a href="#确定图片维度和个数以防止出错" class="headerlink" title="确定图片维度和个数以防止出错"></a>确定图片维度和个数以防止出错</h4><ul><li>训练集的个数：m_train</li><li>测试集的个数：m_test</li><li>图片（正方形）的尺寸即边长的像素数：num_px</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定维度和个数</span></span><br><span class="line"><span class="comment"># train_set_x_orig 形状为 (m_train, num_px, num_px, 3)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;Number of training examples: m_train = 209</span><br><span class="line">  Number of testing examples: m_test = 50</span><br><span class="line">  Height/Width of each image: num_px = 64</span><br><span class="line">  Each image is of size: (64, 64, 3)</span><br><span class="line">  train_set_x shape: (209, 64, 64, 3)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x shape: (50, 64, 64, 3)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br></pre></td></tr></table></figure><h4 id="重构图片数组变为标准输入矩阵"><a href="#重构图片数组变为标准输入矩阵" class="headerlink" title="重构图片数组变为标准输入矩阵"></a>重构图片数组变为标准输入矩阵</h4><p>把尺寸为 (num_px, num_px, 3) 的图片变为 shape 为 (num_px ∗ num_px ∗ 3, 1) 的向量 </p><p>把一个 shape 为 (a,b,c,d) 的矩阵变为一个 shape 为 (b∗c∗d, a) 的矩阵的技巧：<code>x_flatten = X.reshape(X.shape[0], -1).T</code></p><blockquote><p>实际上，reshape() 是按<strong>行</strong>取元素，按<strong>行</strong>放元素</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重构图片数组</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))<span class="comment">#重构后完整性检查</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;train_set_x_flatten shape: (12288, 209)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x_flatten shape: (12288, 50)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br><span class="line">  sanity check after reshaping: [17 31 56 22 33]</span><br></pre></td></tr></table></figure><h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><p>为了使得数据在一个合适的尺度上，我们需要将数据标准化，数据标准化的办法见上一篇博文，但是对于图片来说，由于图片的每个像素的 RGB 值介于 0 到 255 之间，所以我们可以将每个特征值除以 255，这样就能将它们标准化了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></span><br></pre></td></tr></table></figure><h3 id="算法的一般体系结构"><a href="#算法的一般体系结构" class="headerlink" title="算法的一般体系结构"></a>算法的一般体系结构</h3><ul><li>初始化模型参数</li><li>通过最优化代价函数学习参数<ul><li>计算目前的损失函数（前向传播）</li><li>计算现在的梯度（反向传播）</li><li>更新参数（梯度下降）</li></ul></li><li>使用学习后的参数对测试集进行预测</li><li>分析结果得出结论</li></ul><h3 id="构建算法的各个部分"><a href="#构建算法的各个部分" class="headerlink" title="构建算法的各个部分"></a>构建算法的各个部分</h3><h4 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h4><p>实现 sigmoid 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid([0, 2]) = "</span> + str(sigmoid(np.array([<span class="number">0</span>,<span class="number">2</span>]))))</span><br></pre></td></tr></table></figure><p>>&gt; sigmoid([0, 2]) = [ 0.5         0.88079708]</p><h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><p>使用一系列的 0 初始化我们的参数 w 和 b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    w = np.zeros((dim,<span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>)) <span class="comment"># 确保 w 的维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int)) <span class="comment">#确保 b 是浮点数或者整数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dim = <span class="number">2</span></span><br><span class="line">w, b = initialize_with_zeros(dim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(w))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(b))</span><br></pre></td></tr></table></figure><p>>&gt; w = [[0] [0]]      b = 0</p><h4 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h4><p>步骤：</p><ul><li>输入 X</li><li>计算预测值 $A = \sigma(w^T X + b)$</li><li>计算代价函数 $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</li><li>计算  $ dw = \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$</li><li>计算 $ db=\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算前向传播和反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播 (从 X 到 COST)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X ) + b) <span class="comment"># 计算预测值                    </span></span><br><span class="line">    cost = (<span class="number">-1</span>/m)*(np.sum(Y*np.log(A) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))) <span class="comment"># 计算代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播 (计算梯度)</span></span><br><span class="line">    dw = (<span class="number">1</span>/m)*np.dot(X,(A-Y).T)</span><br><span class="line">    db = (<span class="number">1</span>/m)*np.sum(A-Y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,   <span class="comment">#返回梯度 dict</span></span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w, b, X, Y = np.array([[<span class="number">1.</span>],[<span class="number">2.</span>]]), <span class="number">2.</span>, np.array([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">-1.</span>],[<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">-3.2</span>]]), np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">grads, cost = propagate(w, b, X, Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cost = "</span> + str(cost))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;dw = [[ 0.99845601]</span><br><span class="line">       [ 2.39507239]]</span><br><span class="line">  db = 0.00145557813678</span><br><span class="line">  cost = 5.80154531939</span><br></pre></td></tr></table></figure><h4 id="梯度下降优化参数"><a href="#梯度下降优化参数" class="headerlink" title="梯度下降优化参数"></a>梯度下降优化参数</h4><p>更新参数方法：$ \theta = \theta - \alpha \text{ } d\theta$ ，其中 $\alpha$ 为<strong>学习率</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- 循环的迭代次数</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 代价函数和梯度计算 </span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line">        </span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b - learning_rate*db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每100次迭代记录一次代价函数到 costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每一百步打印一次代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params, grads, costs = optimize(w, b, X, Y, num_iterations= <span class="number">100</span>, learning_rate = <span class="number">0.009</span>, print_cost = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(params[<span class="string">"w"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(params[<span class="string">"b"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; w = [[ 0.19033591]</span><br><span class="line">       [ 0.12259159]]</span><br><span class="line">   b = 1.92535983008</span><br><span class="line">   dw = [[ 0.67752042]</span><br><span class="line">        [ 1.41625495]]</span><br><span class="line">   db = 0.219194504541</span><br></pre></td></tr></table></figure><h4 id="用得到的参数预测数据集的标签"><a href="#用得到的参数预测数据集的标签" class="headerlink" title="用得到的参数预测数据集的标签"></a>用得到的参数预测数据集的标签</h4><ul><li>先计算预测值 $\hat{Y} = A = \sigma(w^T X + b)$</li><li>若 $\hat Y &gt; 0.5$ ，则预测的标签为 1</li><li>若 $\hat Y &lt;= 0.5$ ，则预测的标签为 0</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m)) <span class="comment">#初始化</span></span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>) <span class="comment">#确保 w shape 正确</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算图片中是猫的概率</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X) + b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 把概率转化为标签值</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([[<span class="number">0.1124579</span>],[<span class="number">0.23106775</span>]])</span><br><span class="line">b = <span class="number">-0.3</span></span><br><span class="line">X = np.array([[<span class="number">1.</span>,<span class="number">-1.1</span>,<span class="number">-3.2</span>],[<span class="number">1.2</span>,<span class="number">2.</span>,<span class="number">0.1</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"predictions = "</span> + str(predict(w, b, X)))</span><br></pre></td></tr></table></figure><p>>&gt; predictions = [[ 1.  1.  0.]]</p><h3 id="把所有的函数聚合到主函数"><a href="#把所有的函数聚合到主函数" class="headerlink" title="把所有的函数聚合到主函数"></a>把所有的函数聚合到主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用 0 初始化参数</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations , learning_rate , print_cost = <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从参数 dict 中获取参数</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测训练集/测试集的标签</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印预测误差</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练模型</span></span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train accuracy: 99.04306220095694 % # 训练集的预测精确度</span><br><span class="line">test accuracy: 70.0 % #测试集的预测精确度</span><br></pre></td></tr></table></figure><p>画出错误图形：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture that was wrongly classified.</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(test_set_y[<span class="number">0</span>,index]) + <span class="string">", you predicted that it is a \""</span> + classes[d[<span class="string">"Y_prediction_test"</span>][<span class="number">0</span>,index]].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><p>画出代价函数的下降曲线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>]) <span class="comment"># np.squeeze() 确保它是一维数组</span></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2.png" alt=""></p><h3 id="进一步分析"><a href="#进一步分析" class="headerlink" title="进一步分析"></a>进一步分析</h3><h4 id="学习率的选择"><a href="#学习率的选择" class="headerlink" title="学习率的选择"></a>学习率的选择</h4><p>为了梯度下降正常工作，必须选择合适的学习率。学习率 $\alpha$ 决定了更新参数的快慢，如果学习率太大，我们可能错过最优值，同样，如果太小，我们需要迭代很多次到达最优值，下面比较不同学习率的差别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (hundreds)'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="keyword">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning rate is: 0.01</span><br><span class="line">train accuracy: 99.52153110047847 %</span><br><span class="line">test accuracy: 68.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate is: 0.001</span><br><span class="line">train accuracy: 88.99521531100478 %</span><br><span class="line">test accuracy: 64.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate is: 0.0001</span><br><span class="line">train accuracy: 68.42105263157895 %</span><br><span class="line">test accuracy: 36.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.3.png" alt=""></p><p>结论：</p><ul><li>不同的学习率导致不同的代价函数和预测结果</li><li>如果学习率太大（0.01），代价函数可能会上下摆动甚至偏离（尽管在这个例子中 0.01 最后收敛得很好）</li><li>更小的学习率不意味着更好的模型，因为有可能出现 <strong>过拟合</strong>，一般出现在训练数据精确度比测试数据高得多时</li><li>深度学习中，一般推荐：<ul><li>选择更好降低代价函数的学习率</li><li>如果发生过拟合，用其他的方式减小过拟合</li></ul></li></ul><h3 id="作业结论"><a href="#作业结论" class="headerlink" title="作业结论"></a>作业结论</h3><ul><li>预处理数据很重要</li><li>先分开构建函数： initialize(), propagate(), optimize()，最后再搭建模型 model()</li><li>调整学习率（超参数 的一个例子）对算法影响很大</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 2）—— 神经网络基础</title>
      <link href="/2018/08/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w2/"/>
      <url>/2018/08/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w2/</url>
      <content type="html"><![CDATA[<p>本周主要介绍<strong>逻辑回归</strong>算法 (Logistics Regression)。</p><h2 id="二元分类"><a href="#二元分类" class="headerlink" title="二元分类"></a>二元分类</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>输出值为离散的两个值，即 1 或者 0</p><h3 id="例子——判断图片是（1）不是（0）猫"><a href="#例子——判断图片是（1）不是（0）猫" class="headerlink" title="例子——判断图片是（1）不是（0）猫"></a>例子——判断图片是（1）不是（0）猫</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1.png" alt=""></p><ul><li>目标：输入图片的特征向量 x，预测对应的输出是 1 还是 0 </li></ul><h3 id="图片特征值的提取"><a href="#图片特征值的提取" class="headerlink" title="图片特征值的提取"></a>图片特征值的提取</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.2.png" alt=""><br><a id="more"></a></p><ul><li>图片特征值的提取：每张图片有三个颜色通道，把每个通道的所有像素值取出展成一个向量，每张图片（像素为 64*64）的所有特征值为 64x64x3 = 12288 个，组成一个维度为 12288 的列向量 X </li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.4.png" alt=""></p><ul><li>输入向量的维度 n =  $n_x$= 12288</li></ul><h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.3.png" alt=""></p><h2 id="逻辑回归-Logistics-Regression（针对二元分类问题）"><a href="#逻辑回归-Logistics-Regression（针对二元分类问题）" class="headerlink" title="逻辑回归 Logistics Regression（针对二元分类问题）"></a>逻辑回归 Logistics Regression（针对二元分类问题）</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>逻辑回归是一个很小的神经网络，已知一个数据集，我们用它拟合出一个模型，这个模型能够实现：</p><ul><li><p>输入特征值 $x$</p></li><li><p>输出预测值 $\hat y$： $\hat{y} = P(y = 1|x) , 0 \leq\hat{y} \leq1 $，即标签值 y = 1 的概率</p></li></ul><p>那么如何得到某个输入 x 的标签值为 1 的概率呢，用如下式子来进行预测：</p><script type="math/tex; mode=display">\hat{y} = w^Tx + b</script><p>但是由于概率的值在 0 和 1 之间，而 $w^T x+ b$ 可能大于 1 或者小于 0，所以这不是一个好的算法，于是我们将其通过一个 <strong>sigmoid函数</strong> 改造一下：</p><script type="math/tex; mode=display">\hat{y} =\sigma( w^Tx + b)</script><ul><li>输入特征向量 x： 𝑥 ∈ $ℝ^{n_x}$, 即 $x = \left ( x_{1},x_{2},x_{3},…,x_{n_x} \right )^T$,$n_x$ 为特征数目</li><li>训练标签 y：y $\in$ { 0 , 1 }</li><li>权重 w：$w = \left ( w_{1},w_{2},w_{3},…,w_{n_x} \right )^T$, $n_x$ 为特征数目</li><li>偏置量 b：实数</li><li>sigmoid 函数：$s=\sigma(z)=\frac{1}{1+e^{-z}}$</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.5.png" alt=""></p><blockquote><p>sigmoid 函数的合理性在于：</p><ul><li>当 z 是一个很大的数时， $\sigma (z) = 1$</li><li>当 z 是一个很小的数时， $\sigma(z)=0$</li><li>当 z = 0 时，$\sigma(z)=0.5$</li></ul><p>这样就能很好地估计介于 0 和 1 之间的概率</p></blockquote><h3 id="另外一种描述方式"><a href="#另外一种描述方式" class="headerlink" title="另外一种描述方式"></a>另外一种描述方式</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.6.png" alt=""></p><script type="math/tex; mode=display">\hat{y} =\sigma( \Theta^Tx)</script><p>其中：</p><ul><li>$x = (1,x_{1},x_{2},x_{3},…,x_{n_x})^T$</li><li>$\Theta = (b,w_{1},w_{2},w_{3},…,w_{n_x})^T$</li></ul><h3 id="损失函数（lost-function）"><a href="#损失函数（lost-function）" class="headerlink" title="损失函数（lost function）"></a>损失函数（lost function）</h3><p>为了训练参数 w 和 b，也就是找到最优的拟合模型，我们定义一个<strong>损失函数</strong></p><h4 id="概括"><a href="#概括" class="headerlink" title="　概括"></a>　概括</h4><blockquote><p>右上角的括号 (i) 表示第 i 个训练实例</p></blockquote><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.7.png" alt=""></p><h4 id="损失函数定义"><a href="#损失函数定义" class="headerlink" title="损失函数定义"></a>损失函数定义</h4><p>损失函数评估了预测值 $\hat y^{(i)}$ 和 已知的期望值 $y^{(i)}$ 之间的差异，或者说计算了单个训练样本的偏差</p><h4 id="常用的损失函数"><a href="#常用的损失函数" class="headerlink" title="常用的损失函数"></a>常用的损失函数</h4><p>（1）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.9.png" alt=""></p><blockquote><p>逻辑回归中一般不适用此损失函数，因为会使优化问题变成非凸问题，无法使用梯度下降法找到全局最优解</p></blockquote><p>（2）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.10.png" alt=""></p><blockquote><p>该损失函数的合理性在于：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.11.png" alt=""></p></blockquote><h3 id="代价函数（Cost-function）"><a href="#代价函数（Cost-function）" class="headerlink" title="代价函数（Cost function）"></a>代价函数（Cost function）</h3><p>代价函数指的是<strong>整个</strong>训练集的损失函数的<strong>平均值</strong>，我们的最终目的就是找到使整个代价函数值最小的参数 w 和 b</p><blockquote><ul><li>我们希望用 $\hat{y} =\sigma( w^Tx + b)$ 模型得出的预测值和已知的实际值之间的差异最小，也就是代价函数最小，这样才能证明这个模型（假设参数为 $w_{best},b_{best}$）是所有包含参数 w，b 的模型里最合理的，也是最符合实际的</li><li>从输入到求得代价函数的过程称之为<strong>前向传播 (forward propagation)</strong> </li></ul></blockquote><p>代价函数：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.12.png" alt=""></p><h3 id="代价函数的证明"><a href="#代价函数的证明" class="headerlink" title="代价函数的证明"></a>代价函数的证明</h3><p>二元分类问题类似于概率论里的“0-1分布”.</p><ul><li>假设当结果为 1 的概率  $P(y=1|x)=\hat y$</li><li>则当结果为 0 的概率就是  $P(y=0|x)=1-\hat y$</li><li>则结果为 y（0或1）的概率为 $P(y|x)=\hat y^y(1-\hat y)^{1-y}$，可带入y=0或1进行检验，发现该式子正确</li><li>假设有一个容量为 m 的数据集（假设符合独立同分布），根据最大似然原理，我们要使得我们的假设的 $\hat y$ 最正确，最符合实际，那么必须满足使得“用 $\hat y$ 估计到的目前整个数据集的概率”<strong>最大</strong>（之所以越大越接近实际，是因为这些数据集已经存在，发生的概率便是 1），那么根据独立事件概率相乘原理：</li><li>“用 $\hat y$ 估计到的目前整个数据集的概率”为 $L(\hat y)=P(y^{(1)}|x^{(1)})P(y^{(2)}|x^{(2)})…P(y^{(m)}|x^{(m)})\\\quad\quad =\prod\limits_{i=1}^m P(y^{(i)}|x^{(i)})\\ \quad\quad =\prod\limits_{i=1}^m    {({\hat y^{(i)}})}   ^{y^{(i)}}          (   1   -    {\hat y^{(i)}}  )^{1-   y^{(i)}} $</li><li>为了让 $L(\hat y)$ <strong>最大</strong>，由于是连乘，我们让 $logL(\hat y)$ 最大，则 $logL(\hat y) = \sum\limits ^m_{i=1} y^{(i)}log{\hat y}^{(i)}+(1-y^{(i)})log(1-{\hat y}^{(i)})$</li><li>而代价函数必须要<strong>尽量小</strong>，所以我们将 $logL(\hat y)$ 取负号，即 $-\sum\limits ^m_{i=1} y^{(i)}log{\hat y}^{(i)}+(1-y^{(i)})log(1-{\hat y}^{(i)})$</li><li>为了让代价函数处于更好的尺度上，我们加上系数 1/m，即 $J(w,b)=-\frac{1}{m}\sum\limits ^m_{i=1} y^{(i)}log{\hat y}^{(i)}+(1-y^{(i)})log(1-{\hat y}^{(i)})$</li><li>代价函数的逻辑为：某个 w，b 算出的代价函数越小 → $L(\hat y)$ 越大 → 用其估计到的目前整个数据集的概率就越大 → 用 w，b 为参数的模型的估计距离实际情况就越接近 → 该模型就越准确</li></ul><h2 id="用梯度下降法训练参数-w-和-b"><a href="#用梯度下降法训练参数-w-和-b" class="headerlink" title="用梯度下降法训练参数 w 和 b"></a>用梯度下降法训练参数 w 和 b</h2><h3 id="概括-1"><a href="#概括-1" class="headerlink" title="概括"></a>概括</h3><p>已知：</p><ul><li><p>模型 $\hat{y} =\sigma( w^Tx + b)$，其中 $\sigma(z)=\frac{1}{1+e^{-z}}$</p></li><li><p>代价函数</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.12.png" alt=""></p></li></ul><p>期望：</p><ul><li><p>找到使 $J(w,b)$ 最小的 w 和 b，如图中红点所示，找到谷底的 w 和 b 值</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.13.png" alt=""></p></li></ul><h3 id="梯度下降法原理"><a href="#梯度下降法原理" class="headerlink" title="梯度下降法原理"></a>梯度下降法原理</h3><ul><li>用一些初始 w，b 值来初始化，通常用 0</li><li>朝着最陡的的下坡方向走一步，第一次迭代</li><li>第二次迭代</li><li>…</li><li>第 n 次迭代到达或接近全局最优点</li></ul><h3 id="单变量梯度下降法（先忽略b）"><a href="#单变量梯度下降法（先忽略b）" class="headerlink" title="单变量梯度下降法（先忽略b）"></a>单变量梯度下降法（先忽略b）</h3><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.14.png" alt=""></p><p>  假设 J(w) 曲线如图中所示，为了找到谷底的 w 点，我们重复以下更新操作：</p><script type="math/tex; mode=display">  w := w - \alpha\frac{dJ(w)}{dw}</script><p>  其中：</p><ul><li>w <strong>: =</strong> 符号表示对 w 进行迭代更新</li><li>$\alpha$ 为<strong>学习率</strong>，控制每一次迭代的步长大小</li><li>在代码中导数 $\frac{dJ(w)}{dw}$ 的变量名写成 $dw$</li></ul><h3 id="多变量梯度下降法（w，b）"><a href="#多变量梯度下降法（w，b）" class="headerlink" title="多变量梯度下降法（w，b）"></a>多变量梯度下降法（w，b）</h3><script type="math/tex; mode=display">  J(w,b)</script><script type="math/tex; mode=display">  w := w - \alpha\frac{\partial J(w,b) }{\partial w}</script><script type="math/tex; mode=display">  b := b - \alpha\frac{\partial J(w,b) }{\partial b}</script><h2 id="求代价函数的导数"><a href="#求代价函数的导数" class="headerlink" title="求代价函数的导数"></a>求代价函数的导数</h2><h3 id="求损失函数的导数"><a href="#求损失函数的导数" class="headerlink" title="求损失函数的导数"></a>求损失函数的导数</h3><p>  已知公式：</p><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.15.png" alt=""></p><p>  假设训练实例有两个特征 : $x_1,x_2$，则逻辑关系如下：</p><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.16.png" alt=""></p><p>  根据链式法则求偏导数，推导过程如下：</p><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.17.png" alt=""></p><p>  整理可得：</p><script type="math/tex; mode=display">“da”=\frac{dL}{da}= \frac {\partial L(a,y)}{\partial a}=-\frac{y}{a}+\frac{1-y}{1-a}</script><script type="math/tex; mode=display">“dz”= \frac {dL}{dz}=\frac{dL}{da}\frac{da}{dz}=a-y</script><script type="math/tex; mode=display">“dw_1”=\frac{\partial L}{\partial w_1}=x_1dz</script><script type="math/tex; mode=display">“dw_2”=\frac{\partial L}{\partial w_2}=x_2dz</script><script type="math/tex; mode=display">“db” = \frac{\partial L}{\partial b} = dz</script><blockquote><p>其中：$“da””dz””dw_1””dw_2””db”$ 表示损失函数对其导数在 python 中的变量名，这个过程叫做 <strong>反向传播 (back propagation)</strong></p></blockquote><h3 id="求代价函数的导数-1"><a href="#求代价函数的导数-1" class="headerlink" title="求代价函数的导数"></a>求代价函数的导数</h3><p>由于代价函数是所有训练样本损失函数的平均值，故我们可以用如下的伪代码求得 $dw_1,dw_2,db$ (我们仍然假设只有两个特征值)：</p><p>  首先初始化 $J=0;dw_1=0;dw_2=0;db=0$</p><p>  遍历整个训练集：</p><p>  For i = 1 to m:</p><p>​         $z^{(i)}=w^Tx^{(i)}+b\\a^{(i)}=\sigma(z^{(i)})\\J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})]\\dz^{(i)}=a^{(i)}-y^{(i)}\\dw_1+=x_1^{(i)}dz^{(i)}\\dw_2+=x_2^{(i)}dz^{(i)}\\dw_3…\\dw_4… \\ …\\db+=dz^{(i)}$</p><p>  最后除以训练集个数：</p><p>​         $J=J/m\\”dw_1”=\frac {\partial J}{\partial w_1}=dw_1/m\\”dw_2”=\frac {\partial J}{\partial w_2}=dw_2/m \\ … \\”db”=\frac {\partial J}{\partial b}=db/m$</p><blockquote><p>我们可以看到为了求取代价函数的导数，我们需要进行两次遍历，这是一种十分低效的遍历方式，我们可以使用<strong>向量化（Vectorization）</strong>的方式加速我们的运算</p></blockquote><p>  最后用求得的代价函数的导数更新我们的参数：</p><p>​        $w_1:=w_1-\alpha dw_1\\w_2:=w_2-\alpha dw_1 \\ …\\b:=b-\alpha db$</p><h2 id="向量化（Vectorization）"><a href="#向量化（Vectorization）" class="headerlink" title="向量化（Vectorization）"></a>向量化（Vectorization）</h2><h3 id="什么是向量化"><a href="#什么是向量化" class="headerlink" title="什么是向量化"></a>什么是向量化</h3><ul><li>是一门让代码变得高效的艺术</li><li><p>遍历是<strong>非向量化</strong>，而向量化充分利用 CPU 和 GPU 的并行化实现更快的运算</p><blockquote><p>大概是指直接让两个向量或者矩阵进行点乘</p></blockquote></li></ul><h3 id="向量化的好处"><a href="#向量化的好处" class="headerlink" title="向量化的好处"></a>向量化的好处</h3><ul><li>比非向量化计算效率快很多很多，下面的测试显示快 300 多倍</li><li><p>只要可能，尽量避免使用显式的 for 循环</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.18.png" alt=""></p></li></ul><h3 id="一些向量化的例子"><a href="#一些向量化的例子" class="headerlink" title="一些向量化的例子"></a>一些向量化的例子</h3><ol><li><p>计算  $u = A_{i \times j}V_{j \times 1}$ </p><p>(1) 非向量化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">u = np.zero((n,1))</span><br><span class="line">for i ...</span><br><span class="line">    for j ...</span><br><span class="line">    u[i] += A[i][j]*v[j]</span><br></pre></td></tr></table></figure><p>(2) 向量化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">u = np.dot(A,V)</span><br></pre></td></tr></table></figure></li><li><p>计算某个向量所有元素的指数</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.19.png" alt=""></p><p>(1) 非向量化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">u = np.zeros((n,1))</span><br><span class="line">for i in range(n)</span><br><span class="line">u[i] = math.exp(V[i])</span><br></pre></td></tr></table></figure><p>(2) 向量化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">u = np.exp(V)</span><br></pre></td></tr></table></figure></li><li><p>其他例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.log(V)</span><br><span class="line">np.abs(V)</span><br><span class="line">np.maximum(V)</span><br><span class="line">V**<span class="number">2</span></span><br><span class="line"><span class="number">1</span>/V</span><br></pre></td></tr></table></figure></li></ol><h2 id="用向量化实现逻辑回归"><a href="#用向量化实现逻辑回归" class="headerlink" title="用向量化实现逻辑回归"></a>用向量化实现逻辑回归</h2><h3 id="用-for-循环实现"><a href="#用-for-循环实现" class="headerlink" title="用 for 循环实现"></a>用 for 循环实现</h3><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.20.png" alt=""></p><h3 id="向量化实现"><a href="#向量化实现" class="headerlink" title="向量化实现"></a>向量化实现</h3><ol><li><p>前向传播</p><p><strong>求取 Z</strong>：</p><p>$Z=[z^{(1)}\quad z^{(1)}\quad…\quad z^{(m)}]\\=[w^Tx^{(1)}+b\quad w^Tx^{(2)}+b\quad …\quad w^Tx^{(m)}+b] \\=w^T[x^{(1)}\quad x^{(1)}\quad …\quad x^{(m)}]+[b\quad b\quad…\quad b]\\=w^TX+[b\quad b\quad…\quad b]$</p><p>实现代码为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br></pre></td></tr></table></figure><blockquote><p>此处 b 为一个实数，但是 numpy 会自动将这个实数 b 拓展为一个 1 乘 m 的行向量，这个在 python 中称为 <strong>广播（broadcasting）</strong></p></blockquote><p><strong>求取 A</strong>：</p><p>$A=[a^{(1)}  \  a^{(2)} \ … \ a{(m)}]= \sigma(Z)$</p></li><li><p>反向传播</p><p><strong>求取 dZ</strong>：</p><p>$dZ=[dz^{(1)} \ dz^{(2)} \ … \ dz^{(m)}]\\=[a^{(1)}-y^{(1)} \quad a^{(1)}-y^{(1)} \ … \quad a^{(m)}-y^{(m)}]\\=A -Y $</p><p>其中 $Y=[y^{(1)}\quad y^{(2)}\quad … \quad y^{(m)}]$</p><p><strong>求取 dw</strong>：</p><p>$dw=\frac {1}{m}[x^{(1)} dz^{(1)} + x^{(2)} dz^{(2)} +…+x^{(m)} dz^{(m)}]\\=\frac {1}{m}[x^{(1)}\quad x^{(2)}\quad…\quad x^{(m)}][dz^{(1)}\quad dz^{(2)}\quad…\quad dz^{(m)}]^T\\=\frac {1}{m}X(dZ)^T$</p><p><strong>求取 db</strong>：</p><p>$db=\frac {1}{m} \sum\limits _{i=1}^{m} dz^{(i)}=\frac {1}{m}$ np.sum(dZ)</p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>$Z = w^TX + b = np.dot(w.T, X)+b\\A = \sigma (Z)\\dZ=A-Y\\db=\frac {1}{m}np.sum(dZ)\\dw= \frac{1}{m}X(dZ)^T=\frac{1}{m}np.dot(X, dZ.T)\\w:=w-\alpha dw\\b:=b-\alpha db$</p><h2 id="Python中的广播（broadcasting）"><a href="#Python中的广播（broadcasting）" class="headerlink" title="Python中的广播（broadcasting）"></a>Python中的广播（broadcasting）</h2><ul><li>若拿一个<strong>（m，n）</strong>的矩阵加减乘除另一个<strong>（1，n）</strong>的向量，这个向量会复制多次自动拓展成一个<strong>（m，n）</strong>的矩阵然后逐元素进行运算</li><li>若拿一个<strong>（m，1）</strong>的向量加减乘除另一个<strong>实数R</strong>，这个实数会复制多次自动拓展成一个<strong>（m，1）</strong>的向量然后逐元素进行运算</li><li>更多广播功能见 numpy 相关文档</li></ul><h2 id="在使用-numpy-时的注意事项"><a href="#在使用-numpy-时的注意事项" class="headerlink" title="在使用 numpy 时的注意事项"></a>在使用 numpy 时的注意事项</h2><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><ol><li><p>错误的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;<span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt;a = np.random.randn(<span class="number">5</span>)<span class="comment">#产生5个高斯随机变量并存在数组里</span></span><br><span class="line">&gt;&gt;print(a)</span><br><span class="line">[<span class="number">1.37838388</span>  <span class="number">0.53281963</span> <span class="number">-0.41769013</span>  <span class="number">0.69356822</span> <span class="number">-0.30333514</span>]</span><br><span class="line">&gt;&gt;print(a.shape)<span class="comment">#输出 a 的形状</span></span><br><span class="line">(<span class="number">5</span>,)</span><br><span class="line">&gt;&gt;print(a.T)<span class="comment">#输出 a 的转置</span></span><br><span class="line">[<span class="number">1.37838388</span>  <span class="number">0.53281963</span> <span class="number">-0.41769013</span>  <span class="number">0.69356822</span> <span class="number">-0.30333514</span>]</span><br><span class="line">&gt;&gt;print(np.dot(a,a.T))<span class="comment">#输出a 和 a 的转置的点乘</span></span><br><span class="line"><span class="number">8.65042528221</span></span><br></pre></td></tr></table></figure></li><li><p>正确的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">&gt;&gt;print(a)</span><br><span class="line">[[<span class="number">-0.4593413</span> ]</span><br><span class="line"> [<span class="number">-0.54494632</span>]</span><br><span class="line"> [<span class="number">-0.9348879</span> ]</span><br><span class="line"> [<span class="number">-0.51491905</span>]</span><br><span class="line"> [ <span class="number">0.91934378</span>]]</span><br><span class="line">&gt;&gt;print(a.T)</span><br><span class="line">[[<span class="number">-0.4593413</span>  <span class="number">-0.54494632</span> <span class="number">-0.9348879</span>  <span class="number">-0.51491905</span>  <span class="number">0.91934378</span>]]</span><br><span class="line">&gt;&gt;print(np.dot(a,a.T))</span><br><span class="line">[[ <span class="number">0.21099443</span>  <span class="number">0.25031635</span>  <span class="number">0.42943262</span>  <span class="number">0.23652358</span> <span class="number">-0.42229257</span>]</span><br><span class="line"> [ <span class="number">0.25031635</span>  <span class="number">0.29696649</span>  <span class="number">0.50946372</span>  <span class="number">0.28060324</span> <span class="number">-0.50099301</span>]</span><br><span class="line"> [ <span class="number">0.42943262</span>  <span class="number">0.50946372</span>  <span class="number">0.87401539</span>  <span class="number">0.48139159</span> <span class="number">-0.85948338</span>]</span><br><span class="line"> [ <span class="number">0.23652358</span>  <span class="number">0.28060324</span>  <span class="number">0.48139159</span>  <span class="number">0.26514163</span> <span class="number">-0.47338763</span>]</span><br><span class="line"> [<span class="number">-0.42229257</span> <span class="number">-0.50099301</span> <span class="number">-0.85948338</span> <span class="number">-0.47338763</span>  <span class="number">0.84519299</span>]]</span><br></pre></td></tr></table></figure><blockquote><p>错误的例子在于 <code>a = np.random.randn(5)</code> 生成了一个形状为 <strong>(5,)</strong> 的名为 <strong>“秩为1的数组”</strong> ，而不是一个向量，这种数组转置之后显示完全一样，但是性质与向量不同，所以极易造成许多奇怪的 bug，所以要杜绝秩为1的数组的使用，而是使用 <code>a = np.random.randn(5,1)</code> 或者 <code>a = np.random.randn(1,5)</code> </p></blockquote></li></ol><h3 id="减少错误的技巧"><a href="#减少错误的技巧" class="headerlink" title="减少错误的技巧"></a>减少错误的技巧</h3><ol><li><p>不要使用 <strong>秩为1的数组</strong>，始终使用 （n，1）列向量或者（1，n）的行向量，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>经常使用<strong>断言</strong>语句来确保是向量而不是秩为1的数组,例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape == (<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></li><li><p>经常使用 <strong>reshape 语句</strong> 来确保矩阵和向量是需要的维度，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>)<span class="comment">#产生了一个秩为1矩阵</span></span><br><span class="line">a = a.reshape((<span class="number">5</span>,<span class="number">1</span>))<span class="comment">#将其变为（5，1）的列向量</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 1）—— 深度学习介绍</title>
      <link href="/2018/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w1/"/>
      <url>/2018/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w1/</url>
      <content type="html"><![CDATA[<p>本周主要介绍深度学习的概况。</p><h2 id="Specializeion的课程设置"><a href="#Specializeion的课程设置" class="headerlink" title="Specializeion的课程设置"></a>Specializeion的课程设置</h2><ul><li>神经网络和深度学习  </li><li>改进神经网络 </li><li>构建机器学习系统 </li><li>卷积神经网络CNN </li><li>序列模型 </li></ul><h2 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h2><h3 id="例子1——single-neural-network"><a href="#例子1——single-neural-network" class="headerlink" title="例子1——single neural network"></a>例子1——single neural network</h3><p>预测房价（线性回归问题）：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.1.png" alt=""><br><a id="more"></a><br><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.2.png" alt=""></p><ul><li>输入为房子的尺寸（x）</li><li>输出为房价（y）</li><li>注意：房价不能为负数，故用不从零开始的 <strong>修正线性单元（rectified linear unit）</strong>即 <strong>ReLU函数</strong>（蓝线） 表示</li><li>ReLU = max（0，y）</li><li>第二张图片中间的圆圈表示一个“神经元”，从 x 到 y 的整个输出表示一个最小的神经网络，是组成神经网络最基本的单元</li></ul><h3 id="例子2——-Multiple-neural-network"><a href="#例子2——-Multiple-neural-network" class="headerlink" title="例子2—— Multiple neural network"></a>例子2—— Multiple neural network</h3><p>用更多特征（房子尺寸、卧室大小、邮政编码、区域富裕程度）来预测房价：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.3.png" alt=""></p><ul><li>输入的四个特征叫做<strong>输入层</strong></li><li>每个小圆圈都叫做<strong>隐藏神经元</strong>（hidden unit），图中每个神经元都把四个特征当作输入（全连接）</li><li>神经网络自己决定每个网络节点是什么</li></ul><blockquote><p>我的理解：给定足够的输入x，得到若干输出y，神经网络就是建立起 x 和 y 之间映射关系的一个黑箱系统</p></blockquote><h2 id="神经网络的监督学习"><a href="#神经网络的监督学习" class="headerlink" title="神经网络的监督学习"></a>神经网络的监督学习</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>给定一个<strong>已知输出</strong>的数据集，找到输入与输出之间的函数关系</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul><li>回归问题：在连续的输出上预测结果，即将输入变量映射到某个连续函数上</li><li>分类问题：在离散的输出上预测结果，即将输入变量映射到离散的类别上</li></ul><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.4.png" alt=""></p><h3 id="神经网络（NN）的分类"><a href="#神经网络（NN）的分类" class="headerlink" title="神经网络（NN）的分类"></a>神经网络（NN）的分类</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.5.png" alt=""></p><ul><li>标准神经网络（standard NN）：用在房地产、广告</li><li>卷积神经网络（convolution NN）：图片处理</li><li>循环神经网络（recurrent NN）：语音识别、翻译</li><li>混合了其他结构的神经网络： 自动驾驶</li></ul><h3 id="数据的分类"><a href="#数据的分类" class="headerlink" title="数据的分类"></a>数据的分类</h3><ul><li>结构化数据：基于数据库的数据，即标签化了的、具有清晰定义的信息，例如房价预测中数据有房屋面积和卧室数量等标签</li><li>非结构化数据：类似于音频、图片、文本这类的数据，处理更加困难</li></ul><h2 id="为何深度学习蓬勃发展"><a href="#为何深度学习蓬勃发展" class="headerlink" title="为何深度学习蓬勃发展"></a>为何深度学习蓬勃发展</h2><h3 id="规模驱动深度学习的发展"><a href="#规模驱动深度学习的发展" class="headerlink" title="规模驱动深度学习的发展"></a>规模驱动深度学习的发展</h3><ul><li>数字化生活产生的大量数据</li><li>算力的巨幅提高</li><li>算法的创新：例如 signmoid 函数到 ReLU 函数的迁移</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.6.png" alt=""></p><ul><li>线条从下往上表示神经网络的规模在变大</li><li>m 表示训练集的大小</li><li>为了获得好的表现，要么足够大的神经网络，要么足够多的数据</li><li>在小训练集的情况下，模型的好坏与神经网络规模无关</li></ul><h3 id="训练神经网络的过程"><a href="#训练神经网络的过程" class="headerlink" title="训练神经网络的过程"></a>训练神经网络的过程</h3><p>一个 idea → 代码实践 → 试验 → 修正 idea →…</p><h2 id="course-1-的课程计划"><a href="#course-1-的课程计划" class="headerlink" title="course 1 的课程计划"></a>course 1 的课程计划</h2><ul><li>Week 1 : Introduction</li><li>Week 2 : Basics of Neural Network programming</li><li>Week 3 : One hidden layer Networks</li><li>Weel 4 : Deep Neural Networks</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>小白的 hexo+Github Pages 博客搭建之旅（windows）</title>
      <link href="/2018/07/26/%E5%B0%8F%E7%99%BD%E7%9A%84%20hexo+github%20pages%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%B9%8B%E6%97%85%EF%BC%88windows%EF%BC%89/"/>
      <url>/2018/07/26/%E5%B0%8F%E7%99%BD%E7%9A%84%20hexo+github%20pages%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%B9%8B%E6%97%85%EF%BC%88windows%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我的博客的第一篇文章，我想在这篇文章里记录一下自己搭建这个博客的全过程和途中踩过的坑，一是为了记录一下，二是方便以后换电脑需要再次搭建的时候有个参考，如果能帮助到其他人就更好了。</p><h2 id="搭建博客起因"><a href="#搭建博客起因" class="headerlink" title="搭建博客起因"></a>搭建博客起因</h2><p>前段时间刚开始自学python和机器学习，为了找到最佳的笔记方案，我上知乎搜“学习编程怎么做笔记？”有的人说不用做笔记，有的人说用 gitbook，有的人说直接记在代码注释里，有的人说直接记在 github 的 wiki 或者 issue 页面。但是还看到许多人推荐记录在自己的独立博客上，作为一个一直希望有自己博客的人，我立马萌生了自己搭博客的想法，但是碍于麻烦一直有些动摇，直到这篇文章<a href="https://zhuanlan.zhihu.com/p/19743861" target="_blank" rel="noopener">《为什么你要写博客？》</a>打动了我，它让我下定决心搭建自己的博客并坚持写点东西，不仅可以放自己学习过程中的笔记，作为一个话痨，还可以放一些自己的随想和读书笔记，何乐而不为？</p><p>总的来说，我认为写博客能给我以下的好处：</p><ul><li>梳理自己学到的东西</li><li>通过沉下心写点东西消磨自己的浮躁心态</li><li>小时候经常写日记，长大后放弃了这个习惯，希望这个博客能让我重拾记录自己内心，与自己对话的习惯</li><li>记录自己的生活和成长</li><li>搭建独立博客本身就是一件非常 geek 的事情（虽然是用的框架）</li></ul><p>那么废话不多说，直接开始吧！</p><a id="more"></a><h2 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h2><p>既然选择搭博客，那么怎么搭呢？我继续搜“如何搭建独立博客？”知乎上推荐比较多的方案是 <strong>静态博客生成器 + github pages + markdown</strong>，静态博客较动态博客的好处在于浏览速度快、方便简单且无须数据库，让你专注于写作本身。其次，Github Pages 是 github 上用于介绍项目的页面，不过，由于其免费的 300M 空间，把我们的博客放在上面再合适不过了。另外 markdown 指的是一种轻量级的标记语言，可以边写作边排版，非常方便。</p><p>静态博客生成器是一种把你写的文章变成漂亮的静态网页的博客框架，你只要负责写作，它负责帮你生成网页然后部署到 Github Pages 上，这就使得搭建博客变成了一件非常方便的事情。目前比较流行的静态博客生成器有 <strong>hexo</strong> 和 <strong>hugo</strong>。</p><p>我首先尝试了 hugo，他是用GO语言写成的，直接下载二进制文件然后安装就行了，非常方便。但是由于在应用 hugo 一个主题时出现了一个bug，始终无法生成页面，折腾了好几天，最后心力交瘁只能放弃 hugo，但不得不说 hugo 生成页面的速度非常快而且安装很方便，感兴趣的可以尝试一下。</p><p>于是最后的方案为 <strong>hexo + github pages + markdown</strong>。 </p><p>搭建过程参考以下链接：</p><ul><li><a href="http://blog.haoji.me/build-blog-website-by-hexo-github.html?from=xa" target="_blank" rel="noopener">使用 hexo+github 搭建免费个人博客详细教程</a></li><li><a href="https://www.jianshu.com/p/05289a4bc8b2" target="_blank" rel="noopener">如何搭建一个独立博客——简明 Github Pages 与 Hexo 教程</a></li><li><a href="https://www.youtube.com/watch?v=Ud1xAhu7t2Y&amp;list=PLXbU-2B80FvDjD_RiuNwsSQ4eF8pkwAIa" target="_blank" rel="noopener">快速使用 Hexo 搭建个人博客</a>(youtube)</li><li><a href="http://www.lovebxm.com/2018/06/24/hexo-github-blog/#%E6%90%AD%E5%BB%BA-Git-%E7%8E%AF%E5%A2%83" target="_blank" rel="noopener">可能是最详细的 Hexo + GitHub Pages 搭建个人博客的教程</a></li></ul><h3 id="hexo的下载安装"><a href="#hexo的下载安装" class="headerlink" title="hexo的下载安装"></a>hexo的下载安装</h3><h4 id="hexo介绍"><a href="#hexo介绍" class="headerlink" title="hexo介绍"></a>hexo介绍</h4><ul><li>hexo 是一个基于 Node.js 快速、简洁且高效的博客框架。</li><li><a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">hexo 中文官网</a></li></ul><h4 id="安装前的准备"><a href="#安装前的准备" class="headerlink" title="安装前的准备"></a>安装前的准备</h4><p>由于 hexo 是一个基于 Node.js 的博客框架，所以我们需要安装 Node.js，另外因为我们需要使用 Git 命令部署到 github 上，所以还需要安装 Git，两者的安装教程如下。</p><ul><li><a href="http://www.runoob.com/nodejs/nodejs-install-setup.html" target="_blank" rel="noopener">Node.js</a></li><li><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137396287703354d8c6c01c904c7d9ff056ae23da865a000" target="_blank" rel="noopener">Git</a></li></ul><blockquote><p>特别要注意安装 Node.js 和 Git 时是否设置了环境变量，点击<strong>win+R</strong>  =》输入 “cmd” =&gt; 输入命令 “path”，查看是否有 Node.js 和 Git 所在的文件夹，如果没有则可以根据网上教程设置。另外不要忘了根据说明配置Git。最后我们在 cmd 中分别输入 <code>node -v</code> 和 <code>npm -v</code> 和 <code>git version</code> 看看是否安装成功，如果返回版本号则说明安装成功。如下图所示。</p></blockquote><p><img src="/img/博客搭建之旅/1.png" alt=""></p><h4 id="安装-hexo"><a href="#安装-hexo" class="headerlink" title="安装 hexo"></a>安装 hexo</h4><p>在某个盘创建文件夹 hexo，进入该文件夹，输入以下命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure><blockquote><p>有时会得到如下图所示的 warn，无视之。</p></blockquote><p><img src="/img/博客搭建之旅/2.png" alt=""></p><p>然后输入 <code>hexo version</code> 命令判断是否安装成功，正确输出如下图所示。</p><p><img src="/img/博客搭建之旅/3.png" alt=""> </p><blockquote><p>我第一次安装时报错显示<strong>‘hexo’ 不是内部或外部命令，也不是可运行的程序</strong>，解决办法如下：</p><p>在 <code>hexo</code> 文件夹下用命令行输入 <code>npm install hexo --save</code> 安装 hexo，然后在 hexo 文件夹可以发现一个名为 <code>node_modules</code> 的文件夹，进入该文件夹，再进入第一个文件夹 <code>.bin</code>，复制当前路径 <code>F:\hexo\node_modules\.bin</code> 加入环境变量（如果不知道怎么加可以自行搜索），现在使用 <code>hexo version</code> 命令就可以得到正确的输出了！</p></blockquote><h4 id="博客文件夹的初始化"><a href="#博客文件夹的初始化" class="headerlink" title="博客文件夹的初始化"></a>博客文件夹的初始化</h4><p>现在我们开始初始化我们的博客，假设我们放博客资源的文件夹叫 chenyichen.github.io (可以把 chenyichen 改成自己的名字)，在 hexo 根目录用命令行输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init chenyichen.github.io</span><br></pre></td></tr></table></figure><p>然后你会发现在根目录下出现了一个名为 <code>chenyichen.github.io</code> 的文件夹，点进去发现如下的目录结构：</p><p><img src="/img/博客搭建之旅/4.png" alt=""></p><p>初始化成功！</p><h3 id="生成你的第一个博客页面！"><a href="#生成你的第一个博客页面！" class="headerlink" title="生成你的第一个博客页面！"></a>生成你的第一个博客页面！</h3><p>下面我们在 chenyichen.github.io 文件夹里用命令行输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g #用来生成网页，g 是 generate 的缩写</span><br><span class="line">hexo s #把网页上传至本地服务器，s 是 server 的缩写</span><br></pre></td></tr></table></figure><p>如图所示：</p><p><img src="/img/博客搭建之旅/5.png" alt=""></p><p><img src="/img/博客搭建之旅/6.png" alt=""></p><p>现在打开浏览器，在搜索框内输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost:4000</span><br></pre></td></tr></table></figure><p>点击回车，我们可以看到我们的第一个博客页面，Hello World！</p><p><img src="/img/博客搭建之旅/7.png" alt=""></p><h3 id="主题的安装和配置"><a href="#主题的安装和配置" class="headerlink" title="主题的安装和配置"></a>主题的安装和配置</h3><h4 id="主题推荐"><a href="#主题推荐" class="headerlink" title="主题推荐"></a>主题推荐</h4><p>我们刚刚建立了我们的第一个博客，有点丑对不对？没事儿，hexo 社区提供各式各样的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">博客主题</a>供你选择。但是这上百个主题对于选择困难症来说真的是太痛苦了！到底怎么选呢？我有两个建议。</p><p>第一就是你可以上知乎搜“hexo 主题”，里面有大神用爬虫爬下来 github 上最火的主题排序，看看哪些主题用的人比较多；第二就是一定不要选主题配置文档含糊不清的，之前我用了一个主题，在配置过程中各种出问题，文档介绍极其不清楚，虽然主题非常好看，但无奈怎么都编译不过，折腾了好几天只得放弃。这是现在我用的主题的<a href="https://molunerfinn.com/hexo-theme-melody-doc/#/" target="_blank" rel="noopener">配置文档</a>，博客的每一个细节都介绍得非常清楚，安利一波。</p><h4 id="主题的安装"><a href="#主题的安装" class="headerlink" title="主题的安装"></a>主题的安装</h4><p>你可以使用 git 命令将主题 clone 下来，但是鉴于墙的原因，速度有时非常慢，所以建议直接下载压缩包然后解压的方式。</p><p>以我现在用的主题 melody 为例，进入该主题的 github 页面，点击右边的绿色按钮，再点击 <code>Download ZIP</code>，保存到你博客文件夹下的 theme 文件夹下，接着解压到当前文件夹，然后我们就得到了一个名字为主题名字的文件夹，为了等下配置方便，我们把该该文件夹重命名为一个好记的名字，如“melody”，如下图所示。</p><p><img src="/img/博客搭建之旅/8.png" alt=""></p><p><img src="/img/博客搭建之旅/9.png" alt=""></p><p>现在我们进入放博客的文件夹的根目录，你会发现一个名为“_config.yml”的文件，用编辑器打开它，把倒数第五行的 <code>theme: landscape</code> 改成 <code>theme: melody</code>，保存文件，现在运行命令 <code>hexo server</code> 然后在浏览器输入 <code>localhost:4000</code>，看看是不是变成了你想要的主题！</p><h4 id="主题的配置"><a href="#主题的配置" class="headerlink" title="主题的配置"></a>主题的配置</h4><p>主题的配置一般在你下载主题的 github 页面会有详细的说明，比如 <a href="https://molunerfinn.com/hexo-theme-melody-doc/#/" target="_blank" rel="noopener">melody 主题说明文档</a>，所以就不详细说明了。有时候一些主题里没有的修改项，比如你需要修改 banner 的字体，只需要到该主题的文件夹里寻找对应的 css 文件然后修改即可，这个就需要你自己慢慢试慢慢找了。</p><h3 id="博客写作"><a href="#博客写作" class="headerlink" title="博客写作"></a>博客写作</h3><h4 id="写作工具"><a href="#写作工具" class="headerlink" title="写作工具"></a>写作工具</h4><p>我们的博客使用 markdown 进行写作，那么什么是 markdown 呢？有关它的介绍可以看这个：</p><ul><li><a href="https://github.com/younghz/Markdown" target="_blank" rel="noopener">markdown 介绍</a></li></ul><p>那么我们该用什么编辑器来写 markdown 呢？请看这个：</p><ul><li><p><a href="http://www.williamlong.info/archives/4319.html" target="_blank" rel="noopener">好用的 Markdown 编辑器一览</a></p></li><li><p><a href="https://www.zhihu.com/question/19637157" target="_blank" rel="noopener">用 Markdown 写作用什么文本编辑器？</a></p></li><li><p><a href="https://sspai.com/post/42126" target="_blank" rel="noopener">在 Windows 上拥有舒适的码字体验，12 款 Markdown 写作工具推荐</a></p></li></ul><h4 id="如何开始？"><a href="#如何开始？" class="headerlink" title="如何开始？"></a>如何开始？</h4><p>你可以在放博客的文件夹打开命令行，输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new 文章的名字</span><br></pre></td></tr></table></figure><p>然后我们发现在 source 文件夹下的 _post 文件夹生成了一个 .md 文件，也就是一个 markdown 文件，如下图所示。</p><p><img src="/img/博客搭建之旅/10.png" alt=""></p><p><img src="/img/博客搭建之旅/11.png" alt=""></p><p>打开它可以发现只有如下文字：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 我的第一篇博客</span><br><span class="line">date: 2018-07-29 16:53:49</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>这是某种类似于“头文件”的东西，决定了这篇文章的属性，通过添加或者修改这些关键字你就可以修改文章的标题、日期、分类、标签等属性，详细的属性介绍在<a href="https://hexo.io/zh-cn/docs/front-matter" target="_blank" rel="noopener">这里</a>可以找到。</p><p>你也可以自己在编辑器里创建 markdown 文件然后开始写文章，但是你必须先在前面加上这些“头文件”，然后把文章移动到 source 文件夹下的 _post 文件夹里。</p><h4 id="有关写作中图片的插入问题"><a href="#有关写作中图片的插入问题" class="headerlink" title="有关写作中图片的插入问题"></a>有关写作中图片的插入问题</h4><p>搭建独立博客写作最麻烦的一件事儿就是给文章添加图片，为啥麻烦呢，如果你把图片随文字一起上传到 github，那么它免费给你的 300M 空间也捉襟见肘，如果你使用图床（也就是把图片先传到网上的服务器，然后添加图片的时候引用它的链接），那么添加图片的步骤将会非常繁琐。</p><p>图床推荐最多的就是七牛图床，还有其他的比如微博图床和极简图床等等，网上的推荐很多，可以自行搜索，上传完图片后会得到一个图片的外链，复制这个链接，在需要引用图片的地方打如下文字就可以啦：</p><p><code>![](图片的外链)</code></p><p>那么如果你要上传本地图片咋办呢？我的建议是绝对路径引用。在 source 文件夹下建立名为 img 的文件夹专门用来放图片，再给每篇文章专门建立文件夹放这篇文章的图片，比如“我的第一篇博客”，图片名为 1.jpg，那么我引用该图片的时候就输入以下命令就可以啦：</p><p><code>![](/img/我的第一篇博客/1.jpg)</code></p><p>有关图片引用的说明还可以参考以下链接：</p><ul><li><a href="https://yanyinhong.github.io/2017/05/02/How-to-insert-image-in-hexo-post/" target="_blank" rel="noopener">Hexo 博客搭建之在文章中插入图片</a></li><li><a href="https://www.jianshu.com/p/c2ba9533088a" target="_blank" rel="noopener">hexo 博客图片问题</a></li></ul><blockquote><p>2018/8/3 更新，找到了更好的图床解决方案：用 github 做图床，然后配合一键上传图床工具 picgo，完美解决 markdown 图片上传问题，详细用法见 <a href="https://molunerfinn.com/PicGo/" target="_blank" rel="noopener">picgo官网</a></p></blockquote><h3 id="把博客部署上线"><a href="#把博客部署上线" class="headerlink" title="把博客部署上线"></a>把博客部署上线</h3><h4 id="设置-github"><a href="#设置-github" class="headerlink" title="设置 github"></a>设置 github</h4><p>最激动人心的时刻到了，当你安装好漂亮的主题，写好优秀的文章，就该部署上线让更多人看到了！由于 github 推出一个 github pages 服务，用来展示我们的网页，所以我们可以把我们的博客网页部署到那个上面，当别人在看我们的博客时，其实就是在看我们的 github pages 页面。</p><p>首先，我们需要注册一个 github 账号，假设你的 github 用户名是 zemin，我们建立一个项目名为：</p><p><code>zemin.github.io</code></p><blockquote><p>注意必须与你的用户名一模一样并严格按照格式来，否则待会儿不能直接通过 zemin.github.io 访问我们的博客，如下图所示，</p></blockquote><p><img src="/img/博客搭建之旅/12.png" alt=""></p><p><img src="/img/博客搭建之旅/13.png" alt=""></p><h4 id="把博客与-github-联系起来"><a href="#把博客与-github-联系起来" class="headerlink" title="把博客与 github 联系起来"></a>把博客与 github 联系起来</h4><p>我们在运行命令 <code>hexo g</code> 之后，可以在博客文件夹根目录发现一个名为 public 的的文件夹，这个就是我们 hexo 框架形成的网页文件，将它上传到我们刚刚建立的项目 zemin.github.io 里，便可以在我们的 github pages 页面看到我们的博客了，而这个页面的网址就是“zemin.github.io”了。</p><p>为了方便地上传我们的博客文件，我们用 SSH 密匙将博客与 github 联系起来，这个步骤有许多教程写的很好，我参考<a href="http://beiyuu.com/github-pages" target="_blank" rel="noopener">使用 Github Pages 建独立博客</a>和<a href="http://www.lovebxm.com/2018/06/24/hexo-github-blog/#%E6%90%AD%E5%BB%BA-Git-%E7%8E%AF%E5%A2%83" target="_blank" rel="noopener">可能是最详细的 Hexo + GitHub Pages 搭建个人博客的教程</a>写下以下教程：</p><p>1.生成 SSH</p><p>在命令行输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;你的邮箱&quot;</span><br></pre></td></tr></table></figure><p>一直点击回车直到碰到以下文字：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Enter passphrase (empty for no passphrase):</span><br><span class="line">Enter same passphrase again:</span><br></pre></td></tr></table></figure><p>第一个要求你输入密码，在部署的时候用得到，第二个要求你重复密码，如果不想要密码直接回车即可，我觉得个人电脑的话没必要输入密码。当碰到如下文字则说明运行成功。</p><p><img src="/img/博客搭建之旅/14.jpg" alt=""></p><p>2.把 SSH 添加进 github</p><p>SSH 一般放在路径 <code>C:\Users\pc\.ssh\id_rsa.pub</code> 下，如果没有再仔细找找，否则是上一步出了问题，用编辑器打开这个文件 .pub 文件，复制所有的文字。</p><p>登陆 github。点击右上角的 Settings—-&gt;SSH and GPG keys—-&gt;New SSH key，Title 可以填你项目的名字，然后把 .pub 里复制的文字再粘贴进 Key 里，最后点击 Add SSH Key 绿色按钮，完成配置。</p><p>3.测试是否配置成功</p><p>在命令行输入以下命令，切记原封不动输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>一直回车，碰到 <code>Are you sure you want to continue connecting (yes/no)?</code> 便输入“yes”，如果最后返回如下文字则配置成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi XXX! You&apos;ve successfully authenticated, but GitHub does not provide shell access.</span><br></pre></td></tr></table></figure><p>4.设置 Git 的个人信息</p><p>命令行输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;你的真名&quot;</span><br><span class="line">git config --global user.email &quot;你的邮箱&quot;</span><br></pre></td></tr></table></figure><p>5.配置 _config.yml</p><p>进入你在 github 的项目并点击右侧 Clone and download 绿色按钮，选择 Clone with SSH，并复制下框里的一串文字，如下图所示。</p><p><img src="/img/博客搭建之旅/14.png" alt=""></p><p>然后用编辑器打开博客根目录下的 _config.yml 文件，找到 <code># Deployment</code>（布署） 部分，并作如下修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:yichenchan/yichenchan.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><blockquote><p>git@github.com:yichenchan/yichenchan.github.io.git 处就是你刚刚复制的一串文字</p></blockquote><p>那如果我们还需要部署在 Coding.net 上该怎么办呢？同样复制下 Coding.net 上该项目的 SSH，然后 # Deployment 修改为以下写法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo:</span><br><span class="line">      github: git@github.com:yichenchan/chenyichen.github.io.git,master</span><br><span class="line">      coding: git@git.coding.net:yichenChan/yichenChan.git,master</span><br></pre></td></tr></table></figure><p>做完这些，我们的本地博客就与 github 或者 coding.net 联系起来了！</p><h4 id="让我们的博客上线吧！"><a href="#让我们的博客上线吧！" class="headerlink" title="让我们的博客上线吧！"></a>让我们的博客上线吧！</h4><p>首先我们需要安装一个插件，在博客根目录用命令行输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p><p>安装成功后，在你博客文件夹的根目录用命令行执行以下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean #删除 public 里的文件</span><br><span class="line">hexo g     #生成新的 public 文件</span><br><span class="line">hexo d     #部署到 github 上</span><br></pre></td></tr></table></figure></p><p>或者合并成一句：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d -g</span><br></pre></td></tr></table></figure></p><p>由于网络的原因，布署的时间可能会非常长，而且经常会卡在一个地方不动，碰到这种情况请耐心等待，当布署成功之后，会有 git done 的提示。</p><p>现在打开浏览器输入 <code>zemin(改成你自己的).github.io</code>，我们就可以看到我们的博客上线啦！</p><h3 id="给博客一个好记的名字（域名）"><a href="#给博客一个好记的名字（域名）" class="headerlink" title="给博客一个好记的名字（域名）"></a>给博客一个好记的名字（域名）</h3><p>虽然到目前为止，我们成功地上线了我们的博客，但是每次看到域名后面的 github.io 总有种不是自己的博客的感觉，既然是独立博客，我们就应该有个性化的独立的名字，最后我们试着给它一个好记的域名。</p><h4 id="注册你自己的域名"><a href="#注册你自己的域名" class="headerlink" title="注册你自己的域名"></a>注册你自己的域名</h4><p>首先我们得有自己的一个域名，在 <a href="https://sg.godaddy.com/zh/" target="_blank" rel="noopener">GoDaddy</a> 或者<a href="https://wanwang.aliyun.com/" target="_blank" rel="noopener">万网</a>都可以用低价买到自己中意的域名，但是由于万网的域名解析现在需要实名制，而 GoDaddy 是国外的网站不需要实名制所以我选择在后者购买域名，买下的 chenyichen.xyz 大概第一年七块钱，还是很便宜的。</p><h4 id="将你的域名与-XXX-github-io-绑定"><a href="#将你的域名与-XXX-github-io-绑定" class="headerlink" title="将你的域名与 XXX.github.io 绑定"></a>将你的域名与 XXX.github.io 绑定</h4><p>我们刚刚注册的域名目前还是空的，我们需要用 DNS解析把该域名指向我们的 xxx.github.io，使得其他人在输入 chenyichen.xyz 的时候可以直接跳转到我的 yichenchan.github.io。DNS 就是专门负责把域名解析为 IP 地址的系统，我们也可以通过 CNAME 直接把我们的域名指向另一个域名，这样就不用去搜 IP 地址，更加方便，下面介绍该怎么做，以 GoDaddy 的 DNS 为例，如果你是在万网买的域名，可以进入万网的域名控制台，操作大同小异。</p><p>1.将购买的域名指向 XXX.github.io</p><p>进入 GoDaddy 的“我的产品”页面，找到你刚刚买的域名，点击右侧的“DNS”，如下图所示：</p><p><img src="/img/博客搭建之旅/15.png" alt=""></p><p>点击右边的铅笔符号即可修改，把 A 类型的“值”（“指向”）改成你网站的IP地址，把 CNAME 类型的“名称”（“主机”）改成“www”，“值”（“指向”）改成你的网站域名即“yichenchan.github.io”，“TTL”都自定义为“6000”，如下图所示。</p><blockquote><p>如何知道你网站的IP地址呢，在命令行输入 <code>ping yichenchan.github.io</code> (换成自己的二级域名) 即可。</p></blockquote><p><img src="/img/博客搭建之旅/16.png" alt=""></p><p>2.修改 _config.yml</p><p>其实不是很确定这步需不需要，但是以防万一做一下。</p><p>用编辑器打开博客根目录的 _config.yml 文件，找到如下代码：</p><p><code>url: http://yoursite.com</code> </p><p>改成：</p><p><code>url: http://chenyichen.xyz（你自己的域名）</code></p><p>3.建立 CNAME 文件</p><p>在博客目录下的 source 文件夹里新建一个<strong>没有拓展名</strong>的文件（可以用编辑器新建），然后命名为<strong>CNAME</strong>，在里面写上你自己购买的域名，然后保存即可。</p><p>4.修改 github pages 设置</p><p>进入你在 github 上的博客项目，点击 setting如下图所示：</p><p><img src="/img/博客搭建之旅/17.png" alt=""></p><p>往下滑，找到“GitHub Pages”设置，在“Custom domain”下的输入框输入你购买的域名，然后点击“save”，如果你需要让它使用 https 的话，可以点击下面的“Enforce HTTPS”，大功告成！！</p><p><img src="/img/博客搭建之旅/18.png" alt=""></p><p>一般来说，DNS解析需要一段时间，第二天再打开你的浏览器，输入只属于你自己的域名，看看你的博客是不是映入眼帘!!赶紧发条朋友圈炫耀吧hiahia！！</p><h2 id="建站感想"><a href="#建站感想" class="headerlink" title="建站感想"></a>建站感想</h2><p>虽然网上 hexo+github pages 建站的教程数不胜数，但是希望按照自己的思路来写一篇教程，一是为了发表一篇博客纪念一下建站，二是为了练习一下 markdow 的语法，如果有人能看到这篇教程并得到帮助那更是再好不过了。</p><p>作为一个小白，第一次拥有自己的小站，就感觉有了自己一个小窝一样。前前后后折腾了半个月，最后在 chenyichen.xyz 打开自己的博客的感觉真的很棒！不过建站的初衷还是为了能够记录一下自己的学习经历，生活感想，希望自己能坚持更新自己的博客吧，毕竟博客的价值在于其内容而不是他的外表，最后贴一下自己建站过程中参考的网站，感谢这些无私提供教程的大佬，还有感谢本博客主题的开发者<a href="https://github.com/Molunerfinn" target="_blank" rel="noopener">Molunerfinn</a>。</p><ul><li><p><a href="http://www.lovebxm.com/2018/06/24/hexo-github-blog/#%E5%B0%86-GitHub-Pages-%E5%9C%B0%E5%9D%80%E8%A7%A3%E6%9E%90%E5%88%B0%E4%B8%AA%E4%BA%BA%E5%9F%9F%E5%90%8D" target="_blank" rel="noopener">可能是最详细的 Hexo + GitHub Pages 搭建个人博客的教程</a></p></li><li><p><a href="https://www.jianshu.com/p/05289a4bc8b2" target="_blank" rel="noopener">如何搭建一个独立博客——简明 Github Pages 与 Hexo 教程</a></p></li><li><p><a href="http://beiyuu.com/github-pages" target="_blank" rel="noopener">使用 Github Pages 建独立博客</a></p></li><li><p><a href="http://blog.haoji.me/build-blog-website-by-hexo-github.html?from=xa" target="_blank" rel="noopener">使用 hexo+github 搭建免费个人博客详细教程</a></p></li><li><p><a href="https://molunerfinn.com/hexo-theme-melody-doc/#/" target="_blank" rel="noopener">melody 主题文档</a></p></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> blog </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>祝丸子同学生日快乐！！！</title>
      <link href="/2018/07/25/%E7%A5%9D%E4%B8%B8%E5%AD%90%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/"/>
      <url>/2018/07/25/%E7%A5%9D%E4%B8%B8%E5%AD%90%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/</url>
      <content type="html"><![CDATA[<h2 id="祝可爱的丸子同学26岁生日快乐！"><a href="#祝可爱的丸子同学26岁生日快乐！" class="headerlink" title="祝可爱的丸子同学26岁生日快乐！"></a>祝可爱的丸子同学26岁生日快乐！</h2><a id="more"></a><h3 id="月亮代表我的心：）"><a href="#月亮代表我的心：）" class="headerlink" title="月亮代表我的心：）"></a>月亮代表我的心：）</h3><p><img src="/img/祝丸子生日快乐/1.jpg" alt=""></p>]]></content>
      
      <categories>
          
          <category> 个人随想 </category>
          
      </categories>
      
      
    </entry>
    
  
  
</search>
