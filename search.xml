<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 2）</title>
      <link href="/2018/10/19/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/10/19/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h2 id="Keras-教程"><a href="#Keras-教程" class="headerlink" title="Keras 教程"></a>Keras 教程</h2><ul><li>Keras 是一个高级神经网络框架，用 Python 写成，在 TensorFlow 和 CNTK 等一些更低级的框架上运行，拥有比 tensorflow 更高的抽象</li><li>Keras 能够快速搭建和试验不同的模型</li><li>Keras 比低级框架限制更多，无法实现一些非常复杂的模型，但是对一些普通模型运行很好</li></ul><p>这里用 Keras 实现一个识别图片中的人是否开心的算法。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_1.png" alt=""></p><p>先引入我们需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> kt_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p>加载数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">Y_train = Y_train_orig.T</span><br><span class="line">Y_test = Y_test_orig.T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_3.png" alt=""></p><p>在 Keras 中训练测试模型有四个步骤：</p><ul><li><p>建立模型</p><ul><li><p>自己定义一个 model() 函数进行前向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HappyModel</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the HappyModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- 输入图片的维度（不包括图片的数量！）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero-Padding: pads the border of X_input with zeroes</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CONV -&gt; BN -&gt; RELU Block applied to X</span></span><br><span class="line">    X = Conv2D(<span class="number">32</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">1</span>, <span class="number">1</span>), name = <span class="string">'conv0'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn0'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MAXPOOL</span></span><br><span class="line">    X = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'max_pool'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FLATTEN X (means convert it to a vector) + FULLYCONNECTED</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>, name=<span class="string">'fc'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model. This creates your Keras model instance, you'll use this instance to train/test the model.</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'HappyModel'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel = HappyModel(X_train.shape[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure></li></ul></li><li><p>编译模型</p><ul><li><p>model.compile(optimizer = “…”, loss = “…”, metrics = [“accuracy”])</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.compile(optimizer = <span class="string">'Adam'</span>, loss = <span class="string">'binary_crossentropy'</span>, metrics = [<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></li></ul></li><li><p>在训练数据上训练模型</p><ul><li><p>model.fit(x = …, y = …, epochs = …, batch_size = …)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.fit(x = X_train, y = Y_train, epochs = <span class="number">40</span>, batch_size = <span class="number">16</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_4.png" alt=""></p></li></ul></li><li><p>在测试数据上测试模型</p><ul><li><p>model.evaluate(x = …, y = …)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">preds = happyModel.evaluate(x = X_test, y = Y_test)</span><br><span class="line">print(preds)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_5.png" alt=""></p></li></ul></li></ul><p>最后可以达到 98% 的训练集精确度，90% 的训练集精确度。</p><p>我们可以用自己的图片进行测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">'images/45.jpg'</span><span class="comment"># 用自己的图片路径</span></span><br><span class="line"></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">imshow(img)</span><br><span class="line"></span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line"></span><br><span class="line">print(happyModel.predict(x))</span><br><span class="line"><span class="keyword">if</span> happyModel.predict(x) == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">'这个人不开心哦 :('</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'这个人在笑哦 :)'</span>)</span><br></pre></td></tr></table></figure><p>我们还可以获得模型的概况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.summary()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_6.png" alt=""></p><p>还可以将模型流程图打印出来并保存为位图文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(happyModel, to_file=<span class="string">'HappyModel.png'</span>)</span><br><span class="line">SVG(model_to_dot(happyModel).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_7.png" alt=""></p><h2 id="用-Keras-创建-ResNet-残差网络"><a href="#用-Keras-创建-ResNet-残差网络" class="headerlink" title="用 Keras 创建 ResNet 残差网络"></a>用 Keras 创建 ResNet 残差网络</h2><p>这节会用残差网络创建一个非常深的卷积网络。</p><ul><li>实现基本的残差网络组块</li><li>将这些组块合在一起实现一个最先进的神经网络图像分类器</li></ul><h3 id="包"><a href="#包" class="headerlink" title="包"></a>包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> resnets_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line">K.set_learning_phase(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="非常深的神经网络的问题所在"><a href="#非常深的神经网络的问题所在" class="headerlink" title="非常深的神经网络的问题所在"></a>非常深的神经网络的问题所在</h3><ul><li>优点：很深的网络能学习到非常多层次的抽象特征，从边缘到复杂特征。</li><li>缺点：训练时会产生梯度下降，使得训练速度极慢。</li></ul><h3 id="残差网络的基本-block-组块"><a href="#残差网络的基本-block-组块" class="headerlink" title="残差网络的基本 block 组块"></a>残差网络的基本 block 组块</h3><p>有两种主要的组块在残差网络中，取决于输入/输出维度是否相同。</p><ol><li><p>The identity block （输入/输出维度相同）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_8.png" alt=""></p><p>First component of main path: </p><ul><li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is “same” and its name should be <code>conv_name_base + &#39;2b&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2c&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li></ul><p>Final step: </p><ul><li>The shortcut and the input are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The identity block 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X, f, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Second component of main path </span></span><br><span class="line">    X = Conv2D(filters = F2, kernel_size = (f,f), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'same'</span>, name = conv_name_base + <span class="string">'2b'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path </span></span><br><span class="line">    X = Conv2D(filters = F3, kernel_size = (<span class="number">1</span>,<span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2c'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2c'</span> )(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation </span></span><br><span class="line">    X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">"relu"</span>)(X)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><blockquote><p>最后的 X 加上 X_shoutcut 应该使用 Keras 内置的 Add()([]) 函数，否则后面会报错：’Tensor’ object has no attribute ‘_keras_history’</p></blockquote></li><li><p>The convolutional block（输入/输出维度不相同）</p><p>当输入输出维度不相同时，在 shourtcut 这条路径使用一个卷积操作来使得维度相同。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_9.png" alt=""></p><p>各层的细节：</p><p>First component of main path:</p><ul><li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. </li><li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be <code>conv_name_base + &#39;2b&#39;</code>.</li><li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be <code>conv_name_base + &#39;2c&#39;</code>.</li><li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li></ul><p>Shortcut path:</p><ul><li>The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;1&#39;</code>.</li><li>The BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;1&#39;</code>. </li></ul><p>Final step: </p><ul><li>The shortcut and the main path values are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>实现的代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The convolutional block 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X, f, filters, stage, block, s = <span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the convolutional block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    s -- Integer, specifying the stride to be used</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value</span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path </span></span><br><span class="line">    X = Conv2D(F1, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path</span></span><br><span class="line">    X = Conv2D(F2, (f, f), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'same'</span>, name = conv_name_base + <span class="string">'2b'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path </span></span><br><span class="line">    X = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), name = conv_name_base + <span class="string">'2c'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### SHORTCUT PATH ####</span></span><br><span class="line">    X_shortcut = Conv2D(F3, (<span class="number">1</span>,<span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'1'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'1'</span> )(X_shortcut)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure></li></ol><h3 id="建立一个-50-层的残差网络模型"><a href="#建立一个-50-层的残差网络模型" class="headerlink" title="建立一个 50 层的残差网络模型"></a>建立一个 50 层的残差网络模型</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_10.png" alt=""></p><p>这个模型的细节为：</p><ul><li>Zero-padding pads the input with a pad of (3,3)</li><li>Stage 1:<ul><li>The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”.</li><li>BatchNorm is applied to the channels axis of the input.</li><li>MaxPooling uses a (3,3) window and a (2,2) stride.</li></ul></li><li>Stage 2:<ul><li>The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>Stage 3:<ul><li>The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”.</li></ul></li><li>Stage 4:<ul><li>The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”.</li></ul></li><li>Stage 5:<ul><li>The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [512, 512, 2048], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”.</li><li>The flatten doesn’t have any hyperparameters or name.</li><li>The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be <code>&#39;fc&#39; + str(classes)</code>.</li></ul><p>实现代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50 层的残差网络模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape = <span class="params">(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span>, classes = <span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the popular ResNet50 the following architecture:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3</span></span><br><span class="line"><span class="string">    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string">    classes -- integer, number of classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">64</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">2</span>, <span class="number">2</span>), name = <span class="string">'conv1'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X) <span class="comment">#  kernel_initializer = glorot_uniform 表示 Xavier 均匀初始化</span></span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn_conv1'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage = <span class="number">2</span>, block=<span class="string">'a'</span>, s = <span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 3 </span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], stage = <span class="number">3</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 4</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'d'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'e'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 5 </span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># AVGPOOL</span></span><br><span class="line">    X = AveragePooling2D((<span class="number">2</span>,<span class="number">2</span>), name = <span class="string">'avg_pool'</span>)(X)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># output layer</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(classes, activation=<span class="string">'softmax'</span>, name=<span class="string">'fc'</span> + str(classes), kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)<span class="comment">#  kernel_initializer = glorot_uniform 表示 Xavier 均匀初始化</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create model</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'ResNet50'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="进行训练和预测"><a href="#进行训练和预测" class="headerlink" title="进行训练和预测"></a>进行训练和预测</h3><ul><li><p>建立模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = ResNet50(input_shape = (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>), classes = <span class="number">6</span>)</span><br></pre></td></tr></table></figure></li><li><p>编译模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></li><li><p>训练模型</p><ul><li><p>载入数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert training and test labels to one hot matrices</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br></pre></td></tr></table></figure></li><li><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, Y_train, epochs = <span class="number">2</span>, batch_size = <span class="number">32</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">preds = model.evaluate(X_test, Y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure></li></ul><h3 id="测试自己的图片"><a href="#测试自己的图片" class="headerlink" title="测试自己的图片"></a>测试自己的图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">'images/my_image.jpg'</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line">print(<span class="string">'Input image shape:'</span>, x.shape)</span><br><span class="line">my_image = scipy.misc.imread(img_path)</span><br><span class="line">imshow(my_image)</span><br><span class="line">print(<span class="string">"class prediction vector [p(0), p(1), p(2), p(3), p(4), p(5)] = "</span>)</span><br><span class="line">print(model.predict(x))</span><br></pre></td></tr></table></figure><h3 id="对模型进行总览"><a href="#对模型进行总览" class="headerlink" title="对模型进行总览"></a>对模型进行总览</h3><ul><li><p>表格</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure></li><li><p>位图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(model, to_file=<span class="string">'model.png'</span>)</span><br><span class="line">SVG(model_to_dot(model).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure></li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>实际中非常深的普通网络不起作用，因为梯度下降它们很难训练</li><li>跳跃连接帮忙解决跳跃连接问题，它们也使得残差网络很容易学习 identity function</li><li>有两种主要的组块：The identity block and the convolutional block</li><li>把这些组块堆叠起来可以形成很深的残差网络</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 4 week 2）—— 深度卷积网络的案例研究</title>
      <link href="/2018/10/17/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w2/"/>
      <url>/2018/10/17/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w2/</url>
      <content type="html"><![CDATA[<p>本周我们直接从研究论文里学习深度 CNN 的一些使用技巧和方法。</p><h2 id="outline"><a href="#outline" class="headerlink" title="outline"></a>outline</h2><ul><li><p>一些经典的网络模型：</p><ul><li>LeNet-5</li><li>AlexNet</li><li>VGGNet</li></ul></li><li><p>残差卷积网络 ResNet</p></li><li>初始神经网络实例</li></ul><h2 id="经典卷积网络模型"><a href="#经典卷积网络模型" class="headerlink" title="经典卷积网络模型"></a>经典卷积网络模型</h2><h3 id="LeNet－５"><a href="#LeNet－５" class="headerlink" title="　LeNet－５"></a>　LeNet－５</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_1.png" alt=""></p><ul><li>60000 个参数</li><li>从左到右 $n_H,n_W$ 减小，$n_C$ 增大</li><li>一个至今使用的模式：conv -&gt; pool -&gt; conv -&gt; pool -&gt; FC -&gt; FC -&gt; output</li><li>这篇论文中使用的是 sigmoid/tanh 函数，而不是 ReLU</li><li>由于论文较久远，吴恩达建议只读论文的第二章，快速看一下第三章</li></ul><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_3.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_4.png" alt=""></p><ul><li>与 LetNet 很相似，但是大得多，大概六千万个参数</li><li>使用 ReLU 激活函数</li><li>使用两块 GPU 训练</li><li>局部响应归一层 LRN</li></ul><h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_5.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_6.png" alt=""></p><ul><li>超过一亿三千万个参数，十分庞大的网络</li><li>结构非常统一，缺点是网络过于庞大</li></ul><p>建议先读 AlexNet 的论文，再读 VGG-16 的论文，最后读最难的 LeNet。</p><h2 id="残差网络-ResNet"><a href="#残差网络-ResNet" class="headerlink" title="残差网络 ResNet"></a>残差网络 ResNet</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_7.png" alt=""></p><p>训练网络很深时会发生梯度消失或者梯度爆炸，利用“跳跃连接” ( skip connection)，可以训练很深很深的残差网络。</p><h3 id="残差结构-residual-block"><a href="#残差结构-residual-block" class="headerlink" title="残差结构 residual block"></a>残差结构 residual block</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_8.png" alt=""></p><ul><li>绿色的叫<strong>主路径 (main  path)</strong></li><li>紫色的叫<strong>快捷路径 (short cut)</strong> 或 <strong>跳跃连接 (skip connection)</strong></li><li>最后两者合一得到的 $a^{[l+1]} = g(z^{[l+2]}+a^{[l]})$ </li></ul><h3 id="残差网络-Residual-Network"><a href="#残差网络-Residual-Network" class="headerlink" title="残差网络 Residual Network"></a>残差网络 Residual Network</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_9.png" alt=""></p><h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>对于一个普通网络 (plain network)，随着层数的增大，理论上训练误差应该减小，但是实际上是先减小后增大，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_10.png" alt=""></p><p>而如果使用 ResNet，即使训练一个 1000 层的网络，性能也不会先增后降。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_11.png" alt=""></p><h3 id="为什么-ResNet-有用"><a href="#为什么-ResNet-有用" class="headerlink" title="为什么 ResNet 有用"></a>为什么 ResNet 有用</h3><p>假设我们在一个网络后面加上一个残差结构，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_12.png" alt=""></p><p> 所有的激活函数使用的都是 ReLU，所以 $a \geq 0 $ </p><p>$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(w^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$</p><p>加入使用正则化使权重衰减，w 和 b 变得很小，如果没有残差结构，那么可能会发生梯度消失，但是如果有残差结构，那么就算 w 和 b 衰减到 0，$a^{[l+2]}=g(a^{[l]})=ReLU(a^{[l]})=a^{[l]}$，所以额外增加两层也不会损害神经网络的表现。</p><p>要实现跳跃连接，我们必须假设 $z^{[l+2]}$ 和 $a^{[l]}$ 是同维度，这样才能相加，所以深度残差网络里面很多  <strong>same 卷积</strong>，保证卷积之后维度不变。</p><p>但是有时候 $z^{[l+2]}$ 和 $a^{[l]}$ 维度不同，比如 256 和 128，我们可以加上一个系数 $W_S$：</p><p>$a^{[l+2]}=g(w^{[l+2]}a^{[l+1]}+b^{[l+2]}+W_sa^{[l]})$</p><p>此处$W_S$ 是一个 256×128 的矩阵。  </p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_13.png" alt=""></p><h2 id="Inception-网络"><a href="#Inception-网络" class="headerlink" title="Inception 网络"></a>Inception 网络</h2><h3 id="1-1-卷积"><a href="#1-1-卷积" class="headerlink" title="1*1 卷积"></a>1*1 卷积</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_14.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_15.png" alt=""></p><p>1×1 卷积本质上就是一个全连接神经网络，逐一作用于这 6×6=36 个不同的位置，这个小神经网络接收一行 32 个数的输入，然后输出过滤器数个输出值。这个有时被称作“网中网”。</p><p>一个实例：</p><p>我们可以用 1×1 卷积来缩小通道数：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_16.png" alt=""></p><h3 id="Inception-network-引例"><a href="#Inception-network-引例" class="headerlink" title="Inception network 引例"></a>Inception network 引例</h3><p>在设计卷积网络的某一层的时候，有可能会用到1×1的卷积层或者3×3的卷积层或者5×5的卷积层或者池化层，Inception 网络在某一层将这些全部用上，使得网络更复杂，效果也更好。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_17.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_18.png" alt=""></p><p>我们将输出进行不同的卷积操作，但是都进行 “same padding”，使得输出维度都一样，最后将所有的输出堆叠起来，这就形成了 Inception 网络的一个模块。</p><h3 id="计算成本问题"><a href="#计算成本问题" class="headerlink" title="计算成本问题"></a>计算成本问题</h3><p>我们看 5×5 的卷积核。</p><ol><li><p>像上图一样直接将 5×5 的卷积核与 28×28×192 的输入进行卷积，得到 28×28×32 的输出</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_19.png" alt=""></p><p>最后进行的乘法运算次数为 28×28×32×5×5×192 约等于 1.2亿 次</p></li><li><p>在进行 5×5 卷积之前进行一个 1×1 卷积（<strong>瓶颈层</strong>），先将通道数减到 16，再进行 5×5 卷积，得到 28×28×32 的输出</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_20.png" alt=""></p><p>最后的运算次数为 28×28×16×192×1×1 + 28×28×32×5×5×16 约为 1240万 次，是第一种方法的十分之一运算量。</p></li></ol><h3 id="Inception-模块"><a href="#Inception-模块" class="headerlink" title="Inception 模块"></a>Inception 模块</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_21.png" alt=""></p><h3 id="Inception-网络-1"><a href="#Inception-网络-1" class="headerlink" title="Inception 网络"></a>Inception 网络</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_22.png" alt=""></p><ul><li><p>红色部分是一个一个 Inception 模块</p></li><li><p>绿色的旁支是以隐藏层作为输入的 softmax 函数，防止网络太深了过拟合</p></li><li><p>这个网络是谷歌开发的 gooleNet，用来纪念 Lecun 的 LeNet</p></li><li><p>Inception 这个名字来源于《盗梦空间 (Inception)》表情包：），作者们把这个图片当成了构建更加有深度的神经网络的动机：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_23.png" alt=""></p></li></ul><h2 id="搭建卷积网络的小技巧"><a href="#搭建卷积网络的小技巧" class="headerlink" title="搭建卷积网络的小技巧"></a>搭建卷积网络的小技巧</h2><h3 id="使用开源资源"><a href="#使用开源资源" class="headerlink" title="使用开源资源"></a>使用开源资源</h3><ul><li>使用文献中的网络结构</li><li>使用开源的网络结构算法实现</li><li>使用人家预训练好的模型然后在自己的数据集上进行微调</li></ul><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>如果你想实现一个计算机视觉应用而不想从零开始训练权重，实现更快的方式通常是下载已经训练好权重的网络结构，把这个作为预训练迁移到你感兴趣的新任务上，计算机视觉的研究社区已经很擅长把许多数据库发布在网络上，如 ImageNet、MSCOCO、PASCAL 等数据库，许多计算机视觉的研究者已经在上面训练了自己的算法，有时训练要耗费好几周时间，占据许多 GPU，事实上有人已经做过这种训练，这意味着你可以下载这些开源的权重为你自己的神经网络做好的初始化开端，且可以用迁移学习来迁移知识，从这些大型公共数据库迁移知识到你自己的问题上。</p><p>假如我们要识别家里的两只猫，训练数据量不够，我们可以从网上下载一些训练好的模型，比如在有 1000 类物体的 ImageNet 数据库上训练的模型。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_24.png" alt=""></p><p>将紫色框住的部分全部“冻结住”，可训练参数为零，将这部分看成一个模块或者函数，我们需要做的是把 1000 分类的 softmax 改成 3 分类的然后训练 softmax 这部分的参数即可。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_25.png" alt=""></p><p>当你有更大的带标签数据集，你可以冻结更少的层数。只冻结前面这些层，然后训练后面这些层，（1）你可以用最后几层的权重，作为初始化开始做梯度下降，把 softmax 改成自己需要的（2）或者你也可以去掉最后几层，然后用自己的新神经元和新 softmax 输出。原则是：你数据越多，所冻结的层数可以越少，自己训练的层数可以越多。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_26.png" alt=""></p><p>最后，如果你有许多数据，你可以用该开源网络和权重来初始化整个网络然后训练。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在计算机视觉领域，更多的数据对几乎所有的计算机视觉工作都有好处，但是对于绝大多数问题，总是不能获得足够多的数据，这就需要数据增强。</p><ol><li><p>普通数据增强方式</p><p>镜像 mirroring</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_27.png" alt=""></p><p>随机裁剪 random cropping</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_28.png" alt=""></p><p>旋转 rotation</p><p>剪切 shearing</p><p>局部弯曲 local warping</p><p>……</p></li><li><p>色彩变化 color shifting</p><p>给三个颜色通道 R G B 增加不同的值，可以使用 PCA 主成份分析算法来实现。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_29.png" alt=""></p></li></ol><h4 id="数据增强线程"><a href="#数据增强线程" class="headerlink" title="数据增强线程"></a>数据增强线程</h4><p>数据存在硬盘中，在进入 CPU/GPU 训练之前使用几个 CPU 线程进行失真处理（数据增强），训练线程和失真线程可以并行处理。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_30.png" alt=""></p><h3 id="目前计算机视觉的状态"><a href="#目前计算机视觉的状态" class="headerlink" title="目前计算机视觉的状态"></a>目前计算机视觉的状态</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_31.png" alt=""></p><p>机器学习的两种知识来源：</p><ul><li>带标签的数据</li><li>手工构建的 特征/网络结构/其他组件</li></ul><p>当我们拥有大量的数据，我们只需要简单的算法和更少的手工构建部分，而当我们数据量不够时，就需要更多的手工构建部分，需要花更多时间在设计网络结构上。传统的计算机视觉论文，相当依赖复杂的手工构建，就算近些年随着数据量的增加，手工构建的数量明显减少，但还是有很多手工设计的网络架构，所以超参数选择在计算机视觉比起其他领域更加复杂。</p><h3 id="在基准数据集上做的好-赢得比赛-的技巧"><a href="#在基准数据集上做的好-赢得比赛-的技巧" class="headerlink" title="在基准数据集上做的好 / 赢得比赛 的技巧"></a>在基准数据集上做的好 / 赢得比赛 的技巧</h3><p>这些方法在比赛时可以使得结果准确度增加一些微弱的优势，但是会增加很多运算成本，所以这些东西并不会真的用在实际的服务客户的系统上。</p><ul><li><p>Ensembling 总效果</p><ul><li>独立训练几个网络然后取输出值的平均值</li></ul></li><li><p>multi-crop at test time 在测试时多重裁剪</p><ul><li><p>在测试图像的多个裁剪子图像中运行分类器然后求结果平均值</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_32.png" alt=""></p></li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 1）</title>
      <link href="/2018/10/16/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/10/16/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c4w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h1 id="cnn-辅助函数的构建"><a href="#cnn-辅助函数的构建" class="headerlink" title="cnn 辅助函数的构建"></a>cnn 辅助函数的构建</h1><p>这周的编程作业内容是使用 numpy 实现卷积层和池化层，包括前向传播和方向传播。</p><h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="作业大纲"><a href="#作业大纲" class="headerlink" title="作业大纲"></a>作业大纲</h2><ul><li>卷积函数，包括：<ul><li>零填充</li><li>卷积窗口</li><li>卷积前向传播</li><li>卷积反向传播</li></ul></li><li>池化函数，包括：<ul><li>池化前向传播</li><li>创造 mask</li><li>Distribute value</li><li>池化反向传播</li></ul></li></ul><p>注意：每一步前向传播都需要储存一些参数在 cache 中，以便方向传播时可以用</p><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>先构建两个辅助函数。</p><h3 id="零填充-Zero-Padding"><a href="#零填充-Zero-Padding" class="headerlink" title="零填充 Zero-Padding"></a>零填充 Zero-Padding</h3><p>该辅助函数的作用是在该图片周围加零，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_1.png" alt=""></p><ul><li>它使图片在通过卷积层时尺寸不会减小，对深层网络尤其有效</li><li>使我们保留边缘的重要信息</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X, pad)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    在图片的长宽方向填充 pad 宽的 0 像素值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- 代表 m 张图片的形状为 (m, n_H, n_W, n_C) 的数组</span></span><br><span class="line"><span class="string">    pad -- 整数，图片水平方向和竖直方向的填充值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.pad(填充对象, ((维度1的前方，维度1的后方),(..,..),(..,..),(..,..)), '填充方式', constant_values = (前方填充值, 后方填充值)) </span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>,<span class="number">0</span>),(pad,pad),(pad,pad),(<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>, constant_values = (<span class="number">0</span>,<span class="number">0</span>) )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br></pre></td></tr></table></figure><h3 id="单步卷积运算"><a href="#单步卷积运算" class="headerlink" title="单步卷积运算"></a>单步卷积运算</h3><ul><li>取出输入向量</li><li>在输入的每个位置使用过滤器进行卷积</li><li>输出另一个向量</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_single_step</span><span class="params">(a_slice_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将过滤器与一小片图片进行卷积操作</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_slice_prev -- 形状为 (f, f, n_C_prev) 的之前图片的一小片</span></span><br><span class="line"><span class="string">    W -- 形状为 (f, f, n_C_prev) 形状与过滤器一样的权重参数矩阵</span></span><br><span class="line"><span class="string">    b -- 形状为 (1, 1, 1) 的偏差参数矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 标量，过滤器与一小片图片卷积的结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先逐元素相乘</span></span><br><span class="line">    s = np.multiply(a_slice_prev, W)</span><br><span class="line">    <span class="comment"># 再全部相加</span></span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    <span class="comment"># 加上偏差 b</span></span><br><span class="line">    Z = Z + float(b) <span class="comment"># b 为矩阵，先用 float() 转为标量再相加</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><h3 id="卷积神经网络——前向传播"><a href="#卷积神经网络——前向传播" class="headerlink" title="卷积神经网络——前向传播"></a>卷积神经网络——前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_2.png" alt=""></p><p>切片方法：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_3.png" alt=""></p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_prev, W, b, hparameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    卷积函数的前向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 形状为 (m, n_H_prev, n_W_prev, n_C_prev)，前一层的激活值</span></span><br><span class="line"><span class="string">    W -- 形状为 (f, f, n_C_prev, n_C)，权重矩阵</span></span><br><span class="line"><span class="string">    b -- 形为 (1, 1, 1, n_C)，偏差矩阵</span></span><br><span class="line"><span class="string">    hparameters -- 包含步长 "stride" 和填充 "pad" 的字典</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 形为 (m, n_H, n_W, n_C) 的卷积输出</span></span><br><span class="line"><span class="string">    cache -- 反向传播中要用到的缓存值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 A_prev 的 shape 中获取维度信息  </span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 W 的 shape 中获取维度信息</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = np.shape(W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取步长和填充信息</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用公式计算输出的维度信息，int() 可用于向下取整</span></span><br><span class="line">    n_H = int((n_H_prev+<span class="number">2</span>*pad-f)/stride) + <span class="number">1</span></span><br><span class="line">    n_W = int((n_W_prev+<span class="number">2</span>*pad-f)/stride) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用 0 初始化输出矩阵</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输入进行填充</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># 对 m 个训练图像的循环</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i,:,:,:]             <span class="comment"># 选出第 i 个图像</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                         <span class="comment"># 对输出向量高度方向的循环</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                     <span class="comment"># 对输出向量宽度方向的循环</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):                 <span class="comment"># 对输出向量通道数（过滤器个数）的循环</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到循环到 (i,h,w,c) 时候对应的图像“小片”</span></span><br><span class="line">                    vert_start = h*stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w*stride </span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 进行切片操作</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line">                    <span class="comment"># 将对应的图像小片和过滤器进行卷积得到 (i,h,w,c) 处的值</span></span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])</span><br><span class="line">                                          </span><br><span class="line">    <span class="comment"># 确保输出维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将一些信息储存在缓存中以便反向传播可以用</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化层减小了输入的高度和宽度，帮助减少计算量，使得特征检测器在输入中的位置更加不变。</p><ul><li>max pooling</li><li>average pooling</li></ul><p>池化层没有参数学习，只需要确定超参数，例如滤波器的大小。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_prev, hparameters, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    池化层的前向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "f" and "stride"</span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出输入的维度信息</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出过滤器的参数 </span></span><br><span class="line">    f = hparameters[<span class="string">"f"</span>]</span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义输出的维度</span></span><br><span class="line">    n_H = int(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = int(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化输出矩阵</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))              </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                           <span class="comment"># 对所有训练样本循环</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                     <span class="comment"># 对高度方向循环</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                 <span class="comment"># 对宽度方向循环</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range (n_C):            <span class="comment"># 对输出通道数的循环</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到切片索引值</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 对输入进行切片</span></span><br><span class="line">                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 进行池化操作</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.max(a_prev_slice)</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.average(a_prev_slice)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缓存</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保证输出维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h2 id="cnn-中的反向传播"><a href="#cnn-中的反向传播" class="headerlink" title="cnn 中的反向传播"></a>cnn 中的反向传播</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>这部分是可选项，在课程中也没有给详细的推导过程和解释，具体的解释可以参考以下几个网站：</p><ul><li><a href="https://grzegorzgwardys.wordpress.com/2016/04/22/8/" target="_blank" rel="noopener">参考1</a></li><li><a href="http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/" target="_blank" rel="noopener">参考2</a></li><li><a href="https://www.cnblogs.com/pinard/p/6494810.html" target="_blank" rel="noopener">参考3</a></li></ul><p>在参考 1 中我们可以知道，$dA^{[l-1]}$ 就是将 $dZ^{[l]}$ 与翻转 180 度的过滤器矩阵进行卷积的结果，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_4.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_5.jpg" alt=""></p><p>这正是下面式子所表示的，+= 就是将四个叠加起来，问题是下面的式子中的 W 并没有旋转 180 度……这点一直无法解释……</p><script type="math/tex; mode=display">d A^{[l-1]} + = \sum _ { h = 0 } ^ { n _ { l I } } \sum _ { w = 0 } ^ { n _ { W } } W _ { c } \times d Z ^{[l]} _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">d W _ { c } + = \sum _ { h = 0 } ^ { n _ { H } } \sum _ { w = 0 } ^ { n _ { W } } a _ { s l i c e } \times d Z _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">d b = \sum _ { h } \sum _ { w } d Z _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db[:,:,:,c] += dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward(), output of conv_forward()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),</span></span><br><span class="line"><span class="string">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    dW -- gradient of the cost with respect to the weights of the conv layer (W)</span></span><br><span class="line"><span class="string">          numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    db -- gradient of the cost with respect to the biases of the conv layer (b)</span></span><br><span class="line"><span class="string">          numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (A_prev, W, b, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = np.shape(W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters"</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from dZ's shape</span></span><br><span class="line">    (m, n_H, n_W, n_C) = np.shape(dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev, dW, db with the correct shapes</span></span><br><span class="line">    dA_prev = np.zeros(np.shape(A_prev))                           </span><br><span class="line">    dW = np.zeros(np.shape(W))   </span><br><span class="line">    db = np.zeros(np.shape(b))   </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pad A_prev and dA_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    dA_prev_pad = zero_pad(dA_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select ith training example from A_prev_pad and dA_prev_pad</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i,:,:,:]</span><br><span class="line">        da_prev_pad = dA_prev_pad[i,:,:,:]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice"</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the slice from a_prev_pad</span></span><br><span class="line">                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Update gradients for the window and the filter's parameters using the code formulas given above</span></span><br><span class="line">                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br><span class="line">                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br><span class="line">                    db[:,:,:,c] += dZ[i, h, w, c]</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span></span><br><span class="line">        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h3 id="池化层-1"><a href="#池化层-1" class="headerlink" title="池化层"></a>池化层</h3><p>虽然池化层没有参数，但是还是需要将梯度反向传播到池化层的上一层，以便反向传播能继续下去。</p><h4 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max pooling"></a>max pooling</h4><p>对于 max pooling 而言，只有原来的最大值才对最终的代价函数有影响，所以我们只需要计算代价函数对这个最大值的梯度即可，其他的置为零，首先创造一个蒙板函数：</p><script type="math/tex; mode=display">X = \left[ \begin{array} { l l } { 1 } & { 3 } \\ { 4 } & { 2 } \end{array} \right] \quad \rightarrow \quad M = \left[ \begin{array} { l l } { 0 } & { 0 } \\ { 1 } & { 0 } \end{array} \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a mask from an input matrix x, to identify the max entry of x.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Array of shape (f, f)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    mask = (x == np.max(x)) <span class="comment"># x 中等于 np.max(x) 的都为真，其他为假</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><h4 id="average-pooling"><a href="#average-pooling" class="headerlink" title="average pooling"></a>average pooling</h4><p>由于 average pooling 中过滤器中每个值都对最终结果有影响，所以这每个值的梯度都是下一层梯度的平均一份，因为它们每个数对最终代价函数的贡献都是一样的，所以我们将一个梯度分散为若干个相等的梯度：</p><script type="math/tex; mode=display">d Z = 1 \quad \rightarrow \quad d Z = \left[ \begin{array} { l l } { 1 / 4 } & { 1 / 4 } \\ { 1 / 4 } & { 1 / 4 } \end{array} \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span><span class="params">(dz, shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Distributes the input value in the matrix of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dz -- input scalar</span></span><br><span class="line"><span class="string">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Array of size (n_H, n_W) for which we distributed the value of dz</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shape (≈1 line)</span></span><br><span class="line">    (n_H, n_W) = shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the value to distribute on the matrix (≈1 line)</span></span><br><span class="line">    average = n_H * n_W</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a matrix where every entry is the "average" value (≈1 line)</span></span><br><span class="line">    a = dz * np.ones((n_H, n_W)) / average</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><h4 id="合并到一个函数"><a href="#合并到一个函数" class="headerlink" title="合并到一个函数"></a>合并到一个函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span><span class="params">(dA, cache, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现池化层的反向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A</span></span><br><span class="line"><span class="string">    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters </span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 cache 取出参数</span></span><br><span class="line">    (A_prev, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出超参数</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    f = hparameters[<span class="string">'f'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出 dA_prev 和 dA 的维度信息</span></span><br><span class="line">    m, n_H_prev, n_W_prev, n_C_prev = np.shape(A_prev)</span><br><span class="line">    m, n_H, n_W, n_C = np.shape(dA)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 dA_prev 初始化</span></span><br><span class="line">    dA_prev = np.zeros(np.shape(A_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                        <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对每个训练样例单独操作</span></span><br><span class="line">        a_prev = A_prev[i, :,:,:]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop on the vertical axis</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop on the horizontal axis</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels (depth)</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到切片索引值</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride </span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 用两种方式计算反向传播</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 进行切片</span></span><br><span class="line">                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                        <span class="comment"># 创造目前切片的蒙板，使得该切片中最大值置 1，其他置 0</span></span><br><span class="line">                        mask = create_mask_from_window(a_prev_slice)</span><br><span class="line">                        <span class="comment"># 将 dA[i,h,w,c] 这个位置的值乘上蒙板得到 [i,h,w,c] 这个位置反向传播的结果，最后根据链式法则将所有支路相加，也就是 +=</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i,h,w,c]</span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 首先得到 [i,h,w,c] 这个特定位置的梯度值 dA[i,h,w,c]</span></span><br><span class="line">                        da = dA[i,h,w,c]</span><br><span class="line">                        <span class="comment"># 过滤器形状</span></span><br><span class="line">                        shape = (f,f)</span><br><span class="line">                        <span class="comment"># 将 dA[i,h,w,c] 这个值分散得到该位置反向传播结果，根据链式法则将所有位置的结果相加</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)</span><br><span class="line">                        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev</span><br></pre></td></tr></table></figure><h1 id="cnn-的应用"><a href="#cnn-的应用" class="headerlink" title="cnn 的应用"></a>cnn 的应用</h1><p>这部分使用 tensorflow 来构建一个分类器。</p><h2 id="包的引入和数据集"><a href="#包的引入和数据集" class="headerlink" title="　包的引入和数据集"></a>　包的引入和数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> cnn_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">＃ 加载数据集</span><br><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>仍然是识别手势的数据集：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_6.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片示例</span></span><br><span class="line">index = <span class="number">6</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(Y_train_orig[:, index])))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_7.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集信息</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br><span class="line">conv_layers = &#123;&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_8.png" alt=""></p><h2 id="创建占位符"><a href="#创建占位符" class="headerlink" title="创建占位符"></a>创建占位符</h2><p>首先需要创建输入数据的占位符，以便在运行 sess 时可以喂数据进去。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建占位符</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_H0, n_W0, n_C0, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_H0 -- scalar, height of an input image</span></span><br><span class="line"><span class="string">    n_W0 -- scalar, width of an input image</span></span><br><span class="line"><span class="string">    n_C0 -- scalar, number of channels of the input</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype "float"</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype "float"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    X = tf.placeholder(tf.float32, shape = [<span class="keyword">None</span>,n_H0, n_W0, n_C0 ]) <span class="comment"># 第一个参数是数据类型，第二个是占位符形状</span></span><br><span class="line">    Y = tf.placeholder(tf.float32, shape = [<span class="keyword">None</span>, n_y ])</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><h2 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h2><ul><li>初始化使用 W = tf.get_variable(“W”, [1,2,3,4], initializer = …)</li><li>初始化器使用 tf.contrib.layers.xavier_initializer(seed = 0)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [4, 4, 3, 8]</span></span><br><span class="line"><span class="string">                        W2 : [2, 2, 8, 16]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, W2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                              <span class="comment"># 这句不用管，确保我们的输出和教程一样</span></span><br><span class="line"></span><br><span class="line">    W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>)) </span><br><span class="line">    W2 = tf.get_variable(<span class="string">"W2"</span>, [<span class="number">2</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">16</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>))</span><br><span class="line">   </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>卷积层 （步长 1，same 填充） -&gt; RELU 激活 -&gt; maxpool 池化（8×8过滤器，8×8步长，same 填充） -&gt; 卷积层（步长 1，same 填充）-&gt; RELU 激活 -&gt; maxpool 池化（4×4 过滤器，4×4 步长）-&gt;拍扁 -&gt; 全连接层（输出结点 6 个，不需要调用 softmax，因为在 tensorflow 中，softmax 和代价函数被整合进一个函数中）</p><p>使用的函数为：</p><ul><li>卷积层：<code>tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = &#39;SAME&#39;)</code>  <ul><li>X 为输入，W1 为过滤器，strides 必须为 [1,s,s,1]，s 为步长，padding 类型为 same </li></ul></li><li>maxpool 池化层：<code>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = &#39;SAME&#39;)</code> <ul><li>f 为过滤器尺寸</li></ul></li><li>relu：<code>tf.nn.relu(Z1)</code> <ul><li>Z1 可以是任意形状</li></ul></li><li>拍扁：<code>tf.contrib.layers.flatten(P)</code><ul><li>返回一个 [batch_size,k] 的张量，也就是说会保留样本个数那个维度</li></ul></li><li>全连接层：<code>tf.contrib.layers.fully_connected(F, num_outputs)</code> <ul><li>num_outputs 为输出层结点个数</li><li>注意：tensorflow 会自动帮我们初始化全连接层的参数并在训练模型的时候自动训练，所以不用初始参数</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "W2"</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># CONV2D: stride of 1, padding 'SAME'</span></span><br><span class="line">    Z1 = tf.nn.conv2d(X, W1, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A1 = tf.nn.relu(Z1)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 8x8, sride 8, padding 'SAME'</span></span><br><span class="line">    P1 = tf.nn.max_pool(A1, ksize = [<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># CONV2D: filters W2, stride 1, padding 'SAME'</span></span><br><span class="line">    Z2 = tf.nn.conv2d(P1, W2, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A2 = tf.nn.relu(Z2)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 4x4, stride 4, padding 'SAME'</span></span><br><span class="line">    P2 = tf.nn.max_pool(A2, ksize = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># FLATTEN</span></span><br><span class="line">    P2 = tf.contrib.layers.flatten(P2)</span><br><span class="line">    <span class="comment"># FULLY-CONNECTED without non-linear activation function (not not call softmax).</span></span><br><span class="line">    <span class="comment"># 6 neurons in output layer. Hint: one of the arguments should be "activation_fn=None" </span></span><br><span class="line">    Z3 = tf.contrib.layers.fully_connected(P2, <span class="number">6</span>, activation_fn = <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><h2 id="计算代价函数"><a href="#计算代价函数" class="headerlink" title="计算代价函数"></a>计算代价函数</h2><ul><li>计算所有样例的损失函数：<code>tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y)</code> <ul><li>返回所有样例的损失函数的一个向量</li></ul></li><li>计算损失函数均值（代价函数）：<code>tf.reduce_mean()</code>  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="总模型"><a href="#总模型" class="headerlink" title="总模型"></a>总模型</h2><ul><li>创造占位符</li><li>初始化参数（全连接层不要）</li><li>前向传播</li><li>计算损失</li><li>创建优化器</li><li>小批量梯度下降</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 总模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.009</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          num_epochs = <span class="number">100</span>, minibatch_size = <span class="number">64</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer ConvNet in Tensorflow:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    train_accuracy -- real number, accuracy on the train set (X_train)</span></span><br><span class="line"><span class="string">    test_accuracy -- real number, testing accuracy on the test set (X_test)</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    ops.reset_default_graph()<span class="comment"># 重置默认的计算图</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)       <span class="comment"># 不用管，保持结果一致 (tensorflow seed)</span></span><br><span class="line">    seed = <span class="number">3</span>                    <span class="comment"># 不用管，保持结果一致 (numpy seed)</span></span><br><span class="line">    (m, n_H0, n_W0, n_C0) = X_train.shape             </span><br><span class="line">    n_y = Y_train.shape[<span class="number">1</span>]                            </span><br><span class="line">    costs = []                  <span class="comment"># 记录代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据输入数据形状创建占位符</span></span><br><span class="line">    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播: 在 tensorflow 计算图中构建前向传播</span></span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 代价函数计算: 往计算图中增加代价函数</span></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播: 定义优化器. Use an AdamOptimizer that minimizes the cost.</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化全局参数</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 开始会话 sess 计算计算图</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 运行全局初始化</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 进行训练循环</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">            minibatch_cost = <span class="number">0.</span></span><br><span class="line">            num_minibatches = int(m / minibatch_size) <span class="comment"># 计算小批量的个数</span></span><br><span class="line">            seed = seed + <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)<span class="comment"># 划分小批量</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 选择小批量</span></span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line">                <span class="comment"># 运行会话以执行 optimizer and the cost, the feedict should contain a minibatch for (X,Y).</span></span><br><span class="line">                _ , temp_cost= sess.run([optimizer,cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)               </span><br><span class="line">                </span><br><span class="line">                minibatch_cost += temp_cost / num_minibatches</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每五次迭代打印一次 cost</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, minibatch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(minibatch_cost)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># plot the cost</span></span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the correct predictions</span></span><br><span class="line">        predict_op = tf.argmax(Z3, <span class="number">1</span>)<span class="comment"># 找到 Z3 中最大值的索引号，1 为按行取</span></span><br><span class="line">        correct_prediction = tf.equal(predict_op, tf.argmax(Y, <span class="number">1</span>))<span class="comment"># 相等则为 1 否则为 0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate accuracy on the test set</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">        print(accuracy)</span><br><span class="line">        train_accuracy = accuracy.eval(&#123;X: X_train, Y: Y_train&#125;)<span class="comment"># accuracy.eval() 相当于 sess.run(accuracy)</span></span><br><span class="line">        test_accuracy = accuracy.eval(&#123;X: X_test, Y: Y_test&#125;)</span><br><span class="line">        print(<span class="string">"Train Accuracy:"</span>, train_accuracy)</span><br><span class="line">        print(<span class="string">"Test Accuracy:"</span>, test_accuracy)                              </span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> train_accuracy, test_accuracy, parameters</span><br></pre></td></tr></table></figure><h2 id="Tensorflow-使用感想"><a href="#Tensorflow-使用感想" class="headerlink" title="Tensorflow 使用感想"></a>Tensorflow 使用感想</h2><p>在 tensorflow 中，计算图中的任何值都不会被计算出来，除非你使用 <code>sess.run(feed_dict)</code>或者 <code>tensor.eval(feed_dict)</code>，在求值的时候，从要求的值往前推，把这一条线上所有需要的 placeholder 找出来然后填入 feed_dict。</p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 4 week 1）—— 卷积神经网络基础</title>
      <link href="/2018/09/28/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w1/"/>
      <url>/2018/09/28/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c4w1/</url>
      <content type="html"><![CDATA[<h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><h3 id="计算机视觉的问题"><a href="#计算机视觉的问题" class="headerlink" title="计算机视觉的问题"></a>计算机视觉的问题</h3><ul><li><p>图像分类：例如猫分类器</p></li><li><p>物体检测：例如自动驾驶不仅需要识别出图片中是否有车，还要计算出这张图片中汽车的位置</p></li><li><p>神经风格转换：如下图</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_1.png" alt=""></p></li></ul><a id="more"></a><h3 id="计算机视觉的挑战"><a href="#计算机视觉的挑战" class="headerlink" title="计算机视觉的挑战"></a>计算机视觉的挑战</h3><p>图片的像素可以任意大，输入维度可以任意大，会导致：</p><ul><li>很难获得足够数据避免过拟合</li><li>对计算量和内存的需求很大</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_2.png" alt=""></p><h2 id="卷积运算-Convolutional-operation"><a href="#卷积运算-Convolutional-operation" class="headerlink" title="卷积运算 (Convolutional operation)"></a>卷积运算 (Convolutional operation)</h2><h3 id="引例"><a href="#引例" class="headerlink" title="引例"></a>引例</h3><p>假设要检测下图的垂直边缘和水平边缘：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_3.png" alt=""></p><p>检测垂直边缘可以使用如下方法：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_4.png" alt=""></p><ul><li>左边是某个图像</li><li>中间是一个 3×3 的<strong>过滤器 (fliter)</strong>，是一个矩阵，有时也称为<strong>核 (kernel)</strong> </li><li><strong>*</strong> 符号表示<strong>卷积运算</strong> </li><li>将蓝框中的值逐元素乘以过滤器矩阵的值然后相加得到第一个结果 -5，下面的以此类推</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_5.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_6.png" alt=""></p><p>中间的过滤器就是一个垂直边缘检测器，那么它是如何检测边缘的？</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_7.png" alt=""></p><ul><li>边缘左侧像素值大，更明亮，右侧像素值小，更暗，最后得到的结果是更亮的区域在正中间，与检测出的垂直边缘相对应</li><li>检测出的边缘看起来很厚，是因为使用 6×6 的小图像，如果使用大图像就不会发生这种比例失调</li><li>过滤器告诉我们一个信息：某个垂直边缘是 3×3 的区域，左边有亮像素，右边有暗像素，而不在意中间有什么，这种区域被认为有垂直边缘</li></ul><h3 id="更多边缘检测例子"><a href="#更多边缘检测例子" class="headerlink" title="更多边缘检测例子"></a>更多边缘检测例子</h3><p>还有许多种精心设计的过滤器，比如水平边缘过滤器：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_8.png" alt=""></p><p>还有 sobel filter：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_9.png" alt=""></p><p>还有 scharr filter：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_10.png" alt=""></p><p>随着深度学习的发展我们发现，如果你想要检测一些复杂图片的边界，可能并不需要计算机视觉的研究人员挑选出这 9 个矩阵元素。你可以把矩阵里的这 9 个元素当做参数，通过反向传播来学习得到他们的数值，它能学到比前述这些人为定义的过滤器更加善于捕捉你的数据的统计学特征的过滤器。除了垂直和水平边界，同样能够学习去检测 45 度的边界 70 度 或 73 度，无论什么角度。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_11.png" alt=""></p><h3 id="填充-paddling"><a href="#填充-paddling" class="headerlink" title="填充 (paddling)"></a>填充 (paddling)</h3><h4 id="卷积运算的一些问题"><a href="#卷积运算的一些问题" class="headerlink" title="卷积运算的一些问题"></a>卷积运算的一些问题</h4><p>如果我们用 3×3 的过滤器去卷积 6×6 的图像，最后得到的是一个 4×4 的图像，这样会出现两个问题：</p><ul><li>输出的图像逐渐缩小，如果网络的层数很深，那么许多层之后会得到一个非常小的图片</li><li>角落上的像素值只会在输出中被使用一次，而中间的像素会重叠许多次，导致丢失了图片靠近边界上的信息</li></ul><h4 id="怎麽办"><a href="#怎麽办" class="headerlink" title="怎麽办"></a>怎麽办</h4><p>为了消除这两个问题，可以在卷积前用一个额外的边缘 (border) 填充图片，例如我们可以在 6×6 的图片边缘用一个 1 像素大小的额外边缘，那么这个图片就变成 8×8，和 3×3 的过滤器卷积之后得到一个 6×6 的图片，和原来尺寸一样，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_12.png" alt=""></p><ul><li>填充的边缘厚度为 p，上图中 p = 1</li></ul><h4 id="到底填充多少？——两种卷积"><a href="#到底填充多少？——两种卷积" class="headerlink" title="到底填充多少？——两种卷积"></a>到底填充多少？——两种卷积</h4><ol><li><p>valid 卷积 —— 没有填充</p><script type="math/tex; mode=display">n \times n \quad * \quad f \times f \rightarrow (n-f+1) \times (n-f+1)\\6 \times 6 \quad * \quad 3 \times 3 \rightarrow 4 \times 4</script></li><li><p>same 卷积 —— 填充后使得输入大小等于输出大小</p><script type="math/tex; mode=display">(n+2p) \times (n+2p) \quad * \quad f \times f \rightarrow (n+2p-f+1) \times (n+2p-f+1)\\\Rightarrow \ \ n+2p-f+1=n\\\Rightarrow \ \ p=\frac{f-1}{2}</script><ul><li>其中 p 为填充的厚度，f 为过滤器的边长</li><li>当 f 为奇数时，p 按照上面公式取值即可以使得输入输出相同</li><li>一般实际中 f 都是使用奇数，有如下两个原因：<ul><li>如果 f 是偶数，则需要一些不对称填充</li><li>奇数过滤器可以有个中心点，称之为中心像素，可以描绘过滤器的位置</li></ul></li></ul></li></ol><h3 id="带步长的-strided-卷积"><a href="#带步长的-strided-卷积" class="headerlink" title="带步长的 (strided) 卷积"></a>带步长的 (strided) 卷积</h3><p>在卷积时过滤器移动的步长不为 1 称为带步长的卷积。如果步长 stride = 2 结果如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_13.png" alt=""></p><p>加入我们有：</p><ul><li>n×n 的图像</li><li>f×f 的过滤器</li><li>填充 padding 为 p</li><li>步长 strides 为 s </li></ul><p>如果将两者卷积，输出为：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_14.png" alt=""></p><ul><li>其中 [ ] 为向下取整，也就是小于它的最大整数，[z] = floor(z)</li></ul><h3 id="一个提法的纠正"><a href="#一个提法的纠正" class="headerlink" title="一个提法的纠正"></a>一个提法的纠正</h3><p>实际上，在正式数学课本中，卷积操作前还需要一个额外的“翻转”操作，即将原来的过滤器先水平翻转再竖直翻转，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_15.png" alt=""></p><p>而深度学习中的“卷积”操作，将翻转操作取消了，数学上应该叫“交叉相关”，但是大多数深度学习文献都叫它卷积操作。</p><h3 id="对三维立方体的卷积"><a href="#对三维立方体的卷积" class="headerlink" title="对三维立方体的卷积"></a>对三维立方体的卷积</h3><p>假如我们的图像是 RGB 图像，那么它不是一个二维的灰度图像，而是一个 6×6×3 的三维图像，其中 3 为图像的<strong>通道数</strong>，这种图像如何卷积呢？</p><h4 id="单过滤器"><a href="#单过滤器" class="headerlink" title="单过滤器"></a>单过滤器</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_17.png" alt=""></p><ul><li>3 为图像的通道数，图像的通道数应该和过滤器的通道数一致</li><li>将每一层的过滤器的 9 个数字与对应通道的图像像素值相乘，最后把这所有的 27 个数字相加得到第一个输出，其他的以此类推</li></ul><h4 id="多过滤器"><a href="#多过滤器" class="headerlink" title="多过滤器"></a>多过滤器</h4><p>如果我们希望检测多个边缘，那么可以使用多个过滤器。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_18.png" alt=""></p><p>总结：</p><script type="math/tex; mode=display">n \times n \times n_c \quad * \quad f \times f \times n_c \ \rightarrow (n-f+1) \times (n-f+1) \times n_c'</script><ul><li><p>其中 $n_c$ 为图像通道数，图像通道数应该和过滤器一致</p><ul><li>有时候 $n_c$ 称之为三维立方体的深度</li></ul></li><li><p>其中 $n_c’$ 为过滤器的数量，即检测的特征数量</p></li></ul><p>现在我们不仅可以直接处理拥有 3 个通道的 RGB 图片，而且更重要的是，可以检测两个特征，比如垂直、水平边缘，或者 10 个，128 个，甚至几百个不同的特征。</p><h2 id="单层卷积神经网络"><a href="#单层卷积神经网络" class="headerlink" title="单层卷积神经网络"></a>单层卷积神经网络</h2><h3 id="引例-1"><a href="#引例-1" class="headerlink" title="引例"></a>引例</h3><p>下面是一个卷积层的计算过程。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_20.png" alt=""></p><ul><li>假设输入图像为一个 6×6×3 的 $a^{[0]}$，那么输出为 4×4×2 的 $a^{[1]}$ </li><li>其中过滤器相当于参数 $w^{[1]}$，第二个蓝框里相当于 $z^{[1]}$，经过激活函数 Relu 之后变为 $a^{[1]}$</li></ul><h3 id="一层卷积层的参数"><a href="#一层卷积层的参数" class="headerlink" title="一层卷积层的参数"></a>一层卷积层的参数</h3><p>假设你有 10 个 3×3×3 的过滤器在一层神经网络中，那么这一层有多少个参数？</p><p>parameters = ( 3×3×3 +1 ) × 10 = 280 个</p><p>这是一个很好的特性：不管输入的图像有多大，比方 1000 x 1000 或者 5000 x 5000，这里的参数个数不变. 依然是 280 个。因此 用这 10 个过滤器来检测一个图片的不同的特征，比方垂直边缘线，水平边缘线或者其他不同的特征，不管图片多大，所用的参数个数都是一样的。</p><h3 id="标记符号总结"><a href="#标记符号总结" class="headerlink" title="标记符号总结"></a>标记符号总结</h3><p>如果第 $l$ 层是一个卷积层：</p><ul><li>过滤器尺寸：$f^{[l]}$</li><li>填充厚度：$p^{[l]}$</li><li>卷积步长：$s^{[l]}$</li><li>过滤器数量：$n_c^{[l]}$</li><li>输入：$n_H^{[l-1]} \times n_W^{[l-1]} \times n_c^{[l-1]}$ <ul><li>其中 H 代表高度，W 代表宽度，C 代表通道 channel</li></ul></li><li>输出：$n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$ <ul><li>$n_H^{[l]}=[\frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]$，$n_W^{[l]}=[\frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1]$ <ul><li>其中 $[ \ ]$ 为向下取整</li></ul></li></ul></li><li>每个过滤器为：$f^{[l]} \times f^{[l]} \times n_c^{[l-1]}$ (注意不是 $n_c^{[l]}$) </li><li>这一层的激活值：$a^{[l]} \rightarrow n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$<ul><li>向量化激活值：$ A^{[l]} \rightarrow m \times n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]}$ </li></ul></li><li>权重：$f^{[l]} \times f^{[l]} \times n_c^{[l-1]} \times n_c^{[l]}$<ul><li>其中 $n_c^{[l]}$ 为 $l$ 层的过滤器数量</li></ul></li><li>偏差：$1 \times 1 \times 1 \times n_c^{[l]}$ </li></ul><h2 id="深层卷积神经网络"><a href="#深层卷积神经网络" class="headerlink" title="深层卷积神经网络"></a>深层卷积神经网络</h2><h3 id="引例-2"><a href="#引例-2" class="headerlink" title="引例"></a>引例</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_21.png" alt=""></p><ul><li>最后我们得到一个 7×7×40 的激活值，一共 1960 个数展开为一个列向量，然后输入到一个逻辑回归或者 softmax 单元得到预测值</li></ul><h3 id="卷积网络中的层类型"><a href="#卷积网络中的层类型" class="headerlink" title="卷积网络中的层类型"></a>卷积网络中的层类型</h3><ul><li>卷积层 Convolution Layers (简称 Conv)</li><li>池化层 Pooling Layers (简称 Pool)</li><li>全连接层 Fully connected Layers (简称 FC)</li></ul><h3 id="池化层-Pooling-Layers"><a href="#池化层-Pooling-Layers" class="headerlink" title="池化层 (Pooling Layers)"></a>池化层 (Pooling Layers)</h3><ul><li><p>Max pooling 最大值采样</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_22.png" alt=""></p><ul><li>Max pooling 和卷积操作基本一样，只是最后输出的是滤波器范围内的最大值</li><li>如果你把这个 4x4 的区域看作某个特征的集合，那么一个大的数字就意味着它或许检测到了一个特定的特征，所以左侧上方的四分之一区域有这样的特征，它或许是一个垂直的边沿 亦或一个更高或更弱，但是右侧上方的四分之一区域没有这个特征。所以 max pooling 做的是检测到所有地方的特征，四个特征中的一个被保留在 max pooling 的输出中。</li></ul></li><li><p>Average pooling 平均值采样</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/img/dl_4.1_23.png" alt=""></p><ul><li>平均值采样输出的是滤波器范围内所有值的平均值</li><li>不如 max pooling 常用</li></ul></li></ul><p>总结：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_24.png" alt=""></p><p>池化层没有任何参数要学习！是确定的函数！</p><h3 id="识别手写数字的例子"><a href="#识别手写数字的例子" class="headerlink" title="识别手写数字的例子"></a>识别手写数字的例子</h3><p>下面是一个用来识别手写数字的神经网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_25.png" alt=""></p><ul><li>conv 是 卷积层，pool 是池化层，FC 是全连接层</li><li>由于池化层没有权重和参数，所以不把它算作一个独立的层，而是视 conv1 和 pool1 为 layer 1，但是在某些文献中把卷积层和池化层视为两个独立的层</li></ul><p>总结：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_26.png" alt=""></p><ul><li>池化层没有参数</li><li>卷积层趋向于拥有越来越少的参数</li></ul><h2 id="为什么卷积如此有用"><a href="#为什么卷积如此有用" class="headerlink" title="为什么卷积如此有用"></a>为什么卷积如此有用</h2><ul><li>参数共享： 在特征检测器中（例如垂直边缘检测）对于图像的一部分是有用的，那么对于另一部分可能也是有用的。</li><li>连接的稀疏性：在每一层中，每个输出值只依靠一小部分输出值。也就是说每个输出值都只依赖于滤波器覆盖到的那几个数字，而剩下的像素值对它没有影响。</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>坎坷开学路</title>
      <link href="/2018/09/26/%E5%9D%8E%E5%9D%B7%E9%80%89%E5%AF%BC%E5%B8%88/"/>
      <url>/2018/09/26/%E5%9D%8E%E5%9D%B7%E9%80%89%E5%AF%BC%E5%B8%88/</url>
      <content type="html"><![CDATA[<p>今天是9月26号，在哈工大深圳校区的荔园写下这篇日记，距离开学已经过去了十天左右的时间，本来以为可以开开心心入学，没想到碰到了许多倒霉事儿：</p><a id="more"></a><ul><li>首先是开学延迟十五天，这导致了后面许多 schedule 非常紧张，开学第三天就选课上课。</li><li>开学前寄来了四个行李包裹，其中一个死活找不到，最后在另外一个快递点找到，寄来的台式电脑里面的独立显卡死活找不到，还以为是快递员偷了，结果是人家帮我另外放在一个盒子里了。</li><li>开学第二天就遭遇所谓“建国以来第三大的台风”——“山竹”，导致各种活动取消，不过这个台风也不过如此～</li><li>the last but not the least :），最倒霉的就是选导师太晚导致最后被学院分了一个没人选的导师：）</li></ul><p>选导师这件事情有必要特别来记录一下，仔细想来确实反映了自己许多缺点。</p><p>其实找导师我行动得并不晚，网上很多人说初试成绩出来后就要联系，于是考研初试成绩一出我就根据网上说的给好几个发了邮件，但都被委婉拒绝。在群里得知机械专业开学有个所谓的“双选会”，在这之前老师都不会理你，于是我就跟大多数同学一样，暑假两个月把选导师的事情抛在脑后。</p><p>其实每个学院那么多导师，在同学的口碑中都有个排序，有些导师方向好人品好自然就很抢手，临近开学，偶然得知许多同学已经被一些抢手的导师收了，这个时候我有点慌了，但还是信心满满，给最热门的一个导师 J 发了邮件，他第二天就回我了，让我给他发封简历，但是之后再也没了下文，我还傻乎乎地等待这个 J 回信，觉得还有希望，准备提前几天去找 J，并自信地认为一定能成功，从而错过了给其他导师发邮件的黄金时间。</p><p>提前两天去的学校，还以为自己去的很早：），其实有些人提前一个月就来了。去找了那个导师 J，他说他不确定能不能收我而且只有一个名额了，我这才知道他让我给他发简历只是说说而已：），为了保险，又去找了其他几个比较热门的导师，结果一个个都不在办公室，在这抢导师最激烈的几天，我还想着，发短信打电话会不会不礼貌啊，发邮件会不会看不到啊，会不会有更好的选择啊，也许 J 老师后面会增加名额啊…… 犹犹豫豫的，于是这段时间又被我错过了。</p><p>到了双选会那天，其实大部分热门的导师已经没有名额了，只是上台介绍一下自己的项目走个过场，但是还是有蛮多导师有名额，可以第二天去面试，这个时候我又犹豫了，因为我对他们的研究方向不太感兴趣（殊不知在机械学院根本没有搞 AI 的老师，要想转行搞 AI 只能靠自己，我的那种犹豫根本没意义）。当时让我印象最深的就是学院的首席学术顾问 clarence 教授，加拿大的两院院士，学术水平非常高，当时室友让我选他，我又担心自己语言问题，跟他无法交流，又纠结了很久。双选会结束当晚，所有的导师办公室都排起了面试的长队，我又怂了，觉得自己竞争不过别人，于是随便找了一个没人选的导师 L 签了双选表（之所以选他还因为当时找他时，他拉着我谈了<strong>两个小时</strong>，说自己如何如何尊重学生，我一冲动就签了）。第二天我才发现，所有人都叫我别选他，说学长口中的口碑不好，但是也说不出哪里不好……这个时候我心里有点发怵，他口碑这么差，是不是该换个人？</p><p>又过了一天，我室友告诉我他一个朋友选了 clarence 教授，教授还给了她一个项目介绍书，我看了看，大概是用神经网络识别流水线上的苹果进行分拣之类的，也就是神经网络在实际中的应用，这不就是我想做的项目吗！于是我下定决心换导师，给教授发了邮件，表明想做这个项目，教授很快就回了我，让我发封英文简历，我赶紧急匆匆地做了个英文简历发给他，然后他让我待会儿去他办公室找他，我心想有戏了！P.S.教授的回信，开头必加 “dear student:”，在许多连邮件都不回的导师中真是一股清流。</p><p>来到他办公室，他给我介绍了另一个项目：模拟油液管道泄漏，拍照片，提取特征，使用 cnn 训练数据然后用来判断现实中管道是否泄漏，我表示非常愿意做这个项目！但是教授说，现在有另外两个人也对这个项目感兴趣，他向学院申请是否能将名额拓展至三个，如果不能就给我做，让我先回去考虑一下，晚上给他答复。我心想这不就成了嘛！于是开开心心上课去了：）。</p><p>上完第一节课到了三点半，我心想要从 L 导师那里把双选表拿回来啊！给自己做了很久思想工作才敢去找他了，跟他说自己看上了另外一个导师的项目，想换导师，他挽留了我几次我都拒绝了，场面极其尴尬，但最后还是把表还给我了，走前还数落我确定了导师还背后联系别的导师……我心想管他呢，反正已经找好下家了：）。</p><p>拿到表准备去找 clarence 教授签字，突然看到手机上来自教授的两封未读邮件，看完内容我顿时石化……他让我在三点之前给他发封邮件确认是否愿意做这个项目，如果没回就给另外两个学生做了，而我一下午都没看手机：），飞奔到教授的办公室，他告诉我没办法了，他已经上报学院了，他没办法再要我了。</p><p>走出他的办公室，我开始联系其他导师，一个又一个地打电话，一个又一个办公室地跑，最后的结果是：所有的导师名额已经满了，我只能灰溜溜地回去找 L 导师……这时我走在学校的广场上，看着高耸的主楼，茫然无措，像个无助的孩子，感觉我这些天的所有折腾，既狼狈又滑稽，一次又一次错过所有找导师的机会，现在连之前根本不想去的导师都去不了咯，极度的绝望让我突然生出了退学的想法，想一切重新来过……</p><p>总之最后我还是回到了被我得罪了的 L 导师那里，四个没找到导师的同学被强制分到了他名下，也就是说没一个学生主动选他……我还向 L 导师道了个歉，希望以后不要被穿小鞋……</p><p>现在看来，这件事可能没我当时想象的那么严重，但是的确反映了我许多毛病：</p><ul><li><p>完美主义太严重</p><ul><li>这导致了选导师的时候拖延，总想在了解所有导师情况之后再选。</li></ul></li><li><p>好高骛远，过分自信</p><ul><li><p>总想找到最好的导师，希望他的研究方向和自己的完美契合，还希望人品好，学术水平高，而且觉得自己一定会被接收，但事实恰恰相反，自己跟大神相比相差太远，根本没有找到好导师的砝码。</p></li><li><p>也许以后找工作也是一样，只要自己够厉害了，不怕没人需要。</p></li></ul></li><li><p>做事情想太多，太纠结，不自信，缺乏勇气</p><ul><li>本来在双选会第二天就可以去找 calrence 教授，但是总纠结自己到底够不够格当他学生，语言交流有没有问题，以后如果要去加拿大怎么办等等问题，等到自己鼓起勇气的时候已经晚了。</li></ul></li><li><p>有时候太冲动</p><ul><li>不应该在当时直接签了 L 导师，也不应该在下家还没确定好就贸然把表拿回来。</li></ul></li><li><p>人云亦云，容易被他人意见左右</p><ul><li>群里有谁说哪个老师好，就跟着一窝蜂找他，大家说哪个老师不好，自己也开始先入为主。</li></ul></li><li><p>太极端，情绪化，把事情想的太严重</p><ul><li>其实很多事情没我想的那么严重，当我知道自己只能回到L导师那里之后，甚至萌生了退学重来的想法，其实现在看来，都是小事儿，但是当时居然情绪波动那么大。</li></ul></li></ul><p>通过这件事情反省一下自己，希望自己能学到一些教训。</p>]]></content>
      
      <categories>
          
          <category> 个人随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 日记 </tag>
            
            <tag> 研究生生活 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 3 week 2）—— 机器学习策略（二）</title>
      <link href="/2018/09/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c3w2/"/>
      <url>/2018/09/10/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c3w2/</url>
      <content type="html"><![CDATA[<h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p>如果你想得到一个训练算法来做人类可以做的任务，但是训练的算法还没有达到人类的效果，你就需要手动地检查算法中的错误，来知道你下一步该做什么，这个过程叫做错误分析。</p><h3 id="如何进行错误分析"><a href="#如何进行错误分析" class="headerlink" title="如何进行错误分析"></a>如何进行错误分析</h3><p>假设训练一个分类猫的算法，准确率为 90%，错误率为 10%，我们对分错的样本进行检查，发现分错的样本中有一些把狗认成了猫，这时是否应该将重心转移到使分类器对狗的表现更好？</p><p>错误分析：</p><ul><li>拿到约 <strong>100</strong> 张分类错误的开发集图片并进行手动检测<ul><li>一般不需要检测上万张图片，几百张就足够了！</li></ul></li><li>输出错误的图片里多少张狗</li></ul><p>假设只有 5 张，那么在错误图片中只有 5% 的狗，如果在狗的问题上花了大量的时间，那么就算是最好的情况，也<strong>最多</strong>只是把错误率从 10% 降到 9.5%，所以并不值得。但是假设 100 张错误图片里有 50 张狗的图片，那么可以确定很值得将时间花在狗身上，因为错误率<strong>最多</strong>可能从 10% 降到 5%.</p><a id="more"></a><p>有时候我们可以并行考虑好几个 idea 是否值得付出努力。</p><p>假设有下面几个提高猫咪分类器性能的 idea：</p><ul><li>提高对狗的识别</li><li>提高对其他猫科动物的识别</li><li>提高模型对模糊图像的性能</li></ul><p>如何评估这几个方案，可以列一张表，记录每个分类错误的图片分属于哪一个错误类别，然后计算百分比：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_1.png" alt=""></p><p>通过上面的分析我们可以知道现在应该专注于提高对模糊的图片（43%）或者猫科动物图片（61%）的准确度，但仅仅是一个参考，因为这取决于获得这些数据<strong>容不容易</strong>，假如要获得猫科动物的图片需要大量的时间，那么这一方面就可以先放放。</p><h3 id="清除错误标记（标签）"><a href="#清除错误标记（标签）" class="headerlink" title="清除错误标记（标签）"></a>清除错误标记（标签）</h3><p>当你检查数据集时发现一些输出标签 Y 标记错误，需不需要花时间修正这些标签。</p><h4 id="对于训练集"><a href="#对于训练集" class="headerlink" title="对于训练集"></a>对于训练集</h4><ul><li>深度学习算法在训练集上对于一些随机错误非常稳健，也就是说基本不需要管训练集上的一些随机的错误标记</li><li>但是注意算法对系统误差比较敏感，如果标记员一直把白色的狗标记为猫，那么训练出来的分类器也会学着这么做</li></ul><h4 id="对于开发-测试集"><a href="#对于开发-测试集" class="headerlink" title="对于开发/测试集"></a>对于开发/测试集</h4><p>推荐的方法是在错误分析时增加一列“因为标签错误而分类错误”的统计：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_2.png" alt=""></p><p>如上如图，加入标签错误是 6%，那么我们值得花时间去纠正这 6% 的样本的标签吗？</p><p>假设算法的总错误率是 10% 或 2%，考虑这两种情况下标签错误对评估的影响：</p><div class="table-container"><table><thead><tr><th>算法的总错误率</th><th>10%</th><th>2%</th></tr></thead><tbody><tr><td>由标签错误引起的错误率</td><td>0.6%</td><td>0.6%</td></tr><tr><td>由其他因素引起的错误率</td><td>9.4%</td><td>1.4%</td></tr></tbody></table></div><p>当总错误率为 10% 时，并不值得花时间去纠正这些错误的标签，因为即使纠正了也只提高 0.6% 的准确度，但是当总错误率为 2% 时，如果我们不纠正标签，那么他们引起的错误就占了一大半，这时纠正开发集中的标签错误就很有必要了。</p><h3 id="纠正开发集-测试集的原则"><a href="#纠正开发集-测试集的原则" class="headerlink" title="纠正开发集/测试集的原则"></a>纠正开发集/测试集的原则</h3><ul><li>对开发集和测试集进行一样的处理，确保他们来自相同的分布，当你纠正开发集中的一些问题，将这个过程也运用到测试集中</li><li>考虑检查你算法预测正确的和预测错误的样本</li><li>训练集 和 开发/测试集可以服从稍微不同的分布</li></ul><h3 id="建立第一个模型的建议"><a href="#建立第一个模型的建议" class="headerlink" title="建立第一个模型的建议"></a>建立第一个模型的建议</h3><ul><li>设立开发/测试集和评估指标</li><li><strong>快速地建立一个不那么复杂的初始模型</strong></li><li>使用偏差/方差分析/错误分析进行模型的迭代</li></ul><h2 id="训练集-和-开发-测试集-的不匹配"><a href="#训练集-和-开发-测试集-的不匹配" class="headerlink" title="训练集 和 开发/测试集 的不匹配"></a>训练集 和 开发/测试集 的不匹配</h2><p>深度学习算法都希望有大量的训练数据，这导致很多团队将能找到的任何数据都塞进训练集，只为有更多的训练数据，即使很多这种数据来自于与开发集和测试集不同的分布。因此在深度学习时代，越来越多的团队正在使用的，训练数据并非来自与开发集和测试集相同的分布。</p><h3 id="在不同的分布上进行训练和测试"><a href="#在不同的分布上进行训练和测试" class="headerlink" title="在不同的分布上进行训练和测试"></a>在不同的分布上进行训练和测试</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_3.png" alt=""></p><p>要训练一个猫咪分类器，目前手上有两种数据，一种是从网上爬下来的十分精美的图片 200000 个，另一种是用户上传的质量很差的图片 10000 张，这两种数据的分布差异非常大，但是我们更关心的是右边用户上传的数据，是我们的“靶子”，现在有两种数据集分配方案。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_4.png" alt=""></p><p>第一种是将所有图片混合洗牌，其中 205000 张作为训练集，剩下的 5000 张在开发/测试集之间平分。这种分法有一个很大的缺点！按照这种分法，开发集和测试集中按照比例可以算出用户上传的照片只有约 119 张，大部分都是网上爬的图片，并没有很好地反映我们所关心的数据（用户的图片），也就是说靶子放错了！</p><p>而正确的第二种分法是：将所有网上爬的 200000 张图片<strong>加上用户的 5000 张作为训练集</strong>，而剩下的用户的 5000 张平分为开发集和测试集，这样一来，你的开发集就瞄准了正确的目标。但缺点就是使得训练集分布和开发集分布有些不同，但是长期来说性能更好。</p><p>总结：</p><ul><li>在某些情况下，可以允许训练集和开发/测试集来自不同的分布，以可以获得更大的训练集</li><li>但是注意：训练集中<strong>也需要</strong>包含足够的开发集分布数据，来避免数据不匹配问题！</li></ul><h3 id="数据分布不匹配时的偏差-方差分析方法"><a href="#数据分布不匹配时的偏差-方差分析方法" class="headerlink" title="数据分布不匹配时的偏差/方差分析方法"></a>数据分布不匹配时的偏差/方差分析方法</h3><p> 当你的训练集、开发集、测试集来自不同的分布时，偏差和方差的分析方法也会相应变化。</p><h4 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h4><p>对于一个猫分类器，假设人类误差是 0，且训练集和开发集数据来自不同分布，训练集误差为 1%，开发集误差为 10%，那么它们之间 9% 的差距到底是因为 (1)模型泛化能力不足导致方差太大，还是因为 (2)训练集数据分布不同？</p><h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p>为了找出原因，我们定义一个新的数据集：训练-开发集 (training-dev set).</p><p>训练-开发集：将所有训练集数据洗牌，取出一小块作为训练-开发集，它的分布与训练集相同。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_5.png" alt=""></p><blockquote><p>分布相同：</p><ul><li>训练集 $\leftrightarrow$ 训练-开发集</li><li>开发集 $\leftrightarrow$ 测试集</li></ul></blockquote><p>划分好之后，计算出模型在如下各个数据集上的误差如下表：</p><div class="table-container"><table><thead><tr><th style="text-align:left">例子</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th></tr></thead><tbody><tr><td style="text-align:left">训练集误差</td><td style="text-align:center">1%</td><td style="text-align:center">1%</td><td style="text-align:center">10%</td><td style="text-align:center">10%</td></tr><tr><td style="text-align:left">训练-开发集误差</td><td style="text-align:center">9%</td><td style="text-align:center">1.5%</td><td style="text-align:center">11%</td><td style="text-align:center">11%</td></tr><tr><td style="text-align:left">开发集误差</td><td style="text-align:center">10%</td><td style="text-align:center">10%</td><td style="text-align:center">12%</td><td style="text-align:center">20%</td></tr></tbody></table></div><ul><li>例子1：训练集误差 1%，训练-开发集误差 9%，两者同分布都差距这么大，说明模型泛化能力不好，属于高方差问题。</li><li>例子2：在同样是在没训练过的数据集上预测，与训练集分布一样的训练-开发集误差很小，而与训练集分布不同的开发集（你所关心的）却误差很大，算法未在你所关心的分布上训练的很好，这就是<strong>数据不匹配</strong>问题 (mismatch problem)。</li><li>例子3：人类误差 0，训练集误差却高达 10%，这是明显是高偏差问题。</li><li>例子4：10% 说明高偏差；11%～20% 说明数据不匹配程度相当大。</li></ul><h4 id="train-sets-和-dev-test-sets-不匹配问题的通用的解决方案"><a href="#train-sets-和-dev-test-sets-不匹配问题的通用的解决方案" class="headerlink" title="train sets 和 dev/test sets 不匹配问题的通用的解决方案"></a>train sets 和 dev/test sets 不匹配问题的通用的解决方案</h4><p>一个模型可能的误差类型：</p><ul><li><p><strong>人类误差</strong> (训练集分布)  $&lt;\overset{\text{反映了不同分布的识别难度大小}}{===========}&gt;$ 人类误差 (开发集分布) </p><p>$\Updownarrow$ 反映可避免偏差</p></li><li><p><strong>训练集误差</strong> (训练集分布)</p><p> $\Updownarrow$ 反映方差情况</p></li><li><p><strong>训练-开发集误差</strong> (训练集分布)</p><p> $\Updownarrow$ 反映（训练集和开发集）数据不匹配情况</p></li><li><p><strong>开发集误差</strong> (开发/测试集分布)</p><p> $\Updownarrow$ 反映算法对开发集的过拟合情况，因为如果这个差距太大，则也许将神经网络</p><p> $\Updownarrow$ 调的太偏向开发集了，此时需要一个更大的开发集</p></li><li><p><strong>测试集误差</strong> (开发/测试集分布)</p></li></ul><h3 id="如何改善数据不匹配问题"><a href="#如何改善数据不匹配问题" class="headerlink" title="如何改善数据不匹配问题"></a>如何改善数据不匹配问题</h3><ul><li>人工分析误差，试着理解训练集与开发/测试集之间的差异<ul><li>举个例子，在识别车载语音识别时，可能开发集中很多车辆噪音，而训练集没有，这是由实际情况决定的</li></ul></li><li>想方设法使得训练集与之更相似：可以收集更多与开发/测试集相似的数据加入训练集，或者进行人工合成数据加入训练集，只要合成的数据让人类觉得很真实，那么就可以骗过算法<ul><li>比如模拟车载噪音数据，将训练集中干净的音频加上一段噪音合成出噪音下的说话声。但是注意：一小段<strong>重复</strong>用来合成的汽车噪音会使得学习算法对这段声音产生过拟合</li><li>再比如无人驾驶中的汽车识别，可以通过三维建模画出我们需要的汽车样子。但是注意：建模所用的汽车模型非常有限，加入只有 20 种，那么神经网络很可能对这 20 种汽车产生过拟合</li></ul></li></ul><h2 id="迁移学习-Transfer-learning"><a href="#迁移学习-Transfer-learning" class="headerlink" title="迁移学习 (Transfer learning)"></a>迁移学习 (Transfer learning)</h2><h3 id="什么叫迁移学习"><a href="#什么叫迁移学习" class="headerlink" title="什么叫迁移学习"></a>什么叫迁移学习</h3><p>深度学习中最有力的方法之一，是有时你可以把在一个任务中神经网络学习到的东西，应用到另一个任务中去。比如，你可以让神经网络学习去识别物体，比如猫，然后用学习到的（一部分）知识来帮助你更好地识别X射线的结果。这就是所谓的迁移学习。</p><h3 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h3><p>假如我们我们用猫狗的图片训练了一个如下的图片识别的神经网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_6.png" alt=""></p><p>如果我们想把这个模型应用到另一个场景，比如 x 光片的诊断，实现方法是移除这个神经网络的最后一层和其相关的权重，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_7.png" alt=""></p><p>要实现迁移学习，要做的是：</p><ul><li>将数据集换成 x 光诊断的图片</li><li>随机初始化新的最后一层的权重 W 和 偏差 b</li><li>重新训练该网络<ul><li>如果数据量小：可以只训练最后一层的参数，保留其他的参数</li><li>如果数据量大：可以训练所有层</li></ul></li></ul><p><strong>预训练 (pre training)</strong>：我们把猫狗图片训练的模型用到其他地方，那么用猫狗图片对模型的训练就称之为预训练。预训练对模型的权重进行了预初始化，其他的训练是在预初始化权重的基础上继续训练。</p><p><strong>微调 (fine tuning)</strong>：在预训练的基础上继续训练，比如上面的再用 x 光诊断图片继续训练，对权重进行更新，称之为“微调”。</p><p>有时候你也可以将最后一层移除后增加一些新的层，然后对新建的几层重新训练：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_8.png" alt=""></p><h3 id="为什么迁移学习有效？"><a href="#为什么迁移学习有效？" class="headerlink" title="为什么迁移学习有效？"></a>为什么迁移学习有效？</h3><p>是因为从从大规模的图像识别数据集中学习到的边界检测，曲线检测，明暗对象检测等低层次的信息，或许能够帮助你的学习算法更好地去进行放射扫描结果的诊断。它会在图像的组织结构和自然特性中学习到很多信息，其中一些信息会非常的有帮助。所以当神经网络学会了图像识别意味着它可能学习到了以下信息：关于不同图片的点，线，曲面等等信息，在不同图片中看起来是什么样子的。或许关于一个物体对象的很小的细节，都能够帮助你在放射信息诊断的神经网络中，学习得更快一些或者减少学习需要的数据。</p><h3 id="什么时候采用迁移学习？"><a href="#什么时候采用迁移学习？" class="headerlink" title="什么时候采用迁移学习？"></a>什么时候采用迁移学习？</h3><ul><li><p>任务 A 有着比任务 B 多得多的数据，可以从 A 迁移到 B</p><ul><li><p>例如，假设你在一个图像识别任务中拥有一百万个样本，这就意味着大量数据中的低层次特征信息或者大量的有帮助的特征信息在神经网络的前几层被学习到了。但是对于放射扫描结果的诊断任务，或许仅仅是100个 X 光扫描样本。所以你从图像识别中学习到的大量的信息可以被用于迁移并且这些信息会有效地帮助你处理好放射诊疗。</p></li><li><p>总结：只能大数据模型迁移到小数据模型，而不能小数据模型迁移到大数据模型。</p></li></ul></li><li><p>任务 A 和任务 B 有着相同的输入</p><ul><li>例如识别猫狗和诊断 X 光输入的都是图片</li></ul></li><li><p>当你认为任务 A 中的低层次特征会帮助到任务 B </p><ul><li>例如你判断两个任务有某种可以互通的特征，比如两者都是识别物体或都是语音识别</li></ul></li></ul><h2 id="多任务学习-multi-task-learning"><a href="#多任务学习-multi-task-learning" class="headerlink" title="多任务学习 (multi-task learning)"></a>多任务学习 (multi-task learning)</h2><p>在多任务学习中，几个任务一起进行，让神经网络同时做几件事情。</p><h3 id="如何实现-1"><a href="#如何实现-1" class="headerlink" title="如何实现"></a>如何实现</h3><p>在自动驾驶时拍下一张照片需要识别出照片中是否存在一些物体，如果有就置 1，否则为 0 ：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_9.png" alt=""></p><div class="table-container"><table><thead><tr><th style="text-align:center">行人</th><th style="text-align:center">0</th></tr></thead><tbody><tr><td style="text-align:center">汽车</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">交通牌</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">信号灯</td><td style="text-align:center">0</td></tr></tbody></table></div><p>于是输出标签变为（4，m）的矩阵：</p><script type="math/tex; mode=display">Y=\begin{bmatrix} |&  |&  & | & \\  y^{(1)}& y^{(2)} &,... , & y^{(m)} & \\  |& | &  & | & \end{bmatrix}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_10.png" alt=""></p><p>搭建上图中的网络进行训练，最后一层有四个单元，表示识别的四种物体的概率，那么代价函数可以表示为：</p><script type="math/tex; mode=display">J=\frac{1}{m} \sum\limits^{m}_{i=1}\sum\limits^{4}_{j=1}L(\hat y^{(i)}_j,y^{(i)}_j)</script><ul><li><p>有时候 4 个标签会有缺失（下图中的 “？”），你仍然可以进行训练，只需要省略缺失项， $\sum\limits^{4}_{j=1}$ 只对有标签的值进行求和即可</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_11.png" alt=""></p></li><li><p>该方法相当于将四个独立的神经网络合在一起，但是效果更好</p></li></ul><h3 id="什么时候适用多任务学习"><a href="#什么时候适用多任务学习" class="headerlink" title="什么时候适用多任务学习"></a>什么时候适用多任务学习</h3><ul><li>你要训练的任务共享一些低层次的特征<ul><li>例如行人、交通灯、汽车、交通牌这些物体都是道路特征</li></ul></li><li>非硬性：多任务中每个任务的数据量相当相似<ul><li>如果你集中在某一任务上，其他任务比这单一任务有多得多的数据</li></ul></li><li>如果你可以训练一个足够大的网络来做好所有的任务<ul><li>如果神经网络不够大，与单独训练每个任务相比，多任务训练会损害准确率</li></ul></li></ul><h2 id="端到端深度学习-end-to-end-deep-learning"><a href="#端到端深度学习-end-to-end-deep-learning" class="headerlink" title="端到端深度学习 (end-to-end deep learning)"></a>端到端深度学习 (end-to-end deep learning)</h2><h3 id="什么是“端到端深度学习”"><a href="#什么是“端到端深度学习”" class="headerlink" title="什么是“端到端深度学习”"></a>什么是“端到端深度学习”</h3><p>一个由许多阶段组成的学习系统，输入 x，输出 y，如果我们将其中的许多阶段用单个神经网络进行替代，直接建立从输入端 x 到输出端 y 的映射，就叫做“端到端深度学习”。</p><h3 id="一些例子"><a href="#一些例子" class="headerlink" title="一些例子"></a>一些例子</h3><ul><li><p>例子 1</p><p>加入要建立一个语音识别系统，那么一般思路为：</p><p>输入音频 x $\xrightarrow{MFCC算法}$ 提取低级特征 $\xrightarrow{ML算法}$ 找到音素 $\xrightarrow{组合}$ 单词  $\xrightarrow{剪辑}$ 输出文字 y                    </p><p>而端到端深度学习的实现方式为直接建立输入语音 x 和文字脚本 y 的映射：</p><p>输入音频 x   $\xrightarrow{ \quad \quad\quad\quad\quad\quad\quad 端到端深度学习 \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}$ 输出文字 y   </p></li><li><p>例子 2</p><p>机器翻译系统，传统机器学习思路：</p><p>英语 $\rightarrow$ 文本分析 $\rightarrow$ 提取特征 $\rightarrow$ … $\rightarrow$ 中文</p><p>端到端机器学习使用大量的英文和中文对应文本的数据集进行直接映射：</p><p>英文$\xrightarrow{ \quad \quad\quad\quad\quad 端到端深度学习 \quad\quad\quad}$ 中文 </p></li></ul><h3 id="end-to-end-DL-的优缺点"><a href="#end-to-end-DL-的优缺点" class="headerlink" title="end-to-end DL 的优缺点"></a>end-to-end DL 的优缺点</h3><ol><li>优点<ul><li>端到端数据学习让数据“说话”，从 X $\rightarrow$ Y 的映射中更好地学习到数据内在的统计学特征，而不是被迫去反映人的先见<ul><li>例如传统机器学习利用到语言中“音素”的概念，但是这个概念是是一个人造的产物，无法确定让算法使用“音素”是不是有利的</li></ul></li><li>需要更少的人工设计组件，简化工作流程</li></ul></li><li>缺点<ul><li>为了得到 X 到 Y 的映射，需要非常大量的 (X $\rightarrow$Y) 数据</li><li>它排除了一些具有潜在用途的手工设计组件<ul><li>如果数据量不够，那么一个精心手工设计的系统实际上可以向算法中注入人类关于这个问题的知识，是很有帮助的</li></ul></li></ul></li></ol><h3 id="什么时候需要运用端到端深度学习"><a href="#什么时候需要运用端到端深度学习" class="headerlink" title="什么时候需要运用端到端深度学习"></a>什么时候需要运用端到端深度学习</h3><p>关键问题：你是否有充足的数据去学习出具有能够映射 X 到 Y 所需复杂度的方程？</p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 3 week 1）—— 机器学习策略（一）</title>
      <link href="/2018/09/08/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c3w1/"/>
      <url>/2018/09/08/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c3w1/</url>
      <content type="html"><![CDATA[<p>现在来到 coursera 的 deeplearning.ai 课程的第三课，这门课程叫 Structuring Machine Learning Projects 结构化机器学习项目，将会学习到构建机器学习项目的一些策略和基本套路，防止南辕北辙。</p><h2 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化 (Orthogonalization)"></a>正交化 (Orthogonalization)</h2><h3 id="何为正交化"><a href="#何为正交化" class="headerlink" title="何为正交化"></a>何为正交化</h3><p>如同老式电视机一样，一个旋钮控制一个确定的功能，如屏幕横向伸长或纵向伸长，而不会一个旋钮控制两个属性。</p><h3 id="ML-的正交化"><a href="#ML-的正交化" class="headerlink" title="ML 的正交化"></a>ML 的正交化</h3><p>我们希望某个旋钮能单独让下面的某个步骤运行良好而不影响其他，这就叫正交化。</p><ul><li>首先训练集要拟合的很好<ul><li>否则减小偏差</li></ul></li><li>如果训练集运行良好，则希望开发集运行良好<ul><li>否则减小方差</li></ul></li><li>如果在训练集和开发集运行良好，则希望在测试集运行良好<ul><li>否则采用更大的开发集</li></ul></li><li>最后希望在真实世界表现良好<ul><li>否则调整开发集或者改变代价函数</li></ul></li></ul><a id="more"></a><p>当你发现某个步骤表现不好的时候，找到一个特定的旋钮进行调节，从而解决问题。</p><h2 id="设置目标"><a href="#设置目标" class="headerlink" title="设置目标"></a>设置目标</h2><h3 id="设置单一化的评估指标"><a href="#设置单一化的评估指标" class="headerlink" title="设置单一化的评估指标"></a>设置单一化的评估指标</h3><ul><li>精确率 precision：模型识别出是猫的集合中，有多少百分比真正是猫</li><li>召回率 recall：对于全部是猫的图片有多少能正确识别出来</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_1.png" alt=""></p><p>A、B 分类器分别有两个指标 P 和 R，各有长处，不好判断，于是可以计算它们的调和平均数，变成一个单一指标 F1 score，公式为：</p><script type="math/tex; mode=display">F1=\frac{2}{\frac{1}{P}+\frac{1}{R}}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_2.png" alt=""></p><p>再比如我们知道各个算法在不同国家的错误率，我们求这些错误率的平均值，得到一个单一指标，从而知道 C 算法性能最好。</p><h3 id="优化指标和满足指标"><a href="#优化指标和满足指标" class="headerlink" title="优化指标和满足指标"></a>优化指标和满足指标</h3><p>优化指标 optimizing metric：指这个指标越大（越小）越好，需要它表现得<strong>尽可能</strong>好，一般一个问题只需要一个优化指标，其他的是满足指标。</p><p>满足指标 satisficing metric：不需要尽可能好，只需要到达某个门槛即可，只要到达了这个门槛就不关心它的值到底是多少。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_3.png" alt=""></p><p>上图中的精确度指标是优化指标，运行时间是满足指标，假设我们设置门槛为 100ms，则首先淘汰 C 分类器，然后由于 B 的优化指标更好，我们需要精确度越大越好，所以最好的分类器是 B，也就是说我们要选出运行时间在 100ms 以内且精确度最大的分类器。</p><h3 id="开发集和测试集的设置"><a href="#开发集和测试集的设置" class="headerlink" title="开发集和测试集的设置"></a>开发集和测试集的设置</h3><h4 id="指导原则"><a href="#指导原则" class="headerlink" title="指导原则"></a>指导原则</h4><ul><li>开发集给我们树立了一个靶子，选择的开发集和测试集要能够反映出将来需要预测的数据和你认为重要的数据</li><li>开发集和测试集应该来自相同的分布</li><li>训练集和开发集可以来自不同的分布</li></ul><p>设定开发集和评估方法就像放置一个目标，然后团队通过各种尝试接近这个目标，使得模型在这个开发集和评估方法上做得更好。如果开发集和测试集来自不同的地方，当你的团队花了几个月靠拢开发集，最后用测试集做测试时，会发现表现并不好。就像让你的团队用数个月的时间瞄准一个目标，但是数个月后你说：“等等，我要测试一下，我要换个目标！”</p><p>假设现在有来自世界八个地区的数据，如果将其中四个地区的数据作为 dev sets，另外四个地区的数据作为 test sets，是完全错误的，会导致开发集和测试集的数据分布不同，正确的做法应该是将所有地区的数据混合再分成开发集和测试集，保证数据分布相同。</p><p>总结：找到将来你需要的预测的数据，同时放入开发集和测试集中，瞄准你需要的目标。</p><h4 id="数据划分比例"><a href="#数据划分比例" class="headerlink" title="数据划分比例"></a>数据划分比例</h4><ul><li>几百到上万个样本：训练集/测试集=70/30；训练集/开发集/测试集=60/20/20</li><li>上百万个样本：训练集/开发集/测试集=98/1/1<ul><li>假如一百万个样本，拿一万当开发集，一万当测试集即可</li></ul></li><li>测试集有时候可以用开发集代替，但是有测试集会更安心，而且需要足够大才能有足够的自信来评估整个模型表现</li></ul><h3 id="什么时候重新定义指标"><a href="#什么时候重新定义指标" class="headerlink" title="什么时候重新定义指标"></a>什么时候重新定义指标</h3><ol><li><p>当你的算法由于某些特殊情况影响了用户体验或者说用户有特殊的需要时</p><p>例如我们设置指标为：开发集上猫咪分类的错误率越低越好。算法 A 错误率：3%；算法 B 错误率：5%；根据我们的原有指标来看，明显算法 A 更好。但是现在算法 A 会在推荐猫咪图片时，错误地推荐一些色情图片，这对用户来说是无法忍受的，而 B 算法就不会，所以这种情况下，我们的评估指标发生了变化，B 算法更符合我们的需要。</p><p>那么如何修改评价指标？</p><script type="math/tex; mode=display">Error=\frac{1}{\sum\limits_iw^{(i)}}\frac{1}{m_{dev}}\sum\limits_{i=1}^{m_{dev}}w^{(i)}I\{y_{predict}^{(i)} \neq y^{(i)} \}\\w^{(i)}= \left\{\begin{aligned}1 && 如果x^{(i)}不是色情图片\\10 &  & 如果x^{(i)}是色情图片\end{aligned}\right.</script><ul><li>$m_{dev}$ 为开发集的个数</li><li>$y_{predict}^{(i)}$ 为第 i 个样本的预测值，取值为 1 或 0</li><li>$I\{y_{predict}^{(i)} \neq y^{(i)} \}$ 如果括号里为真，则值为 1，即错误数加 1</li><li>如果某个图片是色情图片，则权重 w=10， 将会给它产生更大的误差值，也就是说假如一个图片是色情图片，则它相当于十个错误图片，会大大提高错误率</li><li>$\frac{1}{\sum\limits_iw^{(i)}}$ 是归一化操作，使得 error 介于 0～1 之间</li></ul></li><li><p>实际情况发生了变化而当前的算法无法满足实际需要时</p><p>例如你用网上找到十分精美的图片用作训练集和开发集，来开发猫咪识别算法，但是用户使用时上传各种各样的图片，可能取景不好，可能猫没照全，可能图像模糊，也就是说实际情况发生了改变，我们的目标也发生了改变，这个时候你需要从实际情况获取更多的数据，开始修改你的指标或者开发/测试集了，让它们实际中做得更好。</p></li></ol><h3 id="机器学习的两个正交化步骤"><a href="#机器学习的两个正交化步骤" class="headerlink" title="机器学习的两个正交化步骤"></a>机器学习的两个正交化步骤</h3><ul><li>立靶子：讨论如何定义一个指标来评估分类器</li><li>射靶子：如何很好地满足这个指标</li></ul><h2 id="与人类表现相比较"><a href="#与人类表现相比较" class="headerlink" title="与人类表现相比较"></a>与人类表现相比较</h2><h3 id="为什么要和人类表现相比较"><a href="#为什么要和人类表现相比较" class="headerlink" title="为什么要和人类表现相比较"></a>为什么要和人类表现相比较</h3><ul><li>机器学习算法快速发展，慢慢在某些领域变得和人类相比有竞争力</li><li>在某些领域，机器学习效率比人类更高</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_4.png" alt=""></p><p>绿线指<strong>贝叶斯误差 (Bayers error)</strong>，它指从 x 映射到 y 的最好的理论函数，永远无法超越，不管是人类还是机器。</p><p>当机器的表现超越人类之后进展放慢，原因有：</p><ul><li>人类的误差在许多任务中与贝叶斯误差相差不远，超越人类之后没有那么大改善空间</li><li>当表现超越人类之后，就没有什么工具来进行提高了</li></ul><p>当机器的表现不如人类，我们可以用以下工具来进行提高：</p><ul><li>从人类得到标签进行学习</li><li>人类可以对偏差进行分析，改进算法</li><li>可以得到更好的偏差/方差分析，即得到一个改进的标准</li></ul><h3 id="用人类标准评估偏差"><a href="#用人类标准评估偏差" class="headerlink" title="用人类标准评估偏差"></a>用人类标准评估偏差</h3><p>方法：用人类的误差的最低值作为贝叶斯误差的一个估计值。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_5.png" alt=""></p><p>第一个例子训练集误差跟人类误差差距较大，故专注于减小偏差，第二个例子开发集误差跟训练集误差差距更大，所以专注于减小方差。吴恩达把训练集上的误差和人类误差之间的差值称之为“可回避的偏差”，也就是偏差的下限。所以左边的例子可回避偏差为 7%，右边的为 0.5%，左边的偏差可提高空间很大。</p><h3 id="更好地定义“人类表现”"><a href="#更好地定义“人类表现”" class="headerlink" title="更好地定义“人类表现”"></a>更好地定义“人类表现”</h3><p>假设看一张 x 光片，不同的人类误差如下：</p><ul><li>普通人：3%</li><li>普通医生：1%</li><li>有经验的医生：0.7%</li><li>一个有经验的医生团队讨论：0.5% $\rightarrow$ 作为贝叶斯误差的估计值</li></ul><p>那么如何定义人类表现呢？搞清楚你的目的：</p><ul><li>如果是为了超过一个普通人的水平，那么将人类表现定为 1%</li><li>如果目的是作为贝叶斯误差的代替，那么定义为 0.5% 更好</li></ul><p>举个例子：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_6.png" alt=""></p><p>第一种情况，无论人类表现定义为哪种，可避免偏差都很大，所以专注于减小偏差，第种，无论人类表现定义为哪种，都专注于减小方差，第三种情况由于训练集误差只有 0.7%，所以人类表现的最好定义是 0.5%。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_7.png" alt=""></p><p>总之：人类误差（贝叶斯误差的估计值）和训练集误差之间的的差距代表了可避免的偏差，训练集误差和开发集误差之间的差距代表了方差。</p><h3 id="ML-明显超过人类的领域"><a href="#ML-明显超过人类的领域" class="headerlink" title="ML 明显超过人类的领域"></a>ML 明显超过人类的领域</h3><ul><li>在线广告</li><li>产品推荐</li><li>物流（预测运输时间）</li><li>贷款批准</li><li>某些医学领域</li><li>某些语音识别</li><li>某些图像识别</li></ul><p>人类更擅长的是自然感知任务，包括视觉，语音识别，自然语言处理等；机器更适合处理结构化数据相关任务。</p><h2 id="如何提高模型表现"><a href="#如何提高模型表现" class="headerlink" title="如何提高模型表现"></a>如何提高模型表现</h2><ul><li>如何减小可避免偏差<ul><li>训练更大的模型</li><li>训练更久/更高的优化算法<ul><li>动量算法、RMSprop、Adam</li></ul></li><li>改变神经网络结构/超参数搜寻<ul><li>使用 RNN/CNN</li></ul></li></ul></li><li>如何减小方差<ul><li>更多数据</li><li>正则化<ul><li>L2、dropout、数据集增强</li></ul></li><li>改变神经网络架构/超参数搜寻</li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 2 week 3）—— 调参方法、批量归一化和深度学习框架</title>
      <link href="/2018/09/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w3/"/>
      <url>/2018/09/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w3/</url>
      <content type="html"><![CDATA[<p>本周主要学习超参数的系统调整，批量归一化和深度学习框架的相关知识。</p><h2 id="超参数的调整"><a href="#超参数的调整" class="headerlink" title="超参数的调整"></a>超参数的调整</h2><h3 id="调参的优先级"><a href="#调参的优先级" class="headerlink" title="调参的优先级"></a>调参的优先级</h3><ul><li>第一优先级：学习率 $\alpha$ </li><li>第二优先级：动量算法参数 $\beta$ (默认值0.9)，最小批的个数 mini-batch size，每层的隐藏单元数 hidden units</li><li>第三优先级：神经网络的层数 layers，学习率衰减 learning rate decay</li><li>几乎不需要调优：Adam 算法中的 $\beta_1,\beta_2,\varepsilon$，几乎每次只用默认值 $0.9,0.999,10^{-8}$    </li></ul><h3 id="调参的注意事项"><a href="#调参的注意事项" class="headerlink" title="调参的注意事项"></a>调参的注意事项</h3><ol><li><p>随机取样</p><p>当需要调整多个超参数时，尽量在网格中随机取样（下图右），而不是规则抽样（下图左）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_1.png" alt=""></p><a id="more"></a><p>   由于你不知道两个参数哪个更加重要，需要更多尝试。我们先假设超参数 1 更加重要：如果是左边的规则取样，那么即使取样 25 个点，实际上只尝试到了 5 个参数 1 的值，如果像右边那样随机取样，那么就可以尝试到 25 个参数 1 的值，而超参数 2 的值对结果影响并不大，所以随机取样更加合理。</p><p>   如果同时要选择三个超参数，那么就在三维空间中随机取样。</p></li><li><p>区域定位的抽样方案</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_2.png" alt=""></p><p>假设在上图的超参数样本空间中发现右下角那三个打圈的点表现更好，那么接下来对这些点所在的区域进行限定，然后在这个区域进行密度更高的随机抽样，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_3.png" alt=""></p></li><li><p>采用合适的尺度</p><p>假设现在需要调整学习率 $\alpha$，我们认为它的是一个 0.0001～1 之间的一个值，在数轴上画出它的范围：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_4.png" alt=""></p><p>如果我们在这个之间随机取样，如下图，那么将会有 90% 的样本落在 0.1～1 之间，只有 10% 的样本落在 0.001～0.1 之间，这明显是不合理的。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_5.png" alt=""></p><p>正确的方法应该是使用“对数尺度”而不是“线性尺度”来进行取样，即将数轴变成对数数轴，分成 0.0001、0.001、0.01、0.1、1 这几个部分，在每个部分之间进行均匀取样。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_6.png" alt=""></p><p>假设我们要在线性尺度的 a，b 两点之间取样， 那么就需要在对数尺度 $log_{10}^a$ 和 $log_{10}^b$ 之间进行均匀取样得到 $r$ ，那么对应的超参数就是 $10^r$.</p><p>假设现在我们要取样的是动量算法参数 $\beta$，它的合适范围在 0.9～0.999，我们可以先取样 $1-\beta$，它对应的范围在 0.1~0.001。使用上述方法取样得到 r，则最后的超参数为 $\beta=1-10^{r}$ .</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_7.png" alt=""></p><p>随着 $\beta$ 趋近于 1 ，其结果对于 $\beta$ 的改变非常敏感，例如 $\beta$ 从0.9 变成 0.9005 这没什么大不了，结果几乎不变，但是如果 $\beta$ 从 0.999 变成 0.9995，它将会对你正在运行的算法产生巨大的影响，即：在 $\beta$ 趋近于 1 的时候，你能得到更高效的样本分布，搜索超参数时更有效率。</p></li></ol><h3 id="两者超参数搜索策略"><a href="#两者超参数搜索策略" class="headerlink" title="两者超参数搜索策略"></a>两者超参数搜索策略</h3><ol><li><p>熊猫模式</p><p>精心照料某个单一的模型，通常你需要处理一个非常庞大的数据集，但没有充足的计算资源，比如没有很多CPU 没有很多GPU，那你只能一次训练一个或者非常少量的模型，这种情况下 即使枯燥的训练你也要盯着，每天都在照看你的模型，尝试微调参数，就像一个母亲一样每天精心照料着你的模型，即使是在好几天甚至几周的训练过程中都不离不弃，照看着模型，观察性能曲线，耐心地微调学习率，这通常发生在你没有足够的计算资源同时训练几个模型的情况下。就像熊猫产子一样，数量稀少，经常一次一个，但是会投入全部精力，确保孩子平安无事。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_8.png" alt=""></p></li><li><p>鱼子酱模式</p><p>并行训练许多个模型，这种情况下你可能设置一些超参数，然后让模型自己运行一天或几天，与此同时，你可能会使用完全不同的超参数，开始运行另一个不同的模型，同时，你可能开始训练第三个模型，你会同时运行许多不同的模型，用这样的方式，你就可以尝试不同的超参数设置。这可以让超参数选择变得简单，只要找一个最终结果最好的就行了。这更像鱼类的行为，为了简单易懂，称之为鱼子酱策略，有许多鱼在交配季节能产下一亿枚卵，然后无需投入太多精力去照看某个鱼卵，只希望其中一个或者一部分能够存活。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_9.png" alt=""></p></li></ol><p>如何挑选合适的模式？取决于你有多少计算资源，如果你有足够的计算机来并行训练很多模型，那不用考虑别的，采用鱼子酱模式就行了，尝试大量不同的超参数，看看结果如何。但在某些应用领域，例如在线广告设置以及计算机视觉识别，都有海量的数据和大量的模型需要去训练，而同时训练大量模型是极其困难的事情，这时只能采用熊猫模式。</p><h2 id="批量归一化-Batch-Normalization"><a href="#批量归一化-Batch-Normalization" class="headerlink" title="批量归一化 (Batch Normalization)"></a>批量归一化 (Batch Normalization)</h2><p>批量归一化可以使得你的超参数搜索变得简单，即神经网络对于超参数的选择不再那么敏感，让你更容易训练非常深的网络。</p><h3 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h3><p>之前在第二周第一课中提到对训练集的归一化，先求训练集的均值，再求训练集的方差，将训练集每个数据减去均值再处以标准差就对其进行了归一化操作。可以让代价函数从一个扁圆变成一个正圆形，提高梯度下降的效率。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_10.png" alt=""></p><p>上述只适用于没有隐藏层的逻辑回归，如果是一个层数更多的模型呢？每一层的输出又是下一层的输入，那么我们就需要将每一层的输入都进行归一化操作，否则该层以后的训练都不会很有效率。对任何一个隐藏层的输入进行归一化操作就叫做批量归一化 BN。</p><p>对于输入是在激活函数之前做归一化还是在之后做归一化存在争议，一般来说：<strong>对线性值 z 进行归一化，也就是在激活之前进行归一化</strong>。</p><h3 id="批量归一化的实现"><a href="#批量归一化的实现" class="headerlink" title="批量归一化的实现"></a>批量归一化的实现</h3><p>下面是在神经网络某一层的实现过程：</p><p>$\mu = \frac{1}{m} \sum\limits _i z^{(i)} \leftarrow 求数据的均值\\ \sigma^2 = \frac{1}{m}\sum\limits _i (z^{(i)-\mu^{}})^2 \leftarrow 求数据的方差\\ z_{norm}^{(i)}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}} \leftarrow 将数据归一化为正态分布\\ \tilde{z}^{(i)}=\gamma z_{norm}^{(i)}+\beta \leftarrow 变换尺度，使得数据有可控的方差和均值$  </p><ul><li>其中 $\gamma,\beta$  是模型的参数，可以从反向传播中学习，使用梯度下降来更新它们</li><li>$\varepsilon$ 是一个很小的数防止分母为零</li></ul><p>在整个神经网络上如何实现？以下面的神经网络为例。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_11.png" alt=""></p><p>对于每个小批次，都做如下的操作：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_12.png" alt=""></p><p>有效参数：$W^{[l]},\gamma^{[2]},\beta^{[l]}$ </p><p>之所以参数没有 b，是因为在进行归一化时，参数 b 在均值中被减掉，所以永远是零，于是我们的前向传播算法变为</p><p>$z^{[l]}=W^{[l]}a^{[l-1]} \\ z_{norm}^{[l]}=\frac{z^{[l]}-\mu^{[l]}}{\sqrt{ {\sigma^2}^{[l]}+\varepsilon}} \\ \tilde{z}^{[l]}=\gamma^{[l]} z_{norm}^{[l]}+\beta^{[l]}$ </p><p>其中参数 $z^{[l]},z_{norm}^{[l]},\tilde{z}^{[l]},\gamma^{[l]},\beta^{[l]}$ 的维度都是 $(n^{[l]},1)$ </p><p>批量归一化的小批量梯度下降的实现过程：</p><p>$for \quad t=1…num_minibatch:\\ \quad \quad 计算第 t 批次的前向传播\\ \quad \quad \quad \quad 在隐藏层中进行归一化，用 \tilde{z}^{[l]}代替z^{[l]} \\ \quad \quad  用反向传播计算 dW^{[l]},d\beta^{[l]},d\gamma^{[l]} (没有 db^{[l]})\\ \quad \quad 更新参数 W^{[l]}:=W^{[l]}-\alpha dW^{[l]} \\ \quad \quad \quad \quad \quad \quad \beta^{[l]}:=\beta^{[l]}-\alpha d\beta^{[l]} \\ \quad \quad \quad \quad \quad \quad \gamma ^{[l]}:=\gamma^{[l]}-\alpha d\gamma^{[l]}$ </p><h3 id="为什么-BN-如此有效"><a href="#为什么-BN-如此有效" class="headerlink" title="为什么 BN 如此有效"></a>为什么 BN 如此有效</h3><h4 id="协变量"><a href="#协变量" class="headerlink" title="协变量"></a>协变量</h4><p>首先要提到协变量 (Covariate)，什么是协变量？它与自变量是相对的。</p><p>自变量：指研究者主动操纵，而引起因变量发生变化的因素或条件，因此自变量被看作是因变量的原因。在机器学习中，训练的模型可以称之为自变量。 </p><p>协变量：在实验的设计中，协变量是一个独立变量(解释变量)，不为实验者所操纵，但仍影响响应。在机器学习中，模型的输入变量是协变量。</p><h4 id="协变量偏移-Covariate-Shift"><a href="#协变量偏移-Covariate-Shift" class="headerlink" title="协变量偏移 (Covariate Shift)"></a>协变量偏移 (Covariate Shift)</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_13.png" alt=""></p><p>假设我们用左边黑猫的图片训练了一个逻辑回归模型，则训练的模型是自变量，而输入的数据是协变量。因为黑猫和黄猫的颜色深浅明显是不同的，输入数据分布发生了改变，也就是说发生了协变量偏移 (Covariate Shift)，所以我们不能指望这个模型可以适用于右边的猫的图片。</p><p>即使两个完全一样的模型，如果输入变量的分布不同，得到的结果也会有很大差异，所以机器学习算法都要求输入变量在训练集和测试集上的分布是相似的。如果训练了一个模型，然而输入 x 的分布发生了变化，那么我们就得重新训练模型。</p><h4 id="神经网络中的内部协变量偏移-Internal-Covariate-Shift"><a href="#神经网络中的内部协变量偏移-Internal-Covariate-Shift" class="headerlink" title="神经网络中的内部协变量偏移 (Internal Covariate Shift)"></a>神经网络中的内部协变量偏移 (Internal Covariate Shift)</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_14.png" alt=""></p><p>在上面这个神经网络中，第二层的激活值可以当作这一层后面的神经网络的输入，从而影响第三第四第五层的参数的值，那么它就是这个神经网络的内部协变量，但是它的值并不是一成不变的，它又是前面的神经网络的参数所决定，这些参数不停地更新，导致第二层的值不停在改变，从而发生了内部协变量偏移 (Internal Covariate Shift)。</p><h4 id="BN-的作用"><a href="#BN-的作用" class="headerlink" title="BN 的作用"></a>BN 的作用</h4><p>批量归一化将每个隐藏单元的输入进行归一化，减少了隐藏单元值分布的不稳定性，虽然这些隐藏结点的值会发生变化，但是 BN 算法确保无论它怎么变化，它的均值和方差将保持不变，所以限制了先前层中参数的更新对后面的网络的输入的影响，使后层神经网络有更加稳固的基础，从而提高整个网络的学习速度。</p><p>在 BN 中，由于每个小批次都被那一批次的均值和方差归一化，因为我们在该 min-batch 上计算了均值和方差，而不是在整个数据集上计算，所以该均值和方差包含有噪声，所以某个 $\tilde z$ 也会有噪声，就像 dropout 给隐藏层增加噪声一样，所以 BN 会有轻微的正则化效果，但是它不是一个正则化方法。</p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><h3 id="多分类问题概况"><a href="#多分类问题概况" class="headerlink" title="多分类问题概况"></a>多分类问题概况</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_15.png" alt=""></p><p>假设我们要将图片分为四类，则不同的类别用不同的数字代表，比如 1 是小鸡，2 是小猫，3 是小狗，0 是其他动物，则这是一个多分类问题，分类的类别用 C 表示，在这里 C = 4.</p><p>解决这类问题，只需要将神经网络的输出层单元数变成 4，即 $n^{[L]}=4=C$，第一个单元表示图片是小鸡的概率，第二个单元表示图片是小猫的概率，第三个单元表示图片是小狗的概率，第四个单元表示图片是其他的概率，输出向量 $\hat y$ 的维度为 (4,1).</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_16.png" alt=""></p><h3 id="softmax-层的前向传播"><a href="#softmax-层的前向传播" class="headerlink" title="softmax 层的前向传播"></a>softmax 层的前向传播</h3><p>第 L 层为输出层，这一层也叫 softmax 层，这一层的运算如下：</p><p>$z^{[L]}=W^{[L]}a^{[l-1]}+b^{[L]}  \\  激活函数：\\ t=e^{(z^{[L]})} \\ \hat y = a^{[L]}=\frac{t}{\sum\limits _{i=1}^4 t_i},a^{[L]}_i=\frac{t_i}{\sum\limits _{i=1}^4 t_i}$  </p><ul><li>$z^{[L]},t,\hat y$ 的维度都是 (4,1) 向量</li><li>i 代表第 L 的第 i 个结点</li></ul><p>把上面的运算当成一个激活函数：</p><script type="math/tex; mode=display">a^{[L]}=g^{[L]}(z^{[L]})\\g^{[L]}=softmax()</script><ul><li>softmax() 输入一个向量，输出一个尺寸一样的向量</li></ul><p>下面是在逻辑回归中将输入通过 softmax 的线性分类结果：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_18.png" alt=""></p><p>那么 softmax 的实质是什么？</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_19.png" alt=""></p><p>实际上 softmax 将一组向量映射成了它们的概率向量，这组向量中某个值越大，它通过 softmax 映射的概率值就越大。与 softmax 对应的是 hardmax，它也是将输入向量映射为另一个向量，但它是直接找出输入向量的最大值并将它置为 1，其他置为 0，非常简单粗暴，而 softmax 的映射就更加平和。同时 softmax 将逻辑回归中的二分类推广到 C分类，也就是多分类，当 C=2 的时候，softmax 就简化为逻辑回归。</p><h3 id="softmax-的代价函数"><a href="#softmax-的代价函数" class="headerlink" title="softmax 的代价函数"></a>softmax 的代价函数</h3><p>假设某个样本的标签值 $y=\begin{bmatrix} 0\\1\\0\\0 \end{bmatrix}$，预测值 $\hat y=\begin{bmatrix} 0.3\\0.2\\0.1\\0.4 \end{bmatrix}$，用下列公式计算单个样本代价函数：</p><script type="math/tex; mode=display">L(\hat y,y)=-\sum\limits_{j=1}^4 y_jlog\hat y_j</script><p>所有样本的总代价函数为：</p><script type="math/tex; mode=display">J(W^{[1]},b^{[1]},...)=\frac{1}{m}\sum\limits_{i=1}^m L(\hat y,y)</script><ul><li>实际上 $y,\hat y$ 应该是 $Y,\hat Y$ ，是一个维度为 （C，m）的矩阵</li></ul><p>简单的证明：因为 $y_2=1,y_1=y_3=y_4=0$，则 $L(\hat y,y)=-\sum\limits_{j=1}^4 y_jlog\hat y_j=-y_2log \hat y_2=-log \hat y_2$，假设代价函数越小，则 $log \hat y_2$ 越大，则 $\hat y_2$ 越大，则图片的标签值为 2 的概率越大，而这张图片的实际标签值就是 2。于是我们建立逻辑链条：代价函数越小 $\rightarrow$ 预测这张图片标签值是 2 的概率 $\hat y_2$ 越大 $\rightarrow$ 图片真实标签值就是 2 $\rightarrow$ 越符合实际情况 $\rightarrow$ 模型预测越准确 </p><h3 id="softmax-的反向传播"><a href="#softmax-的反向传播" class="headerlink" title="softmax 的反向传播"></a>softmax 的反向传播</h3><p>初始化公式为：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial z^{[L]}}=dz^{[L]}=\hat y -y</script><ul><li>其中 $dz^{[L]},\hat y,y$ 都是 （C，1）的向量</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 超参数 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 2）</title>
      <link href="/2018/09/04/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c2w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/09/04/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c2w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<p>本次作业实现了几种梯度下降算法，比较了它们的不同。</p><h2 id="普通梯度下降-BGD"><a href="#普通梯度下降-BGD" class="headerlink" title="　普通梯度下降 (BGD)"></a>　普通梯度下降 (BGD)</h2><p>所谓普通梯度下降就是一次处理所有的 m 个样本，也叫批量梯度下降 (Batch Gradient Descent)，公式为：</p><script type="math/tex; mode=display">W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\\b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_gd</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    普通梯度下降</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters to be updated:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients to update each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 参数的个数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] =parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="随机梯度下降-Stochastic-Gradient-Descent-SGD"><a href="#随机梯度下降-Stochastic-Gradient-Descent-SGD" class="headerlink" title="随机梯度下降 Stochastic Gradient Descent (SGD)"></a>随机梯度下降 Stochastic Gradient Descent (SGD)</h2><p>SGD 一个只处理一个样本进行梯度下降，速度比 GD 快，但是在朝着最小值行进的过程中会发生震荡，而 GD 是平滑地稳步向最小值走。GD 是考虑周全再行动，每一步都朝着全局最优走，所以很慢，而 SGD 是走一步看一步，并不是每一步的迭代都朝着全局最优的方向走，噪声很多，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_1.png" alt=""></p><p>SGD 算法中有三个 for 循环：</p><ul><li>迭代次数的循环</li><li>所有 m 个训练样例的循环</li><li>神经网络所有层的循环</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):<span class="comment"># 遍历所有的 m 个训练样本</span></span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure><h2 id="小批量梯度下降-Mini-Batch-Gradient-descent-MBGD"><a href="#小批量梯度下降-Mini-Batch-Gradient-descent-MBGD" class="headerlink" title="小批量梯度下降 Mini-Batch Gradient descent (MBGD)"></a>小批量梯度下降 Mini-Batch Gradient descent (MBGD)</h2><p>由于 BGD 和 SGD 是两个极端，如果一次既不处理一个样本，也不处理所有样本，而处理介于两者之间的样本数，即一次处理整个训练样本的一小批，学习速度会更快。</p><p>MBGD 比 SGD 的震荡更小：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/l_cdoe_2.2_2.png" alt=""></p><h3 id="划分-mini-batch-的步骤"><a href="#划分-mini-batch-的步骤" class="headerlink" title="划分 mini -batch 的步骤"></a>划分 mini -batch 的步骤</h3><p>一共有两步：</p><ul><li><p>洗牌 (shuffle)：将训练集 （X，Y）进行随机的洗牌，打乱每一列的顺序，确保所有的样本会被随机分成不同的小批次。注意 X 和 Y 需要进行一样的洗牌操作，一一对应，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_3.png" alt=""></p></li><li><p>划分 (Partition)：将训练集划分为每个大小为 mini_batch_size 的小批次。注意总样本数不一定总是能被 mini_batch_size 整除，所以最后一个小批次比 mini_batch_size 要小，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_4.png" alt=""></p></li></ul><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size = <span class="number">64</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将总样本随机划分为许多小批次</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- 存放划分好的(mini_batch_X, mini_batch_Y)的 list</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]<span class="comment"># 总样本数</span></span><br><span class="line">    mini_batches = []<span class="comment"># 存放划分好的最小批</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 第一步：洗牌</span></span><br><span class="line">    permutation = list(np.random.permutation(m))   <span class="comment"># 生成 [0，1，2，...,m-1] 的数组并随机打乱</span></span><br><span class="line">    shuffled_X = X[:, permutation]<span class="comment"># 将打乱的数组作为列标签，使得 X 的每列被随机打乱</span></span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))<span class="comment"># 同上</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第二步：划分</span></span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># math.floor() 取整函数计算完整的 mini_batch 数目</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):<span class="comment"># 注意是从 0 开始，所以下面都是 k+1</span></span><br><span class="line">        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+<span class="number">1</span>)* mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+<span class="number">1</span>)* mini_batch_size]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)<span class="comment"># 组合成一个 tuple</span></span><br><span class="line">        mini_batches.append(mini_batch)<span class="comment"># 将划分好的 tuple 放入 list 中</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 单独处理最后一个数量小于 mini_batch_size 的批次</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        mini_batch_X = shuffled_X[:, num_complete_minibatches*mini_batch_size:]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*mini_batch_size:]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure><h2 id="动量梯度下降"><a href="#动量梯度下降" class="headerlink" title="动量梯度下降"></a>动量梯度下降</h2><p>由于小批量梯度下降也会产生震荡问题，所以我们采用动量算法，考虑之前几次迭代的梯度来减小震荡。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_5.png" alt=""></p><p>蓝色线是每一次梯度的方向，但是每次下降不沿着梯度方向走，而是沿着红线走，它是前几次梯度方向的加权平均值 v ——称之为“速度”。</p><script type="math/tex; mode=display">\begin{cases}v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \\W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}\end{cases}\\\begin{cases}v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \\b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}} \end{cases}</script><h3 id="初始化-v"><a href="#初始化-v" class="headerlink" title="初始化 v"></a>初始化 v</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_velocity</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes the velocity as a python dictionary with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity.</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 神经网络层数</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))   <span class="comment"># v_dW 和 dW 的维度相同</span></span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))   <span class="comment"># v_db 和 db 的维度相同</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><h3 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用动量算法更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- 初始化过的 v</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># 神经网络层数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 v</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure><h2 id="Adam-算法"><a href="#Adam-算法" class="headerlink" title="Adam 算法"></a>Adam 算法</h2><p>Adam 是训练神经网络最有效的算法之一，结合了动量算法和 RMSprop 算法的优点。</p><p>一共三步：</p><ul><li>计算之前梯度的指数加权平均 v，然后其计算偏差修正值 v_correct</li><li>计算之前梯度的平方的指数加权平均 s，然后计算其偏差修正值 s_correct</li><li>用上面的值更新参数</li></ul><p>公式：</p><script type="math/tex; mode=display">\begin{cases}v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \\v^{corrected}_{dW^{[l]}} = \frac{v_{dW^{[l]}}}{1 - (\beta_1)^t} \\s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \\s^{corrected}_{dW^{[l]}} = \frac{s_{dW^{[l]}}}{1 - (\beta_1)^t} \\W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}_{dW^{[l]}}}{\sqrt{s^{corrected}_{dW^{[l]}}} + \varepsilon}\end{cases}</script><h3 id="初始化-v-和-s"><a href="#初始化-v-和-s" class="headerlink" title="初始化 v 和 s"></a>初始化 v 和 s</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span> :</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   初始化 v 和 s，他们的维度都和对应的梯度相同</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters["W" + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters["b" + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class="line"><span class="string">                    v["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class="line"><span class="string">                    s["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    s["db" + str(l)] = ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span> </span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))</span><br><span class="line">        s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))</span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],  parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)].shape[<span class="number">1</span>] ))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure><h3 id="用-v-和-s-更新参数"><a href="#用-v-和-s-更新参数" class="headerlink" title="用 v 和 s 更新参数"></a>用 v 和 s 更新参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate = <span class="number">0.01</span>,beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用 Adam 算法更新参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有参数的字典</span></span><br><span class="line"><span class="string">    grads -- 包含所有参数梯度的字典</span></span><br><span class="line"><span class="string">    v -- 初始化过的 v</span></span><br><span class="line"><span class="string">    s -- 初始化过的 s</span></span><br><span class="line"><span class="string">    t -- 当前的批次</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class="line"><span class="string">    epsilon -- 防止分母为零的超参数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 参数个数              </span></span><br><span class="line">    v_corrected = &#123;&#125;<span class="comment"># 用来存放 v_correct</span></span><br><span class="line">    s_corrected = &#123;&#125;<span class="comment"># 用来存放 s_correct</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 Adam 算法</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment"># 计算 v 值</span></span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算偏差修正过的 v 值</span></span><br><span class="line">        v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span> - beta1 ** t)<span class="comment"># beta ** t 表示 beta 的 t 次幂 </span></span><br><span class="line">        v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span> - beta1 ** t)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 计算 s 值</span></span><br><span class="line">        s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta2) * np.square(grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)])<span class="comment"># np.square() 表示逐元素平方</span></span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span> - beta2) * np.square(grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算偏差修正过的 s 值</span></span><br><span class="line">        s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line">        s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span> - beta2 ** t)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * ( v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (np.sqrt(s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]) + epsilon))<span class="comment"># np.sqrt() 为逐元素开平方</span></span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * ( v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (np.sqrt(s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]) + epsilon))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure><h2 id="比较几种优化算法的区别"><a href="#比较几种优化算法的区别" class="headerlink" title="比较几种优化算法的区别"></a>比较几种优化算法的区别</h2><ul><li>使用普通的批量梯度下降，调用函数：<ul><li>update_parameters_with_gd()</li></ul></li><li>使用动量算法梯度下降，调用函数：<ul><li>initialize_velocity() 和 update_parameters_with_momentum()</li></ul></li><li>使用 Adam 梯度下降，调用函数：<ul><li>initialize_adam() 和 update_parameters_with_adam()</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, layers_dims, optimizer, learning_rate = <span class="number">0.0007</span>, mini_batch_size = <span class="number">64</span>, beta = <span class="number">0.9</span>,beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>, epsilon = <span class="number">1e-8</span>, num_epochs = <span class="number">10000</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    可以使用不同优化函数的 3 层神经网络</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- python list, containing the size of each layer</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    mini_batch_size -- the size of a mini batch</span></span><br><span class="line"><span class="string">    beta -- Momentum hyperparameter</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(layers_dims)             </span><br><span class="line">    costs = []                       </span><br><span class="line">    t = <span class="number">0</span>                           </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化优化函数</span></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># 普通梯度下降没有初始化要求</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">        v = initialize_velocity(parameters)<span class="comment"># 初始化动量算法</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)<span class="comment"># 初始化 Adam 算法</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优化算法循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        </span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size)<span class="comment"># 划分最小批</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不同批次的循环</span></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 从 minibatchs 中取出当前最小批</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 前向传播</span></span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算代价函数</span></span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数，三种方式</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)<span class="comment"># 批量（普通）梯度下降更新参数</span></span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)<span class="comment"># 动量梯度下降更新参数</span></span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># 当前的批次，Adam 算法与 t 有关</span></span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每一千步打印一次代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># 代价函数画图</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'epochs (per 100)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="使用-MBGD"><a href="#使用-MBGD" class="headerlink" title="使用 MBGD"></a>使用 MBGD</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"gd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Gradient Descent optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_6.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_7.png" alt=""></p><h3 id="使用动量算法"><a href="#使用动量算法" class="headerlink" title="使用动量算法"></a>使用动量算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">"momentum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Momentum optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p>结果：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_8.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_9.png" alt=""></p><h3 id="使用-Adam-算法"><a href="#使用-Adam-算法" class="headerlink" title="使用 Adam 算法"></a>使用 Adam 算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Adam optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_10.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_11.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_cdoe_2.2_12.png" alt=""></p><ul><li>动量算法经常会有帮助，但是在小学习率和简单的的数据集中它的影响几乎可以忽略。</li><li>代价函数的震荡是因为某些小批次的数据噪声更多</li><li>Adam 算法比动量算法和 MBGD 表现精确度更高，但是如果给足够的时间，三个算法都会得到很好的精确度，但是这说明 Adam 算法运行更快</li><li>Adam 算法的优点<ul><li>虽然比动量算法和 MBGD 所需的内存更多，但是相对来说所需内存还是较少</li><li>几乎不需要怎么调整它的超参数 $\beta_1,\beta_2,\varepsilon$ ，直接使用默认值，就能得到很好的结果</li></ul></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 优化算法 </tag>
            
            <tag> 梯度下降 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 2 week 2）—— 优化算法的改进</title>
      <link href="/2018/09/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w2/"/>
      <url>/2018/09/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w2/</url>
      <content type="html"><![CDATA[<p>本周主要学习不同的参数优化方式，提高模型的训练速度。</p><h2 id="小批量梯度下降算法-mini-batch-gradient-descent"><a href="#小批量梯度下降算法-mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降算法 (mini-batch gradient descent)"></a>小批量梯度下降算法 (mini-batch gradient descent)</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><p>向量化可以有效率地同时计算 m 个样本，但是当样本数为几百万时，速度依然会很慢，每一次迭代都必须先处理几百万的数据集才能往前一步，所以我们可以使用这个算法进行加速。</p><p>首先我们将训练集划分为一个一个微小的训练集，也就是小批量训练集 (mini-batch)，比如每一个微小训练集只有 1000 个样本：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_2.png" alt=""></p><a id="more"></a><p>我们把第 t 个批次的数据用上标 {t} 表示，例如 $X^{\{t\}},Y^{\{t\}}$ </p><p>$for \quad t = 1,…,5000: (每一步用 (X^{\{t\}},Y^{\{t\}}) 做一次梯度下降)\\ \quad X^{\{t\}} 为输入的前向传播 (1000 个样例)\\ \quad 计算代价函数 J^{\{t\}}\\ \quad 反向传播计算梯度 (只用  X^{\{t\}} 和 Y^{\{t\}})\\ \quad 更新参数 $   </p><p>以上是对训练集的一次迭代，可以得到 5000 次梯度逼近，继续使用另一个 for 循环进行多次迭代。</p><h3 id="理解算法"><a href="#理解算法" class="headerlink" title="理解算法"></a>理解算法</h3><p>小批量梯度下降 (mini-batch gradient descent) 和批量梯度下降 (batch gradient descent) 中代价函数的变化如下图：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_3.png" alt=""></p><p>梯度下降有三种类型，根据每个批次的样本数不同划分：</p><ul><li>若 minibatch 尺寸 = m，称之为：批量梯度下降 (BGD) ( X{t} , Y{t} ) = ( X , Y ) </li><li>若 minibatch 尺寸 = 1，称之为：随机梯度下降 (SGD)  ( X{t}, Y{t} ) = ( X{1}, Y{1} ), ( X{2}, Y{2} ) ,…, ( X{m}, Y{m} ) </li><li>若 minibatch 尺寸介于 1 到 m 之间：小批量梯度下降 (MBGD)</li></ul><p>在实际中一般采用第三种，因为第一种和第二种都有缺点：</p><ul><li>随机梯度下降失去了使用向量加速的机会，而且会发生震荡，但是速度比批量梯度下降快</li><li>批量梯度下降的缺点是如果训练集太大，在每次的迭代上要花费太长的时间</li><li>小批量梯度下降，则既可以利用到向量化，又可以不用等待整个训练集都遍历一遍就可以进行梯度下降，而且震荡要比随机梯度下降要小，是最好的方案</li></ul><p>下图展示了不同梯度下降的过程，蓝线是批量梯度下降，绿线是小批量梯度下降，紫线是随机梯度下降。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_4.png" alt=""></p><h3 id="如何选择-mini-batch-的-size"><a href="#如何选择-mini-batch-的-size" class="headerlink" title="如何选择 mini-batch 的 size"></a>如何选择 mini-batch 的 size</h3><p>最小批包含的样本数是一个可调的超参数，用如下原则来确定：</p><ul><li>如果训练集很小（m &lt; 2000）：直接使用批量梯度下降</li><li>如果训练集很大：一般选择 <strong>64 ～ 512</strong> 作为每个批次的大小，由于计算机内存的布局和访问方式，把它设为 2 的幂数代码运行会更快，故一般选择 <strong>64，126，256，512</strong> 这几个值</li><li>确保 minibatch 的 X{t} 和 Y{t} 能够放进 CPU 和 GPU 的内存</li></ul><h2 id="指数加权平均-Exponentially-weighted-averages"><a href="#指数加权平均-Exponentially-weighted-averages" class="headerlink" title="指数加权平均 (Exponentially weighted averages)"></a>指数加权平均 (Exponentially weighted averages)</h2><h3 id="什么是指数加权平均"><a href="#什么是指数加权平均" class="headerlink" title="什么是指数加权平均"></a>什么是指数加权平均</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_5.png" alt=""></p><p>上图表示的是伦敦一年 365 天气温的散点图，第 t 天的气温值为 $\theta_t$，给出如下公式计算<strong>指数加权平均</strong> (统计学中称为“指数加权滑动平均”)：</p><script type="math/tex; mode=display">v_t=\beta v_{t-1}+(1-\beta)\theta_t</script><p>其中 $v_t$ 可以近似认为是前 $\frac{1}{1-\beta}$ 天的气温平均值，例如：</p><ul><li>若 $\beta = 0.9$ 则 $v_t$ 可认为计算的是前 10 天的平均气温值，如红线所示</li><li>若 $\beta = 0.98$ 则 $v_t$ 可认为计算的是前 50 天的平均气温值，如绿线所示</li><li>若 $\beta = 0.5$ 则 $v_t$ 可认为计算的是前 2 天的平均气温值，如黄线所示</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_6.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_7.png" alt=""></p><h3 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_8.png" alt=""></p><h3 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h3><script type="math/tex; mode=display">v_0=0\\v_1=\beta v_0+(1-\beta)\theta_1\\v_2=\beta v_1+(1-\beta)\theta_2\\v_3=\beta v_2+(1-\beta)\theta_3\\...</script><p>伪代码：</p><p>$v_\theta=0\\repeat\{\\ \quad \quad \quad get \ next \ \theta_t \\  \quad \quad \quad v_\theta:=\beta v_\theta+(1-\beta)\theta_t\\ \quad \quad \quad \}$ </p><h3 id="小技巧：偏差修正"><a href="#小技巧：偏差修正" class="headerlink" title="小技巧：偏差修正"></a>小技巧：偏差修正</h3><p>当 $\beta=0.98$ 时，我们预计会得到绿色那条线： </p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_6.png" alt=""></p><p>但是实际上我们会得到下图紫色那条线：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_9.png" alt=""></p><p>根据公式 $v_t=\beta v_{t-1}+(1-\beta)\theta_t$ ：<br>$v_0=0\\v1=0.98v_0+0.02\theta_1=0.02\theta_1\\v_2=0.98v_1+0.02\theta_2=0.98*0.02\theta_1+0.02\theta_2$</p><p>可以看到 $v_2$ 的值将远小于 $\theta_1$ 和 $\theta_2$ ，造成估计的偏差，有一种方法可以在估算的初期将偏差修正：</p><script type="math/tex; mode=display">v_t=\frac{v_t}{1-\beta^t}</script><p>$t=2:1-\beta ^t=1-0.98^2=0.0396\\\frac{v_2}{0.0396}=\frac{0.0196 \theta_1 + 0.02\theta_2}{0.0396} \rightarrow \theta_1 和 \theta_2的加权平均数，从而消除了偏差$  </p><p>当 $t​$ 逐渐增大，$\beta^t​$ 的值将趋于 0，修正偏差基本无影响，所以绿线和紫线在后半段几乎重合。</p><h2 id="动量梯度下降算法-Gradient-descent-with-momentum"><a href="#动量梯度下降算法-Gradient-descent-with-momentum" class="headerlink" title="动量梯度下降算法 (Gradient descent with momentum)"></a>动量梯度下降算法 (Gradient descent with momentum)</h2><h3 id="普通梯度下降的困境"><a href="#普通梯度下降的困境" class="headerlink" title="普通梯度下降的困境"></a>普通梯度下降的困境</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_10.png" alt=""></p><ul><li>红点是梯度下降最小点，对于普通的梯度下降，路径如蓝线所示，朝着最小值缓慢地震荡前进，这种震荡会减慢学习的速度</li><li>不能使用太大的学习率，否则会产生超调，产生发散，无法收敛到最小点</li><li>在上下方向希望学习慢一点，因为不希望出现那些震荡，水平方向希望学习快一点，因为希望快速从左到右</li></ul><h3 id="动量梯度下降步骤"><a href="#动量梯度下降步骤" class="headerlink" title="动量梯度下降步骤"></a>动量梯度下降步骤</h3><p>$v_{dW}=0(维度和dW相同)\\v_{db}=0(维度和db相同)\\在第 t 次迭代：\\ \quad \quad 计算当前小批次的 dW，db \\ \quad \quad v_{dW}=\beta v_{dW}+(1-\beta)dW \\ \quad \quad v_{db}=\beta v_{db}+(1-\beta)db \\ \quad \quad W:=W-\alpha v_{dW},b:=b-\alpha v_{db} $    </p><ul><li>超参数 $\alpha, \beta$ ， 其中 $\beta = 0.9$ 效果最好，相当于计算前十次迭代的梯度平均值</li><li>一般不需要对 $v_{dW},v_{db}$ 进行偏差修正 </li><li>另一个版本为 $v_{dW}=\beta v_{dW}+dW,v_{db}=\beta v_{db}+db$ 也是正确的，但吴恩达不推荐</li></ul><h3 id="算法解释"><a href="#算法解释" class="headerlink" title="算法解释"></a>算法解释</h3><p>解释：这些操作可以让梯度下降变得平滑，因为 $v_{dW}$ 计算的是这一次迭代之前的若干次的梯度的平均值，震荡的路径在纵轴方向是 $\nearrow \searrow \nearrow \searrow$ ，这些梯度一正一负，相互抵消，在纵轴方向的平均值趋于 0 ，减弱了震荡，而横轴方向是 $\rightarrow \rightarrow\rightarrow\rightarrow$ ，都指向了右边，所以横轴方向的平均值依然很大。</p><p>形象解释：把代价函数图像想象成一个碗，有一个球滑向碗的最低点，把导数项 dW 和 db 想象成球滚下时的加速度，而把动量项 $v_{dW},v_{db}$ 想象成球的速度。</p><p>导数项给了球一个加速度，然后球向下滚，因为有加速度，所以它滚得越来越快，因为$\beta$ 是一个略小于1的数，可以把它看作摩擦力，让球不至于无限加速下去。与梯度下降中每一步都独立于之前步骤所不同的是，现在你的球可以向下滚并获得动量，沿碗向下加速并获得动量。</p><h2 id="均方根传递梯度下降算法——RMSprop-算法-Root-Mean-Square-prop"><a href="#均方根传递梯度下降算法——RMSprop-算法-Root-Mean-Square-prop" class="headerlink" title="均方根传递梯度下降算法——RMSprop 算法 (Root Mean Square prop)"></a>均方根传递梯度下降算法——RMSprop 算法 (Root Mean Square prop)</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_11.png" alt=""></p><p>仍然是这个震荡的问题，为了便于理解，假设只有两个参数 w 和 b，我们可以看到要减小震荡，必须使得纵轴即参数 b 的学习速度很慢，而横轴即参数 w 的学习速度很快，我们仍然使用指数加权平均的知识来实现梯度下降。</p><h3 id="RMSprop-实现步骤"><a href="#RMSprop-实现步骤" class="headerlink" title="RMSprop 实现步骤"></a>RMSprop 实现步骤</h3><p>$ S_{dW}=0(维度和dW相同)\\S_{db}=0(维度和db相同)\\在第 t 次迭代：\\ \quad \quad 计算当前小批次的 dW，db \\ \quad \quad S_{dW}=\beta’ S_{dW}+(1-\beta’)(dW)^2 \\ \quad \quad S_{db}=\beta’ S_{db}+(1-\beta’)(db)^2 \\ \quad \quad W:=W-\alpha \frac{dW}{\sqrt{S_{dW}}+\varepsilon },b:=b-\alpha \frac{db}{\sqrt{S_{db}}+\varepsilon } $ </p><ul><li>$\beta’$ 是为了和动量算法中的  $\beta$  相区分</li><li>$(dW)^2$ 是逐元素平方</li><li>$\varepsilon$ 是一个非常小的数比如 $10^{-8}$ 确保分母不为零</li></ul><h3 id="算法理解"><a href="#算法理解" class="headerlink" title="算法理解"></a>算法理解</h3><p>如上图所示，我们希望纵轴参数 b 的梯度很小，学习更慢，减小震荡，希望横轴参数 w 的梯度很大，学习更快，也就是希望作为分母的 $\sqrt{S_{dW}}$ 很小，而 $\sqrt{S_{db}}$ 很大。由于纵轴方向上是震荡的，路径更加偏向 b 轴一些，所以 $(db)^2$ 会相对更大，从而$\sqrt{S_{db}}$ 会相对更大，而 $(dW)^2$ 会相对更小，从而 $\sqrt{S_{dW}}$ 相对更小。</p><h2 id="自适应矩估计梯度下降算法——Adam-算法-Adaptive-Moment-Estimation"><a href="#自适应矩估计梯度下降算法——Adam-算法-Adaptive-Moment-Estimation" class="headerlink" title="自适应矩估计梯度下降算法——Adam 算法 (Adaptive Moment Estimation)"></a>自适应矩估计梯度下降算法——Adam 算法 (Adaptive Moment Estimation)</h2><p>这是一种结合了动量算法和 RMSprop 两者优点的算法，被广泛使用且已经被证明在很多不同种类的神经网络架构中都十分有效。</p><h3 id="Adam-实现步骤"><a href="#Adam-实现步骤" class="headerlink" title="Adam 实现步骤"></a>Adam 实现步骤</h3><p>$v_{dW}=0,S_{dW}=0,v_{db}=0,S_{db}=0   \leftarrow  初始化\\ 在第 t 次迭代：\\ \quad  计算当前小批次的 dW，db \\   \quad v_{dW}=\beta_1 v_{dW}+(1-\beta_1)dW  ，v_{db}=\beta_1 v_{db}+(1-\beta_1)db \leftarrow  动量算法\\ \quad S_{dW}=\beta_2 S_{dW}+(1-\beta_2)(dW)^2，S_{db}=\beta_2 S_{db}+(1-\beta_2)(db)^2 \leftarrow RMSprop\\  \quad v_{dW}^{correct}=\frac{v_{dW}}{1-\beta_1^t}，v_{db}^{correct}=\frac{v_{db}}{1-\beta_1^t}   \leftarrow v 的偏差修正   \\ \quad  S_{dW}^{correct}=\frac{S_{dW}}{1-\beta_2^t}，S_{db}^{correct}=\frac{v_{db}}{1-\beta_2^t}     \leftarrow S 的偏差修正   \\ \quad W:=W-\alpha \frac{v_{dW}^{correct}}{\sqrt{S_{dW}^{correct}}+\varepsilon },b:=b-\alpha \frac{v_{db}^{correct}}{\sqrt{S_{db}^{correct}}+\varepsilon }   \leftarrow 更新参数$   </p><h3 id="超参数的默认值"><a href="#超参数的默认值" class="headerlink" title="超参数的默认值"></a>超参数的默认值</h3><ul><li>学习率 $\alpha$ ：尝试不同值比较效果</li><li>$\beta_1：0.9$   </li><li>$\beta_2:0.999$ </li><li>$\varepsilon:10^{-8}$    </li></ul><blockquote><p>参数为 $\beta_1$ 的梯度值的指数加权平均 $v$ 称为第一阶的矩，参数为 $\beta_2$ 的梯度平方值的指数加权平均 $S$ 被称为第二阶的矩</p></blockquote><h2 id="学习率衰减-learning-rate-decay"><a href="#学习率衰减-learning-rate-decay" class="headerlink" title="学习率衰减 (learning rate decay)"></a>学习率衰减 (learning rate decay)</h2><h3 id="为什么需要学习率衰减"><a href="#为什么需要学习率衰减" class="headerlink" title="为什么需要学习率衰减"></a>为什么需要学习率衰减</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_12.png" alt=""></p><p>假如我们采用固定步长，如蓝色路径所示，那么它会逐步向最小值靠近，但不会完全收敛到这点，所以算法会在最小值周围浮动，但是却永远不会真正收敛。</p><p>如果慢慢地衰减学习率，刚开始学习率取值较大，学习速度较快，但随着学习率的的逐渐衰减，步长也会逐渐减小，所以最后将围绕着离极值点更近的的区域摆动，不会继续漂流远离。</p><h3 id="如何实现-1"><a href="#如何实现-1" class="headerlink" title="如何实现"></a>如何实现</h3><p>1 次迭代（1 epoch）：遍历完一次数据集，完成一次梯度更新</p><p>学习率衰减就是使学习率随着迭代次数 epoch _num 单调递减，比如下面这个公式：</p><script type="math/tex; mode=display">\alpha = \frac{1}{1+decay\_rate*epoch\_num}\alpha_0</script><ul><li>decay_rate 为衰减率，是一个超参数</li><li>epoch_num 是迭代次数</li><li>$\alpha_0$ 是初始学习率，也是一个超参数</li></ul><p>代一些具体值看看学习率衰减的情况：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_13.png" alt=""></p><p>还有一些其他的学习率衰减公式，比如下面这个：</p><script type="math/tex; mode=display">\alpha = 0.95 ^{epoch\_num}\alpha_0</script><ul><li>使得学习率呈指数级下降</li></ul><p>还有：</p><script type="math/tex; mode=display">\alpha = \frac{k}{\sqrt{epoch\_num}}\alpha_0 \ \ 或者\frac{k}{\sqrt{t}}\alpha_0</script><p>还有采用离散阶梯函数的学习率衰减，比如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_14.png" alt=""></p><p>有时候人们还会使用手动的梯度衰减，但是比较少。</p><h2 id="局部最优问题-the-problem-of-local-optima"><a href="#局部最优问题-the-problem-of-local-optima" class="headerlink" title="局部最优问题 (the problem of local optima)"></a>局部最优问题 (the problem of local optima)</h2><p>深度学习早期，人们担心优化算法会陷入局部最优之中。</p><h3 id="什么是局部最优"><a href="#什么是局部最优" class="headerlink" title="什么是局部最优"></a>什么是局部最优</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_15.png" alt=""></p><p>假设上图中是参数 w1 和 w2关于代价函数的三维图像，人们担心在对这些参数进行优化的时候会优化到图中蓝点所在的位置，这些值只是在“谷底”，而不是真正的最小值，是局部最优。</p><h3 id="为什么不用担心局部最优问题"><a href="#为什么不用担心局部最优问题" class="headerlink" title="为什么不用担心局部最优问题"></a>为什么不用担心局部最优问题</h3><p>在训练一个神经网络时，代价函数中大部分梯度为零的点并不是上图中的局部最优，而是<strong>鞍点 (saddle point)</strong>，这种点周围的函数从一个方向看是凸函数，从另一个是凹函数，就像马鞍一样，如下图所示。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_16.png" alt=""></p><p>如果一个点要是局部最优点，那么这个点周围的函数需要在所有方向看都是凸函数或者都是凹函数，但是由于神经网络的参数有上万乃至上百万个，如果某个点要成为局部最优，那么在所有的上百万个维度中函数的方向都得是一样的凹函数或者凸函数，这件事发生的概率非常非常低。更有可能碰到的是鞍点，某些方向的曲线向上弯曲，某些方向的向下弯曲，并非所有的曲线都向上或者向下弯曲，所以在高维空间中，不用担心发生局部最优问题！</p><h3 id="停滞区问题-Plateaus"><a href="#停滞区问题-Plateaus" class="headerlink" title="停滞区问题 (Plateaus)"></a>停滞区问题 (Plateaus)</h3><p>停滞区：导数长时间接近于零的一段区域</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_17.png" alt=""></p><p>当沿着马鞍面向下移动，移动到鞍点，但鞍点不是我们需要的最小值点，需要继续往下移动。但是这个地方梯度为零或者接近于零，曲面很平，需要花费很长时间缓慢地在这个停滞区内找到这个点而不能继续往下，当左侧或右侧有随机扰动，才能继续往下走，这会让学习速度变得非常慢。</p><p>解决办法：采用 动量算法、RMSprop算法、Adam算法等可以改善这个情况。</p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络优化 </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>《Beck》观后感</title>
      <link href="/2018/09/01/beck%E8%A7%82%E5%90%8E%E6%84%9F/"/>
      <url>/2018/09/01/beck%E8%A7%82%E5%90%8E%E6%84%9F/</url>
      <content type="html"><![CDATA[<p>　　我终于找到我的本命番了，它的名字叫《Beck》，为啥我知道这是本命番呢，因为这是第一部我看到最后迟迟舍不得看完的番，一共２６集，每天睡前看几集，看了有十几天，不敢一次看完，因为看完了我的梦就碎了，是的，这是一个追梦的故事，我相信任何热爱摇滚，热爱音乐，热爱吉他，渴望或者已经开始玩乐队的人，都会对这部番有着深深的感动和发自内心的共鸣。因为这部番，我拿起了好久没碰的木琴，并开始挑选我的第一把电吉他……</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_topimg.jpg" alt=""></p><a id="more"></a><p>　　我是怎么发现这部如此小众的动漫的呢，是群里一个新加的朋友告诉我的，这个群是我建的摇滚群，只有我一个人（笑），本来准备将群解散，在前段时间突然加进来一个人。我本以为和以前那些加入者一样，只是些悦耳旋律的伪摇滚迷，没想到谈论一番，此人对摇滚很有自己的理解，是我所理解的“硬核摇滚迷”，最让我惊讶的是，此人居然会变听歌边做笔记，我瞬间对他刮目相看。虽然我自己对摇滚乐理解也不深，但是感觉找到了能与之一谈的对摇滚乐有较强鉴赏能力的人。他给我看了他的四张专辑，其中两张都是名为《beck》的专辑，我以为是某个乐队名，结果他告诉我这是一部动漫的名字，并且强烈建议我去看，本来是抱着试试的心态去看，结果发现了自己的本命番哈哈。这人的昵称叫“Lucille”，就是动漫里那把传奇吉他的名字。</p><p>　　这部番是少见的乐队题材动漫，讲的是一个日本少年从只听流行乐的普通少年因为喜欢的女孩爱上摇滚开始弹吉他，加入乐队，最后乐队开始全美巡演的故事。这个少年叫小雄，在接触摇滚之前是你能想象的最普通的少年，偶然的机会碰到自己好久不见的童年伙伴小泉，她是一个喜欢摇滚的女生，有一次和她出去听乐队 live，结束后她和别人激烈地谈论着自己喜欢的乐队的 The dying breed，不听摇滚的小雄完全无法一起谈论，这让他非常低落，后来小泉给了他一张卷子DB的专辑，从此小雄打开了新世界的大门……不得不说，小雄刚开始对摇滚有点爱屋及乌的感觉，也许每个人都会因为迷恋一个人而爱上另一样东西吧。如果说小泉是带小雄进入摇滚世界的启蒙者，那么龙介就是带小雄进入乐队世界的启蒙者，他因为被龙介的狗咬伤而认识龙介，他给了小雄第一把吉他，然后发生了一段微妙而真实的小插曲。刚拿到吉他的小雄，也许是想要装逼，直接拿着吉他而不放进琴箱就上街溜达，结果一个平地摔把吉他摔断了，导致龙介暂时和他绝交，心灰意冷的小雄走在街上，好心的斋藤先生路过，决定帮他修吉他，代价是在他店里打工，没想到斋藤先生是个老摇滚迷，所以带小雄学吉他，又成了小雄吉他的启蒙者。小雄刚学吉他想要让全世界知道的心理，学几下就嫌手指疼放弃的描写，种种细节，对我们这些玩吉他的人来说真的是再亲切不过了，关于情节，实在非常的有趣且跌宕起伏，真想一口气把它们都详细地叙述出来！可惜篇幅有限，只能靠你们自己去看啦。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_3.png" alt="乐队成员"></p><p>　　这部动漫对人物的塑造和对乐队的描绘真的是入木三分。懦弱敏感而胆小的小雄，面对霸凌任由欺负，却在最后敢于面对上万的观众单独上台，乐队的主心骨主音吉他手龙介经常一副冷漠脸，可他的心里却隐藏着比谁都大的野心和热情，低调的眯眯眼鼓手阿樱（眯眯眼都是怪物hhh），贝斯技术高超而稳重的贝斯手阿平以及集正直、幽默、直率、满腔热血于一体的极富感染力的主唱千叶。龙介把这么一群人组合到一起，用他的狗的名字成立一个乐队——beck。这部动漫还有一个我印象很深的人物是斋藤先生，他四十多岁还没有结婚，独自生活经营着一家快要倒闭的纸店，他好色、猥琐、自私但是善良且忠厚，就是这个一个猥琐穷困的中年男人，居然是一个被摇滚拯救人生的摇滚老炮，家里收藏了几把吉他，虽然每天在外面跑业务低声下气，但是只要一有空，就进入摇滚的世界开始享受，他有一只鸟，只对好听的音乐欢呼，斋藤先生的梦想就是有一天能让它欢呼。我真的被这个角色感动到了，因为从他身上，我看到了一种可能性，在生活的重压下就算是一个四十岁的中年人都能享受摇滚的可能性，希望自己像他一样，不管到多老，都能享受摇滚，享受音乐。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_2.png" alt="斋藤先生"></p><p>　　这部动漫另一个有趣之处在于，虽然里面所有乐队和人都是虚构的，但是致敬了许多经典的摇滚乐队，比如刚开始提到最多的乐队The dying breed，我感觉对应现实中就是 nirvana，因为DB的吉他手艾迪的形象跟科本真的是太像了，金色长发，传奇吉他手，另外还有斋藤先生最爱的乐队rocket boy，风格完全和披头士一模一样，另外里面还经常提到滚石，齐柏林飞艇，地下丝绒等许多乐队，还有衬衫上专辑上都能看到许多经典乐队的影子，详细的可以见知乎有些大神的分析，这些细节对于摇滚迷来说真的不能再亲切了。</p><p>　　既然是音乐番，那么怎么能少的了音乐。这部番的音乐制作非常精良，我最喜欢的是小雄妙手偶得的那首《spit out》，不仅打动了动漫里音乐节的一万多人，更打动了屏幕外的我，但是最让人印象深刻的应该是剧里DB的《moon on the water》，这首歌见证了小雄的成长，刚开始觉得这首歌不好听，但是随着剧情深入，越听越有味道，尤其是月色下小雄和真帆在游泳池里的场景，让人无法忘怀，引用网易云一句评论：“真帆在泳池唱这首歌，月光皎洁，风吹过草地，水面泛起波纹，那一瞬间真的恍若天籁，又真实简单的让人动容。”到 youtube 上一查，果然许多人在翻唱这首歌，看完这部番的那天，我把这首歌听了一整天，然后花了一天学会弹，唱着歌词，满脑子都是真帆的样子。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_4.jpg" alt="moon on the water"></p><p>　　之所以迟迟不舍得看完，是因为这部动漫构建了一个梦，构建了一个普通少年的摇滚梦，它细腻的描写和真实的情感，给这个梦添上了翔实的细节，看完了，美好的梦就没了。不得不说，小雄是我所希望成为的那一种人，小雄的人生是我想过的人生，有摇滚，有朋友，有吉他，有乐队，但对我来说这些只是一个飘渺的幻影，因为我并不知道我该怎么样获得这种生活，而这部动漫给我描述另一个和我一样普通的少年如何走上这条路的过程，我将自己，也许每个热爱摇滚的青年都会这么做，带入小雄这个角色，为乐队的繁荣而开心，为乐队的解散而难受，所以这部番才能带给我如此多的感动。乐队，一个如此遥远而熟悉的名词啊，总有一天，我要像小雄一样，组一个最强的乐队！</p><p>　　不知不觉写了好多，但是我觉得我拙劣的文笔还是无法表达出我对这部作品有多喜爱。吉他在考研之后好久没碰，看完这部番，我又重新拿起了吉他，开始重新出发，谢谢你 beck，我又找回了我最初的梦。也许我不能像小雄一样碰到龙介，但是我希望成为自己的龙介。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_5.png" alt="谢谢你小雄"></p>]]></content>
      
      <categories>
          
          <category> 读书观影笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 摇滚 </tag>
            
            <tag> 音乐 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 1）</title>
      <link href="/2018/09/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c2w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/09/01/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c2w1%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>这次编程作业对比了三种不同的初始化方法的不同，三种方法分别是“零初始化”、“随机初始化”、“He 初始化”。</p><p>这是所用的数据，我们要将红点和蓝点分类：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_1.png" alt=""></p><a id="more"></a><h3 id="零初始化"><a href="#零初始化" class="headerlink" title="零初始化"></a>零初始化</h3><p>也就是将参数 w 和 b 全都初始化为 0，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l],layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>最后的代价函数随迭代次数的变化如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_2.png" alt=""></p><p>训练集的精确度为 0.5，测试集的精确度为 0.5</p><p>分类结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_3.png" alt=""></p><p>我们可以发现将所有参数初始化为零无法分类任何数据，因为无法打破对称性，这意味着每层的每个神经元都在学习一样的东西。</p><h3 id="随机初始化（为很大的值）"><a href="#随机初始化（为很大的值）" class="headerlink" title="随机初始化（为很大的值）"></a>随机初始化（为很大的值）</h3><p>将权重矩阵随机地初始化为很大的值（×10），偏差向量继续初始化为零，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)  </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*<span class="number">10</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>代价函数的变化如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_4.png" alt=""></p><p>训练集精确度为 0.83，测试集为 0.86</p><p>分类的结果为：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_5.png" alt=""></p><p>分析：</p><ul><li>代价函数开始的值很高，是因为用很大的随机值初始化权重会使得最后的激活（sigmoid）输出值 $a^{[L]}$ 非常接近 0 或者 1，代价函数公式为$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L] (i)}\right) \large{)} \small$ ，当 $a^{<a href="i">L</a>}\approx0$ 时，$log(a^{<a href="i">L</a>})=log(0)\rightarrow$ 无穷大</li><li>不好的初始化可能导致梯度消失/爆炸，这会减慢优化算法的速度</li><li>如果训练上面的的网络更长时间，可以得到更好的结果，但是用大随机值初始化会减慢优化的速度</li></ul><h3 id="He初始化"><a href="#He初始化" class="headerlink" title="He初始化"></a>He初始化</h3><p>这是用某个人名命名的初始化，与上面的随机初始化相似，只是在末尾不是乘以 10 而是 $\sqrt{\frac{2}{n^{[l-1]}}}$ ，这个推荐用来初始化包含 relu 激活函数的层，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span> </span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/layers_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>代价函数图像如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_6.png" alt=""></p><p>训练集上的精确度达到了 0.99，测试集上的精确度达到了 0.96 </p><p>分类的结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_7.png" alt=""></p><p>分析：我们可以看到 He 初始化在很少的迭代次数上就将蓝点和红点分类得很好</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul><li>不同的初始化有不同的结果</li><li>随机初始化用来打破权重对称确保不同的隐藏层能学习到不同的东西</li><li>不要把任何值初始化得太大</li><li>对于有 relu 激活函数的网络 He 初始化非常有效</li></ul><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>本次编程作业将会学到如何在深度学习模型中运用正则化。</p><h3 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> compute_cost, predict, forward_propagation, backward_propagation, update_parameters</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y, test_X, test_Y = load_2D_dataset()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_8.png" alt=""></p><h3 id="非正则化模型"><a href="#非正则化模型" class="headerlink" title="非正则化模型"></a>非正则化模型</h3><p>将正则化系数 lambd 设为 0，将 keep_prob 设为 1，模型如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, print_cost = True, lambd = <span class="number">0</span>, keep_prob = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个三层的神经网络: 线性-&gt;RELU-&gt;线性-&gt;RELU-&gt;线性-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- If True, print the cost every 10000 iterations</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learned by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            <span class="comment"># 记录代价函数值</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                        <span class="comment"># 样本总数</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="keyword">if</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 代价函数</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span>:</span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        <span class="keyword">assert</span>(lambd==<span class="number">0</span> <span class="keyword">or</span> keep_prob==<span class="number">1</span>)    <span class="comment"># 可以同时使用 L2 正则化和 dropout，但是这个例子只使用两者之一</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span> <span class="keyword">and</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        <span class="keyword">elif</span> lambd != <span class="number">0</span>:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每一万步打印代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 画出代价函数点图</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (x1,000)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>我们先不使用正则化试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the training set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p>结果是：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_9.png" alt=""></p><p>在测试集上的精确度为 0.91，而在训练集上的有 0.95，打印出分类图像看看：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_10.png" alt=""></p><p>很明显，分类器将一些训练集中的噪声学习进去了，发生了过拟合，接下来用正则化试试。</p><h3 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h3><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>代价函数为：</p><script type="math/tex; mode=display">J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)}</script><p>L2 正则化是在原本代价函数的基础上加上一个正则化项：</p><script type="math/tex; mode=display">J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost}</script><p>计算带有正则化项的代价函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算带有 L2 正则化项的代价函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing parameters of the model</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - value of the regularized loss function (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) <span class="comment"># 没加正则化项的代价函数</span></span><br><span class="line">    </span><br><span class="line">    L2_regularization_cost = (lambd/(<span class="number">2</span>*m))*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))<span class="comment"># np.square() 用来平方</span></span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p>由于代价函数变了，所以反向传播计算某个参数的梯度也要加上正则化项对它的梯度：$\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   用带了正则化项的代价函数计算反向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- 前向传播缓存</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y<span class="comment"># 反向传播初始化</span></span><br><span class="line">    </span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + (lambd / m * W3)<span class="comment"># (lambd / m * W3) 是加上的正则化项梯度</span></span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">   </span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) + (lambd / m * W2)<span class="comment"># 多加了额外项</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) + (lambd / m * W1)<span class="comment"># 多加了额外项</span></span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>现在将正则化系数 lambd 设为 0.7 看看效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_11.png" alt=""></p><p>测试集精确度提高到了 0.93，打印分类图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with L2-regularization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_12.png" alt=""></p><p>可以看到噪点已经没有被学习进去。</p><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><ul><li>正则化参数 $\lambda$ 是一个可以在开发集上调整的超参数</li><li>L2 正则化让分类的边界更加平滑，但是如果正则化参数太大，则很可能导致“过平滑”，即变成一根直线，造成很大的偏差</li></ul><h4 id="L2-正则化的原理"><a href="#L2-正则化的原理" class="headerlink" title="L2 正则化的原理"></a>L2 正则化的原理</h4><p>L2 正则化依赖于一个假设，即权重更小的模型比权重更大的模型更简单，所以，通过在代价函数里<strong>惩罚</strong>权重的平方值，驱使所有的权重值变得更小，因为如果你有高权重值，那么代价函数就会变得非常大！这生成了一个更加平滑的模型，在模型中输出改变得比输入更慢。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>代价函数计算：正则化项应该加进代价函数中</li><li>反向传播：在代价函数对权重的梯度中应该加入正则化项对其的梯度</li><li>权重最后被驱使变得更小：权重衰减</li></ul><h3 id="dropout-正则化"><a href="#dropout-正则化" class="headerlink" title="dropout 正则化"></a>dropout 正则化</h3><h4 id="dropout-技术的原理"><a href="#dropout-技术的原理" class="headerlink" title="dropout 技术的原理"></a>dropout 技术的原理</h4><p>dropout 正则化在每次迭代中丢弃一些结点，也就是将这些结点的激活值变成零，每个结点被保留的概率是 keep_prob，被丢弃的结点在整个这次迭代过程中都不会出现。</p><p>当你丢弃某些神经元时，实际上改变了模型的结构，每一次迭代，你都在训练不同的模型，而这些模型是原有模型的子集。使用 dropout 让神经元们对某个特定的神经元的激活不再那么敏感，因为它随时可能会被丢弃。</p><h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><p>首先在前向传播中实现 dropout，假设是第 l 层，一共有如下四步：</p><ul><li><p>创造一个和 $A^{[l]}$ 形状一样的筛选矩阵，她每个元素取 1 的概率是 keep_prob</p><ul><li>先创造一个值在 0 到 1 之间的随机矩阵： D[i] = np.random.rand(A[i].shape[0],A[i].shape[1])，其中 np.random.rand() 创造的随机值范围为 (0,1)</li><li>对这个随机矩阵设置门槛，小于它的值取 1，否则取 0：D[i] &lt; keep_prob</li></ul></li><li><p>将 l 层的激活值矩阵逐元素乘以筛选矩阵，完成 dropout：A[i] = A[i] * D[i]</p></li><li>为了补偿丢弃的值，将上一步的结果处以 keep_prob：A[i] = A[i] / keep_prob</li></ul><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现带 dropout 的前向传播: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (20, 2)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (20, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 20)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span></span><br><span class="line"><span class="string">    cache -- tuple, information stored for computing the backward propagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取出初始化后的参数</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 第一层的 dropout</span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>])<span class="comment"># 第一步 1: 初始化矩阵 D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D1 = D1 &lt; keep_prob<span class="comment"># 第二步: 把 D1 的值变成 0 或 1 (用 keep_prob 作为门槛)得到筛选矩阵</span></span><br><span class="line">    A1 = A1 * D1<span class="comment"># 第三步: 乘上筛选矩阵</span></span><br><span class="line">    A1 = A1 / keep_prob <span class="comment"># 第四步：补偿没有被丢弃的结点的值</span></span><br><span class="line">  </span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># 第二层的 dropout</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])<span class="comment"># 第一步 1: 初始化矩阵 D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D2 = D2 &lt; keep_prob<span class="comment"># 第二步: 把 D1 的值变成 0 或 1 (用 keep_prob 作为门槛)得到筛选矩阵</span></span><br><span class="line">    A2 = A2 * D2<span class="comment"># 第三步: 乘上筛选矩阵</span></span><br><span class="line">    A2 = A2 / keep_prob <span class="comment"># 第四步：补偿没有被丢弃的结点的值</span></span><br><span class="line">   </span><br><span class="line">    Z3 = np.dot(W3, A2) + b3<span class="comment"># 输出层不用 dropout</span></span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)<span class="comment"># 切记将每层的筛选矩阵缓存</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure><p>然后我们在反向传播中使用 dropout，一共两步：</p><ul><li>将每层的筛选矩阵 D[i] 从缓存中取出，将 dA[i] 也乘上筛选矩阵，因为一个结点被丢弃后该结点的梯度值也归零</li><li>由于 A[i] 除以了 keep_prob，它对应的 dA[i] 也应该除以 keep_prob 来进行补偿</li></ul><p>反向传播函数代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现加了 dropout 的反向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_with_dropout()</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y<span class="comment"># 反向传播初始化</span></span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第二层的筛选</span></span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dA2 = dA2 * D2<span class="comment"># 第一步: 将 dA2 的值乘上筛选矩阵</span></span><br><span class="line">    dA2 = dA2 / keep_prob<span class="comment"># 第二步: 补偿没被丢弃的 dA2 的值</span></span><br><span class="line"></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第一层筛选</span></span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dA1 = dA1 * D1<span class="comment">#第一步: 将 dA1 的值乘上筛选矩阵</span></span><br><span class="line">    dA1 = dA1 / keep_prob<span class="comment"># 第二步: 补偿没被丢弃的 dA1 的值</span></span><br><span class="line"></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>现在运行使用了 dropout 的模式试试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, keep_prob = <span class="number">0.86</span>, learning_rate = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><blockquote><p>注意：不要在测试过程中使用 dropout！！</p></blockquote><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_13.png" alt=""></p><p>测试集精度提高到了 0.95！画图看看分类的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with dropout"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_14.png" alt=""></p><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><ul><li>dropout 是一门正则化技术</li><li>只在训练时使用 dropout，不要在测试时使用</li><li>在前向和反向传播中都要同时使用 dropout </li><li>在前向传播和反向传播中都要记得进行值的补偿，即除以 keep_prob</li></ul><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>三种方式的最终结果对比如下表：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_15.png" alt=""></p><p>我们可以学到：</p><ul><li>正则化可以帮助减少过拟合</li><li>正则化可以迫使权重的值变得更小</li><li>L2 正则化和 dropout 正则化是两种非常有效的正则化技术</li></ul><h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>假设你正在搭建一个深度学习模型来检测诈骗，但是反向传播经常会有 bug，由于这是一个关键步骤，所以你的 boss 想要你的反向传播一定完全正确，所以我们在模型搭建好之后进行梯度检查。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>反向传播计算梯度 $\frac{\partial J}{\partial \theta}$, 其中 $\theta$ 代表模型的所有参数， $J$ 是前向传播的代价函数</p><p>由于前向传播很好实现，有十足的把握是正确的，而且非常确信代价函数 $J$ 百分百正确， 所以可以用 $J$ 来检验 $\frac{\partial J}{\partial \theta}$ 的正确性。</p><p>梯度的定义式如下：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}</script><ul><li>$\frac{\partial J}{\partial \theta}$ 是我们需要检验是否计算正确的梯度</li><li>我们需要计算 $J(\theta + \varepsilon)$ 和 $J(\theta - \varepsilon)$ ，因为 $J$ 是一定正确的</li></ul><p>假设我们有个三层的模型：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_16.png" alt=""></p><p>先引入所需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> gc_utils <span class="keyword">import</span> sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector</span><br></pre></td></tr></table></figure><p>然后是我们已经实现好的前向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_n</span><span class="params">(X, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation (and computes the cost) presented in Figure 3.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- training set for m examples</span></span><br><span class="line"><span class="string">    Y -- labels for m examples </span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (5, 4)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (5, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 5)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- the cost function (logistic cost for one example)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cost</span></span><br><span class="line">    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(<span class="number">1</span> - A3), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">1.</span>/m * np.sum(logprobs)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, cache</span><br></pre></td></tr></table></figure><p>实现好的反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_n</span><span class="params">(X, Y, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in figure 2.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    Y -- true "label"</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_n()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) </span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,</span><br><span class="line">                 <span class="string">"dA2"</span>: dA2, <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2,</span><br><span class="line">                 <span class="string">"dA1"</span>: dA1, <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>不确定我们刚刚实现的反向传播是否正确，所以我们写一个梯度检验的函数。</p><h3 id="多参数梯度检验的实现"><a href="#多参数梯度检验的实现" class="headerlink" title="多参数梯度检验的实现"></a>多参数梯度检验的实现</h3><p>对于下式：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}</script><p>其中 $\theta$ 不是一个标量，而是一个字典 parameters，所以我们需要先将这个字典转化为一个向量 parameters_value，方便进行取值，转化的过程如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2%2C1_17.png" alt=""></p><p>将字典转化为向量的函数 dictionary_to_vector() 和将向量转化回字典的函数 vector_to_dictionary() 和将梯度字典转化为梯度向量的函数 gradients_to_vector() 代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dictionary_to_vector</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Roll all our parameters dictionary into a single vector satisfying our specific required shape.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    keys = []</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">"W1"</span>, <span class="string">"b1"</span>, <span class="string">"W2"</span>, <span class="string">"b2"</span>, <span class="string">"W3"</span>, <span class="string">"b3"</span>]:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># flatten parameter</span></span><br><span class="line">        new_vector = np.reshape(parameters[key], (<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">        keys = keys + [key]*new_vector.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">            theta = new_vector</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            theta = np.concatenate((theta, new_vector), axis=<span class="number">0</span>)</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta, keys</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vector_to_dictionary</span><span class="params">(theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    parameters[<span class="string">"W1"</span>] = theta[:<span class="number">20</span>].reshape((<span class="number">5</span>,<span class="number">4</span>))</span><br><span class="line">    parameters[<span class="string">"b1"</span>] = theta[<span class="number">20</span>:<span class="number">25</span>].reshape((<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">    parameters[<span class="string">"W2"</span>] = theta[<span class="number">25</span>:<span class="number">40</span>].reshape((<span class="number">3</span>,<span class="number">5</span>))</span><br><span class="line">    parameters[<span class="string">"b2"</span>] = theta[<span class="number">40</span>:<span class="number">43</span>].reshape((<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">    parameters[<span class="string">"W3"</span>] = theta[<span class="number">43</span>:<span class="number">46</span>].reshape((<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">    parameters[<span class="string">"b3"</span>] = theta[<span class="number">46</span>:<span class="number">47</span>].reshape((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradients_to_vector</span><span class="params">(gradients)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Roll all our gradients dictionary into a single vector satisfying our specific required shape.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">"dW1"</span>, <span class="string">"db1"</span>, <span class="string">"dW2"</span>, <span class="string">"db2"</span>, <span class="string">"dW3"</span>, <span class="string">"db3"</span>]:</span><br><span class="line">        <span class="comment"># flatten parameter</span></span><br><span class="line">        new_vector = np.reshape(gradients[key], (<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">            theta = new_vector</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            theta = np.concatenate((theta, new_vector), axis=<span class="number">0</span>)</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure><p>现在我们得到了 paremeters_values (也就是 $\theta$) 的向量，下面是进行梯度检查的步骤：</p><p>for  each i in len(paremeters_values):</p><ul><li><p>计算 $J(…,\theta[i]+\varepsilon,…)$，即 J_plus[i]：</p><ul><li>$\theta^+$ = np.copy(parameters_values)</li><li>$\theta^+[i]=\theta^+[i]+\varepsilon$  </li><li>将 $\theta^+$ 重新转换回参数字典（使用 vector_to_dictionary 函数）</li><li>用新的参数带入前向传播计算 J_plus[i]</li></ul></li><li><p>同样的方法计算$J(…,\theta[i]-\varepsilon,…)$，即 J_minus[i]</p></li><li>用梯度估算式计算梯度 $gradapprox[i]=\frac{J_plus[i]-J_minus[i]}{2\varepsilon}$ </li><li>计算估算值和实际值的差异：$ difference = \frac {| grad - gradapprox |_2}{| grad |_2 + | gradapprox |_2 } $</li></ul><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check_n</span><span class="params">(parameters, gradients, X, Y, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    检查反向传播计算的梯度是否正确</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">    gradients -- 反向传播的输出，包含了代价函数对 theta 中每个参数的梯度的实际计算值的字典，需要检验正确性</span></span><br><span class="line"><span class="string">    X -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    Y -- true "label"</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 设置变量</span></span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters) <span class="comment"># 将参数值字典拍扁成一个向量存，_ 表示没有用到的返回值</span></span><br><span class="line">    grad = gradients_to_vector(gradients) <span class="comment"># 将梯度字典也拍扁成一个向量</span></span><br><span class="line">    num_parameters = parameters_values.shape[<span class="number">0</span>] <span class="comment"># 参数的总个数 </span></span><br><span class="line">    J_plus = np.zeros((num_parameters, <span class="number">1</span>))<span class="comment"># 初始化</span></span><br><span class="line">    J_minus = np.zeros((num_parameters, <span class="number">1</span>))<span class="comment"># 初始化</span></span><br><span class="line">    gradapprox = np.zeros((num_parameters, <span class="number">1</span>))<span class="comment"># 初始化</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算梯度的估算值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第一步：计算 J_plus[i]</span></span><br><span class="line">        thetaplus = np.copy(parameters_values)    <span class="comment"># 1.将参数向量 copy 过来，"_" 表示函数输出两个参数但是第二个用不到</span></span><br><span class="line">        thetaplus[i][<span class="number">0</span>] =  thetaplus[i][<span class="number">0</span>] + epsilon<span class="comment"># 2.给第 i 个参数加上微小量</span></span><br><span class="line">        J_plus[i], _ =forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))<span class="comment"># 3.计算参数加上微小量之后的代价函数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第二步：计算 J_minus[i]</span></span><br><span class="line">        thetaminus = np.copy(parameters_values)<span class="comment"># 1.将参数向量 copy 过来                    </span></span><br><span class="line">        thetaminus[i][<span class="number">0</span>] = thetaminus[i][<span class="number">0</span>] - epsilon<span class="comment"># 2.给第 i 个参数减去微小量    </span></span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X,Y,vector_to_dictionary(thetaminus))   <span class="comment"># 3.计算减去微小量之后的代价函数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 第三步：计算第 i 个参数的梯度估算值 gradapprox[i]</span></span><br><span class="line">        gradapprox[i] = (J_plus[i]-J_minus[i]) / np.float(<span class="number">2</span> * epsilon)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第四步：计算估算值和实际值之间的差异</span></span><br><span class="line">    numerator = np.linalg.norm(grad-gradapprox)<span class="comment"># 计算分子</span></span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)<span class="comment"># 计算分母</span></span><br><span class="line">    difference = numerator / denominator<span class="comment"># 计算差异   </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> difference &gt; <span class="number">1e-7</span>:<span class="comment"># 若差异值大于10的-7次方，则可能有错误</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[93m"</span> + <span class="string">"There is a mistake in the backward propagation! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    <span class="keyword">else</span>:<span class="comment"># 若差异值小于10的-7次方，则梯度检验通过</span></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[92m"</span> + <span class="string">"Your backward propagation works perfectly fine! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><p>下面运用这个函数来对我们的反向传播进行检验：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, Y, parameters = gradient_check_n_test_case()</span><br><span class="line"></span><br><span class="line">cost, cache = forward_propagation_n(X, Y, parameters)</span><br><span class="line">gradients = backward_propagation_n(X, Y, cache)</span><br><span class="line">difference = gradient_check_n(parameters, gradients, X, Y)</span><br></pre></td></tr></table></figure><p>>&gt; There is a mistake in the backward propagation! difference = 1.18904178788e-07</p><p>结果显示，梯度检验可能没有通过，反向传播中可能有错误，于是我们可以返回之前写的反向传播中进行仔细检查！</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul><li>由于 $\frac{\partial J}{\partial \theta} \approx  \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}$ 的计算成本很高，所以梯度检验非常慢，因此不要在训练的时候进行梯度检验，只需要调用几次检验反向传播函数是否正确即可</li><li>梯度检验不能与 dropout 一起运行，可以在运行梯度检验之前先关掉 dropout，检验完后再打开</li></ul><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><ul><li>梯度检验核实了反向传播中计算的梯度和公式估计的梯度之间的接近程度</li><li>梯度检验很慢，所以不要在训练的每一次迭代中运行，只有当你想确保你的代码正确的时候才要用，然后在实际的学习过程中关掉它</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络优化 </tag>
            
            <tag> 正则化 </tag>
            
            <tag> 初始化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 2 week 1）—— 正则化等</title>
      <link href="/2018/08/29/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w1/"/>
      <url>/2018/08/29/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c2w1/</url>
      <content type="html"><![CDATA[<p>deeplearning.ai 的第二个课程名为 <strong>改进深度神经网络：超参数调整，正则化和优化 (Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization)</strong> ，这门课程教你使深度学习表现更好的“魔法”，而不是把神经网络当作一个黑箱，你会理解什么使得神经网络表现更好，而且能更系统地得到好的结果，你还会学到一些 tensorflow 知识。通过三周的学习，你将能够：</p><ul><li>了解构建深度学习应用程序的行业最佳实践</li><li>能够有效地使用常见的神经网络技巧，包括初始化，L2 正则化和丢失正则化，批量归一化，梯度检查</li><li>能够实现和应用各种优化算法，例如小批量梯度下降，动量，PMSprop 和 Adam，并检查它们的收敛性</li><li>了解如何设置训练/开发/测试集和分析偏差/方差</li><li>能够在 TensorFlow 上实现神经网络</li></ul><a id="more"></a><h2 id="配置你的机器学习应用"><a href="#配置你的机器学习应用" class="headerlink" title="配置你的机器学习应用"></a>配置你的机器学习应用</h2><h3 id="训练-开发-测试集-Train-Dev-Test-sets"><a href="#训练-开发-测试集-Train-Dev-Test-sets" class="headerlink" title="训练/开发/测试集 (Train/Dev/Test sets)"></a>训练/开发/测试集 (Train/Dev/Test sets)</h3><p>假设我们有一组数据，我们将这组数据分成 <strong>训练集 train sets</strong> 和 <strong>开发集 dev sets</strong>（有时称作 hold-out 交叉验证集）和 <strong>测试集 test sets</strong>。</p><p>训练集用来训练模型，开发集用来评估不同超参数下的模型哪一个在开发集上效果最好，测试集用来评估最终的训练效果。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_1.png" alt=""></p><p>它们在一组数据中的比例是：</p><ul><li>在“前机器学习时代”即小数据时代：训练集/测试集 = 70%/30%；训练集/开发集/测试集 = 60%/20%/20%，当数据为10000以下时，这是被认为最好的分法</li><li>在大数据时代：假设有一百万个样本，开发集只需要一万个即可，测试集也只需要一万个便可评估模型性能，所以此时的比例为 训练/开发/测试 = 98%/1%/1%</li></ul><p>对于训练集和测试集的数据分布不匹配的问题，例如训练集来自网上爬取的精美图片，而训练集和开发集来自用户上传的模糊图片，这种情况的经验法则是：确保<strong>开发集和测试集</strong>的数据分布相同。</p><p>当不需要无偏估计时，可以没有测试集，这个时候开发集有时被称为“测试集”。</p><h3 id="偏差-bias-和方差-variance"><a href="#偏差-bias-和方差-variance" class="headerlink" title="偏差 (bias) 和方差 (variance)"></a>偏差 (bias) 和方差 (variance)</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_2.png" alt=""></p><ul><li>第一幅图中的分类器发生了<strong>欠拟合</strong>现象，我们称这个模型为<strong>“高偏差”</strong></li><li>第三幅图发生了<strong>过拟合</strong>现象，我们称这个模型为<strong>“高方差”</strong></li></ul><p>假设训练一个识别猫的分类器，有以下四种结果：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_3.png" alt=""></p><ul><li>第一个分类器训练集误差 1%，开发集 11%，则说明训练集拟合得较好，但是模型的泛化能力不行，也就是高方差，有可能发生过拟合现象</li><li>第二个分类器训练集误差 15%，开发集 16%，说明训练集拟合得不是很好，但是开发集误差与训练集相近，说明泛化能力较好，模型具有高偏差，有可能发生欠拟合现象</li><li>第三个分类器不仅训练集误差较大，而且开发集误差与训练集误差差距很多，说明此时模型既有高偏差，又有高方差，是最差的情况</li><li>第四个分类器训练集误差小，开发集误差也很小，此时的模型是低偏差、低方差，是最好的情况</li></ul><blockquote><p>这种分析方法基于人类识别出猫的误差约为零，如果图片模糊，连真人都无法识别出来，则理想误差（贝叶斯误差）就会非常高</p></blockquote><p>用一张图片更直观地展现“高偏差，高方差”的情况：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_4.png" alt=""></p><p>一个好的分类器如虚线所示，作为直线分类器具有高偏差，但是紫色线表示的分类器不仅大部分是直线，而且在途中扭曲地绕过了两个错误的样本，这样使得它具有了高方差，所以这个分类器既具有高方差又有高偏差。</p><h3 id="机器学习基本准测"><a href="#机器学习基本准测" class="headerlink" title="机器学习基本准测"></a>机器学习基本准测</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_5.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_7.png" alt=""></p><blockquote><p>在深度学习之前的时代中我们能用的工具不是很多，我们没有太多那种能够单独减小偏差或单独减小方差而不顾此失彼的工具。但在当前这个深度学习和大数据的时代，只要你能不断扩大所训练的网络的规模或不断获得更多数据，那扩大网络几乎总是能够减小偏差而不增大方差，而获得更多数据（用恰当的方式正则化）几乎总是能够减小方差而不增大偏差。有了这两步，再加上能够选取不同的网络来训练，我们就有了能够单独削减偏差或单独削减方差而不会过多影响另一个指标的能力，这样就不需小心地平衡两者。它能够解释为何深度学习在监督学习中如此有用以及为何在深度学习中，偏差与方差的权衡要不明显得多。</p></blockquote><h2 id="神经网络的正则化-regularization"><a href="#神经网络的正则化-regularization" class="headerlink" title="神经网络的正则化 (regularization)"></a>神经网络的正则化 (regularization)</h2><p>如果你怀疑你的神经网络在数据上发生了过拟合，也就是存在高方差问题，需要首先尝试使用正则化，虽然获取更多数据也是解决高方差问题的一个很可靠的方法，但你并不是总能获取到更多的训练数据或者获取更多数据的代价太大，但使用正则化通常有助于防止过拟合并降低网络的误差。</p><h3 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h3><h4 id="什么是-L2-正则化"><a href="#什么是-L2-正则化" class="headerlink" title="什么是 L2 正则化"></a>什么是 L2 正则化</h4><p>假如考虑最简单的逻辑回归模型，我们的优化目标是找到参数 $w，b$ 使得代价函数 $J(w,b)$ 最小，其中 $w \in \mathbb{R^{n_x}} , b \in \mathbb{R},$</p><script type="math/tex; mode=display">J(w,b)= \frac {1}{m} \sum\limits_{i=1}^{m} L({\hat y}^{(i)},y^{(i)})</script><p>当发生过拟合问题时，我们对上式引入正则化项，L2 正则化则是使用 L2 范数：</p><script type="math/tex; mode=display">J(w,b)= \frac {1}{m} \sum\limits_{i=1}^{m} L({\hat y}^{(i)},y^{(i)}) + \frac{\lambda}{2m}||w||^2_2</script><ul><li>$\lambda$ 称之为<strong>正则化参数</strong>，是一个需要调优的超参数，通常在开发集上配置这个参数，尝试一系列的值找出最好的那个，lambda 是 python 保留的关键字，在编程中我们把它写为“lambd”，避免和保留关键字冲突</li><li>$||w||_2$ 为 L2 范数 (欧几里德范数)，且 $||w||_2^2=\sum\limits_{j=1}^{n_x}w_j^2=w^Tw$ </li><li>我们通常省略 b 的相关项 $\frac{\lambda}{2m}b^2$，因为 w 往往是一个非常高维的参数矢量，几乎所有的参数都集中在 w 中，而 b 只是大量参数中的一个参数，即使加上了 b 的相关项也不会起到太大作用</li><li>L1 正则化使用的是 L1 范数，正则化项为 $\frac{\lambda}{2m}\sum\limits_{j=1}^{n_x}|w_j|=\frac{\lambda}{2m}||w||_1$，使用 L1 正则化会使得模型变得“稀疏”，这意味着 w 中有很多 0，吴恩达认为 L1 并没有压缩模型的作用且 L2 正则化用的更多</li></ul><p>对于 L 层的神经网络来说，正则化项为各层范数之和：</p><script type="math/tex; mode=display">J(W^{[1]},b^{[1]},...,W^{[L]},b^{[L]})= \frac {1}{m} \sum\limits_{i=1}^{m} L({\hat y}^{(i)},y^{(i)}) + \frac{\lambda}{2m}\sum\limits_{l=1}^{L}||W^{[l]}||^2_F</script><ul><li>其中：$||W^{[l]}||^2_F=\sum\limits_{i=1}^{n^{[l-1]}}\sum\limits_{j=1}^{n^{[l]}}(W_{ij}^{[l]})^2，W:(n^{[l]},n^{[l-1]})$，即 $W$ 矩阵的每个元素的平方求和，这个矩阵的范数，称为矩阵的<strong>费罗贝尼乌斯范数</strong> (注意，这不叫矩阵的 L2 范数)，使用角标 F表示 </li><li>式子最后加的正则化项也叫<strong>“惩罚项”</strong>，用来防止权重过大</li></ul><p>那么该如何使用 L2 正则化呢，我们可以证明，在代价函数添加正则项之后，其对 $W^{[l]}$ 的偏导 $\frac{\partial J}{\partial W^{[l]}}=dW^{[l]}$ 也要添加一项：</p><script type="math/tex; mode=display">dW^{[l]}=(正则化之前的dW^{[l]}) + \frac{\lambda}{m}W^{[l]}</script><p>然后更新参数即可：</p><script type="math/tex; mode=display">W^{[l]}:=W^{[l]}-\alpha dW^{[l]}\\ \quad \quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad \ \ \ =W^{[l]}-\alpha((正则化之前的dW^{[l]}) + \frac{\lambda}{m}W^{[l]})\\   \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad  =(1-\frac{\alpha\lambda}{m})W^{[l]}-\alpha(正则化之前的dW^{[l]})</script><p>我们可以看到矩阵 $W^{[l]}$前多了一个略小于1的系数，也就是说无论 $W$ 是多少，都让它变小一点，由于这个原因，L2 正则化有时被称为<strong>“权重衰减”</strong></p><h4 id="为什么-L2-正则化可以减小过拟合"><a href="#为什么-L2-正则化可以减小过拟合" class="headerlink" title="为什么 L2 正则化可以减小过拟合"></a>为什么 L2 正则化可以减小过拟合</h4><ol><li><p>例子1</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_8.png" alt=""></p><p>假如目前发生了过拟合，是第三种情况，根据正则化公式：</p><script type="math/tex; mode=display">W^{[l]}=(1-\frac{\alpha\lambda}{m})W^{[l]}-\alpha(正则化之前的dW^{[l]})</script><p>假如我们把正则化参数 $\lambda$ 调得足够大，使得 $W$ 变得很小，接近于0，这样会使得隐藏单元的影响被消除了，因为如果权重是零，那么这个单元便可有可无，那么这个大的神经网络就被简化为一个很小的神经网络，如上图左上角红框框起来的部分，这种情况与逻辑回归单元很类似 (只是深度变深)。而逻辑回归正是图中第一种高偏差的情况，于是上述操作使得我们将模型从第三种过拟合的情况变成第一种高偏差的情况！如果我们找到一个合适的中间值 $\lambda$，那么就可以将过拟合状态变成中间那种“刚刚好”的情况！</p></li><li><p>例子2</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_9.png" alt=""></p><p>对于上图的 tanh 激活函数，中间的一部分（红色的）接近于线性函数，如果 $\lambda$ 增大，则 $W^{[l]}$减小，由于$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$，则$Z^{[l]}$减小，更接近于在中间简单的线性部分取值，那么这个函数呈现相对线性，从而使神经网络只能计算一些离线性函数很近的相对简单的值，不能计算复杂的非线性函数，因此不太容易发生过拟合</p><blockquote><p>我的理解是，线性函数可以类比为某种“直脑筋”，直来直去，大大咧咧，所以容易产生高偏差，而非线性我类比为那种想得很多的人，过度思虑，所以经常会把一些噪声学进模型里，造成过度的拟合，正如太过焦虑某个事情容易走火入魔，L2 正则化大概是往这种“过度思虑”的心态里加上一些更加坦率更加直接的力量来矫正它吧 ：）</p></blockquote></li></ol><h3 id="丢弃正则化-Dropout-Regularization"><a href="#丢弃正则化-Dropout-Regularization" class="headerlink" title="丢弃正则化 (Dropout Regularization )"></a>丢弃正则化 (Dropout Regularization )</h3><h4 id="什么是丢弃正则化"><a href="#什么是丢弃正则化" class="headerlink" title="什么是丢弃正则化"></a>什么是丢弃正则化</h4><p>假设下图所示的神经网络发生过拟合：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_10.png" alt=""></p><p>使用随机失活技术来处理，首先为丢弃网络中的某个节点设置一个概率值，假设为 50%，遍历这个网络的每一层，对每一层的每一个结点作一次公平投币，使得这个结点有 50% 的几率被保留，50% 的几率被丢弃（取值为0），丢弃完这些结点后，我们得到一个小得多的网络，再进行反向传播，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_11.png" alt=""></p><h4 id="丢弃正则化的一种实现——反向随机失活-inverted-dropout"><a href="#丢弃正则化的一种实现——反向随机失活-inverted-dropout" class="headerlink" title="丢弃正则化的一种实现——反向随机失活 (inverted dropout)"></a>丢弃正则化的一种实现——反向随机失活 (inverted dropout)</h4><p>假设在神经网络的第 3 层上，即 $l=3$，首先设置一个向量 d3，它表示第 3 层的失活向量，它和 a3 的形状一样：</p><script type="math/tex; mode=display">第一步：d3=np.random.rand(a3.shape[0],a3.shape[1])<keep.prob</script><ul><li>np.random.randn() 产生的随机数介于 [0,1) 之间</li><li>keep.prob 是某个结点被保留的概率，小于它的值都取 1，反之取 0，也就是说上面这条语句产生了一个维度是 (a3.shape[0],a3.shape[1]) 的只包含 0 或者 1 的矩阵，某个元素取 1 的概率是 keep.prob，这是一个筛选矩阵</li></ul><p>接着我们用这个筛选矩阵来将 a3 随机失活：</p><script type="math/tex; mode=display">第二步：a3= np.multiply(a3,d3)</script><ul><li>np.multiply() 是逐元素相乘</li><li>如果 a3 某个元素乘到了 d3 中对应的某个刚好是 1 的元素（概率为 keep.prob），那么这个值保留，如果乘到了刚好是 0 的元素（概率为 1-keep.prob）,则这个值清零，即这个单元失活。也就是说： 每个结点都有 keep.prob 的概率被保留，1-keep.prob 的概率被失活</li></ul><p>接着我们将 a3 除以保留概率 keep.prob:</p><script type="math/tex; mode=display">第三步：a3 = a3/keep.prob</script><ul><li>假设 a3 有 50 个单元，keep.prob 为 0.8，则意味着平均会有 50×0.2=10 个单元失活清零，那么$z^{[4]}=w^{[4]}a^{[3]}+b^{[4]}$也会减少约 20%，所以为了不减少 z4 的期望值，我们需要除以 0.8，提供大约 20% 的校正值</li></ul><p>注意：<strong>不要在预测使用随机失活算法！</strong>因为我们不想我们预测值的输出也是随机的，这样做只会给预测带来噪声！</p><h4 id="为什么-dropout-正则化有效？"><a href="#为什么-dropout-正则化有效？" class="headerlink" title="为什么 dropout 正则化有效？"></a>为什么 dropout 正则化有效？</h4><p>一个直觉：不能依赖任何一个特征，所以必须分散权重。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_12.png" alt=""></p><p>对于上面这个神经元来说，如果使用 dropout，这些输入会被随机丢弃，这就意味着，它不能依赖于任何一个特征，因为每一个都有可能被随机丢弃，或者说每一个输入都有可能随机失活，所以在特定的时候，就不愿把所有的赌注或权重只放在某一个输入上，因此这个神经元会更积极地使用这种方式对于每个输入给一个较小的权重。泛化这些权值有利于压缩这些权重的平方和（平方范数）。</p><p>就和 L2 正则化一样，使用 dropout 有助于收缩权值，防止过拟合。</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_13.png" alt=""></p><h4 id="使用-dropout-的注意事项"><a href="#使用-dropout-的注意事项" class="headerlink" title="使用 dropout 的注意事项"></a>使用 dropout 的注意事项</h4><p>对于上图中的神经网络，每一层的权重矩阵维度都不一样，例如第一层是 (7,3)，第二层是 (7,7)，第三层是 (3,7)，我们可以对每一层设置不同的留存率 (keep.prob)，由于第二层有最大的 (7,7) 权值矩阵，参数最多，这一层最容易发生过拟合，于是我们可以在这一层设置最低的留存率，比如 0.5，而对于不那么担心会发生过拟合的层，例如权重矩阵较小的第三层，我们可以设为 0.7，而第四第五层我们完全不担心会发生过拟合，可以将留存率设为 1.0。另外在实际中，不对输入层进行随机失活。</p><p>最早对dropout技术的成功应用，是在计算机视觉领域，在这个领域中，输入层向量维度非常大，因为要包含每个像素点的值，几乎不可能有足够的数据，因此 dropout 在计算机视觉领域使用非常频繁，几乎已经成为一种默认了，但是除非算法已经过拟合了，所以不需考虑使用 dropout，所以相对计算机视觉领域，它在其他应用领域使用会少一些。</p><p>dropout 的一个缺点：代价函数变得不是那么明确，因为每次都有结点随机失活，所以代价函数定义不明确，也就是说你看不出它随着迭代的变化曲线，不能使用绘图的方法进行调参，这个时候可以先关闭 dropout，把留存率都设为 1，确保代价函数是单调递减的再打开 dropout。</p><h3 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h3><h4 id="数据集扩增-data-augmentation"><a href="#数据集扩增-data-augmentation" class="headerlink" title="数据集扩增 (data augmentation)"></a>数据集扩增 (data augmentation)</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_14.png" alt=""></p><p>增加数据的方法：</p><ul><li>图片水平翻转，可以使数据量翻倍</li><li>随机裁剪图片</li><li>数字的随机旋转或者变形</li></ul><p>增加数据集可以做出额外的伪训练样本，但这些额外的伪训练样本能增加的信息量不如全新的、独立的猫照片多，但因为这么做只有一些计算代价，所以这是一个获得更多数据的廉价方式，因此可以算作正则化，减少了过拟合。</p><h4 id="早终止法-early-stopping"><a href="#早终止法-early-stopping" class="headerlink" title="早终止法 (early stopping)"></a>早终止法 (early stopping)</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_15.png" alt=""></p><p>假设某个模型的代价函数随迭代次数在训练集上的变化（蓝线）和开发集误差（紫线）如上图所示，代价函数一直下降，而开发集误差先下降一段，然后开始增大，在最低点的那次迭代附近，神经网络表现最好，早终止法指的是，在这里把神经网络的训练停住，并选取这个最小开发集误差对应的参数值。</p><p>为什么有效？在迭代初期 w 很小，迭代后期 w 很大，正如 L2 正则化一样，通过停在半路，我们得到一个不大不小的 w 值，就能减小过拟合且不至于偏差太大了。</p><p>缺点：如果停止了梯度下降，意味着打断了优化代价函数 J 的过程，所以使得在降低代价函数这件事上做得就不够好，但同时你又想做到避免过拟合，也就是想用一个方法来干<strong>降低代价函数</strong>和<strong>减少过拟合</strong>这两件事情，而没有用不同方法来解决这两个问题，使问题更加复杂了。</p><p>优点：不用尝试大量的正则化参数 $\lambda$ 的值，只要运行一次梯度下降过程。</p><h2 id="问题的优化-optimization"><a href="#问题的优化-optimization" class="headerlink" title="问题的优化 (optimization )"></a>问题的优化 (optimization )</h2><h3 id="训练集的归一化-Normalization"><a href="#训练集的归一化-Normalization" class="headerlink" title="训练集的归一化 (Normalization)"></a>训练集的归一化 (Normalization)</h3><h4 id="什么是归一化"><a href="#什么是归一化" class="headerlink" title="什么是归一化"></a>什么是归一化</h4><p>假设某个训练集输入只有两个维度，$x=(x_1,x_2)^T$，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_16.png" alt=""></p><p>将输入归一化有两个步骤：</p><ol><li><p>将均值归零</p><p>$均值\mu = \frac{1}{m}\sum\limits ^m_{i=1}x^{(i)}\\x:=x- \mu $   </p><p>于是训练集变成下图的样子，使样本点分布在原点周围：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_17.png" alt=""></p><p>我们可以注意到特征 $x_1$的方差比 $x_2$的方差大，即更加分散，于是我们进行第二步。</p></li><li><p>将方差归一化</p><p>$方差\sigma^2=\frac{1}{m} \sum\limits ^m_{i=1}(x^{(i)} )^2\\x=\frac{x}{\sqrt{\sigma^2}}​$   </p><blockquote><p>注意：第一个式子本来应该是 $\sigma^2=\frac{1}{m} \sum\limits ^m_{i=1}(x^{(i)} - \mu)$，但是在第一步中 $x$ 已经减了 $\mu$，所以此处不需要</p></blockquote><p>于是训练集变成下面图里的样子：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_18.png" alt=""></p></li></ol><p>值得注意的是：当使用归一化对数据进行缩放时，<strong>训练集和测试集应该使用相同的 $\mu$ 和 $\sigma^2$ </strong>，也就是从训练集计算出来的 $\mu$ 和 $ \sigma^2$ </p><h4 id="为什么要对数据归一化"><a href="#为什么要对数据归一化" class="headerlink" title="为什么要对数据归一化"></a>为什么要对数据归一化</h4><p>假设 $x_1$ 的取值介于 1～10000，而 $x_2$ 的取值介于 0～1，那么会导致参数 w1 和 w2 的取值范围有很大不同，那么代价函数就如下图所示（两根轴用 w 和 b 代替）：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_19.png" alt=""></p><p>这个代价函数就像一个“扁平的碗”，它的等高线就是一个长长的椭圆，那么梯度下降必须采用很小的步长，经历许多步，反复辗转才能下降到最小值，而经过归一化的代价函数如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_20.png" alt=""></p><p>代价函数变成了一个更倾向正球体的图形，等高线也趋近于圆形，那么梯度下降能直接朝着最小值而去，而且采用更长的步长。</p><p>归一化适用于输入特征的尺寸非常不同的情况， 如果输入特征本来尺度就相近，那么这一步就不那么重要，不过因为归一化的步骤几乎从来没有任何害处，所以可以总是进行归一化。</p><h3 id="梯度消失和梯度爆炸-vanishing-exploding-gradients"><a href="#梯度消失和梯度爆炸-vanishing-exploding-gradients" class="headerlink" title="梯度消失和梯度爆炸 (vanishing / exploding gradients)"></a>梯度消失和梯度爆炸 (vanishing / exploding gradients)</h3><p>梯度消失或爆炸指的是，当你在训练一个深度神经网络的时候，损失函数的导数或者说斜率，有时会变得非常大或者非常小甚至是呈指数级减小的一种现象。</p><h4 id="为什么会发生梯度消失或梯度爆炸"><a href="#为什么会发生梯度消失或梯度爆炸" class="headerlink" title="为什么会发生梯度消失或梯度爆炸"></a>为什么会发生梯度消失或梯度爆炸</h4><p>有一个非常深的神经网络如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_21.png" alt=""></p><p>假设激活函数为线性函数，即 $g(z)=z$，且 $b^{[l]}=0$，那么我们可以求出最终的预测值：$\hat y=W^{[1]}W^{[2]}W^{[3]}…W^{[L-1]}W^{[L]}X$，假设每个权重矩阵只比单位矩阵大一点点，例如 $W^{[l]}=\begin{bmatrix}<br>1.5 &amp; 0\\<br>0 &amp; 1.5<br>\end{bmatrix}$，那么 $\hat y = W^{[L]}{\begin{bmatrix}<br>1.5 &amp; 0\\<br>0 &amp; 1.5<br>\end{bmatrix}}^{L-1}X$，如果 L 很大即层数很深，那么激活函数值会呈指数级增长，最后的预测值发生爆炸。反之，如果每个权重矩阵只比单位矩阵小一点点，那么激活函数值会呈指数级减少，最后的预测值可能消失。同理，反向传播中的梯度也会在层数很大时指数级增长或者减少，这样会使得梯度下降变得非常非常慢。</p><h4 id="如何改善梯度消失或爆炸——参数初始化"><a href="#如何改善梯度消失或爆炸——参数初始化" class="headerlink" title="如何改善梯度消失或爆炸——参数初始化"></a>如何改善梯度消失或爆炸——参数初始化</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1_22.png" alt=""></p><p>对于一个单神经元，如果我们忽略参数 b，那么 $z=w_1x_1+w_2x_2+…+w_nx_n$，我们不想让 z 变大或变小，如果 n 越大，那么 $w_i$ 就要越小，因为 z 是 n 个 $w_ix_i$ 的和，而 x 是不变的。一个解决办法是用方差为 $\frac{k}{n}$ 的正态分布初始化参数 w，其中 k 值取决于激活函数：</p><ol><li><p>对于 tanh 函数的 <strong>“Xavier 初始化”</strong>：</p><script type="math/tex; mode=display">将随机生成的 W 值乘以\sqrt{\frac{1}{n^{[l-1]}}}\\W^{[l]}=np.random.randn(n^{[l]},n^{[l-1]})*np.sqrt(\frac{1}{n^{[l-1]}})</script><ul><li>np.sqrt() 是所有元素开平方，* 是逐元素相乘</li></ul></li><li><p>对于 ReLU 函数有 <strong>“He 初始化”</strong>：</p><script type="math/tex; mode=display">将随机生成的W值乘以\sqrt{\frac{2}{n^{[l-1]}}}\\W^{[l]}=np.random.randn(n^{[l]},n^{[l-1]})*np.sqrt(\frac{2}{n^{[l-1]}})</script></li></ol><ol><li>另一种变体：<script type="math/tex; mode=display">\sqrt {\frac{2}{n^{[l-1]}+n^{[l]}}}\\W^{[l]}=np.random.randn(n^{[l]},n^{[l-1]})*np.sqrt(\frac{2}{n^{[l-1]}+n^{[l]}})</script></li></ol><h3 id="梯度检查-gradient-cheking"><a href="#梯度检查-gradient-cheking" class="headerlink" title="梯度检查 (gradient cheking)"></a>梯度检查 (gradient cheking)</h3><p>梯度检查是检查反向传播中计算的梯度是否正确并找出错误的方法，使用梯度检查可以节省时间，调试代码，检验反向传播算法。</p><h4 id="梯度检查的公式"><a href="#梯度检查的公式" class="headerlink" title="梯度检查的公式"></a>梯度检查的公式</h4><p>我们用一个近似公式来计算代价函数对参数的梯度：</p><script type="math/tex; mode=display">f'(\theta)\approx\frac{f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2\varepsilon}</script><ul><li>其中 $\varepsilon$ 是一个很小的数</li><li>这种双向导数公式比单向更精确</li></ul><p>接下来：</p><ol><li><p>把 $W^{[1]},b^{[1]},W^{[2]},b^{[2]},…,W^{[L]},b^{[L]}$ reshape 成一个大向量 $\theta$ </p></li><li><p>把 $dW^{[1]},db^{[1]},dW^{[2]},db^{[2]},…,dW^{[L]},db^{[L]}$ reshape 成一个大向量 $d\theta$</p></li><li><p>梯度检查算法：</p><p> $for \quad each \quad  i:\\  \quad \quad  \  d\theta_{approx}{[i]}=\frac{J(\theta_1,\theta_2,…,\theta_i+\varepsilon,…)-J(\theta_1,\theta_2,…,\theta_i-\varepsilon,…)}{2\varepsilon}\\  \hspace{3.5cm} \approx d\theta[i]=\frac{\partial J}{\partial \theta_i}$</p><ul><li>其中 $d\theta_{approx}[i]$ 指的用上面的公式计算的值，$d\theta[i]$ 指的是程序计算出的值</li><li>$\theta[i]$ 指的是第 i 层的参数</li></ul></li><li><p>检查 $d\theta_{approx}[i]$ 和 $d\theta[i]$ 是否相等，用下式来评估它们之间的差距：</p><script type="math/tex; mode=display">\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||_2}<\varepsilon</script><ul><li>其中分子为两个向量的欧几里德距离，值为两个向量的每个分量的差的平方之和再开方，分母为两个向量的欧几里德长度（L1 范数）之和</li><li>若 $\varepsilon=10^{-7}$，则可认为误差很小， 若 $\varepsilon$ 在 $10^{-5}$ 量级，则可能需要再三检查这个向量的每个分量，若在 $10^{-3}$ 量级，则很可能有错误，仔细检查每个部分，总之这个数字应该非常非常小</li></ul></li></ol><h4 id="实现梯度检查的注意事项"><a href="#实现梯度检查的注意事项" class="headerlink" title="实现梯度检查的注意事项"></a>实现梯度检查的注意事项</h4><ul><li>不要在训练的时候进行梯度检查——只在 debug 阶段</li><li>如果算法没通过梯度检查，需要检查它的组成，找出漏洞，也就是如果 $d\theta_{approx}[i]$ 和 $d\theta[i]$ 差距很大，检查不同的 i 值，看看哪些 $d\theta_{approx}[i]$ 和 $d\theta[i]$ 很大</li><li>记着正则化，$J(\theta)= \frac {1}{m} \sum\limits_{i=1}^{m} L({\hat y}^{(i)},y^{(i)}) + \frac{\lambda}{2m}\sum\limits_{l=1}^{L}||W^{[l]}||^2_F$，在用公式计算时 $d\theta_{approx}$ 时，别忘了正则化项</li><li>不要在使用 dropout 时进行梯度检查，可以将 keep.prob 设为 1</li><li>反向传播算法在 w 和 b 接近 0 的时候是正确的，但是当 w 和 b 变大的时候，算法精确度有所下降，可以在进行几次训练的迭代后，再运行梯度检验</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络优化 </tag>
            
            <tag> 正则化 </tag>
            
            <tag> 初始化 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 4）</title>
      <link href="/2018/08/27/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w4%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/08/27/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w4%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h1 id="第一部分：基本架构"><a href="#第一部分：基本架构" class="headerlink" title="第一部分：基本架构"></a>第一部分：基本架构</h1><p>这是深度学习专项课程第一课第四周的编程作业的第一部分，通过这一部分，可以学到：</p><ul><li>使用非线性单元比如 ReLU 来提高模型</li><li>建立一个更深的神经网络（大于一个隐藏层）</li><li>实现一个易用的神经网络类</li></ul><h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 包的引入</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline <span class="comment"># %符号为ipython的魔法函数，与画图有关，在pycharm中会报错</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h2><ul><li>初始化一个两层神经网络的参数以及一个 L 层的神经网络的参数</li><li><p>实现前向传播模块（下图中的紫色模块）</p><ul><li>实现某一层的前向传播步骤的<strong>线性部分</strong>（得到 $Z^{[l]}$）</li><li>给定激活函数（relu/sigmoid）</li><li>组合之前的两步形成一个<strong>[线性—&gt;激活]</strong>前向传播函数</li><li>重复<strong>[线性—&gt;ReLU]</strong>前向传播函数 L-1 次（对于 1 到 L-1 层），再加上<strong>[线性—&gt;sigmoid]</strong>在末尾（对于二元分类的最终层 L）。这形成了一个新的 L_model_forward 函数</li></ul></li><li><p>计算损失函数</p></li><li><p>实现反向传播模块（下图中的红色部分）</p><ul><li>计算某一层的反向传播函数步骤的<strong>线性部分</strong></li><li>计算激活函数的梯度（relu_backward/sigmoid_backward）</li><li>组合之前的两步形成一个新的<strong>[线性—&gt;激活]</strong>反向传播函数</li><li>重复<strong>[线性—&gt;relu]</strong>反向传播 L-1 次，然后加上<strong>[线性—&gt;sigmoid]</strong>反向传播在末尾。这形成了一个新的 L_model_backward 函数</li></ul></li><li><p>更新参数</p></li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1.png" alt=""></p><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><h3 id="两层神经网路初始化"><a href="#两层神经网路初始化" class="headerlink" title="两层神经网路初始化"></a>两层神经网路初始化</h3><ul><li>模型结构为：<em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</em></li><li>权重 W 采用随机初始化</li><li>偏差 b 采用初始化为零的方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单隐层神经网络的初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="L-层的神经网络的初始化"><a href="#L-层的神经网络的初始化" class="headerlink" title="L 层的神经网络的初始化"></a>L 层的神经网络的初始化</h3><p>假设输入 X 维度是（12288，209），则其他的维度如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2.png" alt=""></p><ul><li>模型结构为：<em>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</em></li><li>权重 W 采用随机初始化</li><li>偏差 b 采用初始化为零</li><li>将每层单元个数储存在 list 变量 layer_dims 里面</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L 层神经网络的初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- 包含每一层的维度数据的list数组</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">   </span><br><span class="line">    parameters = &#123;&#125;    <span class="comment"># 先建立一个空的参数dict</span></span><br><span class="line">    L = len(layer_dims)   <span class="comment"># 神经网络的总层数L </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数 W1，b1，W2，b1... 初始化</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l],layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l],<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="前向传播模块"><a href="#前向传播模块" class="headerlink" title="前向传播模块"></a>前向传播模块</h2><ul><li>线性前向传播</li><li>线性—&gt;激活前向传播，其中激活函数可以是 ReLU 或者 sigmoid</li><li>整个模型为：[线性 —&gt; relu] ×（L-1）—&gt; 线性 —&gt; sigmoid</li></ul><h3 id="线性前向传播"><a href="#线性前向传播" class="headerlink" title="线性前向传播"></a>线性前向传播</h3><ul><li>输入上一层的激活值 A_prev，这一层的参数 W，b，</li><li>输出这一层的线性值 Z 和 “线性缓存”</li><li>“线性缓存”是一个值为 (A_prev,W,b) 的 tuple</li><li>$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 之前层的激活值 (或输入数据): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- 权重矩阵: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- 偏差向量, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 这一层激活函数的输入值，也叫“前激活”参数 </span></span><br><span class="line"><span class="string">    cache -- “线性缓存”，值为 (A_prev,W,b) 的 tuple，可以更有效率地计算反向传播</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算 Z 值</span></span><br><span class="line">    Z = np.dot(W,A_prev) + b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A_prev, W, b)<span class="comment"># “线性缓存”</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h3 id="一层线性-激活前向传播"><a href="#一层线性-激活前向传播" class="headerlink" title="一层线性-激活前向传播"></a>一层线性-激活前向传播</h3><ul><li>输入上一层的激活值 A_prev，这一层的参数 W 和 b，以及激活函数名 (字符串)</li><li>输出这一层的激活值 A 和 “前向传播缓存”</li><li>“前向传播缓存” = “线性缓存”(A_prev,W,b) + “激活缓存”Z，是一个值为 ( (A_prev,W,b), Z) 的 tuple，简称“缓存”</li><li>$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一层线性-激活前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 之前层的激活值 (或输入数据): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- 权重矩阵: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- 偏差向量, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- 在这一层用到的激活函数, 用字符串储存: "sigmoid" 或 "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- 这一层的激活值 </span></span><br><span class="line"><span class="string">    cache -- 前向传播缓存，是一个包含 "linear_cache"（线性缓存，A_prev，W，b） 和 "activation_cache"（激活缓存，Z） 的 tuple：((A_prev,W,b),Z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 判断激活函数是 sigmoid 还是 relu</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev,W,b)<span class="comment"># 先得到这一层的线性值 Z 和“线性缓存” (A_prev,W,b)</span></span><br><span class="line">        A, activation_cache = sigmoid(Z)<span class="comment"># 再将 Z 激活得到 A 和“激活缓存” Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev,W,b) <span class="comment"># 同理</span></span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)<span class="comment"># 将线性缓存和激活缓存合并成一个“前向传播缓存” tuple</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><p>其中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid 函数和 relu 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(Z)</span>:</span></span><br><span class="line"></span><br><span class="line">    A = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    cache = Z <span class="comment"># 激活缓存 Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z)</span>:</span></span><br><span class="line">    </span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    cache = Z<span class="comment"># 激活缓存 Z</span></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h3 id="L-层模型的前向传播"><a href="#L-层模型的前向传播" class="headerlink" title="L 层模型的前向传播"></a>L 层模型的前向传播</h3><p>对于二元分类，先 [线性-relu激活] L-1 次，再 [线性-sigmoid激活] 一次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二元分类 L 层模型的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 初始训练集, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- 初始化函数 initialize_parameters_deep() 输出的初始化之后的参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- 最终层即 L 层的激活值</span></span><br><span class="line"><span class="string">    caches -- 包含每一层"前向传播缓存"的 list (一共有 L-1 个, 索引值为 0 到 L-1)</span></span><br><span class="line"><span class="string">              其中每一层的缓存是一个 tuple：((A,W,b),Z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 神经网络的层数，由于参数有 W 和 b，故要除以 2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 [线性 -&gt; RELU] 前向传播 L-1 次，并把每一次的缓存加到总缓存 caches 中.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span> + str(l)], parameters[<span class="string">'b'</span> + str(l)], <span class="string">'relu'</span>)<span class="comment"># 切记是 'relu'字符串 而不是 relu</span></span><br><span class="line">        caches.append(cache)<span class="comment"># 将每一次的缓存加入总缓存 caches</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 [线性 -&gt; sigmoid] 前向传播 1 次，并把这一次的缓存加到总缓存 caches 中.</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span> + str(L)], parameters[<span class="string">'b'</span> + str(L)], <span class="string">'sigmoid'</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><blockquote><p>关于缓存，在对输入进行线性运算时，生成<strong>“线性缓存”</strong>，是一个 (A_prev,W,b) 的 tuple，在对线性值进行激活运算时，生成<strong>“激活缓存”</strong>，是一个值为 Z 的数组，在进行 [线性-激活] 的前向传播时，生成的是<strong>“前向传播缓存”</strong>，简称<strong>“缓存”</strong>，是一个值为 ((A_prev,W,b),Z) 的 tuple</p></blockquote><h2 id="计算代价函数"><a href="#计算代价函数" class="headerlink" title="计算代价函数"></a>计算代价函数</h2><script type="math/tex; mode=display">cost = -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right))</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">  </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]<span class="comment"># 总样本个数</span></span><br><span class="line"></span><br><span class="line">    cost = <span class="number">-1</span>/m * ( np.dot(Y, np.log(AL).T) + np.dot((<span class="number">1</span>-Y), np.log(<span class="number">1</span>-AL).T) )</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># 确保代价函数是一个数而不是数组 (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="反向传播模块"><a href="#反向传播模块" class="headerlink" title="反向传播模块"></a>反向传播模块</h2><ul><li>线性反向传播</li><li>线性—&gt;激活反向传播，其中“激活”计算 relu 或者 sigmoid 函数的梯度</li><li>整个模型为：[线性 —&gt; relu] ×（L-1）—&gt; 线性 —&gt; sigmoid</li></ul><h3 id="线性反向传播"><a href="#线性反向传播" class="headerlink" title="线性反向传播"></a>线性反向传播</h3><p>已知 $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$ 和该层 “线性缓存”(A_prev,W,b)，求 $dW^{[l]}, db^{[l]} ,dA^{[l-1]}$ ，如下图所示</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3.png" alt=""></p><p>计算公式为：</p><script type="math/tex; mode=display">Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\\dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T}\\db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}\\dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- 代价函数对某l层线性输出 Z 的偏导 (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- 该l层的前向传播“线性缓存” tuple：(A_prev, W, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- 代价函数对前一层 l-1 层的激活值的偏导数，与 A_prev 的形状相同</span></span><br><span class="line"><span class="string">    dW -- 代价函数对当前层 l 层的权重 W 的偏导数，形状与 W 相同</span></span><br><span class="line"><span class="string">    db -- 代价函数对当前层 l 层的偏差 b 的偏导数，形状与 b 相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache<span class="comment"># 将该层“线性缓存”提取出来</span></span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]<span class="comment"># 总样本数m</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式计算 dW，db，dA_prev</span></span><br><span class="line">    dW = <span class="number">1</span>/m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h3 id="一层线性-激活反向传播"><a href="#一层线性-激活反向传播" class="headerlink" title="一层线性-激活反向传播"></a>一层线性-激活反向传播</h3><ul><li>已知 $dA^{[l]} = \frac{\partial \mathcal{L} }{\partial A^{[l]}}$ 和 $g(.)$ 和该层 “前向传播缓存”((A_prev,W,b),Z)，先求 $dZ^{[l]}$, 再求 $dW^{[l]}, db^{[l]} ,dA^{[l-1]}$ </li><li>$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一层线性-激活反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- 当前层l层的激活值梯度 </span></span><br><span class="line"><span class="string">    cache -- “前向传播缓存”tuple：((A_prev,W,b),Z)</span></span><br><span class="line"><span class="string">    activation -- 在这一层用到的激活函数, 用字符串储存: "sigmoid" 或 "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- 代价函数对前一层 l-1 层的激活值的偏导数，与 A_prev 的形状相同</span></span><br><span class="line"><span class="string">    dW -- 代价函数对当前层 l 层的权重 W 的偏导数，形状与 W 相同</span></span><br><span class="line"><span class="string">    db -- 代价函数对当前层 l 层的偏差 b 的偏导数，形状与 b 相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache<span class="comment"># 从“前向传播缓存”中提取“线性缓存”和“激活缓存”</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)<span class="comment"># 先用 dA 求得 dZ</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)  <span class="comment"># 再运行线性反向传播</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)<span class="comment">#同理</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p>其中 dA 求得 dZ 的函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现单个 relu 单元的反向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- 某l层的激活值</span></span><br><span class="line"><span class="string">    cache -- 该层的“激活缓存” Z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- 代价函数对l层的 Z 的梯度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    Z = cache<span class="comment"># 将 Z 值从“激活缓存”取出</span></span><br><span class="line">    dZ = np.array(dA, copy=<span class="keyword">True</span>) <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据 relu 函数，当 Z&lt;=0，dZ 也为零</span></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, cache)</span>:</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (<span class="number">1</span>-s)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br></pre></td></tr></table></figure><h3 id="L-层模型的反向传播"><a href="#L-层模型的反向传播" class="headerlink" title="L 层模型的反向传播"></a>L 层模型的反向传播</h3><p>对于二元分类，先 [sigmoid—&gt;线性] 一次，再 [relu—&gt;线性] L-1 次</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.4.png" alt=""></p><ul><li>反向传播初始化，即求 dAL 可以使用公式：$dAL=-\frac{Y}{AL}-\frac{1-Y}{1-AL}$ </li><li>将梯度数据存入名为 grads 的 dict 中：$grads[“dW” + str(l)] = dW^{[l]}$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L 层模型的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- 前向传播的最终激活值向量</span></span><br><span class="line"><span class="string">    Y -- 真实的“标签”向量 (包含 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- 总缓存，包含每一层的“前向传播缓存”，用 sigmoid 函数进行激活的缓存是 cache[L-1]，用 relu 函数进行激活的缓存是 cache[l] (for l in range(L-1),即 l = 0,1...,L-2)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- 包含代价函数对 A,W,b 的梯度的 dict</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;<span class="comment"># 先建立一个储存梯度的空 dict</span></span><br><span class="line">    L = len(caches)<span class="comment"># 神经网络的总层数</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]<span class="comment"># 训练集样本的个数</span></span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># 让 Y 和 AL 的形状相同</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播的初始化，即求 dAL</span></span><br><span class="line">    dAL =  - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))<span class="comment"># 逐元素相除</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第 L 层先 [sigmoid -&gt; 线性] 反向传播一次. 输入：dAL, current_cache. 输出：grads["dAL-1"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]  <span class="comment">#从总缓存 caches 中取出当前缓存 </span></span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L<span class="number">-1</span>)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">'sigmoid'</span>) <span class="comment"># 进行一次反向传播，，并把dAL-1，dWL，dbL 放入字典中</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 继续反向传播，从 l=L-2 循环到 l=0，故从 l+1 开始</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):  <span class="comment"># 将 range() 倒序</span></span><br><span class="line">        <span class="comment"># 第 l 层: [relu -&gt; 线性] 反向传播求梯度.</span></span><br><span class="line">        <span class="comment"># 输入：grads["dA" + str(l + 1)], current_cache. 输出：grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]<span class="comment"># 取出当前层的缓存</span></span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">'dA'</span> + str(L<span class="number">-1</span>)], current_cache, <span class="string">'relu'</span>)<span class="comment"># 输入的是 dAL-1</span></span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h2 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h2><script type="math/tex; mode=display">W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \\b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用梯度下降更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有初始化后的参数的字典</span></span><br><span class="line"><span class="string">    grads -- 包含所有参数的梯度的字典</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有更新过的参数的字典</span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = Wl 的值</span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = bl 的值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span><span class="comment"># 神经网络的层数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用公式更新参数</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="第二部分：图片识别应用"><a href="#第二部分：图片识别应用" class="headerlink" title="第二部分：图片识别应用"></a>第二部分：图片识别应用</h1><p>这是深度学习专项课程第一课第四周的编程作业的第二部分，通过这一部分，可以学到：</p><ul><li>学习如何使用第一部分中构建的辅助函数来建立我们需要的任何结构的模型</li><li>用不同的模型结构进行实验观察每一种的表现</li><li>认识到在从头开始构建神经网络之前构建辅助函数使得任务更加容易</li></ul><h2 id="包的引入-1"><a href="#包的引入-1" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v3 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>数据集包括：</p><ul><li>m_train 个训练集，包括图片集 train_set_x_orig 和标签集 train_set_y</li><li>m_test 个测试集，包括图片集 test_set_x_orig 和标签集 test_set_y</li><li>每张图片都是<strong>方形</strong> (height = num_px, width = num_px)，有三个颜色通道，所以数组形状是 (num_px, num_px, 3)</li><li>每个图片集都要进行预处理，所以原始数据加上 <strong>_orig</strong>，但是标签集不需要预处理</li></ul><h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><h4 id="加载原始数据集"><a href="#加载原始数据集" class="headerlink" title="加载原始数据集"></a>加载原始数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(train_set_x_orig[index]) <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</span><br><span class="line"><span class="comment"># np.squeeze() 用于把一个数组的 shape 中为 1 的维度删掉，即让 train_set_y[:, index] 变为一个数</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_1.png" alt=""></p><h4 id="确定图片维度和个数以防止出错"><a href="#确定图片维度和个数以防止出错" class="headerlink" title="确定图片维度和个数以防止出错"></a>确定图片维度和个数以防止出错</h4><ul><li>训练集的个数：m_train</li><li>测试集的个数：m_test</li><li>图片（正方形）的尺寸即边长的像素数：num_px</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定维度和个数</span></span><br><span class="line"><span class="comment"># train_set_x_orig 形状为 (m_train, num_px, num_px, 3)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;Number of training examples: m_train = 209</span><br><span class="line">  Number of testing examples: m_test = 50</span><br><span class="line">  Height/Width of each image: num_px = 64</span><br><span class="line">  Each image is of size: (64, 64, 3)</span><br><span class="line">  train_set_x shape: (209, 64, 64, 3)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x shape: (50, 64, 64, 3)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br></pre></td></tr></table></figure><h4 id="重构图片数组变为标准输入矩阵"><a href="#重构图片数组变为标准输入矩阵" class="headerlink" title="重构图片数组变为标准输入矩阵"></a>重构图片数组变为标准输入矩阵</h4><p>把尺寸为 (num_px, num_px, 3) 的图片变为 shape 为 (num_px ∗ num_px ∗ 3, 1) 的向量 </p><p>把一个 shape 为 (a,b,c,d) 的矩阵变为一个 shape 为 (b∗c∗d, a) 的矩阵的技巧：<code>x_flatten = X.reshape(X.shape[0], -1).T</code></p><blockquote><p>实际上，reshape() 是按<strong>行</strong>取元素，按<strong>行</strong>放元素</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重构图片数组</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))<span class="comment">#重构后完整性检查</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;train_set_x_flatten shape: (12288, 209)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x_flatten shape: (12288, 50)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br><span class="line">  sanity check after reshaping: [17 31 56 22 33]</span><br></pre></td></tr></table></figure><h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><p>为了使得数据在一个合适的尺度上，我们需要将数据标准化，对于图片来说，由于图片的每个像素的 RGB 值介于 0 到 255 之间，所以我们可以将每个特征值除以 255，这样就能将它们标准化了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></span><br></pre></td></tr></table></figure><h2 id="一般方法"><a href="#一般方法" class="headerlink" title="一般方法"></a>一般方法</h2><ul><li>初始化参数 / 定义超参数</li><li><p>进行 num_iterations 次循环：</p><ul><li>前向传播</li><li>计算代价函数</li><li>反向传播</li><li>更新参数</li></ul></li><li><p>用训练过的参数来预测标签值</p></li></ul><h2 id="两层的神经网络构建"><a href="#两层的神经网络构建" class="headerlink" title="两层的神经网络构建"></a>两层的神经网络构建</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.5.png" alt=""></p><p>会用到的之前构建的函数有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>确定每层的单元数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># 输入向量维度</span></span><br><span class="line">n_h = <span class="number">7</span><span class="comment"># 隐藏层单元数</span></span><br><span class="line">n_y = <span class="number">1</span><span class="comment"># 输出层单元数</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><p>将构建的函数组合起来形成主函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个两层的神经网络: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 输入数据, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- 真实的标签向量 (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- 每一层的单元数 (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- 梯度下降的迭代次数</span></span><br><span class="line"><span class="string">    learning_rate -- 梯度下降学习率</span></span><br><span class="line"><span class="string">    print_cost -- 如果值为 True 则每一百步打印一次代价函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 包含更新后的参数 W1，W2，b1，b2 的字典</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []<span class="comment"># 用来记录每百步的代价函数的值</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]<span class="comment"># 样本数</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数字典中取出 W1，W2，b1，b2</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降循环 num_iterations 次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向传播，输出下一层的激活值和“前向传播缓存”</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">'relu'</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算代价函数</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播初始化，即求得 dAL</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播，获得前一层的激活值梯度和这一层的参数梯度</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">'relu'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将参数梯度都存入 grads 字典</span></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 从参数字典中取出参数并赋值以便下一次迭代</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每百步打印一次代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)<span class="comment"># 方便等下打印图表</span></span><br><span class="line">       </span><br><span class="line">    <span class="comment"># 打印出代价函数的图表</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters<span class="comment"># 返回更新过的参数</span></span><br></pre></td></tr></table></figure><p>用我们的数据集进行测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.7.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.8.png" alt=""></p><p>看看预测训练集的精确度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure><p>>&gt; Accuracy: 1.0</p><p>再看看预测测试集的精确度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p>>&gt; Accuracy: 0.72</p><p>我们可以看到比第二周的逻辑回归的 70% 的精确度要稍高，接下来我们建立一个 L 层的神经网络进行试验。</p><h2 id="L-层的神经网络"><a href="#L-层的神经网络" class="headerlink" title="L 层的神经网络"></a>L 层的神经网络</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.9.png" alt=""></p><p>可以用到的之前构建的函数有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>确定每层的单元数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>]<span class="comment"># 这是一个四层的模型</span></span><br></pre></td></tr></table></figure><p>将辅助函数组合到主函数中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个 L 层的神经网络: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 输入数据向量, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- 真实的标签向量 (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- 包含输入向量维度和每层的单元数的列表, 长度为层数 + 1</span></span><br><span class="line"><span class="string">    learning_rate -- 梯度下降学习率</span></span><br><span class="line"><span class="string">    num_iterations -- 梯度下降循环迭代次数</span></span><br><span class="line"><span class="string">    print_cost -- 若为 True，则每百步打印一次代价函数 cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 模型训练出来的参数，可用于预测标签值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    costs = []<span class="comment"># 用来记录代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数初始化</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向传播: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)  <span class="comment"># 注意在前向传播的过程中会得到总缓存 caches</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算代价函数</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)  <span class="comment"># 注意反向传播时要用上前向传播的总缓存 caches</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 每百步打印一次代价函数值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 画出代价函数图表</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>用数据集训练模型试试看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>得到的结果如下：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.10.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.11.png" alt=""></p><p>看看预测训练集的精确度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure><p>>&gt; Accuracy: 0.985645933014</p><p>看看预测测试集的精确度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><p>>&gt; Accuracy: 0.8</p><p>我们可以看到 4 层神经网络的 80% 精确度比逻辑回归的 70% 和 2 层神经网络的 72% 精确度要高很多！</p><p>还可以拿自己的图片进行预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先将自己的图片加到 image 文件夹</span></span><br><span class="line">my_image = <span class="string">"my_image.jpg"</span> <span class="comment"># 将这个改成你图片的名字 </span></span><br><span class="line">my_label_y = [<span class="number">1</span>] <span class="comment"># 图片的真实标签值 (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class="line"></span><br><span class="line">fname = <span class="string">"images/"</span> + my_image</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="keyword">False</span>))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">my_image = my_image/<span class="number">255.</span></span><br><span class="line">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your L-layer model predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.13.png" alt=""></p><p>找了十张动物图片来进行测试，前五张不是猫，后五张是猫，测试结果是：第一张熊猫识别错误，第二张北极熊识别错误，第三张大象识别正确，第四张兔子识别正确，第五张非洲狮识别正确（其实对这张最没信心，因为外形和猫实在是太像了，反而识别正确了==），第六张猫识别错误，第七张猫识别错误，第八张猫识别正确，第九张猫识别正确，第十张猫识别正确，粗略看来，这十张图的正确率有六成，模型仍需改进！</p><h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><p>试着打印出分类错误的图片：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.12.png" alt=""></p><p>我们可以发现有以下特征的图片更难判断正确：</p><ul><li>猫的身体在不寻常的位置</li><li>猫的颜色与背景颜色相似</li><li>特殊的猫的颜色和种类</li><li>相机角度</li><li>图片的明亮度</li><li>大小变化（猫在图片中很大或者很小）</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 4）—— 深度神经网络</title>
      <link href="/2018/08/15/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w4/"/>
      <url>/2018/08/15/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w4/</url>
      <content type="html"><![CDATA[<p>本周主要介绍<strong>深度神经网络</strong> (Deep Neural Networks)。</p><h2 id="深度神经网络概况"><a href="#深度神经网络概况" class="headerlink" title="深度神经网络概况"></a>深度神经网络概况</h2><h3 id="什么是深度神经网络"><a href="#什么是深度神经网络" class="headerlink" title="什么是深度神经网络"></a>什么是深度神经网络</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.1.png" alt=""></p><p>所谓深浅取决于神经网络的层数，例如左上角的逻辑回归模型是一个“最浅的”神经网络，而右下角的神经网络具有五个隐藏层，可以算得上是深度神经网络。</p><a id="more"></a><h3 id="神经网络的符号含义"><a href="#神经网络的符号含义" class="headerlink" title="神经网络的符号含义"></a>神经网络的符号含义</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.2.png" alt=""></p><ul><li><strong>l</strong> —— 表示神经网络的层数</li><li>$n^{[l]}$  表示 l 层的单元数，图中 $n^{[0]}=n_x=3,n^{[1]}=5,n^{[2]}=5,n^{[3]}=3,n^{[4]}=n^{[l]}=1$</li><li>$a^{[l]}=g^{[l]}(z^{[l]})$ 表示 l 层的激活向量</li><li>$W^{[l]}$ 表示产生 l 层的权重</li><li>$b^{[l]}$ 表示产生 l 层的偏差</li></ul><h3 id="为什么我们需要“深度”"><a href="#为什么我们需要“深度”" class="headerlink" title="为什么我们需要“深度”"></a>为什么我们需要“深度”</h3><h4 id="例子1-——-面部识别"><a href="#例子1-——-面部识别" class="headerlink" title="例子1 —— 面部识别"></a>例子1 —— 面部识别</h4><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.3.png" alt=""></p><p>神经网络的第一层可以被认为是一个边缘检测器，这一层的神经元正试图找到人脸的边缘在哪里，通过将像素分组的方法形成边缘，然后取消边缘检测，并将边缘组合在一起，形成面部的一部分，最后将面部的不同部位组合在一起我们可以识别不同的面部，我们可以将神经网络的浅层当作简单的检测函数，然后后一层将他们组合在一起，以便我们可以学习更复杂的功能</p><h4 id="例子2-——-电路理论"><a href="#例子2-——-电路理论" class="headerlink" title="例子2 —— 电路理论"></a>例子2 —— 电路理论</h4><p>用一个隐藏神经元数量较少但是具有深度的神经网络来计算某些函数时，如果我们尝试用浅层神经网络来计算同样的函数，则需要指数级的隐藏神经元来进行计算。</p><h2 id="建立深度神经网络的基本框架"><a href="#建立深度神经网络的基本框架" class="headerlink" title="建立深度神经网络的基本框架"></a>建立深度神经网络的基本框架</h2><h3 id="框架的建立过程"><a href="#框架的建立过程" class="headerlink" title="框架的建立过程"></a>框架的建立过程</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.4.png" alt=""></p><p>假设方框中的隐藏层为第 l 层：</p><p>参数：$W^{[l]},b^{[l]}$</p><p>前向传播：输入 $a^{[l-1]}$ ，输出 $a^{[l]}$，缓存 $z^{[l]}$</p><p>反向传播：输入 $da^{[l]}$ 和缓存 $z^{[l]}$，输出 $da^{[l-1]}$，$dW^{[l]}$，$db^{[l]}$</p><p>画成框图如下所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.5.png" alt=""></p><h3 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.6.png" alt=""></p><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p>输入 $a^{[l-1]}$ ，输出 $a^{[l]}$，缓存 $z^{[l]}$</p><ol><li><p>非向量化</p><script type="math/tex; mode=display">z^{[l]}=W^{[l]}a^{[a-1]}+b^{[l]}\\a^{[l]}=g^{[l]}(z^{[l]})</script></li><li><p>向量化</p><script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[a-1]}+b^{[l]}\\A^{[l]}=g^{[l]}(Z^{[l]})</script></li></ol><blockquote><p>$X=A^{[0]} → A^{[1]}→A^{[2]}→…→ A^{[l]}$ </p></blockquote><h3 id="维度的确定"><a href="#维度的确定" class="headerlink" title="维度的确定"></a>维度的确定</h3><p>有如下的一个5层的神经网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.7!.png" alt=""></p><p>对于单个训练样本：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.8.png" alt=""></p><p>$z^{[l]},dz^{[l]},a^{[l]},da^{[l]}:(n^{[l]},1)\\W^{[l]},dW^{[l]}:(n^{[l]},n^{[l-1]})\\b^{[l]},db^{[l]}:(n^{[l]},1)$</p><p>对于多个训练样本：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.9.png" alt=""></p><p>$ Z^{[l]},dZ^{[l]},A^{[l]},dA^{[l]}:(n^{[l]},m)\\W^{[l]},dW^{[l]}:(n^{[l]},n^{[l-1]})\\b^{[l]},db^{[l]}:(n^{[l]},1)$</p><blockquote><p>其中 $l=0$ 时，$A^{[0]}=X=(n^{[0]},m)$</p></blockquote><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><h3 id="公式-1"><a href="#公式-1" class="headerlink" title="公式"></a>公式</h3><p>输入 $da^{[l]}$ 和缓存 $z^{[l]}$，输出 $da^{[l-1]}$，$dW^{[l]}$，$db^{[l]}$</p><ol><li><p>非向量化</p><script type="math/tex; mode=display">dz^{[l]}=da^{[l]}*g^{[l]'}(z^{[l]})\\dW^{[l]}=dz^{[l]}a^{[l-1]}\\db^{[l]}=dz^{[l]}\\da^{[l-1]}=W^{[l]T}dz^{[l]}\\上式可得：da^{[l]}=W^{[l+1]T}dz^{[l+1]}\\带入第一个式子可得：dz^{[l]}=W^{[l+1]T}dz^{[l+1]}*g^{[l]'}(z^{[l]})</script></li><li><p>向量化</p><script type="math/tex; mode=display">dZ^{[l]}=dA^{[l]}*g^{[l]'}(Z^{[l]})\\dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}\\db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)\\dA^{[l-1]}=W^{[l]T}dZ^{[l]}</script></li></ol><h2 id="参数-parameters-与超参数-hyperparameters"><a href="#参数-parameters-与超参数-hyperparameters" class="headerlink" title="参数 (parameters) 与超参数 (hyperparameters)"></a>参数 (parameters) 与超参数 (hyperparameters)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>什么是参数：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},W^{[3]},b^{[3]}…$ </p><p>什么是<strong>超参数</strong>：学习率 (learning rate) $\alpha$ ，迭代次数 (iterations)，隐藏层数 (hidden layers) L，隐藏单元数 (hidden units) $n^{[1]},n^{[2]}…$ ，激活函数 (activation function) 的选择，动量，最小批大小，各种形式的正则化参数等等，由于这些参数都会影响参数 W 和 b 的最终结果，所以我们称之为<strong>超参数</strong></p><h3 id="深度学习是一个基于试验的过程"><a href="#深度学习是一个基于试验的过程" class="headerlink" title="深度学习是一个基于试验的过程"></a>深度学习是一个基于试验的过程</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.10.png" alt=""></p><h3 id="深度学习和大脑有什么关系"><a href="#深度学习和大脑有什么关系" class="headerlink" title="深度学习和大脑有什么关系"></a>深度学习和大脑有什么关系</h3><p>吴恩达：深度学习和大脑相似度并不高，但深度学习和大脑的确有某些可以对比的地方，如下图，但是人脑中的神经元是如何进行学习的还是一个迷，而且至今我们还不清楚到底人脑中有没有一个类似于反向传播或者梯度下降的算法</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.11.png" alt=""></p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 3）</title>
      <link href="/2018/08/14/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w3%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/08/14/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w3%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<p>这是深度学习专项课程第一课第三周的编程作业，通过这次编程作业，可以学到：</p><ul><li>用一个单隐层神经网络实现一个二元分类器</li><li>使用非线性的激活函数</li><li>计算交叉熵损失</li><li>实现前向和后向传播</li></ul><h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> * <span class="comment"># 提供测试例子评估函数正确性</span></span><br><span class="line"><span class="keyword">import</span> sklearn <span class="comment"># 提供简单有效的数据挖掘和数据分析工具</span></span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets <span class="comment"># 提供一些有用的函数</span></span><br><span class="line"></span><br><span class="line">%matplotlib.inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># 使得每次生成的随机数都和第一次相同</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载一个花朵形状的二分类数据集进 X 和 Y</span></span><br><span class="line">X, Y = load_planar_dataset()</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment"># load_planar_dataset 的具体代码</span></span><br><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">load_planar_dataset</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     np.random.seed(<span class="number">1</span>)</span><br><span class="line">&gt;     m = <span class="number">400</span> <span class="comment"># number of examples</span></span><br><span class="line">&gt;     N = int(m/<span class="number">2</span>) <span class="comment"># number of points per class</span></span><br><span class="line">&gt;     D = <span class="number">2</span> <span class="comment"># dimensionality</span></span><br><span class="line">&gt;     X = np.zeros((m,D)) <span class="comment"># data matrix where each row is a single example</span></span><br><span class="line">&gt;     Y = np.zeros((m,<span class="number">1</span>), dtype=<span class="string">'uint8'</span>) <span class="comment"># labels vector (0 for red, 1 for blue)</span></span><br><span class="line">&gt;     a = <span class="number">4</span> <span class="comment"># maximum ray of the flower</span></span><br><span class="line">&gt; </span><br><span class="line">&gt;     <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">&gt;         ix = range(N*j,N*(j+<span class="number">1</span>))</span><br><span class="line">&gt;         t = np.linspace(j*<span class="number">3.12</span>,(j+<span class="number">1</span>)*<span class="number">3.12</span>,N) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># theta</span></span><br><span class="line">&gt;         r = a*np.sin(<span class="number">4</span>*t) + np.random.randn(N)*<span class="number">0.2</span> <span class="comment"># radius</span></span><br><span class="line">&gt;         X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]</span><br><span class="line">&gt;         Y[ix] = j</span><br><span class="line">&gt;         </span><br><span class="line">&gt;     X = X.T</span><br><span class="line">&gt;     Y = Y.T</span><br><span class="line">&gt; </span><br><span class="line">&gt;     <span class="keyword">return</span> X, Y</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.spectral) <span class="comment"># 画散点图，其中参数 c 是颜色索引值，s 是尺寸，cmap 是颜色索引方式</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1.png" alt=""></p><h3 id="确定数据维度"><a href="#确定数据维度" class="headerlink" title="　确定数据维度"></a>　确定数据维度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定数据集的尺寸</span></span><br><span class="line">shape_X = np.shape(X)</span><br><span class="line">shape_Y = np.shape(Y)</span><br><span class="line">m = X.shape[<span class="number">1</span>] <span class="comment"># 训练集尺寸</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'the shape of X is:'</span> + str(shape_X))</span><br><span class="line">print(<span class="string">'the shape of Y is:'</span> + str(shape_Y))</span><br><span class="line">print(<span class="string">'I have m = %d training examples!'</span> %(m))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.2.png" alt=""></p><h2 id="用-sklearn-库实现简单的逻辑回归"><a href="#用-sklearn-库实现简单的逻辑回归" class="headerlink" title="用 sklearn 库实现简单的逻辑回归"></a>用 sklearn 库实现简单的逻辑回归</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练逻辑回归分类器</span></span><br><span class="line">clf = sklearn.linear_model.logisticRegressionCV() <span class="comment"># 建立一个逻辑回归的对象 clf</span></span><br><span class="line">clf.fit(X.T, Y.T) <span class="comment"># 用数据集拟合模型，返回一个训练过的模型对象，注意数据集的尺寸为 (n_samples, n_features)，所以要进行转置</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印逻辑回归的边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x),X,Y)</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">&gt;     <span class="comment"># 设置图形最大最小值</span></span><br><span class="line">&gt;     x_min, x_max = X[<span class="number">0</span>, :].min() - <span class="number">1</span>, X[<span class="number">0</span>, :].max() + <span class="number">1</span></span><br><span class="line">&gt;     y_min, y_max = X[<span class="number">1</span>, :].min() - <span class="number">1</span>, X[<span class="number">1</span>, :].max() + <span class="number">1</span></span><br><span class="line">&gt;     h = <span class="number">0.01</span></span><br><span class="line">&gt;     <span class="comment"># np.meshgrid() 形成间隔为 h 的点阵</span></span><br><span class="line">&gt;     xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">&gt;     <span class="comment"># 预测点阵的标签值</span></span><br><span class="line">&gt;     Z = model(np.c_[xx.ravel(), yy.ravel()]) <span class="comment"># np.c_[] 按行连接两个矩阵，ravel() 把多维数组变成一维数组</span></span><br><span class="line">&gt;     Z = Z.reshape(xx.shape)</span><br><span class="line">&gt;     <span class="comment"># 生成等高线图像</span></span><br><span class="line">&gt;     plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) <span class="comment"># 生成等高线图并添加颜色</span></span><br><span class="line">&gt;     plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">&gt;     plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">&gt;     plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=y, cmap=plt.cm.Spectral)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印精确度</span></span><br><span class="line">LR_predictions = clf.predict(X.T) <span class="comment"># 用模型预测的 X 的标签值</span></span><br><span class="line">print(<span class="string">'Accuracy of logistic regression: %d '</span> % float( (np.dot(Y,LR_prediction) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.4.png" alt=""></p><p>我们可以发现使用逻辑回归进行预测的准确度非常低！</p><h2 id="神经网络模型"><a href="#神经网络模型" class="headerlink" title="神经网络模型"></a>神经网络模型</h2><h3 id="建立神经网络的一般方法"><a href="#建立神经网络的一般方法" class="headerlink" title="建立神经网络的一般方法"></a>建立神经网络的一般方法</h3><ul><li>定义神经网络结构，包括输入特征个数，隐藏神经元个数等</li><li>初始化模型参数</li><li>循环以下步骤：<ul><li>实现前向传播</li><li>计算代价函数</li><li>实现反向传播，得到梯度</li><li>使用梯度下降更新参数</li></ul></li><li>将所有辅助函数合并到一个主函数 nn_model() 中</li></ul><h3 id="定义神经网络结构"><a href="#定义神经网络结构" class="headerlink" title="定义神经网络结构"></a>定义神经网络结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定 n_x n_h n_y 三个变量</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    </span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># 计算输入层的尺寸</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment"># 计算隐藏层的尺寸，即隐藏神经元个数</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># 计算输出层的尺寸</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><p>注意：</p><ul><li>确保所有参数的尺寸正确</li><li>对于权重 W 采用<strong>随机初始化</strong>的方法</li><li>对于偏差 b 采用初始化为<strong>零</strong>的方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数的初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    </span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span> <span class="comment"># 0.01 使得初始化后的参数更小，梯度下降更快，W1(n_h, n_x)</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>)) <span class="comment"># 切记是 np.zeros(()),b1(n_h,1)</span></span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span> <span class="comment"># W2(n_y, n_h)</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>)) <span class="comment"># b2(n_y,1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保参数唯独正确</span></span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数值放入字典 parameters</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>步骤：</p><ul><li>从 parameters 字典中取出每个参数</li><li>实现前向传播，计算 Z1,A1,Z2,A2 的值</li><li>把这些值放在字典 cache 中</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 把参数从 parameters 取出来</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算Z1 Z2 A1 A2 的值</span></span><br><span class="line">    Z1 = np.dot(W1,X) + b1</span><br><span class="line">    A1 = np.tanh(Z1) <span class="comment"># numpy 内置的 tanh() 函数</span></span><br><span class="line">    Z2 = np.dot(W2,A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2) <span class="comment"># sigmoid 已经 imported</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保 A2 的尺寸正确</span></span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 把求到的值放入字典 cache</span></span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><h3 id="计算代价函数"><a href="#计算代价函数" class="headerlink" title="计算代价函数"></a>计算代价函数</h3><script type="math/tex; mode=display">J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(<span class="number">1</span>-A2), (<span class="number">1</span>-Y)) <span class="comment"># np.multiply() 为逐元素相乘</span></span><br><span class="line">    cost = -(<span class="number">1</span>/m)*np.sum(logprobs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保 cost 的维度是我们需要的，比如把 [[17]] 变成 17</span></span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost,float))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.14.png" alt=""></p><ul><li>因为  $g^{[1]}(z) = tanh(z)$ ， 所以 $g^{[1]’}(z) = 1-a^2$，所以可以使用 <code>(1 - np.power(A1, 2))</code> 来计算 $g^{[1]’}(Z^{[1]})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">back_propgation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 首先将 W1 和 W2 从 parameters 里取出来,将 A1 和 A2 从 cache 里取出来</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = paramaters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算梯度</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = (<span class="number">1</span>/m)*np.dot(dZ2,A1.T)</span><br><span class="line">    db2 = (<span class="number">1</span>/m)*np.sum(dZ2,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T,dZ2),(<span class="number">1</span>-np.power(A1,<span class="number">2</span>)))</span><br><span class="line">    dW1 = (<span class="number">1</span>/m)*np.dot(dZ1,X.T)</span><br><span class="line">    db1 = (<span class="number">1</span>/m)*np.sum(dZ1,axis=<span class="number">1</span>,keepdims=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将数据存入 grads 中</span></span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><script type="math/tex; mode=display"> \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度下降法更新数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 首先将参数从 parameters 中取出来</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 然后将梯度从 grads 中取出来</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数，此处只更新一次参数</span></span><br><span class="line">    W1 = W1 - learning_rate*dW1</span><br><span class="line">    b1 = b1 - learning_rate*db1</span><br><span class="line">    W2 = W2 - learning_rate*dW2</span><br><span class="line">    b2 = b2 - learning_rate*db2</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将更新后的参数放到 parameters 中</span></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="将以上所有辅助函数合并到主函数中"><a href="#将以上所有辅助函数合并到主函数中" class="headerlink" title="将以上所有辅助函数合并到主函数中"></a>将以上所有辅助函数合并到主函数中</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iteration = <span class="number">10000</span>, print_cost=False )</span>:</span></span><br><span class="line">    <span class="comment"># 定义模型结构</span></span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sized(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数取出</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降迭代</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        A2, cache = forword_propagation(X, paramaters)</span><br><span class="line">        <span class="comment"># 计算代价函数</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line">        <span class="comment"># 梯度下降更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate = <span class="number">1.2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每千步打印一次 cost 值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>：</span><br><span class="line">        print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="用模型进行预测"><a href="#用模型进行预测" class="headerlink" title="用模型进行预测"></a>用模型进行预测</h3><p>如果最后的预测值大于 0.5，则标签值为 1，反之标签值为 0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用前向传递函数进行预测</span></span><br><span class="line">    A2, cache = forward_propagation(X,parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>) </span><br><span class="line">    <span class="comment"># predictions = np.where(A2 &gt; 0.5, 1, 0)</span></span><br><span class="line">    <span class="comment"># numpy.where(condition, x, y)当 conditon 的某个位置的为 true 时，输出 x 的对应位置的元素，否则选择 y 对应位置的元素；</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h2 id="分析讨论"><a href="#分析讨论" class="headerlink" title="分析讨论"></a>分析讨论</h2><h3 id="当隐藏节点个数为-4-时"><a href="#当隐藏节点个数为-4-时" class="headerlink" title="当隐藏节点个数为 4 时"></a>当隐藏节点个数为 4 时</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立 n_h = 4 的模型</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印出分类边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印出精确度</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))  /  float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.7.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.8.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.9.png" alt=""></p><p>我们可以看到对比逻辑回归，四个隐藏节点的神经网络模型的预测精确度非常高！</p><p>接下来我们看看不同隐藏节点对预测结果的影响。</p><h3 id="改变隐藏节点个数观察结果"><a href="#改变隐藏节点个数观察结果" class="headerlink" title="改变隐藏节点个数观察结果"></a>改变隐藏节点个数观察结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印不同隐藏节点模型的预测结果</span></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>] <span class="comment"># 不同隐藏节点个数</span></span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.10.png" alt=""></p><h3 id="得出结论"><a href="#得出结论" class="headerlink" title="得出结论"></a>得出结论</h3><ul><li>隐藏节点的个数越多，对数据集的拟合程度越好，直到最终发生过拟合</li><li>最好的隐藏节点个数似乎大概在 5 个左右，在这个值附近的的模型对数据集拟合得很好且没有发生过拟合</li><li>regularization 是减小大型模型（比如 n_h = 50）过拟合的一个方法，在后面会学到 </li></ul><h3 id="更换其他数据集进行试验"><a href="#更换其他数据集进行试验" class="headerlink" title="更换其他数据集进行试验"></a>更换其他数据集进行试验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line"></span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"></span><br><span class="line">dataset = <span class="string">"noisy_circles"</span> <span class="comment"># 选择数据集</span></span><br><span class="line"></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">5</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出分类边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印精确度</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_3.11.png" alt=""></p>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 3）—— 浅层神经网络</title>
      <link href="/2018/08/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w3/"/>
      <url>/2018/08/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w3/</url>
      <content type="html"><![CDATA[<p>本周主要介绍<strong>单隐层神经网络</strong> (one hidden layer Neural Network)。</p><h2 id="神经网络概况"><a href="#神经网络概况" class="headerlink" title="神经网络概况"></a>神经网络概况</h2><h3 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h3><p>逻辑回归的计算图如下，这是一个最小的神经网络：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.2.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.1.png" alt=""></p><p>堆叠一系列的 $\sigma$ 单元，构建一个单隐层神经网络：</p><a id="more"></a><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.3.png" alt=""></p><h3 id="神经网络的含义"><a href="#神经网络的含义" class="headerlink" title="神经网络的含义"></a>神经网络的含义</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.4.png" alt=""></p><ul><li>第一个方框称之为 <strong>输入层 (Input layer)</strong> ，这一层的激活向量用 $a^{[0]}$ 表示，$x = a^{[0]}$，其中字母 a 表示 activate(激活值)</li><li>第二个方框称之为 <strong>隐藏层 (Hidden layer )</strong>，这一层的激活向量用 $a^{[1]}$ 表示</li><li>第三个方框称之为 <strong>输出层 (Output layer)</strong>，这一层的激活向量用 $a^{[2]}$ 表示，$\hat y = a^{[2]}$</li><li>神经网络的层数用右上角的方括号表示，输入层不算入层数</li><li>第一层的参数为 $W^{[1]} (4,3)$ 和 $b^{[1]} (4,1)$</li><li>第二层的参数为 $W^{[2]} (1,4)$ 和 $b^{[2]} (1,4)$</li><li>每个圆圈称之为 <strong>节点 (node)</strong></li></ul><h2 id="神经网络的前向传播及其向量化"><a href="#神经网络的前向传播及其向量化" class="headerlink" title="神经网络的前向传播及其向量化"></a>神经网络的前向传播及其向量化</h2><h3 id="非向量化公式"><a href="#非向量化公式" class="headerlink" title="非向量化公式"></a>非向量化公式</h3><p>在逻辑回归中一个圆圈代表两步运算，如下图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.5.png" alt=""></p><p>在神经网络中每个圆圈也代表两步运算，将圆圈隔离出来看便是一个逻辑回归模型，如图所示：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.6.png" alt=""></p><p>整理成如下的样子：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.7.png" alt=""></p><h3 id="单训练样本的向量化"><a href="#单训练样本的向量化" class="headerlink" title="单训练样本的向量化"></a>单训练样本的向量化</h3><p>已知：</p><script type="math/tex; mode=display">z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1, a^{[1]}_1= \sigma (z^{[1]}_1) \\z^{[1]}_2 = w^{[1]T}_2 x + b^{[1]}_2, a^{[1]}_2= \sigma (z^{[1]}_2) \\z^{[1]}_3 = w^{[1]T}_3 x + b^{[1]}_3, a^{[1]}_3= \sigma (z^{[1]}_3) \\z^{[1]}_4 = w^{[1]T}_4 x + b^{[1]}_4, a^{[1]}_4= \sigma (z^{[1]}_4) \\</script><p>推导：</p><script type="math/tex; mode=display">z^{[1]}=\begin{bmatrix}z^{[1]}_1\\ z^{[1]}_2\\ z^{[1]}_3\\ z^{[1]}_4\end{bmatrix}=\begin{bmatrix}w^{[1]T}_1x+b^{[1]}_1\\ w^{[1]T}_2x+b^{[1]}_2\\ w^{[1]T}_3x+b^{[1]}_3\\ w^{[1]T}_4x+b^{[1]}_4\end{bmatrix}=\begin{bmatrix}--& w^{[1]T}_1 &-- \\  --& w^{[1]T}_2 &-- \\ -- & w^{[1]T}_3 & --\\  --& w^{[1]T}_4 & --\end{bmatrix}_{(4,3)}\begin{bmatrix}x_1\\ x_2\\ x_3\end{bmatrix}+\begin{bmatrix}b^{[1]}_1\\ b^{[1]}_2\\ b^{[1]}_3\\ b^{[1]}_4\end{bmatrix}=W^{[1]}x + b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=\begin{bmatrix}a^{[1]}_1\\ a^{[1]}_2\\ a^{[1]}_3\\ a^{[1]}_4\end{bmatrix}=\sigma (z^{[1]})</script><p>最后得到计算神经网络前向传播的四个公式：</p><script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]} =W^{[1]}a^{[0]}+b^{[1]}\\a^{[1]}=\sigma (z^{[1]}) \\z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} \\a^{[2]}=\sigma (z^{[2]}) \\</script><blockquote><p>注意：</p><script type="math/tex; mode=display">\begin{bmatrix}--& w^{[1]T}_1 &-- \\  --& w^{[1]T}_2 &-- \\ -- & w^{[1]T}_3 & --\\  --& w^{[1]T}_4 & --\end{bmatrix}_{(4,3)}=W^{[1]} \neq W^{[1]T}</script><p>这里与逻辑回归中有所不同，参数 $W^{[1]} (4,3)$ 和 $W^{[2]} (1,4)$ 的右上角没有转置符号，所有的参数 w 都成了行向量而不是列向量，也就是说 $W^{[2]}$ 相当于逻辑回归中的 $w^T$ </p></blockquote><h3 id="多个训练样本的向量化"><a href="#多个训练样本的向量化" class="headerlink" title="多个训练样本的向量化"></a>多个训练样本的向量化</h3><ol><li>记法：</li></ol><p>对于第 i 个训练样本的预测值我们记为：$\hat y^{(i)} = a^{ [2]  (i)}$，其中 [2] 表示第二层，(i) 表示第 i 个训练样本。</p><p>为了输出所有样本的预测值，我们可以遍历所有的样本：</p><p>$for i = 1 \quad to \quad  m: \\ \quad z^{[1] (i)}=W^{[1]} x^{(i)}+b^{[1]} =W^{[1]} a^{[0] (i)}+b^{[1]} \\  \quad a^{[1] (i)}=\sigma (z^{[1] (i)}) \\ \quad z^{[2] (i)}=W^{[2]} a^{[1] (i)}+b^{[2]} \\ \quad a^{[2] (i)}=\sigma (z^{[2] (i)}) $</p><ol><li>公式：</li></ol><script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}\\A^{[1]}=\sigma (Z^{[1]})\\Z^{[2]}=W^{[2]}A^{[2]}+b^{[1]}\\A^{[2]}=\sigma (Z^{[2]})\\</script><p>其中：</p><script type="math/tex; mode=display">Z^{[1]}= \begin{bmatrix}| &|  &  &| \\  z^{[1](1)}&  z^{[1](2)} & ... &  z^{[1](m)}\\  |& | &  &| \end{bmatrix}\quadA^{[1]}= \begin{bmatrix}| &|  &  &| \\  a^{[1](1)}&  a^{[1](2)} & ... &  a^{[1](m)}\\  |& | &  &| \end{bmatrix}\quadX^{[1]}= \begin{bmatrix}| &|  &  &| \\  x^{[1](1)}&  x^{[1](2)} & ... &  x^{[1](m)}\\  |& | &  &| \end{bmatrix}\\</script><script type="math/tex; mode=display">Z^{[2]}= \begin{bmatrix}| &|  &  &| \\  z^{[2](1)}&  z^{[2](2)} & ... &  z^{[2](m)}\\  |& | &  &| \end{bmatrix}\quadA^{[2]}= \begin{bmatrix}| &|  &  &| \\  a^{[2](1)}&  a^{[2](2)} & ... &  a^{[2](m)}\\  |& | &  &| \end{bmatrix}\quadX^{[2]}= \begin{bmatrix}| &|  &  &| \\  x^{[2](1)}&  x^{[2](2)} & ... &  x^{[2](m)}\\  |& | &  &| \end{bmatrix}\\</script><blockquote><p>矩阵的<strong>横向</strong>代表不同的<strong>训练样本</strong>，<strong>纵向</strong>代表不同的<strong>隐藏节点 (hidden unit)</strong> </p></blockquote><ol><li>推导过程：</li></ol><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.8.png" alt=""></p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h3><h4 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h4><script type="math/tex; mode=display">sigmoid: g(z)=a=\frac {1}{1+e^{-z}}\\ \quad \quad \quad g'(z)=a(1-a)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.9.png" alt=""></p><ul><li>由于 tanh 函数比 sigmoid 函数更好，所以 sigmoid 函数只用在<strong>二元分类问题的输出层</strong></li></ul><h4 id="tanh-函数"><a href="#tanh-函数" class="headerlink" title="tanh 函数"></a>tanh 函数</h4><script type="math/tex; mode=display">tanh：g(z)=a=\frac {e^{z}-e^{-z}}{e^{z}+e^{-z}}\\\quad \quad \quad \quad \quad \quad \quad \quad g'(z)=1-(tanh(z))^2=1-a^2</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.10.png" alt=""></p><ul><li>也叫双曲正切函数，是一个移位后重新调整比例使输出范围在 -1 到 1 之间的 sigmoid 函数</li><li>sigmoid 函数使隐藏层输出的平均值逼近于 0.5 ，而 tanh 函数更加逼近 0，这样使得下一层的学习更加简单，所以 tanh 函数在大多数情况下都严格优于 sigmoid 函数</li><li>缺点是在 z 很大时，函数的斜率会接近 0 ，这样使得学习速率变得很慢</li></ul><h4 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="ReLU 函数"></a>ReLU 函数</h4><script type="math/tex; mode=display">ReLU:g(z)=a=max\{0,z\}\\\quad \quad \quad \quad g'(z)=\left\{\begin{matrix}0 &  &if \ z<0 \\  1&  & if \ z\geqslant 0\end{matrix}\right.</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.11.png" alt=""></p><ul><li>实际上当 z = 0 时导数无定义，但是工程上由于 z = 0 的概率无限小，所以 g’(0) 可以设为任意值</li><li>也叫线性整流函数</li><li>减少了激活函数导数趋向 0 的现象</li><li>隐藏层的激活函数的首选，用的最多</li></ul><h4 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h4><script type="math/tex; mode=display">Leaky ReLU:g(z)= a = max\{ 0.01z,z\}\\\quad \quad \quad \quad \quad \quad \quad g'(z)=\left\{\begin{matrix}0.01 &  &if \ z<0 \\  1&  & if \ z\geqslant 0\end{matrix}\right.</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.12.png" alt=""></p><ul><li>实际上当 z = 0 时导数无定义，但是工程上由于 z = 0 的概率无限小，所以 g’(0) 可以设为任意值</li><li>一般效果好于 ReLU，但是用的较少</li></ul><h3 id="为什么神经网络需要非线性激活函数"><a href="#为什么神经网络需要非线性激活函数" class="headerlink" title="为什么神经网络需要非线性激活函数"></a>为什么神经网络需要非线性激活函数</h3><p>已知公式：</p><p>$z^{[1]}=W^{[1]} x+b^{[1]} \\ a^{[1]}=g^{[1]} (z^{[1]}) \\ z^{[2]}=W^{[2]} a^{[1]}+b^{[2]} \\ a^{[2]} = g^{[2]} (z^{[2]})$</p><p>假设 $g^{[1]} 和g^{[2]}$ 是线性激活函数：$g^{[1]}=g^{[2]}=z$  </p><p>即：$a^{[1]}=z^{[1]}$，$a^{[2]}=z^{[2]}$ </p><p>可以推导出：</p><p>$a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}z^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}\\=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W’x+b’$</p><p>我们可以发现，当激活函数是线性函数时，不管神经网络有多少层，最后的输出值只是输入值的线性函数，这样还不如去除所有的隐藏层，因为我们可以看到最后的结果和逻辑回归模型一模一样，所以线性的隐藏层<strong>没有任何作用</strong>，线性函数的组合还是线性函数。</p><blockquote><p>只有一种情况会用到线性激活函数，就是例如预测房价这类回归问题的<strong>输出层</strong></p></blockquote><h2 id="神经网络的反向传播及其向量化"><a href="#神经网络的反向传播及其向量化" class="headerlink" title="神经网络的反向传播及其向量化"></a>神经网络的反向传播及其向量化</h2><h3 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h3><p>$输入: n_x=n^{[0]}, n^{[1]},n^{[2]}=1\\参数: W^{[1]} \rightarrow (n^{[1]},n^{[0]}),b^{[1]}\rightarrow (n^{[1]},1),W^{[2]} \rightarrow  (n^{[2]},n^{[1]}),b^{[2]} \rightarrow (n^{[1]},1)\\ 代价函数：J(W^{[1]},b^{[1]},W^{[2]},b^{[2]})=\frac {1}{m} \sum\limits_{i=1}^nL(\hat y,y),\hat y=a^{[1]}\\梯度下降：\\ repeat:\{ \\ 计算预测值 \hat y,i=1,2…,m\\计算 dW^{[1]}=\frac {dJ}{dW^{[1]}}, db^{[1]}=\frac {dJ}{db^{[1]}},…\\ W^{[1]}:=  W^{[1]}-\alpha dW^{[1]}\\b^{[1]}:=  b^{[1]}-\alpha db^{[1]}\\W^{[2]}:=  W^{[2]}-\alpha dW^{[2]}\\b^{[2]}:=  b^{[2]}-\alpha db^{[2]}\\ \}$</p><h3 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.14.png" alt=""></p><ul><li>左边是单个训练样本的向量化，右边是多个训练样本的向量化</li><li>keepdims = True 是为了保持输出我们需要的维度，防止出现秩为 1 的数组</li><li>第四行的 <strong>*</strong> 号代表 <strong>逐元素相乘</strong>  </li><li>由于 W 的每个 w 都变成行向量，这点与逻辑回归不同，故第二行、第五行的 $a^{[1]T},x^T,A^{[1]T},X^T$ 都要加转置符号</li><li>只有当是二元分类时，第一行公式才成立</li><li>$dZ^{[1]} \rightarrow (n^{[1]},m),W^{[2]T}dZ^{[2]} \rightarrow (n^{[1]},m),g^{[1]’}(Z^{[1]}) \rightarrow (n^{[1]},m)$</li></ul><h3 id="向量化的推导过程"><a href="#向量化的推导过程" class="headerlink" title="向量化的推导过程"></a>向量化的推导过程</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.15.png" alt=""></p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.17.png" alt=""></p><h2 id="参数的随机初始化"><a href="#参数的随机初始化" class="headerlink" title="参数的随机初始化"></a>参数的随机初始化</h2><h3 id="为什么不能将权重-W-全初始化为零"><a href="#为什么不能将权重-W-全初始化为零" class="headerlink" title="为什么不能将权重  W 全初始化为零"></a>为什么不能将权重  W 全初始化为零</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl3.18.png" alt=""></p><p>对于上面的的神经网络，我们假设将 $W^{[1]}$ 和 $b^{[1]}$ 全都初始化为零，正如我们在逻辑回归中做的那样，即 $W^{[1]}=\begin{bmatrix}0 &amp;0 \\0 &amp; 0\end{bmatrix}\quad b^{[1]}=\begin{bmatrix}0\\0\end{bmatrix}$ ，导致 $a^{[1]}_1=a^{[2]}_2$ ，在反向传播时有，$dz^{[1]}_1=dz^{[2]}_2$ ，于是 $dW=\begin{bmatrix}u&amp;v \\u &amp; v\end{bmatrix}$ ，由于 $W^{[1]}=W^{[1]}-\alpha dW$，所以 $W^{[1]}=\begin{bmatrix}a&amp;b \\a &amp; b\end{bmatrix}$，我们可以看到，无论迭代多少次，无论神经网络训练多久，这两个隐藏神经元的参数始终都相同，他们始终都在做相同的运算，称这两个神经元是<strong>对称的</strong>，不能给我们带来任何帮助。</p><h3 id="参数随机初始化"><a href="#参数随机初始化" class="headerlink" title="参数随机初始化"></a>参数随机初始化</h3><script type="math/tex; mode=display">W^{[1]}=np.random.randn((2,2))*0.01\\b^{[1]}=np,zero((2,1))\\W^{[2]}=np.random.randn((1,2))*0.01\\b^{[2]}=0</script><blockquote><p>数据 <strong>0.0.1</strong> 必须比较小，因为如果 W 非常大，那么 z 也相应会非常大，那么可以发现此处对应的激活函数比如 sigmoid 或者 tanh 函数的斜率会非常小，这意味着梯度下降得非常慢，所以应该使 W 为一个较小的值</p></blockquote>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 2）</title>
      <link href="/2018/08/09/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2018/08/09/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0c1w2%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      <content type="html"><![CDATA[<h2 id="numpy-使用基础"><a href="#numpy-使用基础" class="headerlink" title="numpy 使用基础"></a>numpy 使用基础</h2><p>Numpy 是 Python 里用于科学计算的模块，由一个<a href="www.numpy.org">开源社区</a>进行维护，下面介绍一些用于神经网络搭建的函数的构建</p><h3 id="sigmoid-函数，np-exp"><a href="#sigmoid-函数，np-exp" class="headerlink" title="sigmoid 函数，np.exp( )"></a>sigmoid 函数，np.exp( )</h3><p>回忆：$sigmoid(x)=\frac {1}{1+e^{-x}}$</p><p>如果 $ x = (x_1, x_2, …, x_n)$ 是一个行向量，那么 $np.exp(x)$ 会将 $exp( )$ 函数用于 x 的每个元素，输出 $np.exp(x) = (e^{x_1}, e^{x_2}, …, e^{x_n})$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.exp的例子</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])<span class="comment">#x 为一个行向量</span></span><br><span class="line">print(np.exp(x)) <span class="comment"># 结果为 (exp(1), exp(2), exp(3))</span></span><br></pre></td></tr></table></figure><p>>> [  2.71828183   7.3890561   20.08553692]</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更多向量操作例子</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (x + <span class="number">3</span>)</span><br><span class="line">print(<span class="number">1</span>/x)</span><br></pre></td></tr></table></figure><p>>&gt;[4 5 6]</p><p>>&gt;[ 1.          0.5         0.33333333]</p><script type="math/tex; mode=display">\text{For } x \in \mathbb {R}^n \text{,     } sigmoid(x) = sigmoid\begin{pmatrix}x_1  \\x_2  \\    ...  \\    x_n  \\\end{pmatrix} = \begin{pmatrix}\frac{1}{1+e^{-x_1}}  \\\frac{1}{1+e^{-x_2}}  \\    ...  \\    \frac{1}{1+e^{-x_n}}  \\\end{pmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#用 np.exp() 代替 numpy.exp()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">sigmoid(x)</span><br></pre></td></tr></table></figure><p>>&gt;array([ 0.73105858,  0.88079708,  0.95257413])</p><h3 id="求-sigmoid-函数的导数-dZ"><a href="#求-sigmoid-函数的导数-dZ" class="headerlink" title="求 sigmoid 函数的导数 (dZ)"></a>求 sigmoid 函数的导数 (dZ)</h3><blockquote><p>注意：在前面的笔记中，吴恩达的课程视频中写的 dZ 的求法是：$dZ=A-Y$，但是在课程 ppt 和编程作业中的求法是：$dZ=\sigma(x)(1-\sigma(x))$</p></blockquote><script type="math/tex; mode=display">sigmoid\_derivative(x) = {\sigma' (x)} = \sigma(x)(1-\sigma(x))=A(1-A)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span></span><br><span class="line"><span class="string">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    s = sigmoid(x)</span><br><span class="line">    ds = s * (<span class="number">1</span> - s)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid_derivative(x) = "</span> + str(sigmoid_derivative(x)))</span><br></pre></td></tr></table></figure><p>>&gt;sigmoid_derivative(x) = [ 0.19661193  0.10499359  0.04517666]</p><h3 id="reshape-命令重构矩阵"><a href="#reshape-命令重构矩阵" class="headerlink" title="reshape 命令重构矩阵"></a>reshape 命令重构矩阵</h3><ul><li>X.shape 命令用于得到一个矩阵或向量的维度（形状）</li><li>X.reshape 命令用于把 X 重构为某个维度，例如把 shape (length,height,depth=3) 的图片变成输入，重构为 shape (length∗height∗3,1) 的向量</li></ul><p>下面的代码把一张图片变为一个向量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    v = image.reshape((image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>],<span class="number">1</span>)) </span><br><span class="line">    <span class="comment">#image.shape[0] 获得图片或数组第一个维度的长度</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values</span></span><br><span class="line">image = np.array([[[ <span class="number">0.67826139</span>,  <span class="number">0.29380381</span>],</span><br><span class="line">        [ <span class="number">0.90714982</span>,  <span class="number">0.52835647</span>],</span><br><span class="line">        [ <span class="number">0.4215251</span> ,  <span class="number">0.45017551</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">0.92814219</span>,  <span class="number">0.96677647</span>],</span><br><span class="line">        [ <span class="number">0.85304703</span>,  <span class="number">0.52351845</span>],</span><br><span class="line">        [ <span class="number">0.19981397</span>,  <span class="number">0.27417313</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">0.60659855</span>,  <span class="number">0.00533165</span>],</span><br><span class="line">        [ <span class="number">0.10820313</span>,  <span class="number">0.49978937</span>],</span><br><span class="line">        [ <span class="number">0.34144279</span>,  <span class="number">0.94630077</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"image2vector(image) = "</span> + str(image2vector(image)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">image2vector(image) = [[ 0.67826139]</span><br><span class="line"> [ 0.29380381]</span><br><span class="line"> [ 0.90714982]</span><br><span class="line"> [ 0.52835647]</span><br><span class="line"> [ 0.4215251 ]</span><br><span class="line"> [ 0.45017551]</span><br><span class="line"> [ 0.92814219]</span><br><span class="line"> [ 0.96677647]</span><br><span class="line"> [ 0.85304703]</span><br><span class="line"> [ 0.52351845]</span><br><span class="line"> [ 0.19981397]</span><br><span class="line"> [ 0.27417313]</span><br><span class="line"> [ 0.60659855]</span><br><span class="line"> [ 0.00533165]</span><br><span class="line"> [ 0.10820313]</span><br><span class="line"> [ 0.49978937]</span><br><span class="line"> [ 0.34144279]</span><br><span class="line"> [ 0.94630077]]</span><br></pre></td></tr></table></figure><h3 id="输入矩阵的行向量标准化"><a href="#输入矩阵的行向量标准化" class="headerlink" title="输入矩阵的行向量标准化"></a>输入矩阵的行向量标准化</h3><p>将数据标准化会使得模型表现得更好，因为标准化之后梯度下降收敛速度更快，我们可以通过将输入矩阵 x 每一个<strong>行向量</strong>除以该行向量的模，即 $ \frac{x}{| x|} $</p><p>如果 <script type="math/tex">x = \begin{bmatrix}    0 & 3 & 4 \\    2 & 6 & 4 \\\end{bmatrix}</script> 那么 $| x| = np.linalg.norm(x, axis = 1, keepdims = True) =\begin{bmatrix}    5 \\    \sqrt{56} \\\end{bmatrix} $ 而且 $ x_normalized = \frac{x}{| x|} = \begin{bmatrix}    0 &amp; \frac{3}{5} &amp; \frac{4}{5} \\    \frac{2}{\sqrt{56}} &amp; \frac{6}{\sqrt{56}} &amp; \frac{4}{\sqrt{56}} \\\end{bmatrix}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    x_norm = np.linalg.norm(x, ord = <span class="number">2</span>,axis = <span class="number">1</span>,keepdims = <span class="keyword">True</span>)</span><br><span class="line">    <span class="comment">#计算 x 的范数或者说行向量的模，其中 ord = 2 表示范数类型为 2，即平方和的开方，axis =1 表示按行向量处理，keepdims = True 表示保持矩阵的二维特性</span></span><br><span class="line">    x = x / x_norm <span class="comment">#用 x 矩阵除以行向量的模，自动进行广播拓展</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line">print(<span class="string">"normalizeRows(x) = "</span> + str(normalizeRows(x)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; normalizeRows(x) = [[ 0.          0.6         0.8       ]</span><br><span class="line"> [ 0.13736056  0.82416338  0.54944226]]</span><br></pre></td></tr></table></figure><h3 id="广播（broadcasting）和-softmax-函数"><a href="#广播（broadcasting）和-softmax-函数" class="headerlink" title="广播（broadcasting）和 softmax 函数"></a>广播（broadcasting）和 softmax 函数</h3><p>你可以把 softmax 函数看作一个用来标准化的函数，当算法需要进行二元或者更多元分类时</p><ul><li><p>$ \text{for  } \  x \in \mathbb{R}^{1\times n} \text{,     } softmax(x) = softmax(\begin{bmatrix}  x_1  &amp;&amp;    x_2 &amp;&amp;    …  &amp;&amp;    x_n  \end{bmatrix}) = \begin{bmatrix}     \frac{e^{x_1}}{\sum_{j}e^{x_j}}  &amp;&amp;    \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &amp;&amp;    …  &amp;&amp;    \frac{e^{x_n}}{\sum_{j}e^{x_j}} \end{bmatrix} $</p></li><li><p>$\text{for a matrix } x \in \mathbb{R}^{m \times n} \text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  </p><script type="math/tex; mode=display">softmax(x) = softmax\begin{bmatrix}    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\    \vdots & \vdots & \vdots & \ddots & \vdots \\    x_{m1} & x_{m2} & x_{m3} & \dots  & x_{mn}\end{bmatrix} \\= \begin{bmatrix}    \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} & \dots  & \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\    \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} & \dots  & \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\    \vdots & \vdots & \vdots & \ddots & \vdots \\    \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} & \dots  & \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}\end{bmatrix} = \begin{pmatrix}    softmax\text{(first row of x)}  \\    softmax\text{(second row of x)} \\    ...  \\    softmax\text{(last row of x)} \\\end{pmatrix}</script></li></ul><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Calculates the softmax for each row of the input x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Your code should work for a row vector and also for matrices of shape (n, m).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n,m)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    s -- A numpy matrix equal to the softmax of x, of shape (n,m)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对 x 每个元素求 exp()</span></span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 x_exp 每行的和，使用 np.sum(..., axis = 1, keepdims = True)</span></span><br><span class="line">    x_sum = np.sum(x_exp, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用广播特性自动计算 softmax 函数</span></span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span> ,<span class="number">0</span>]])</span><br><span class="line">print(<span class="string">"softmax(x) = "</span> + str(softmax(x)))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; softmax(x) = [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04</span><br><span class="line">    1.21052389e-04]</span><br><span class="line"> [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04</span><br><span class="line">    8.01252314e-04]]</span><br></pre></td></tr></table></figure><h3 id="两种损失函数-L1-和-L2"><a href="#两种损失函数-L1-和-L2" class="headerlink" title="两种损失函数 L1 和 L2"></a>两种损失函数 L1 和 L2</h3><ol><li><p>L1 损失函数定义为：</p><script type="math/tex; mode=display">\begin{align*} & L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}| \end{align*}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L1 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = np.sum(np.abs(y - yhat))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L1 = "</span> + str(L1(yhat,y)))</span><br></pre></td></tr></table></figure><p>>&gt; L1 = 1.1</p></li><li><p>L2 损失函数定义为：</p><script type="math/tex; mode=display">\begin{align*} & L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2 \end{align*}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L2 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = np.dot(y-yhat, y-yhat) <span class="comment"># 直接写成自己和自己做点积</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L2 = "</span> + str(L2(yhat,y)))</span><br></pre></td></tr></table></figure><p>>&gt; L2 = 0.43</p></li></ol><h2 id="用逻辑回归识别猫的图片"><a href="#用逻辑回归识别猫的图片" class="headerlink" title="用逻辑回归识别猫的图片"></a>用逻辑回归识别猫的图片</h2><p>实现步骤：</p><ul><li>建立一个学习算法的一般结构，包括：<ul><li>初始化参数</li><li>计算代价函数和它对参数的导数</li><li>使用梯度下降法迭代参数</li></ul></li><li>把这三个函数用正确的顺序聚合到一个模型主函数中</li></ul><h3 id="相关包"><a href="#相关包" class="headerlink" title="相关包"></a>相关包</h3><ul><li>numpy：python 科学计算基础包</li><li>h5py：与存储在 H5 文件中的数据集交互的常用的包 </li><li>matplotlib：python 中用来画图像的一个包</li><li>PIL 和 scipy：最后用来测试自己的模型使用</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 方便调用</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><p>建立一个简单的判断图片是不是猫的识别算法，数据集包括：</p><ul><li>m_train 个训练集，包括图片集 train_set_x_orig 和标签集 train_set_y</li><li>m_test 个测试集，包括图片集 test_set_x_orig 和标签集 test_set_y</li><li>每张图片都是<strong>方形</strong> (height = num_px, width = num_px)，有三个颜色通道，所以数组形状是 (num_px, num_px, 3)</li><li>每个图片集都要进行预处理，所以原始数据加上 <strong>_orig</strong>，但是标签集不需要预处理</li></ul><h4 id="加载原始数据集"><a href="#加载原始数据集" class="headerlink" title="加载原始数据集"></a>加载原始数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(train_set_x_orig[index]) <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</span><br><span class="line"><span class="comment"># np.squeeze() 用于把一个数组的 shape 中为 1 的维度删掉，即让 train_set_y[:, index] 变为一个数</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_1.png" alt=""></p><h4 id="确定图片维度和个数以防止出错"><a href="#确定图片维度和个数以防止出错" class="headerlink" title="确定图片维度和个数以防止出错"></a>确定图片维度和个数以防止出错</h4><ul><li>训练集的个数：m_train</li><li>测试集的个数：m_test</li><li>图片（正方形）的尺寸即边长的像素数：num_px</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定维度和个数</span></span><br><span class="line"><span class="comment"># train_set_x_orig 形状为 (m_train, num_px, num_px, 3)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;Number of training examples: m_train = 209</span><br><span class="line">  Number of testing examples: m_test = 50</span><br><span class="line">  Height/Width of each image: num_px = 64</span><br><span class="line">  Each image is of size: (64, 64, 3)</span><br><span class="line">  train_set_x shape: (209, 64, 64, 3)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x shape: (50, 64, 64, 3)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br></pre></td></tr></table></figure><h4 id="重构图片数组变为标准输入矩阵"><a href="#重构图片数组变为标准输入矩阵" class="headerlink" title="重构图片数组变为标准输入矩阵"></a>重构图片数组变为标准输入矩阵</h4><p>把尺寸为 (num_px, num_px, 3) 的图片变为 shape 为 (num_px ∗ num_px ∗ 3, 1) 的向量 </p><p>把一个 shape 为 (a,b,c,d) 的矩阵变为一个 shape 为 (b∗c∗d, a) 的矩阵的技巧：<code>x_flatten = X.reshape(X.shape[0], -1).T</code></p><blockquote><p>实际上，reshape() 是按<strong>行</strong>取元素，按<strong>行</strong>放元素</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重构图片数组</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))<span class="comment">#重构后完整性检查</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;train_set_x_flatten shape: (12288, 209)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x_flatten shape: (12288, 50)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br><span class="line">  sanity check after reshaping: [17 31 56 22 33]</span><br></pre></td></tr></table></figure><h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><p>为了使得数据在一个合适的尺度上，我们需要将数据标准化，数据标准化的办法见上一篇博文，但是对于图片来说，由于图片的每个像素的 RGB 值介于 0 到 255 之间，所以我们可以将每个特征值除以 255，这样就能将它们标准化了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></span><br></pre></td></tr></table></figure><h3 id="算法的一般体系结构"><a href="#算法的一般体系结构" class="headerlink" title="算法的一般体系结构"></a>算法的一般体系结构</h3><ul><li>初始化模型参数</li><li>通过最优化代价函数学习参数<ul><li>计算目前的损失函数（前向传播）</li><li>计算现在的梯度（反向传播）</li><li>更新参数（梯度下降）</li></ul></li><li>使用学习后的参数对测试集进行预测</li><li>分析结果得出结论</li></ul><h3 id="构建算法的各个部分"><a href="#构建算法的各个部分" class="headerlink" title="构建算法的各个部分"></a>构建算法的各个部分</h3><h4 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h4><p>实现 sigmoid 函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid([0, 2]) = "</span> + str(sigmoid(np.array([<span class="number">0</span>,<span class="number">2</span>]))))</span><br></pre></td></tr></table></figure><p>>&gt; sigmoid([0, 2]) = [ 0.5         0.88079708]</p><h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><p>使用一系列的 0 初始化我们的参数 w 和 b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    w = np.zeros((dim,<span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>)) <span class="comment"># 确保 w 的维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int)) <span class="comment">#确保 b 是浮点数或者整数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dim = <span class="number">2</span></span><br><span class="line">w, b = initialize_with_zeros(dim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(w))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(b))</span><br></pre></td></tr></table></figure><p>>&gt; w = [[0] [0]]      b = 0</p><h4 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h4><p>步骤：</p><ul><li>输入 X</li><li>计算预测值 $A = \sigma(w^T X + b)$</li><li>计算代价函数 $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</li><li>计算  $ dw = \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$</li><li>计算 $ db=\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算前向传播和反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播 (从 X 到 COST)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X ) + b) <span class="comment"># 计算预测值                    </span></span><br><span class="line">    cost = (<span class="number">-1</span>/m)*(np.sum(Y*np.log(A) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))) <span class="comment"># 计算代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播 (计算梯度)</span></span><br><span class="line">    dw = (<span class="number">1</span>/m)*np.dot(X,(A-Y).T)</span><br><span class="line">    db = (<span class="number">1</span>/m)*np.sum(A-Y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,   <span class="comment">#返回梯度 dict</span></span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w, b, X, Y = np.array([[<span class="number">1.</span>],[<span class="number">2.</span>]]), <span class="number">2.</span>, np.array([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">-1.</span>],[<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">-3.2</span>]]), np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">grads, cost = propagate(w, b, X, Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cost = "</span> + str(cost))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;dw = [[ 0.99845601]</span><br><span class="line">       [ 2.39507239]]</span><br><span class="line">  db = 0.00145557813678</span><br><span class="line">  cost = 5.80154531939</span><br></pre></td></tr></table></figure><h4 id="梯度下降优化参数"><a href="#梯度下降优化参数" class="headerlink" title="梯度下降优化参数"></a>梯度下降优化参数</h4><p>更新参数方法：$ \theta = \theta - \alpha \text{ } d\theta$ ，其中 $\alpha$ 为<strong>学习率</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- 循环的迭代次数</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 代价函数和梯度计算 </span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line">        </span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b - learning_rate*db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每100次迭代记录一次代价函数到 costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每一百步打印一次代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params, grads, costs = optimize(w, b, X, Y, num_iterations= <span class="number">100</span>, learning_rate = <span class="number">0.009</span>, print_cost = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(params[<span class="string">"w"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(params[<span class="string">"b"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; w = [[ 0.19033591]</span><br><span class="line">       [ 0.12259159]]</span><br><span class="line">   b = 1.92535983008</span><br><span class="line">   dw = [[ 0.67752042]</span><br><span class="line">        [ 1.41625495]]</span><br><span class="line">   db = 0.219194504541</span><br></pre></td></tr></table></figure><h4 id="用得到的参数预测数据集的标签"><a href="#用得到的参数预测数据集的标签" class="headerlink" title="用得到的参数预测数据集的标签"></a>用得到的参数预测数据集的标签</h4><ul><li>先计算预测值 $\hat{Y} = A = \sigma(w^T X + b)$</li><li>若 $\hat Y &gt; 0.5$ ，则预测的标签为 1</li><li>若 $\hat Y &lt;= 0.5$ ，则预测的标签为 0</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m)) <span class="comment">#初始化</span></span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>) <span class="comment">#确保 w shape 正确</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算图片中是猫的概率</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X) + b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 把概率转化为标签值</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([[<span class="number">0.1124579</span>],[<span class="number">0.23106775</span>]])</span><br><span class="line">b = <span class="number">-0.3</span></span><br><span class="line">X = np.array([[<span class="number">1.</span>,<span class="number">-1.1</span>,<span class="number">-3.2</span>],[<span class="number">1.2</span>,<span class="number">2.</span>,<span class="number">0.1</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"predictions = "</span> + str(predict(w, b, X)))</span><br></pre></td></tr></table></figure><p>>&gt; predictions = [[ 1.  1.  0.]]</p><h3 id="把所有的函数聚合到主函数"><a href="#把所有的函数聚合到主函数" class="headerlink" title="把所有的函数聚合到主函数"></a>把所有的函数聚合到主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用 0 初始化参数</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations , learning_rate , print_cost = <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从参数 dict 中获取参数</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测训练集/测试集的标签</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印预测误差</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练模型</span></span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train accuracy: 99.04306220095694 % # 训练集的预测精确度</span><br><span class="line">test accuracy: 70.0 % #测试集的预测精确度</span><br></pre></td></tr></table></figure><p>画出错误图形：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture that was wrongly classified.</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(test_set_y[<span class="number">0</span>,index]) + <span class="string">", you predicted that it is a \""</span> + classes[d[<span class="string">"Y_prediction_test"</span>][<span class="number">0</span>,index]].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><p>画出代价函数的下降曲线：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>]) <span class="comment"># np.squeeze() 确保它是一维数组</span></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2.png" alt=""></p><h3 id="进一步分析"><a href="#进一步分析" class="headerlink" title="进一步分析"></a>进一步分析</h3><h4 id="学习率的选择"><a href="#学习率的选择" class="headerlink" title="学习率的选择"></a>学习率的选择</h4><p>为了梯度下降正常工作，必须选择合适的学习率。学习率 $\alpha$ 决定了更新参数的快慢，如果学习率太大，我们可能错过最优值，同样，如果太小，我们需要迭代很多次到达最优值，下面比较不同学习率的差别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (hundreds)'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="keyword">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning rate is: 0.01</span><br><span class="line">train accuracy: 99.52153110047847 %</span><br><span class="line">test accuracy: 68.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate is: 0.001</span><br><span class="line">train accuracy: 88.99521531100478 %</span><br><span class="line">test accuracy: 64.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate is: 0.0001</span><br><span class="line">train accuracy: 68.42105263157895 %</span><br><span class="line">test accuracy: 36.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.3.png" alt=""></p><p>结论：</p><ul><li>不同的学习率导致不同的代价函数和预测结果</li><li>如果学习率太大（0.01），代价函数可能会上下摆动甚至偏离（尽管在这个例子中 0.01 最后收敛得很好）</li><li>更小的学习率不意味着更好的模型，因为有可能出现 <strong>过拟合</strong>，一般出现在训练数据精确度比测试数据高得多时</li><li>深度学习中，一般推荐：<ul><li>选择更好降低代价函数的学习率</li><li>如果发生过拟合，用其他的方式减小过拟合</li></ul></li></ul><h3 id="作业结论"><a href="#作业结论" class="headerlink" title="作业结论"></a>作业结论</h3><ul><li>预处理数据很重要</li><li>先分开构建函数： initialize(), propagate(), optimize()，最后再搭建模型 model()</li><li>调整学习率（超参数 的一个例子）对算法影响很大</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 逻辑回归 </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 2）—— 神经网络基础</title>
      <link href="/2018/08/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w2/"/>
      <url>/2018/08/07/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w2/</url>
      <content type="html"><![CDATA[<p>本周主要介绍<strong>逻辑回归</strong>算法 (Logistics Regression)。</p><h2 id="二元分类"><a href="#二元分类" class="headerlink" title="二元分类"></a>二元分类</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>输出值为离散的两个值，即 1 或者 0</p><h3 id="例子——判断图片是（1）不是（0）猫"><a href="#例子——判断图片是（1）不是（0）猫" class="headerlink" title="例子——判断图片是（1）不是（0）猫"></a>例子——判断图片是（1）不是（0）猫</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.1.png" alt=""></p><ul><li>目标：输入图片的特征向量 x，预测对应的输出是 1 还是 0 </li></ul><h3 id="图片特征值的提取"><a href="#图片特征值的提取" class="headerlink" title="图片特征值的提取"></a>图片特征值的提取</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.2.png" alt=""><br><a id="more"></a></p><ul><li>图片特征值的提取：每张图片有三个颜色通道，把每个通道的所有像素值取出展成一个向量，每张图片（像素为 64*64）的所有特征值为 64x64x3 = 12288 个，组成一个维度为 12288 的列向量 X </li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.4.png" alt=""></p><ul><li>输入向量的维度 n =  $n_x$= 12288</li></ul><h2 id="符号说明"><a href="#符号说明" class="headerlink" title="符号说明"></a>符号说明</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.3.png" alt=""></p><h2 id="逻辑回归-Logistics-Regression（针对二元分类问题）"><a href="#逻辑回归-Logistics-Regression（针对二元分类问题）" class="headerlink" title="逻辑回归 Logistics Regression（针对二元分类问题）"></a>逻辑回归 Logistics Regression（针对二元分类问题）</h2><h3 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h3><p>逻辑回归是一个很小的神经网络，已知一个数据集，我们用它拟合出一个模型，这个模型能够实现：</p><ul><li><p>输入特征值 $x$</p></li><li><p>输出预测值 $\hat y$： $\hat{y} = P(y = 1|x) , 0 \leq\hat{y} \leq1 $，即标签值 y = 1 的概率</p></li></ul><p>那么如何得到某个输入 x 的标签值为 1 的概率呢，用如下式子来进行预测：</p><script type="math/tex; mode=display">\hat{y} = w^Tx + b</script><p>但是由于概率的值在 0 和 1 之间，而 $w^T x+ b$ 可能大于 1 或者小于 0，所以这不是一个好的算法，于是我们将其通过一个 <strong>sigmoid函数</strong> 改造一下：</p><script type="math/tex; mode=display">\hat{y} =\sigma( w^Tx + b)</script><ul><li>输入特征向量 x： 𝑥 ∈ $ℝ^{n_x}$, 即 $x = \left ( x_{1},x_{2},x_{3},…,x_{n_x} \right )^T$,$n_x$ 为特征数目</li><li>训练标签 y：y $\in$ { 0 , 1 }</li><li>权重 w：$w = \left ( w_{1},w_{2},w_{3},…,w_{n_x} \right )^T$, $n_x$ 为特征数目</li><li>偏置量 b：实数</li><li>sigmoid 函数：$s=\sigma(z)=\frac{1}{1+e^{-z}}$</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.5.png" alt=""></p><blockquote><p>sigmoid 函数的合理性在于：</p><ul><li>当 z 是一个很大的数时， $\sigma (z) = 1$</li><li>当 z 是一个很小的数时， $\sigma(z)=0$</li><li>当 z = 0 时，$\sigma(z)=0.5$</li></ul><p>这样就能很好地估计介于 0 和 1 之间的概率</p></blockquote><h3 id="另外一种描述方式"><a href="#另外一种描述方式" class="headerlink" title="另外一种描述方式"></a>另外一种描述方式</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.6.png" alt=""></p><script type="math/tex; mode=display">\hat{y} =\sigma( \Theta^Tx)</script><p>其中：</p><ul><li>$x = (1,x_{1},x_{2},x_{3},…,x_{n_x})^T$</li><li>$\Theta = (b,w_{1},w_{2},w_{3},…,w_{n_x})^T$</li></ul><h3 id="损失函数（lost-function）"><a href="#损失函数（lost-function）" class="headerlink" title="损失函数（lost function）"></a>损失函数（lost function）</h3><p>为了训练参数 w 和 b，也就是找到最优的拟合模型，我们定义一个<strong>损失函数</strong></p><h4 id="概括"><a href="#概括" class="headerlink" title="　概括"></a>　概括</h4><blockquote><p>右上角的括号 (i) 表示第 i 个训练实例</p></blockquote><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.7.png" alt=""></p><h4 id="损失函数定义"><a href="#损失函数定义" class="headerlink" title="损失函数定义"></a>损失函数定义</h4><p>损失函数评估了预测值 $\hat y^{(i)}$ 和 已知的期望值 $y^{(i)}$ 之间的差异，或者说计算了单个训练样本的偏差</p><h4 id="常用的损失函数"><a href="#常用的损失函数" class="headerlink" title="常用的损失函数"></a>常用的损失函数</h4><p>（1）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.9.png" alt=""></p><blockquote><p>逻辑回归中一般不适用此损失函数，因为会使优化问题变成非凸问题，无法使用梯度下降法找到全局最优解</p></blockquote><p>（2）</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.10.png" alt=""></p><blockquote><p>该损失函数的合理性在于：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.11.png" alt=""></p></blockquote><h3 id="代价函数（Cost-function）"><a href="#代价函数（Cost-function）" class="headerlink" title="代价函数（Cost function）"></a>代价函数（Cost function）</h3><p>代价函数指的是<strong>整个</strong>训练集的损失函数的<strong>平均值</strong>，我们的最终目的就是找到使整个代价函数值最小的参数 w 和 b</p><blockquote><ul><li>我们希望用 $\hat{y} =\sigma( w^Tx + b)$ 模型得出的预测值和已知的实际值之间的差异最小，也就是代价函数最小，这样才能证明这个模型（假设参数为 $w_{best},b_{best}$）是所有包含参数 w，b 的模型里最合理的，也是最符合实际的</li><li>从输入到求得代价函数的过程称之为<strong>前向传播 (forward propagation)</strong> </li></ul></blockquote><p>代价函数：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.12.png" alt=""></p><h3 id="代价函数的证明"><a href="#代价函数的证明" class="headerlink" title="代价函数的证明"></a>代价函数的证明</h3><p>二元分类问题类似于概率论里的“0-1分布”.</p><ul><li>假设当结果为 1 的概率  $P(y=1|x)=\hat y$</li><li>则当结果为 0 的概率就是  $P(y=0|x)=1-\hat y$</li><li>则结果为 y（0或1）的概率为 $P(y|x)=\hat y^y(1-\hat y)^{1-y}$，可带入y=0或1进行检验，发现该式子正确</li><li>假设有一个容量为 m 的数据集（假设符合独立同分布），根据最大似然原理，我们要使得我们的假设的 $\hat y$ 最正确，最符合实际，那么必须满足使得“用 $\hat y$ 估计到的目前整个数据集的概率”<strong>最大</strong>（之所以越大越接近实际，是因为这些数据集已经存在，发生的概率便是 1），那么根据独立事件概率相乘原理：</li><li>“用 $\hat y$ 估计到的目前整个数据集的概率”为 $L(\hat y)=P(y^{(1)}|x^{(1)})P(y^{(2)}|x^{(2)})…P(y^{(m)}|x^{(m)})\\\quad\quad =\prod\limits_{i=1}^m P(y^{(i)}|x^{(i)})\\ \quad\quad =\prod\limits_{i=1}^m    {({\hat y^{(i)}})}   ^{y^{(i)}}          (   1   -    {\hat y^{(i)}}  )^{1-   y^{(i)}} $</li><li>为了让 $L(\hat y)$ <strong>最大</strong>，由于是连乘，我们让 $logL(\hat y)$ 最大，则 $logL(\hat y) = \sum\limits ^m_{i=1} y^{(i)}log{\hat y}^{(i)}+(1-y^{(i)})log(1-{\hat y}^{(i)})$</li><li>而代价函数必须要<strong>尽量小</strong>，所以我们将 $logL(\hat y)$ 取负号，即 $-\sum\limits ^m_{i=1} y^{(i)}log{\hat y}^{(i)}+(1-y^{(i)})log(1-{\hat y}^{(i)})$</li><li>为了让代价函数处于更好的尺度上，我们加上系数 1/m，即 $J(w,b)=-\frac{1}{m}\sum\limits ^m_{i=1} y^{(i)}log{\hat y}^{(i)}+(1-y^{(i)})log(1-{\hat y}^{(i)})$</li><li>代价函数的逻辑为：某个 w，b 算出的代价函数越小 → $L(\hat y)$ 越大 → 用其估计到的目前整个数据集的概率就越大 → 用 w，b 为参数的模型的估计距离实际情况就越接近 → 该模型就越准确</li></ul><h2 id="用梯度下降法训练参数-w-和-b"><a href="#用梯度下降法训练参数-w-和-b" class="headerlink" title="用梯度下降法训练参数 w 和 b"></a>用梯度下降法训练参数 w 和 b</h2><h3 id="概括-1"><a href="#概括-1" class="headerlink" title="概括"></a>概括</h3><p>已知：</p><ul><li><p>模型 $\hat{y} =\sigma( w^Tx + b)$，其中 $\sigma(z)=\frac{1}{1+e^{-z}}$</p></li><li><p>代价函数</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.12.png" alt=""></p></li></ul><p>期望：</p><ul><li><p>找到使 $J(w,b)$ 最小的 w 和 b，如图中红点所示，找到谷底的 w 和 b 值</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.13.png" alt=""></p></li></ul><h3 id="梯度下降法原理"><a href="#梯度下降法原理" class="headerlink" title="梯度下降法原理"></a>梯度下降法原理</h3><ul><li>用一些初始 w，b 值来初始化，通常用 0</li><li>朝着最陡的的下坡方向走一步，第一次迭代</li><li>第二次迭代</li><li>…</li><li>第 n 次迭代到达或接近全局最优点</li></ul><h3 id="单变量梯度下降法（先忽略b）"><a href="#单变量梯度下降法（先忽略b）" class="headerlink" title="单变量梯度下降法（先忽略b）"></a>单变量梯度下降法（先忽略b）</h3><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.14.png" alt=""></p><p>  假设 J(w) 曲线如图中所示，为了找到谷底的 w 点，我们重复以下更新操作：</p><script type="math/tex; mode=display">  w := w - \alpha\frac{dJ(w)}{dw}</script><p>  其中：</p><ul><li>w <strong>: =</strong> 符号表示对 w 进行迭代更新</li><li>$\alpha$ 为<strong>学习率</strong>，控制每一次迭代的步长大小</li><li>在代码中导数 $\frac{dJ(w)}{dw}$ 的变量名写成 $dw$</li></ul><h3 id="多变量梯度下降法（w，b）"><a href="#多变量梯度下降法（w，b）" class="headerlink" title="多变量梯度下降法（w，b）"></a>多变量梯度下降法（w，b）</h3><script type="math/tex; mode=display">  J(w,b)</script><script type="math/tex; mode=display">  w := w - \alpha\frac{\partial J(w,b) }{\partial w}</script><script type="math/tex; mode=display">  b := b - \alpha\frac{\partial J(w,b) }{\partial b}</script><h2 id="求代价函数的导数"><a href="#求代价函数的导数" class="headerlink" title="求代价函数的导数"></a>求代价函数的导数</h2><h3 id="求损失函数的导数"><a href="#求损失函数的导数" class="headerlink" title="求损失函数的导数"></a>求损失函数的导数</h3><p>  已知公式：</p><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.15.png" alt=""></p><p>  假设训练实例有两个特征 : $x_1,x_2$，则逻辑关系如下：</p><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.16.png" alt=""></p><p>  根据链式法则求偏导数，推导过程如下：</p><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.17.png" alt=""></p><p>  整理可得：</p><script type="math/tex; mode=display">“da”=\frac{dL}{da}= \frac {\partial L(a,y)}{\partial a}=-\frac{y}{a}+\frac{1-y}{1-a}</script><script type="math/tex; mode=display">“dz”= \frac {dL}{dz}=\frac{dL}{da}\frac{da}{dz}=a-y</script><script type="math/tex; mode=display">“dw_1”=\frac{\partial L}{\partial w_1}=x_1dz</script><script type="math/tex; mode=display">“dw_2”=\frac{\partial L}{\partial w_2}=x_2dz</script><script type="math/tex; mode=display">“db” = \frac{\partial L}{\partial b} = dz</script><blockquote><p>其中：$“da””dz””dw_1””dw_2””db”$ 表示损失函数对其导数在 python 中的变量名，这个过程叫做 <strong>反向传播 (back propagation)</strong></p></blockquote><h3 id="求代价函数的导数-1"><a href="#求代价函数的导数-1" class="headerlink" title="求代价函数的导数"></a>求代价函数的导数</h3><p>由于代价函数是所有训练样本损失函数的平均值，故我们可以用如下的伪代码求得 $dw_1,dw_2,db$ (我们仍然假设只有两个特征值)：</p><p>  首先初始化 $J=0;dw_1=0;dw_2=0;db=0$</p><p>  遍历整个训练集：</p><p>  For i = 1 to m:</p><p>​         $z^{(i)}=w^Tx^{(i)}+b\\a^{(i)}=\sigma(z^{(i)})\\J+=-[y^{(i)}loga^{(i)}+(1-y^{(i)})log(1-a^{(i)})]\\dz^{(i)}=a^{(i)}-y^{(i)}\\dw_1+=x_1^{(i)}dz^{(i)}\\dw_2+=x_2^{(i)}dz^{(i)}\\dw_3…\\dw_4… \\ …\\db+=dz^{(i)}$</p><p>  最后除以训练集个数：</p><p>​         $J=J/m\\”dw_1”=\frac {\partial J}{\partial w_1}=dw_1/m\\”dw_2”=\frac {\partial J}{\partial w_2}=dw_2/m \\ … \\”db”=\frac {\partial J}{\partial b}=db/m$</p><blockquote><p>我们可以看到为了求取代价函数的导数，我们需要进行两次遍历，这是一种十分低效的遍历方式，我们可以使用<strong>向量化（Vectorization）</strong>的方式加速我们的运算</p></blockquote><p>  最后用求得的代价函数的导数更新我们的参数：</p><p>​        $w_1:=w_1-\alpha dw_1\\w_2:=w_2-\alpha dw_1 \\ …\\b:=b-\alpha db$</p><h2 id="向量化（Vectorization）"><a href="#向量化（Vectorization）" class="headerlink" title="向量化（Vectorization）"></a>向量化（Vectorization）</h2><h3 id="什么是向量化"><a href="#什么是向量化" class="headerlink" title="什么是向量化"></a>什么是向量化</h3><ul><li>是一门让代码变得高效的艺术</li><li><p>遍历是<strong>非向量化</strong>，而向量化充分利用 CPU 和 GPU 的并行化实现更快的运算</p><blockquote><p>大概是指直接让两个向量或者矩阵进行点乘</p></blockquote></li></ul><h3 id="向量化的好处"><a href="#向量化的好处" class="headerlink" title="向量化的好处"></a>向量化的好处</h3><ul><li>比非向量化计算效率快很多很多，下面的测试显示快 300 多倍</li><li><p>只要可能，尽量避免使用显式的 for 循环</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.18.png" alt=""></p></li></ul><h3 id="一些向量化的例子"><a href="#一些向量化的例子" class="headerlink" title="一些向量化的例子"></a>一些向量化的例子</h3><ol><li><p>计算  $u = A_{i \times j}V_{j \times 1}$ </p><p>(1) 非向量化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">u = np.zero((n,1))</span><br><span class="line">for i ...</span><br><span class="line">    for j ...</span><br><span class="line">    u[i] += A[i][j]*v[j]</span><br></pre></td></tr></table></figure><p>(2) 向量化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">u = np.dot(A,V)</span><br></pre></td></tr></table></figure></li><li><p>计算某个向量所有元素的指数</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.19.png" alt=""></p><p>(1) 非向量化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">u = np.zeros((n,1))</span><br><span class="line">for i in range(n)</span><br><span class="line">u[i] = math.exp(V[i])</span><br></pre></td></tr></table></figure><p>(2) 向量化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">u = np.exp(V)</span><br></pre></td></tr></table></figure></li><li><p>其他例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.log(V)</span><br><span class="line">np.abs(V)</span><br><span class="line">np.maximum(V)</span><br><span class="line">V**<span class="number">2</span></span><br><span class="line"><span class="number">1</span>/V</span><br></pre></td></tr></table></figure></li></ol><h2 id="用向量化实现逻辑回归"><a href="#用向量化实现逻辑回归" class="headerlink" title="用向量化实现逻辑回归"></a>用向量化实现逻辑回归</h2><h3 id="用-for-循环实现"><a href="#用-for-循环实现" class="headerlink" title="用 for 循环实现"></a>用 for 循环实现</h3><p>  <img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl2.20.png" alt=""></p><h3 id="向量化实现"><a href="#向量化实现" class="headerlink" title="向量化实现"></a>向量化实现</h3><ol><li><p>前向传播</p><p><strong>求取 Z</strong>：</p><p>$Z=[z^{(1)}\quad z^{(1)}\quad…\quad z^{(m)}]\\=[w^Tx^{(1)}+b\quad w^Tx^{(2)}+b\quad …\quad w^Tx^{(m)}+b] \\=w^T[x^{(1)}\quad x^{(1)}\quad …\quad x^{(m)}]+[b\quad b\quad…\quad b]\\=w^TX+[b\quad b\quad…\quad b]$</p><p>实现代码为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br></pre></td></tr></table></figure><blockquote><p>此处 b 为一个实数，但是 numpy 会自动将这个实数 b 拓展为一个 1 乘 m 的行向量，这个在 python 中称为 <strong>广播（broadcasting）</strong></p></blockquote><p><strong>求取 A</strong>：</p><p>$A=[a^{(1)}  \  a^{(2)} \ … \ a{(m)}]= \sigma(Z)$</p></li><li><p>反向传播</p><p><strong>求取 dZ</strong>：</p><p>$dZ=[dz^{(1)} \ dz^{(2)} \ … \ dz^{(m)}]\\=[a^{(1)}-y^{(1)} \quad a^{(1)}-y^{(1)} \ … \quad a^{(m)}-y^{(m)}]\\=A -Y $</p><p>其中 $Y=[y^{(1)}\quad y^{(2)}\quad … \quad y^{(m)}]$</p><p><strong>求取 dw</strong>：</p><p>$dw=\frac {1}{m}[x^{(1)} dz^{(1)} + x^{(2)} dz^{(2)} +…+x^{(m)} dz^{(m)}]\\=\frac {1}{m}[x^{(1)}\quad x^{(2)}\quad…\quad x^{(m)}][dz^{(1)}\quad dz^{(2)}\quad…\quad dz^{(m)}]^T\\=\frac {1}{m}X(dZ)^T$</p><p><strong>求取 db</strong>：</p><p>$db=\frac {1}{m} \sum\limits _{i=1}^{m} dz^{(i)}=\frac {1}{m}$ np.sum(dZ)</p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>$Z = w^TX + b = np.dot(w.T, X)+b\\A = \sigma (Z)\\dZ=A-Y\\db=\frac {1}{m}np.sum(dZ)\\dw= \frac{1}{m}X(dZ)^T=\frac{1}{m}np.dot(X, dZ.T)\\w:=w-\alpha dw\\b:=b-\alpha db$</p><h2 id="Python中的广播（broadcasting）"><a href="#Python中的广播（broadcasting）" class="headerlink" title="Python中的广播（broadcasting）"></a>Python中的广播（broadcasting）</h2><ul><li>若拿一个<strong>（m，n）</strong>的矩阵加减乘除另一个<strong>（1，n）</strong>的向量，这个向量会复制多次自动拓展成一个<strong>（m，n）</strong>的矩阵然后逐元素进行运算</li><li>若拿一个<strong>（m，1）</strong>的向量加减乘除另一个<strong>实数R</strong>，这个实数会复制多次自动拓展成一个<strong>（m，1）</strong>的向量然后逐元素进行运算</li><li>更多广播功能见 numpy 相关文档</li></ul><h2 id="在使用-numpy-时的注意事项"><a href="#在使用-numpy-时的注意事项" class="headerlink" title="在使用 numpy 时的注意事项"></a>在使用 numpy 时的注意事项</h2><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><ol><li><p>错误的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;<span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">&gt;&gt;a = np.random.randn(<span class="number">5</span>)<span class="comment">#产生5个高斯随机变量并存在数组里</span></span><br><span class="line">&gt;&gt;print(a)</span><br><span class="line">[<span class="number">1.37838388</span>  <span class="number">0.53281963</span> <span class="number">-0.41769013</span>  <span class="number">0.69356822</span> <span class="number">-0.30333514</span>]</span><br><span class="line">&gt;&gt;print(a.shape)<span class="comment">#输出 a 的形状</span></span><br><span class="line">(<span class="number">5</span>,)</span><br><span class="line">&gt;&gt;print(a.T)<span class="comment">#输出 a 的转置</span></span><br><span class="line">[<span class="number">1.37838388</span>  <span class="number">0.53281963</span> <span class="number">-0.41769013</span>  <span class="number">0.69356822</span> <span class="number">-0.30333514</span>]</span><br><span class="line">&gt;&gt;print(np.dot(a,a.T))<span class="comment">#输出a 和 a 的转置的点乘</span></span><br><span class="line"><span class="number">8.65042528221</span></span><br></pre></td></tr></table></figure></li><li><p>正确的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">&gt;&gt;print(a)</span><br><span class="line">[[<span class="number">-0.4593413</span> ]</span><br><span class="line"> [<span class="number">-0.54494632</span>]</span><br><span class="line"> [<span class="number">-0.9348879</span> ]</span><br><span class="line"> [<span class="number">-0.51491905</span>]</span><br><span class="line"> [ <span class="number">0.91934378</span>]]</span><br><span class="line">&gt;&gt;print(a.T)</span><br><span class="line">[[<span class="number">-0.4593413</span>  <span class="number">-0.54494632</span> <span class="number">-0.9348879</span>  <span class="number">-0.51491905</span>  <span class="number">0.91934378</span>]]</span><br><span class="line">&gt;&gt;print(np.dot(a,a.T))</span><br><span class="line">[[ <span class="number">0.21099443</span>  <span class="number">0.25031635</span>  <span class="number">0.42943262</span>  <span class="number">0.23652358</span> <span class="number">-0.42229257</span>]</span><br><span class="line"> [ <span class="number">0.25031635</span>  <span class="number">0.29696649</span>  <span class="number">0.50946372</span>  <span class="number">0.28060324</span> <span class="number">-0.50099301</span>]</span><br><span class="line"> [ <span class="number">0.42943262</span>  <span class="number">0.50946372</span>  <span class="number">0.87401539</span>  <span class="number">0.48139159</span> <span class="number">-0.85948338</span>]</span><br><span class="line"> [ <span class="number">0.23652358</span>  <span class="number">0.28060324</span>  <span class="number">0.48139159</span>  <span class="number">0.26514163</span> <span class="number">-0.47338763</span>]</span><br><span class="line"> [<span class="number">-0.42229257</span> <span class="number">-0.50099301</span> <span class="number">-0.85948338</span> <span class="number">-0.47338763</span>  <span class="number">0.84519299</span>]]</span><br></pre></td></tr></table></figure><blockquote><p>错误的例子在于 <code>a = np.random.randn(5)</code> 生成了一个形状为 <strong>(5,)</strong> 的名为 <strong>“秩为1的数组”</strong> ，而不是一个向量，这种数组转置之后显示完全一样，但是性质与向量不同，所以极易造成许多奇怪的 bug，所以要杜绝秩为1的数组的使用，而是使用 <code>a = np.random.randn(5,1)</code> 或者 <code>a = np.random.randn(1,5)</code> </p></blockquote></li></ol><h3 id="减少错误的技巧"><a href="#减少错误的技巧" class="headerlink" title="减少错误的技巧"></a>减少错误的技巧</h3><ol><li><p>不要使用 <strong>秩为1的数组</strong>，始终使用 （n，1）列向量或者（1，n）的行向量，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>经常使用<strong>断言</strong>语句来确保是向量而不是秩为1的数组,例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape == (<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></li><li><p>经常使用 <strong>reshape 语句</strong> 来确保矩阵和向量是需要的维度，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>)<span class="comment">#产生了一个秩为1矩阵</span></span><br><span class="line">a = a.reshape((<span class="number">5</span>,<span class="number">1</span>))<span class="comment">#将其变为（5，1）的列向量</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 1）—— 深度学习介绍</title>
      <link href="/2018/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w1/"/>
      <url>/2018/08/04/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0c1w1/</url>
      <content type="html"><![CDATA[<p>本周主要介绍深度学习的概况。</p><h2 id="Specializeion的课程设置"><a href="#Specializeion的课程设置" class="headerlink" title="Specializeion的课程设置"></a>Specializeion的课程设置</h2><ul><li>神经网络和深度学习  </li><li>改进神经网络 </li><li>构建机器学习系统 </li><li>卷积神经网络CNN </li><li>序列模型 </li></ul><h2 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h2><h3 id="例子1——single-neural-network"><a href="#例子1——single-neural-network" class="headerlink" title="例子1——single neural network"></a>例子1——single neural network</h3><p>预测房价（线性回归问题）：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.1.png" alt=""><br><a id="more"></a><br><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.2.png" alt=""></p><ul><li>输入为房子的尺寸（x）</li><li>输出为房价（y）</li><li>注意：房价不能为负数，故用不从零开始的 <strong>修正线性单元（rectified linear unit）</strong>即 <strong>ReLU函数</strong>（蓝线） 表示</li><li>ReLU = max（0，y）</li><li>第二张图片中间的圆圈表示一个“神经元”，从 x 到 y 的整个输出表示一个最小的神经网络，是组成神经网络最基本的单元</li></ul><h3 id="例子2——-Multiple-neural-network"><a href="#例子2——-Multiple-neural-network" class="headerlink" title="例子2—— Multiple neural network"></a>例子2—— Multiple neural network</h3><p>用更多特征（房子尺寸、卧室大小、邮政编码、区域富裕程度）来预测房价：</p><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.3.png" alt=""></p><ul><li>输入的四个特征叫做<strong>输入层</strong></li><li>每个小圆圈都叫做<strong>隐藏神经元</strong>（hidden unit），图中每个神经元都把四个特征当作输入（全连接）</li><li>神经网络自己决定每个网络节点是什么</li></ul><blockquote><p>我的理解：给定足够的输入x，得到若干输出y，神经网络就是建立起 x 和 y 之间映射关系的一个黑箱系统</p></blockquote><h2 id="神经网络的监督学习"><a href="#神经网络的监督学习" class="headerlink" title="神经网络的监督学习"></a>神经网络的监督学习</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>给定一个<strong>已知输出</strong>的数据集，找到输入与输出之间的函数关系</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul><li>回归问题：在连续的输出上预测结果，即将输入变量映射到某个连续函数上</li><li>分类问题：在离散的输出上预测结果，即将输入变量映射到离散的类别上</li></ul><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.4.png" alt=""></p><h3 id="神经网络（NN）的分类"><a href="#神经网络（NN）的分类" class="headerlink" title="神经网络（NN）的分类"></a>神经网络（NN）的分类</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.5.png" alt=""></p><ul><li>标准神经网络（standard NN）：用在房地产、广告</li><li>卷积神经网络（convolution NN）：图片处理</li><li>循环神经网络（recurrent NN）：语音识别、翻译</li><li>混合了其他结构的神经网络： 自动驾驶</li></ul><h3 id="数据的分类"><a href="#数据的分类" class="headerlink" title="数据的分类"></a>数据的分类</h3><ul><li>结构化数据：基于数据库的数据，即标签化了的、具有清晰定义的信息，例如房价预测中数据有房屋面积和卧室数量等标签</li><li>非结构化数据：类似于音频、图片、文本这类的数据，处理更加困难</li></ul><h2 id="为何深度学习蓬勃发展"><a href="#为何深度学习蓬勃发展" class="headerlink" title="为何深度学习蓬勃发展"></a>为何深度学习蓬勃发展</h2><h3 id="规模驱动深度学习的发展"><a href="#规模驱动深度学习的发展" class="headerlink" title="规模驱动深度学习的发展"></a>规模驱动深度学习的发展</h3><ul><li>数字化生活产生的大量数据</li><li>算力的巨幅提高</li><li>算法的创新：例如 signmoid 函数到 ReLU 函数的迁移</li></ul><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl1.6.png" alt=""></p><ul><li>线条从下往上表示神经网络的规模在变大</li><li>m 表示训练集的大小</li><li>为了获得好的表现，要么足够大的神经网络，要么足够多的数据</li><li>在小训练集的情况下，模型的好坏与神经网络规模无关</li></ul><h3 id="训练神经网络的过程"><a href="#训练神经网络的过程" class="headerlink" title="训练神经网络的过程"></a>训练神经网络的过程</h3><p>一个 idea → 代码实践 → 试验 → 修正 idea →…</p><h2 id="course-1-的课程计划"><a href="#course-1-的课程计划" class="headerlink" title="course 1 的课程计划"></a>course 1 的课程计划</h2><ul><li>Week 1 : Introduction</li><li>Week 2 : Basics of Neural Network programming</li><li>Week 3 : One hidden layer Networks</li><li>Weel 4 : Deep Neural Networks</li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习笔记 </tag>
            
            <tag> AI </tag>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>小白的 hexo+Github Pages 博客搭建之旅（windows）</title>
      <link href="/2018/07/26/%E5%B0%8F%E7%99%BD%E7%9A%84%20hexo+github%20pages%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%B9%8B%E6%97%85%EF%BC%88windows%EF%BC%89/"/>
      <url>/2018/07/26/%E5%B0%8F%E7%99%BD%E7%9A%84%20hexo+github%20pages%20%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E4%B9%8B%E6%97%85%EF%BC%88windows%EF%BC%89/</url>
      <content type="html"><![CDATA[<p>这是我的博客的第一篇文章，我想在这篇文章里记录一下自己搭建这个博客的全过程和途中踩过的坑，一是为了记录一下，二是方便以后换电脑需要再次搭建的时候有个参考，如果能帮助到其他人就更好了。</p><h2 id="搭建博客起因"><a href="#搭建博客起因" class="headerlink" title="搭建博客起因"></a>搭建博客起因</h2><p>前段时间刚开始自学python和机器学习，为了找到最佳的笔记方案，我上知乎搜“学习编程怎么做笔记？”有的人说不用做笔记，有的人说用 gitbook，有的人说直接记在代码注释里，有的人说直接记在 github 的 wiki 或者 issue 页面。但是还看到许多人推荐记录在自己的独立博客上，作为一个一直希望有自己博客的人，我立马萌生了自己搭博客的想法，但是碍于麻烦一直有些动摇，直到这篇文章<a href="https://zhuanlan.zhihu.com/p/19743861" target="_blank" rel="noopener">《为什么你要写博客？》</a>打动了我，它让我下定决心搭建自己的博客并坚持写点东西，不仅可以放自己学习过程中的笔记，作为一个话痨，还可以放一些自己的随想和读书笔记，何乐而不为？</p><p>总的来说，我认为写博客能给我以下的好处：</p><ul><li>梳理自己学到的东西</li><li>通过沉下心写点东西消磨自己的浮躁心态</li><li>小时候经常写日记，长大后放弃了这个习惯，希望这个博客能让我重拾记录自己内心，与自己对话的习惯</li><li>记录自己的生活和成长</li><li>搭建独立博客本身就是一件非常 geek 的事情（虽然是用的框架）</li></ul><p>那么废话不多说，直接开始吧！</p><a id="more"></a><h2 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h2><p>既然选择搭博客，那么怎么搭呢？我继续搜“如何搭建独立博客？”知乎上推荐比较多的方案是 <strong>静态博客生成器 + github pages + markdown</strong>，静态博客较动态博客的好处在于浏览速度快、方便简单且无须数据库，让你专注于写作本身。其次，Github Pages 是 github 上用于介绍项目的页面，不过，由于其免费的 300M 空间，把我们的博客放在上面再合适不过了。另外 markdown 指的是一种轻量级的标记语言，可以边写作边排版，非常方便。</p><p>静态博客生成器是一种把你写的文章变成漂亮的静态网页的博客框架，你只要负责写作，它负责帮你生成网页然后部署到 Github Pages 上，这就使得搭建博客变成了一件非常方便的事情。目前比较流行的静态博客生成器有 <strong>hexo</strong> 和 <strong>hugo</strong>。</p><p>我首先尝试了 hugo，他是用GO语言写成的，直接下载二进制文件然后安装就行了，非常方便。但是由于在应用 hugo 一个主题时出现了一个bug，始终无法生成页面，折腾了好几天，最后心力交瘁只能放弃 hugo，但不得不说 hugo 生成页面的速度非常快而且安装很方便，感兴趣的可以尝试一下。</p><p>于是最后的方案为 <strong>hexo + github pages + markdown</strong>。 </p><p>搭建过程参考以下链接：</p><ul><li><a href="http://blog.haoji.me/build-blog-website-by-hexo-github.html?from=xa" target="_blank" rel="noopener">使用 hexo+github 搭建免费个人博客详细教程</a></li><li><a href="https://www.jianshu.com/p/05289a4bc8b2" target="_blank" rel="noopener">如何搭建一个独立博客——简明 Github Pages 与 Hexo 教程</a></li><li><a href="https://www.youtube.com/watch?v=Ud1xAhu7t2Y&amp;list=PLXbU-2B80FvDjD_RiuNwsSQ4eF8pkwAIa" target="_blank" rel="noopener">快速使用 Hexo 搭建个人博客</a>(youtube)</li><li><a href="http://www.lovebxm.com/2018/06/24/hexo-github-blog/#%E6%90%AD%E5%BB%BA-Git-%E7%8E%AF%E5%A2%83" target="_blank" rel="noopener">可能是最详细的 Hexo + GitHub Pages 搭建个人博客的教程</a></li></ul><h3 id="hexo的下载安装"><a href="#hexo的下载安装" class="headerlink" title="hexo的下载安装"></a>hexo的下载安装</h3><h4 id="hexo介绍"><a href="#hexo介绍" class="headerlink" title="hexo介绍"></a>hexo介绍</h4><ul><li>hexo 是一个基于 Node.js 快速、简洁且高效的博客框架。</li><li><a href="https://hexo.io/zh-cn/docs/index.html" target="_blank" rel="noopener">hexo 中文官网</a></li></ul><h4 id="安装前的准备"><a href="#安装前的准备" class="headerlink" title="安装前的准备"></a>安装前的准备</h4><p>由于 hexo 是一个基于 Node.js 的博客框架，所以我们需要安装 Node.js，另外因为我们需要使用 Git 命令部署到 github 上，所以还需要安装 Git，两者的安装教程如下。</p><ul><li><a href="http://www.runoob.com/nodejs/nodejs-install-setup.html" target="_blank" rel="noopener">Node.js</a></li><li><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/00137396287703354d8c6c01c904c7d9ff056ae23da865a000" target="_blank" rel="noopener">Git</a></li></ul><blockquote><p>特别要注意安装 Node.js 和 Git 时是否设置了环境变量，点击<strong>win+R</strong>  =》输入 “cmd” =&gt; 输入命令 “path”，查看是否有 Node.js 和 Git 所在的文件夹，如果没有则可以根据网上教程设置。另外不要忘了根据说明配置Git。最后我们在 cmd 中分别输入 <code>node -v</code> 和 <code>npm -v</code> 和 <code>git version</code> 看看是否安装成功，如果返回版本号则说明安装成功。如下图所示。</p></blockquote><p><img src="/img/博客搭建之旅/1.png" alt=""></p><h4 id="安装-hexo"><a href="#安装-hexo" class="headerlink" title="安装 hexo"></a>安装 hexo</h4><p>在某个盘创建文件夹 hexo，进入该文件夹，输入以下命令。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure><blockquote><p>有时会得到如下图所示的 warn，无视之。</p></blockquote><p><img src="/img/博客搭建之旅/2.png" alt=""></p><p>然后输入 <code>hexo version</code> 命令判断是否安装成功，正确输出如下图所示。</p><p><img src="/img/博客搭建之旅/3.png" alt=""> </p><blockquote><p>我第一次安装时报错显示<strong>‘hexo’ 不是内部或外部命令，也不是可运行的程序</strong>，解决办法如下：</p><p>在 <code>hexo</code> 文件夹下用命令行输入 <code>npm install hexo --save</code> 安装 hexo，然后在 hexo 文件夹可以发现一个名为 <code>node_modules</code> 的文件夹，进入该文件夹，再进入第一个文件夹 <code>.bin</code>，复制当前路径 <code>F:\hexo\node_modules\.bin</code> 加入环境变量（如果不知道怎么加可以自行搜索），现在使用 <code>hexo version</code> 命令就可以得到正确的输出了！</p></blockquote><h4 id="博客文件夹的初始化"><a href="#博客文件夹的初始化" class="headerlink" title="博客文件夹的初始化"></a>博客文件夹的初始化</h4><p>现在我们开始初始化我们的博客，假设我们放博客资源的文件夹叫 chenyichen.github.io (可以把 chenyichen 改成自己的名字)，在 hexo 根目录用命令行输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init chenyichen.github.io</span><br></pre></td></tr></table></figure><p>然后你会发现在根目录下出现了一个名为 <code>chenyichen.github.io</code> 的文件夹，点进去发现如下的目录结构：</p><p><img src="/img/博客搭建之旅/4.png" alt=""></p><p>初始化成功！</p><h3 id="生成你的第一个博客页面！"><a href="#生成你的第一个博客页面！" class="headerlink" title="生成你的第一个博客页面！"></a>生成你的第一个博客页面！</h3><p>下面我们在 chenyichen.github.io 文件夹里用命令行输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo g #用来生成网页，g 是 generate 的缩写</span><br><span class="line">hexo s #把网页上传至本地服务器，s 是 server 的缩写</span><br></pre></td></tr></table></figure><p>如图所示：</p><p><img src="/img/博客搭建之旅/5.png" alt=""></p><p><img src="/img/博客搭建之旅/6.png" alt=""></p><p>现在打开浏览器，在搜索框内输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">localhost:4000</span><br></pre></td></tr></table></figure><p>点击回车，我们可以看到我们的第一个博客页面，Hello World！</p><p><img src="/img/博客搭建之旅/7.png" alt=""></p><h3 id="主题的安装和配置"><a href="#主题的安装和配置" class="headerlink" title="主题的安装和配置"></a>主题的安装和配置</h3><h4 id="主题推荐"><a href="#主题推荐" class="headerlink" title="主题推荐"></a>主题推荐</h4><p>我们刚刚建立了我们的第一个博客，有点丑对不对？没事儿，hexo 社区提供各式各样的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">博客主题</a>供你选择。但是这上百个主题对于选择困难症来说真的是太痛苦了！到底怎么选呢？我有两个建议。</p><p>第一就是你可以上知乎搜“hexo 主题”，里面有大神用爬虫爬下来 github 上最火的主题排序，看看哪些主题用的人比较多；第二就是一定不要选主题配置文档含糊不清的，之前我用了一个主题，在配置过程中各种出问题，文档介绍极其不清楚，虽然主题非常好看，但无奈怎么都编译不过，折腾了好几天只得放弃。这是现在我用的主题的<a href="https://molunerfinn.com/hexo-theme-melody-doc/#/" target="_blank" rel="noopener">配置文档</a>，博客的每一个细节都介绍得非常清楚，安利一波。</p><h4 id="主题的安装"><a href="#主题的安装" class="headerlink" title="主题的安装"></a>主题的安装</h4><p>你可以使用 git 命令将主题 clone 下来，但是鉴于墙的原因，速度有时非常慢，所以建议直接下载压缩包然后解压的方式。</p><p>以我现在用的主题 melody 为例，进入该主题的 github 页面，点击右边的绿色按钮，再点击 <code>Download ZIP</code>，保存到你博客文件夹下的 theme 文件夹下，接着解压到当前文件夹，然后我们就得到了一个名字为主题名字的文件夹，为了等下配置方便，我们把该该文件夹重命名为一个好记的名字，如“melody”，如下图所示。</p><p><img src="/img/博客搭建之旅/8.png" alt=""></p><p><img src="/img/博客搭建之旅/9.png" alt=""></p><p>现在我们进入放博客的文件夹的根目录，你会发现一个名为“_config.yml”的文件，用编辑器打开它，把倒数第五行的 <code>theme: landscape</code> 改成 <code>theme: melody</code>，保存文件，现在运行命令 <code>hexo server</code> 然后在浏览器输入 <code>localhost:4000</code>，看看是不是变成了你想要的主题！</p><h4 id="主题的配置"><a href="#主题的配置" class="headerlink" title="主题的配置"></a>主题的配置</h4><p>主题的配置一般在你下载主题的 github 页面会有详细的说明，比如 <a href="https://molunerfinn.com/hexo-theme-melody-doc/#/" target="_blank" rel="noopener">melody 主题说明文档</a>，所以就不详细说明了。有时候一些主题里没有的修改项，比如你需要修改 banner 的字体，只需要到该主题的文件夹里寻找对应的 css 文件然后修改即可，这个就需要你自己慢慢试慢慢找了。</p><h3 id="博客写作"><a href="#博客写作" class="headerlink" title="博客写作"></a>博客写作</h3><h4 id="写作工具"><a href="#写作工具" class="headerlink" title="写作工具"></a>写作工具</h4><p>我们的博客使用 markdown 进行写作，那么什么是 markdown 呢？有关它的介绍可以看这个：</p><ul><li><a href="https://github.com/younghz/Markdown" target="_blank" rel="noopener">markdown 介绍</a></li></ul><p>那么我们该用什么编辑器来写 markdown 呢？请看这个：</p><ul><li><p><a href="http://www.williamlong.info/archives/4319.html" target="_blank" rel="noopener">好用的 Markdown 编辑器一览</a></p></li><li><p><a href="https://www.zhihu.com/question/19637157" target="_blank" rel="noopener">用 Markdown 写作用什么文本编辑器？</a></p></li><li><p><a href="https://sspai.com/post/42126" target="_blank" rel="noopener">在 Windows 上拥有舒适的码字体验，12 款 Markdown 写作工具推荐</a></p></li></ul><h4 id="如何开始？"><a href="#如何开始？" class="headerlink" title="如何开始？"></a>如何开始？</h4><p>你可以在放博客的文件夹打开命令行，输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new 文章的名字</span><br></pre></td></tr></table></figure><p>然后我们发现在 source 文件夹下的 _post 文件夹生成了一个 .md 文件，也就是一个 markdown 文件，如下图所示。</p><p><img src="/img/博客搭建之旅/10.png" alt=""></p><p><img src="/img/博客搭建之旅/11.png" alt=""></p><p>打开它可以发现只有如下文字：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 我的第一篇博客</span><br><span class="line">date: 2018-07-29 16:53:49</span><br><span class="line">tags:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>这是某种类似于“头文件”的东西，决定了这篇文章的属性，通过添加或者修改这些关键字你就可以修改文章的标题、日期、分类、标签等属性，详细的属性介绍在<a href="https://hexo.io/zh-cn/docs/front-matter" target="_blank" rel="noopener">这里</a>可以找到。</p><p>你也可以自己在编辑器里创建 markdown 文件然后开始写文章，但是你必须先在前面加上这些“头文件”，然后把文章移动到 source 文件夹下的 _post 文件夹里。</p><h4 id="有关写作中图片的插入问题"><a href="#有关写作中图片的插入问题" class="headerlink" title="有关写作中图片的插入问题"></a>有关写作中图片的插入问题</h4><p>搭建独立博客写作最麻烦的一件事儿就是给文章添加图片，为啥麻烦呢，如果你把图片随文字一起上传到 github，那么它免费给你的 300M 空间也捉襟见肘，如果你使用图床（也就是把图片先传到网上的服务器，然后添加图片的时候引用它的链接），那么添加图片的步骤将会非常繁琐。</p><p>图床推荐最多的就是七牛图床，还有其他的比如微博图床和极简图床等等，网上的推荐很多，可以自行搜索，上传完图片后会得到一个图片的外链，复制这个链接，在需要引用图片的地方打如下文字就可以啦：</p><p><code>![](图片的外链)</code></p><p>那么如果你要上传本地图片咋办呢？我的建议是绝对路径引用。在 source 文件夹下建立名为 img 的文件夹专门用来放图片，再给每篇文章专门建立文件夹放这篇文章的图片，比如“我的第一篇博客”，图片名为 1.jpg，那么我引用该图片的时候就输入以下命令就可以啦：</p><p><code>![](/img/我的第一篇博客/1.jpg)</code></p><p>有关图片引用的说明还可以参考以下链接：</p><ul><li><a href="https://yanyinhong.github.io/2017/05/02/How-to-insert-image-in-hexo-post/" target="_blank" rel="noopener">Hexo 博客搭建之在文章中插入图片</a></li><li><a href="https://www.jianshu.com/p/c2ba9533088a" target="_blank" rel="noopener">hexo 博客图片问题</a></li></ul><blockquote><p>2018/8/3 更新，找到了更好的图床解决方案：用 github 做图床，然后配合一键上传图床工具 picgo，完美解决 markdown 图片上传问题，详细用法见 <a href="https://molunerfinn.com/PicGo/" target="_blank" rel="noopener">picgo官网</a></p></blockquote><h3 id="把博客部署上线"><a href="#把博客部署上线" class="headerlink" title="把博客部署上线"></a>把博客部署上线</h3><h4 id="设置-github"><a href="#设置-github" class="headerlink" title="设置 github"></a>设置 github</h4><p>最激动人心的时刻到了，当你安装好漂亮的主题，写好优秀的文章，就该部署上线让更多人看到了！由于 github 推出一个 github pages 服务，用来展示我们的网页，所以我们可以把我们的博客网页部署到那个上面，当别人在看我们的博客时，其实就是在看我们的 github pages 页面。</p><p>首先，我们需要注册一个 github 账号，假设你的 github 用户名是 zemin，我们建立一个项目名为：</p><p><code>zemin.github.io</code></p><blockquote><p>注意必须与你的用户名一模一样并严格按照格式来，否则待会儿不能直接通过 zemin.github.io 访问我们的博客，如下图所示，</p></blockquote><p><img src="/img/博客搭建之旅/12.png" alt=""></p><p><img src="/img/博客搭建之旅/13.png" alt=""></p><h4 id="把博客与-github-联系起来"><a href="#把博客与-github-联系起来" class="headerlink" title="把博客与 github 联系起来"></a>把博客与 github 联系起来</h4><p>我们在运行命令 <code>hexo g</code> 之后，可以在博客文件夹根目录发现一个名为 public 的的文件夹，这个就是我们 hexo 框架形成的网页文件，将它上传到我们刚刚建立的项目 zemin.github.io 里，便可以在我们的 github pages 页面看到我们的博客了，而这个页面的网址就是“zemin.github.io”了。</p><p>为了方便地上传我们的博客文件，我们用 SSH 密匙将博客与 github 联系起来，这个步骤有许多教程写的很好，我参考<a href="http://beiyuu.com/github-pages" target="_blank" rel="noopener">使用 Github Pages 建独立博客</a>和<a href="http://www.lovebxm.com/2018/06/24/hexo-github-blog/#%E6%90%AD%E5%BB%BA-Git-%E7%8E%AF%E5%A2%83" target="_blank" rel="noopener">可能是最详细的 Hexo + GitHub Pages 搭建个人博客的教程</a>写下以下教程：</p><p>1.生成 SSH</p><p>在命令行输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;你的邮箱&quot;</span><br></pre></td></tr></table></figure><p>一直点击回车直到碰到以下文字：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Enter passphrase (empty for no passphrase):</span><br><span class="line">Enter same passphrase again:</span><br></pre></td></tr></table></figure><p>第一个要求你输入密码，在部署的时候用得到，第二个要求你重复密码，如果不想要密码直接回车即可，我觉得个人电脑的话没必要输入密码。当碰到如下文字则说明运行成功。</p><p><img src="/img/博客搭建之旅/14.jpg" alt=""></p><p>2.把 SSH 添加进 github</p><p>SSH 一般放在路径 <code>C:\Users\pc\.ssh\id_rsa.pub</code> 下，如果没有再仔细找找，否则是上一步出了问题，用编辑器打开这个文件 .pub 文件，复制所有的文字。</p><p>登陆 github。点击右上角的 Settings—-&gt;SSH and GPG keys—-&gt;New SSH key，Title 可以填你项目的名字，然后把 .pub 里复制的文字再粘贴进 Key 里，最后点击 Add SSH Key 绿色按钮，完成配置。</p><p>3.测试是否配置成功</p><p>在命令行输入以下命令，切记原封不动输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>一直回车，碰到 <code>Are you sure you want to continue connecting (yes/no)?</code> 便输入“yes”，如果最后返回如下文字则配置成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hi XXX! You&apos;ve successfully authenticated, but GitHub does not provide shell access.</span><br></pre></td></tr></table></figure><p>4.设置 Git 的个人信息</p><p>命令行输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;你的真名&quot;</span><br><span class="line">git config --global user.email &quot;你的邮箱&quot;</span><br></pre></td></tr></table></figure><p>5.配置 _config.yml</p><p>进入你在 github 的项目并点击右侧 Clone and download 绿色按钮，选择 Clone with SSH，并复制下框里的一串文字，如下图所示。</p><p><img src="/img/博客搭建之旅/14.png" alt=""></p><p>然后用编辑器打开博客根目录下的 _config.yml 文件，找到 <code># Deployment</code>（布署） 部分，并作如下修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: git@github.com:yichenchan/yichenchan.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><blockquote><p>git@github.com:yichenchan/yichenchan.github.io.git 处就是你刚刚复制的一串文字</p></blockquote><p>那如果我们还需要部署在 Coding.net 上该怎么办呢？同样复制下 Coding.net 上该项目的 SSH，然后 # Deployment 修改为以下写法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Deployment</span><br><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo:</span><br><span class="line">      github: git@github.com:yichenchan/chenyichen.github.io.git,master</span><br><span class="line">      coding: git@git.coding.net:yichenChan/yichenChan.git,master</span><br></pre></td></tr></table></figure><p>做完这些，我们的本地博客就与 github 或者 coding.net 联系起来了！</p><h4 id="让我们的博客上线吧！"><a href="#让我们的博客上线吧！" class="headerlink" title="让我们的博客上线吧！"></a>让我们的博客上线吧！</h4><p>首先我们需要安装一个插件，在博客根目录用命令行输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p><p>安装成功后，在你博客文件夹的根目录用命令行执行以下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean #删除 public 里的文件</span><br><span class="line">hexo g     #生成新的 public 文件</span><br><span class="line">hexo d     #部署到 github 上</span><br></pre></td></tr></table></figure></p><p>或者合并成一句：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d -g</span><br></pre></td></tr></table></figure></p><p>由于网络的原因，布署的时间可能会非常长，而且经常会卡在一个地方不动，碰到这种情况请耐心等待，当布署成功之后，会有 git done 的提示。</p><p>现在打开浏览器输入 <code>zemin(改成你自己的).github.io</code>，我们就可以看到我们的博客上线啦！</p><h3 id="给博客一个好记的名字（域名）"><a href="#给博客一个好记的名字（域名）" class="headerlink" title="给博客一个好记的名字（域名）"></a>给博客一个好记的名字（域名）</h3><p>虽然到目前为止，我们成功地上线了我们的博客，但是每次看到域名后面的 github.io 总有种不是自己的博客的感觉，既然是独立博客，我们就应该有个性化的独立的名字，最后我们试着给它一个好记的域名。</p><h4 id="注册你自己的域名"><a href="#注册你自己的域名" class="headerlink" title="注册你自己的域名"></a>注册你自己的域名</h4><p>首先我们得有自己的一个域名，在 <a href="https://sg.godaddy.com/zh/" target="_blank" rel="noopener">GoDaddy</a> 或者<a href="https://wanwang.aliyun.com/" target="_blank" rel="noopener">万网</a>都可以用低价买到自己中意的域名，但是由于万网的域名解析现在需要实名制，而 GoDaddy 是国外的网站不需要实名制所以我选择在后者购买域名，买下的 chenyichen.xyz 大概第一年七块钱，还是很便宜的。</p><h4 id="将你的域名与-XXX-github-io-绑定"><a href="#将你的域名与-XXX-github-io-绑定" class="headerlink" title="将你的域名与 XXX.github.io 绑定"></a>将你的域名与 XXX.github.io 绑定</h4><p>我们刚刚注册的域名目前还是空的，我们需要用 DNS解析把该域名指向我们的 xxx.github.io，使得其他人在输入 chenyichen.xyz 的时候可以直接跳转到我的 yichenchan.github.io。DNS 就是专门负责把域名解析为 IP 地址的系统，我们也可以通过 CNAME 直接把我们的域名指向另一个域名，这样就不用去搜 IP 地址，更加方便，下面介绍该怎么做，以 GoDaddy 的 DNS 为例，如果你是在万网买的域名，可以进入万网的域名控制台，操作大同小异。</p><p>1.将购买的域名指向 XXX.github.io</p><p>进入 GoDaddy 的“我的产品”页面，找到你刚刚买的域名，点击右侧的“DNS”，如下图所示：</p><p><img src="/img/博客搭建之旅/15.png" alt=""></p><p>点击右边的铅笔符号即可修改，把 A 类型的“值”（“指向”）改成你网站的IP地址，把 CNAME 类型的“名称”（“主机”）改成“www”，“值”（“指向”）改成你的网站域名即“yichenchan.github.io”，“TTL”都自定义为“6000”，如下图所示。</p><blockquote><p>如何知道你网站的IP地址呢，在命令行输入 <code>ping yichenchan.github.io</code> (换成自己的二级域名) 即可。</p></blockquote><p><img src="/img/博客搭建之旅/16.png" alt=""></p><p>2.修改 _config.yml</p><p>其实不是很确定这步需不需要，但是以防万一做一下。</p><p>用编辑器打开博客根目录的 _config.yml 文件，找到如下代码：</p><p><code>url: http://yoursite.com</code> </p><p>改成：</p><p><code>url: http://chenyichen.xyz（你自己的域名）</code></p><p>3.建立 CNAME 文件</p><p>在博客目录下的 source 文件夹里新建一个<strong>没有拓展名</strong>的文件（可以用编辑器新建），然后命名为<strong>CNAME</strong>，在里面写上你自己购买的域名，然后保存即可。</p><p>4.修改 github pages 设置</p><p>进入你在 github 上的博客项目，点击 setting如下图所示：</p><p><img src="/img/博客搭建之旅/17.png" alt=""></p><p>往下滑，找到“GitHub Pages”设置，在“Custom domain”下的输入框输入你购买的域名，然后点击“save”，如果你需要让它使用 https 的话，可以点击下面的“Enforce HTTPS”，大功告成！！</p><p><img src="/img/博客搭建之旅/18.png" alt=""></p><p>一般来说，DNS解析需要一段时间，第二天再打开你的浏览器，输入只属于你自己的域名，看看你的博客是不是映入眼帘!!赶紧发条朋友圈炫耀吧hiahia！！</p><h2 id="建站感想"><a href="#建站感想" class="headerlink" title="建站感想"></a>建站感想</h2><p>虽然网上 hexo+github pages 建站的教程数不胜数，但是希望按照自己的思路来写一篇教程，一是为了发表一篇博客纪念一下建站，二是为了练习一下 markdow 的语法，如果有人能看到这篇教程并得到帮助那更是再好不过了。</p><p>作为一个小白，第一次拥有自己的小站，就感觉有了自己一个小窝一样。前前后后折腾了半个月，最后在 chenyichen.xyz 打开自己的博客的感觉真的很棒！不过建站的初衷还是为了能够记录一下自己的学习经历，生活感想，希望自己能坚持更新自己的博客吧，毕竟博客的价值在于其内容而不是他的外表，最后贴一下自己建站过程中参考的网站，感谢这些无私提供教程的大佬，还有感谢本博客主题的开发者<a href="https://github.com/Molunerfinn" target="_blank" rel="noopener">Molunerfinn</a>。</p><ul><li><p><a href="http://www.lovebxm.com/2018/06/24/hexo-github-blog/#%E5%B0%86-GitHub-Pages-%E5%9C%B0%E5%9D%80%E8%A7%A3%E6%9E%90%E5%88%B0%E4%B8%AA%E4%BA%BA%E5%9F%9F%E5%90%8D" target="_blank" rel="noopener">可能是最详细的 Hexo + GitHub Pages 搭建个人博客的教程</a></p></li><li><p><a href="https://www.jianshu.com/p/05289a4bc8b2" target="_blank" rel="noopener">如何搭建一个独立博客——简明 Github Pages 与 Hexo 教程</a></p></li><li><p><a href="http://beiyuu.com/github-pages" target="_blank" rel="noopener">使用 Github Pages 建独立博客</a></p></li><li><p><a href="http://blog.haoji.me/build-blog-website-by-hexo-github.html?from=xa" target="_blank" rel="noopener">使用 hexo+github 搭建免费个人博客详细教程</a></p></li><li><p><a href="https://molunerfinn.com/hexo-theme-melody-doc/#/" target="_blank" rel="noopener">melody 主题文档</a></p></li></ul>]]></content>
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
            <tag> blog </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>祝丸子同学生日快乐！！！</title>
      <link href="/2018/07/25/%E7%A5%9D%E4%B8%B8%E5%AD%90%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/"/>
      <url>/2018/07/25/%E7%A5%9D%E4%B8%B8%E5%AD%90%E7%94%9F%E6%97%A5%E5%BF%AB%E4%B9%90/</url>
      <content type="html"><![CDATA[<h2 id="祝可爱的丸子同学26岁生日快乐！"><a href="#祝可爱的丸子同学26岁生日快乐！" class="headerlink" title="祝可爱的丸子同学26岁生日快乐！"></a>祝可爱的丸子同学26岁生日快乐！</h2><a id="more"></a><h3 id="月亮代表我的心：）"><a href="#月亮代表我的心：）" class="headerlink" title="月亮代表我的心：）"></a>月亮代表我的心：）</h3><p><img src="/img/祝丸子生日快乐/1.jpg" alt=""></p>]]></content>
      
      <categories>
          
          <category> 个人随想 </category>
          
      </categories>
      
      
    </entry>
    
  
  
</search>
