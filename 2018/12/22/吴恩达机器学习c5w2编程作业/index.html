<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="coursera 吴恩达深度学习 Specialization 编程作业（course 5 week 2）"><meta name="keywords" content="吴恩达深度学习笔记,AI,词向量,Debiasing word vectors"><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>coursera 吴恩达深度学习 Specialization 编程作业（course 5 week 2） | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Part1-Operations-on-word-vectors"><span class="toc-number">1.</span> <span class="toc-text">Part1 - Operations on word vectors</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#cosine-相似度"><span class="toc-number">1.1.</span> <span class="toc-text">cosine 相似度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#词语类比"><span class="toc-number">1.2.</span> <span class="toc-text">词语类比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#词向量去偏见"><span class="toc-number">1.3.</span> <span class="toc-text">词向量去偏见</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#中立化-Neutralize-bias-for-non-gender-specific-words"><span class="toc-number">1.3.1.</span> <span class="toc-text">中立化 Neutralize bias for non-gender specific words</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#平均化-Equalization-algorithm-for-gender-specific-words"><span class="toc-number">1.3.2.</span> <span class="toc-text">平均化  Equalization algorithm for gender-specific words</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Part2-Emojify"><span class="toc-number">2.</span> <span class="toc-text">Part2 - Emojify</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#版本1-基本模型"><span class="toc-number">2.1.</span> <span class="toc-text">版本1 - 基本模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Emoji-数据集"><span class="toc-number">2.1.1.</span> <span class="toc-text">Emoji 数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#版本1-的模型"><span class="toc-number">2.1.2.</span> <span class="toc-text">版本1 的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#构建模型"><span class="toc-number">2.1.3.</span> <span class="toc-text">构建模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#在测试集上试验"><span class="toc-number">2.1.4.</span> <span class="toc-text">在测试集上试验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#版本-2-在-Keras-中使用-LSTM-模型"><span class="toc-number">2.2.</span> <span class="toc-text">版本 2 - 在 Keras 中使用 LSTM 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#模型概况"><span class="toc-number">2.2.1.</span> <span class="toc-text">模型概况</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Keras-和-minibatch"><span class="toc-number">2.2.2.</span> <span class="toc-text">Keras 和 minibatch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#嵌入层"><span class="toc-number">2.2.3.</span> <span class="toc-text">嵌入层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#建立模型"><span class="toc-number">2.2.4.</span> <span class="toc-text">建立模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#结论"><span class="toc-number">2.2.5.</span> <span class="toc-text">结论</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">33</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_topimg.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="post-info"><div id="post-title">coursera 吴恩达深度学习 Specialization 编程作业（course 5 week 2）</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-22</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习笔记/">学习笔记</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="Part1-Operations-on-word-vectors"><a href="#Part1-Operations-on-word-vectors" class="headerlink" title="Part1 - Operations on word vectors"></a>Part1 - Operations on word vectors</h1><p>通过这部分我们将学会：</p>
<ul>
<li>加载预训练词向量，用 cos 相似度测量相似度</li>
<li>使用词嵌入解决词语类比问题</li>
<li>减少词向量中的性别偏差</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> w2v_utils <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<p>接下来加载词向量，这次作业使用的是 50 维的 GloVe 向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words, word_to_vec_map = read_glove_vecs(<span class="string">'data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure>
<p>读取的函数为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_glove_vecs</span><span class="params">(glove_file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(glove_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        words = set()</span><br><span class="line">        word_to_vec_map = &#123;&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            line = line.strip().split()</span><br><span class="line">            curr_word = line[<span class="number">0</span>]</span><br><span class="line">            words.add(curr_word)</span><br><span class="line">            word_to_vec_map[curr_word] = np.array(line[<span class="number">1</span>:], dtype=np.float64)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> words, word_to_vec_map</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li>word：词汇表中的单词 set</li>
<li>word_to_veec_map：将单词映射到 GloVe 向量的 dict</li>
</ul>
<h2 id="cosine-相似度"><a href="#cosine-相似度" class="headerlink" title="cosine 相似度"></a>cosine 相似度</h2><p>用下列公式测量两个词向量的相似度：</p>
<script type="math/tex; mode=display">
\text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Cosine similarity reflects the degree of similariy between u and v</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        u -- a word vector of shape (n,)          </span></span><br><span class="line"><span class="string">        v -- a word vector of shape (n,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cosine_similarity -- the cosine similarity between u and v defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    distance = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the dot product between u and v (≈1 line)</span></span><br><span class="line">    dot = np.dot(u,v)</span><br><span class="line">    <span class="comment"># Compute the L2 norm of u (≈1 line)</span></span><br><span class="line">    norm_u = np.sqrt(np.sum(np.square(u)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the L2 norm of v (≈1 line)</span></span><br><span class="line">    norm_v = np.sqrt(np.sum(np.square(v)))</span><br><span class="line">    <span class="comment"># Compute the cosine similarity defined by formula (1) (≈1 line)</span></span><br><span class="line">    cosine_similarity = dot/(norm_u*norm_v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure>
<h2 id="词语类比"><a href="#词语类比" class="headerlink" title="词语类比"></a>词语类比</h2><p>要实现  “<em>a</em> is to <em>b</em> as <em>c</em> is to <strong>__</strong>“ 这种类比任务，我们可以从词向量词汇表里面找出这么一个词使得 $e_b - e_a \approx e_d - e_c$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a, word_b, word_c, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the word analogy task as explained above: a is to b as c is to ____. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_a -- a word, string</span></span><br><span class="line"><span class="string">    word_b -- a word, string</span></span><br><span class="line"><span class="string">    word_c -- a word, string</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary that maps words to their corresponding vectors. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert words to lower case</span></span><br><span class="line">    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Get the word embeddings v_a, v_b and v_c (≈1-3 lines)</span></span><br><span class="line">    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]</span><br><span class="line">    </span><br><span class="line">    words = word_to_vec_map.keys()</span><br><span class="line">    max_cosine_sim = <span class="number">-100</span>              <span class="comment"># Initialize max_cosine_sim to a large negative number</span></span><br><span class="line">    best_word = <span class="keyword">None</span>                   <span class="comment"># Initialize best_word with None, it will help keep track of the word to output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over the whole word vector set</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:        </span><br><span class="line">        <span class="comment"># to avoid best_word being one of the input words, pass on them.</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> [word_a, word_b, word_c] :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)</span></span><br><span class="line">        cosine_sim = cosine_similarity(word_to_vec_map[w], e_b-e_a+e_c)<span class="comment">#找出使得这个值最小的</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the cosine_sim is more than the max_cosine_sim seen so far,</span></span><br><span class="line">            <span class="comment"># then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> cosine_sim &gt; max_cosine_sim:</span><br><span class="line">            max_cosine_sim = cosine_sim</span><br><span class="line">            best_word = w</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> best_word</span><br></pre></td></tr></table></figure>
<p>我们进行一下测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">triads_to_try = [(<span class="string">'small'</span>, <span class="string">'smaller'</span>, <span class="string">'big'</span>), (<span class="string">'india'</span>, <span class="string">'delhi'</span>, <span class="string">'japan'</span>), (<span class="string">'man'</span>, <span class="string">'woman'</span>, <span class="string">'boy'</span>), (<span class="string">'small'</span>, <span class="string">'smaller'</span>, <span class="string">'large'</span>)]</span><br><span class="line"><span class="keyword">for</span> triad <span class="keyword">in</span> triads_to_try:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'&#123;&#125; -&gt; &#123;&#125; :: &#123;&#125; -&gt; &#123;&#125;'</span>.format( *triad, complete_analogy(*triad,word_to_vec_map)))</span><br></pre></td></tr></table></figure>
<p>small -&gt; smaller :: big -&gt; bigger<br>india -&gt; delhi :: japan -&gt; tokyo<br>man -&gt; woman :: boy -&gt; girl<br>small -&gt; smaller :: large -&gt; larger</p>
<p>效果不错！！</p>
<h2 id="词向量去偏见"><a href="#词向量去偏见" class="headerlink" title="词向量去偏见"></a>词向量去偏见</h2><p>我们首先要找到代表性别偏见的偏见轴向量 g，做法就是用 “woman” 这个词的词向量减去 “man” 这个词的词向量，即 $g = e_{woman}-e_{man}$，如果要更精确的结果，我们可以计算 $g_1 = e_{mother}-e_{father}$, $g_2 = e_{girl}-e_{boy}$ …… 然后取平均值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g = word_to_vec_map[<span class="string">'woman'</span>] - word_to_vec_map[<span class="string">'man'</span>]</span><br></pre></td></tr></table></figure>
<p>现在看看男生女生的名字和偏见轴的相似度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">'List of names and their similarities with constructed vector:'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># girls and boys name</span></span><br><span class="line">name_list = [<span class="string">'john'</span>, <span class="string">'marie'</span>, <span class="string">'sophie'</span>, <span class="string">'ronaldo'</span>, <span class="string">'priya'</span>, <span class="string">'rahul'</span>, <span class="string">'danielle'</span>, <span class="string">'reza'</span>, <span class="string">'katy'</span>, <span class="string">'yasmin'</span>,<span class="string">'bill'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> name_list:</span><br><span class="line">    <span class="keyword">print</span> (w, cosine_similarity(word_to_vec_map[w], g))</span><br></pre></td></tr></table></figure>
<p>List of names and their similarities with constructed vector:<br>john -0.23163356146<br>marie 0.315597935396<br>sophie 0.318687898594<br>ronaldo -0.312447968503<br>priya 0.17632041839<br>rahul -0.169154710392<br>danielle 0.243932992163<br>reza -0.079304296722<br>katy 0.283106865957<br>yasmin 0.233138577679<br>bill -0.0306830313755</p>
<p>我们发现，男生的名字倾向于有负的相似度，女生名字倾向于有正的相似度，由于男女有别，这个结果很正常，接下来让我们换一些中性的词语。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Other words and their similarities:'</span>)</span><br><span class="line">word_list = [<span class="string">'lipstick'</span>, <span class="string">'guns'</span>, <span class="string">'science'</span>, <span class="string">'arts'</span>, <span class="string">'literature'</span>, <span class="string">'warrior'</span>,<span class="string">'doctor'</span>, <span class="string">'tree'</span>, <span class="string">'receptionist'</span>, </span><br><span class="line">             <span class="string">'technology'</span>,  <span class="string">'fashion'</span>, <span class="string">'teacher'</span>, <span class="string">'engineer'</span>, <span class="string">'pilot'</span>, <span class="string">'computer'</span>, <span class="string">'singer'</span>]</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> word_list:</span><br><span class="line">    <span class="keyword">print</span> (w, cosine_similarity(word_to_vec_map[w], g))</span><br></pre></td></tr></table></figure>
<p>Other words and their similarities:<br>lipstick 0.276919162564<br>guns -0.18884855679<br>science -0.0608290654093<br>arts 0.00818931238588<br>literature 0.0647250443346<br>warrior -0.209201646411<br>doctor 0.118952894109<br>tree -0.0708939917548<br>receptionist 0.330779417506<br>technology -0.131937324476<br>fashion 0.0356389462577<br>teacher 0.179209234318<br>engineer -0.0803928049452<br>pilot 0.00107644989919<br>computer -0.103303588739<br>singer 0.185005181365</p>
<p>我们可以发现，“computer” 更接近于 “man”，而 “literature” 更接近于 “woman”！什么鬼？女生就不能学计算机？男生就不能学文学？屁！让我们纠正这个偏见！</p>
<h3 id="中立化-Neutralize-bias-for-non-gender-specific-words"><a href="#中立化-Neutralize-bias-for-non-gender-specific-words" class="headerlink" title="中立化 Neutralize bias for non-gender specific words"></a>中立化 Neutralize bias for non-gender specific words</h3><p>首先我们的词向量是 50 维的，将它分为两个部分，偏差轴方向 g 和剩下的 49 维向量，称之为 $g_{\perp}$，在线性代数里，我们称它们是正交的，也就是垂直的，所以 $g_{\perp}$ 是非偏差轴。我们将有偏差的词向量投影到非偏差轴上，便获得了中立化后的词向量。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_2.png" alt=""></p>
<p> 公式为：</p>
<script type="math/tex; mode=display">
词向量在偏差轴上的投影：e^{bias\_component} = \frac{e \cdot g}{||g||_2^2} * g
\\词向量在非偏差轴上的投影：e^{debiased} = e - e^{bias\_component}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word, g, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. </span></span><br><span class="line"><span class="string">    This function ensures that gender neutral words are zero in the gender subspace.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        word -- string indicating the word to debias</span></span><br><span class="line"><span class="string">        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)</span></span><br><span class="line"><span class="string">        word_to_vec_map -- dictionary mapping words to their corresponding vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        e_debiased -- neutralized word vector representation of the input "word"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line)</span></span><br><span class="line">    e = word_to_vec_map[word]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute e_biascomponent using the formula give above. (≈ 1 line)</span></span><br><span class="line">    e_biascomponent = (np.dot(e,g)/np.linalg.norm(g)**<span class="number">2</span>) * g</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Neutralize e by substracting e_biascomponent from it </span></span><br><span class="line">    <span class="comment"># e_debiased should be equal to its orthogonal projection. (≈ 1 line)</span></span><br><span class="line">    e_debiased = e - e_biascomponent</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e_debiased</span><br></pre></td></tr></table></figure>
<p>我们看看“接待员”这个词在中立化前后中立化后与偏差轴的相似度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">e = <span class="string">"receptionist"</span></span><br><span class="line">print(<span class="string">"cosine similarity between "</span> + e + <span class="string">" and g, before neutralizing: "</span>, cosine_similarity(word_to_vec_map[<span class="string">"receptionist"</span>], g))</span><br><span class="line"></span><br><span class="line">e_debiased = neutralize(<span class="string">"receptionist"</span>, g, word_to_vec_map)</span><br><span class="line">print(<span class="string">"cosine similarity between "</span> + e + <span class="string">" and g, after neutralizing: "</span>, cosine_similarity(e_debiased, g))</span><br></pre></td></tr></table></figure>
<p>cosine similarity between receptionist and g, before neutralizing:  0.330779417506<br>cosine similarity between receptionist and g, after neutralizing:  -3.26732746085e-17</p>
<h3 id="平均化-Equalization-algorithm-for-gender-specific-words"><a href="#平均化-Equalization-algorithm-for-gender-specific-words" class="headerlink" title="平均化  Equalization algorithm for gender-specific words"></a>平均化  Equalization algorithm for gender-specific words</h3><p>ok，我们已经将与性别无关的词投影到非偏差轴了，那么如果与性别有关的词到这根轴的距离不相等，那么这些词距离这类无性别词的距离就不相等了，造成了相对的偏差，比如 “保姆” 距离 “男演员” 和 “女演员” 的距离。我们要做的就是将男演员对应的词向量和女演员对应的词向量调整为关于非偏差轴对称。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_3.png" alt=""></p>
<p>作业的公式再一次出现了错误……</p>
<p>论坛一个大佬的更正后的公式：</p>
<script type="math/tex; mode=display">
\mu = \frac{e_{w1} + e_{w2}}{2}\\
 \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\\
  \mu_{\perp} = \mu - \mu_{B} \\
   e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\\
    e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis}\\
    e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {||e_{w1B}-\mu_B||} \\
    e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {||e_{w2B}-\mu_B||} \\
    e_1 = e_{w1B}^{corrected} + \mu_{\perp} \\
    e_2 = e_{w2B}^{corrected} + \mu_{\perp}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_4.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_5.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pair, bias_axis, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Debias gender specific words by following the equalize method described in the figure above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") </span></span><br><span class="line"><span class="string">    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their corresponding vectors</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    e_1 -- word vector corresponding to the first word</span></span><br><span class="line"><span class="string">    e_2 -- word vector corresponding to the second word</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines)</span></span><br><span class="line">    w1, w2 = pair</span><br><span class="line">    e_w1, e_w2 = word_to_vec_map[w1],word_to_vec_map[w2]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)</span></span><br><span class="line">    mu = (e_w1 + e_w2)/<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)</span></span><br><span class="line">    mu_B = (np.dot(mu,bias_axis)/(np.linalg.norm(bias_axis)**<span class="number">2</span>)) * bias_axis</span><br><span class="line">    mu_orth = mu - mu_B</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Use equations (7) and (8) to compute e_w1B and e_w2B (≈2 lines)</span></span><br><span class="line">    e_w1B = (np.dot(e_w1,bias_axis)/(np.linalg.norm(bias_axis)**<span class="number">2</span>)) * bias_axis</span><br><span class="line">    e_w2B = (np.dot(e_w2,bias_axis)/(np.linalg.norm(bias_axis)**<span class="number">2</span>)) * bias_axis</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 5: Adjust the Bias part of e_w1B and e_w2B using the formulas (9) and (10) given above (≈2 lines)</span></span><br><span class="line">    corrected_e_w1B = np.sqrt(abs(<span class="number">1</span>-np.linalg.norm(mu_orth)**<span class="number">2</span>)) * ((e_w1B-mu_B)/np.linalg.norm(e_w1B-mu_B))</span><br><span class="line">    corrected_e_w2B = np.sqrt(abs(<span class="number">1</span>-np.linalg.norm(mu_orth)**<span class="number">2</span>)) * ((e_w2B-mu_B)/np.linalg.norm(e_w2B-mu_B))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)</span></span><br><span class="line">    e1 = corrected_e_w1B + mu_orth</span><br><span class="line">    e2 = corrected_e_w2B + mu_orth                                                           </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e1, e2</span><br></pre></td></tr></table></figure>
<p>现在看看平均化前后的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"cosine similarities before equalizing:"</span>)</span><br><span class="line">print(<span class="string">"cosine_similarity(word_to_vec_map[\"doctor\"], gender) = "</span>, cosine_similarity(word_to_vec_map[<span class="string">"doctor"</span>], g))</span><br><span class="line">print(<span class="string">"cosine_similarity(word_to_vec_map[\"nurse\"], gender) = "</span>, cosine_similarity(word_to_vec_map[<span class="string">"nurse"</span>], g))</span><br><span class="line">print()</span><br><span class="line">e1, e2 = equalize((<span class="string">"doctor"</span>, <span class="string">"nurse"</span>), g, word_to_vec_map)</span><br><span class="line">print(<span class="string">"cosine similarities after equalizing:"</span>)</span><br><span class="line">print(<span class="string">"cosine_similarity(e1, gender) = "</span>, cosine_similarity(e1, g))</span><br><span class="line">print(<span class="string">"cosine_similarity(e2, gender) = "</span>, cosine_similarity(e2, g))</span><br></pre></td></tr></table></figure>
<p>cosine similarities before equalizing:<br>cosine_similarity(word_to_vec_map[“doctor”], gender) =  0.118952894109<br>cosine_similarity(word_to_vec_map[“nurse”], gender) =  0.380308796807</p>
<p>cosine similarities after equalizing:<br>cosine_similarity(e1, gender) =  -0.698086236808<br>cosine_similarity(e2, gender) =  0.698086236808</p>
<p>参考：The debiasing algorithm is from Bolukbasi et al., 2016, <a href="https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf" target="_blank" rel="noopener">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a></p>
<h1 id="Part2-Emojify"><a href="#Part2-Emojify" class="headerlink" title="Part2 - Emojify"></a>Part2 - Emojify</h1><p>这部分作业主要就是用两种方法实现句子的 Emoji 化，目的是输入一个句子，输出跟这个句子有关的 Emoji.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> emo_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> emoji</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h2 id="版本1-基本模型"><a href="#版本1-基本模型" class="headerlink" title="版本1 - 基本模型"></a>版本1 - 基本模型</h2><h3 id="Emoji-数据集"><a href="#Emoji-数据集" class="headerlink" title="Emoji 数据集"></a>Emoji 数据集</h3><ul>
<li>X 包含 127 条句子（字符串）</li>
<li>Y 包含了每个句子对应的标签值，从 0 - 4 的整数，对应了五个 emoji </li>
</ul>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_6.png" alt=""></p>
<p>加载训练集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, Y_train = read_csv(<span class="string">'data/train_emoji.csv'</span>)</span><br><span class="line">X_test, Y_test = read_csv(<span class="string">'data/tesss.csv'</span>)</span><br></pre></td></tr></table></figure>
<p>找出具有最长单词数的句子的长度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">maxLen = len(max(X_train, key=len).split())</span><br></pre></td></tr></table></figure>
<h3 id="版本1-的模型"><a href="#版本1-的模型" class="headerlink" title="版本1 的模型"></a>版本1 的模型</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_7.png" alt=""></p>
<p>为了能够计算损失函数，将标签值变为 one-hot 向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y_oh_train = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br><span class="line">Y_oh_test = convert_to_one_hot(Y_test, C = <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><p>第一步是加载预训练的 50 维的 Glove 词向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(<span class="string">'data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>word_to_index</code> 是将 word 映射到词汇表中 index 的字典（400001个词）</li>
<li><code>index_to_word</code> 是将 index 映射到 word 的索引值的字典</li>
<li><code>word_to_vec_map</code> 是将 word 映射到其词向量的字典</li>
</ul>
<p>接下来我们实现输入句子输出其所有词向量的平均值的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_avg</span><span class="params">(sentence, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word</span></span><br><span class="line"><span class="string">    and averages its value into a single vector encoding the meaning of the sentence.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    sentence -- string, one training example from X</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Step 1: Split sentence into list of lower case words (≈ 1 line)</span></span><br><span class="line">    words = sentence.lower().split()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the average word vector, should have the same shape as your word vectors.</span></span><br><span class="line">    avg = np.zeros((<span class="number">50</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: average the word vectors. You can loop over the words in the list "words".</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        avg += word_to_vec_map[w]</span><br><span class="line">    avg = avg / len(words)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure>
<p>以上函数用在前向传播中，接下来是反向传播的公式：</p>
<script type="math/tex; mode=display">
z^{(i)} = W . avg^{(i)} + b\\
a^{(i)} = softmax(z^{(i)})\\
\mathcal{L}^{(i)} = - \sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)</script><p>其中 Yoh 是标签 Y 的 onehot 向量。</p>
<p>下面是模型函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, word_to_vec_map, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">400</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Model to train word vector representations in numpy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, numpy array of sentences as strings, of shape (m, 1)</span></span><br><span class="line"><span class="string">    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    learning_rate -- learning_rate for the stochastic gradient descent algorithm</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    pred -- vector of predictions, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    W -- weight matrix of the softmax layer, of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">    b -- bias of the softmax layer, of shape (n_y,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define number of training examples</span></span><br><span class="line">    m = Y.shape[<span class="number">0</span>]                          <span class="comment"># number of training examples</span></span><br><span class="line">    n_y = <span class="number">5</span>                                 <span class="comment"># number of classes  </span></span><br><span class="line">    n_h = <span class="number">50</span>                                <span class="comment"># dimensions of the GloVe vectors </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters using Xavier initialization</span></span><br><span class="line">    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)</span><br><span class="line">    b = np.zeros((n_y,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert Y to Y_onehot with n_y classes</span></span><br><span class="line">    Y_oh = convert_to_one_hot(Y, C = n_y) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_iterations):  <span class="comment"># 遍历完所有的 epoch</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):           <span class="comment"># 每一个 epoch 遍历完所有样本，每一个样本更新一次参数，应该是随机梯度下降</span></span><br><span class="line">          </span><br><span class="line">            <span class="comment"># Average the word vectors of the words from the i'th training example</span></span><br><span class="line">            avg = sentence_to_avg(X[i], word_to_vec_map)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagate the avg through the softmax layer</span></span><br><span class="line">            z = np.dot(W,avg)+b</span><br><span class="line">            a = softmax(z)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax)</span></span><br><span class="line">            cost = -np.sum(Y_oh*np.log(a))</span><br><span class="line">          </span><br><span class="line">            <span class="comment"># Compute gradients </span></span><br><span class="line">            dz = a - Y_oh[i]</span><br><span class="line">            dW = np.dot(dz.reshape(n_y,<span class="number">1</span>), avg.reshape(<span class="number">1</span>, n_h))</span><br><span class="line">            db = dz</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters with Stochastic Gradient Descent</span></span><br><span class="line">            W = W - learning_rate * dW</span><br><span class="line">            b = b - learning_rate * db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch: "</span> + str(t) + <span class="string">" --- cost = "</span> + str(cost))</span><br><span class="line">            pred = predict(X, Y, W, b, word_to_vec_map)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred, W, b</span><br></pre></td></tr></table></figure>
<p>训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred, W, b = model(X_train, Y_train, word_to_vec_map)</span><br><span class="line">print(pred)</span><br></pre></td></tr></table></figure>
<p>Epoch: 0 —- cost = 227.527181633<br>Accuracy: 0.348484848485<br>Epoch: 100 —- cost = 418.198641202<br>Accuracy: 0.931818181818<br>Epoch: 200 —- cost = 482.727277095<br>Accuracy: 0.954545454545<br>Epoch: 300 —- cost = 516.659063961<br>Accuracy: 0.969696969697</p>
<h3 id="在测试集上试验"><a href="#在测试集上试验" class="headerlink" title="在测试集上试验"></a>在测试集上试验</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Training set:"</span>)</span><br><span class="line">pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)</span><br><span class="line">print(<span class="string">'Test set:'</span>)</span><br><span class="line">pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)</span><br></pre></td></tr></table></figure>
<p>Training set:<br>Accuracy: 0.977272727273<br>Test set:<br>Accuracy: 0.857142857143</p>
<p>自己随便写几个句子进行测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_my_sentences = np.array([<span class="string">"i hate you"</span>, <span class="string">"i love you"</span>, <span class="string">"funny lol"</span>, <span class="string">"lets play with a ball"</span>, <span class="string">"food is ready"</span>, <span class="string">"not feeling happy"</span>])</span><br><span class="line">Y_my_labels = np.array([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">2</span>], [<span class="number">1</span>], [<span class="number">4</span>],[<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)</span><br><span class="line">print_predictions(X_my_sentences, pred)</span><br></pre></td></tr></table></figure>
<p>Accuracy: 0.666666666667</p>
<p>i hate you 😞<br>i love you ❤️<br>funny lol 😄<br>lets play with a ball ⚾<br>food is ready 🍴<br>not feeling happy 😄</p>
<p>我们可以打印一下混淆矩阵，帮助理解哪一类emoji更加对模型难以分辨。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'           '</span>+ label_to_emoji(<span class="number">0</span>)+ <span class="string">'    '</span> + label_to_emoji(<span class="number">1</span>) + <span class="string">'    '</span> +  label_to_emoji(<span class="number">2</span>)+ <span class="string">'    '</span> + label_to_emoji(<span class="number">3</span>)+<span class="string">'   '</span> + label_to_emoji(<span class="number">4</span>))</span><br><span class="line">print(pd.crosstab(Y_test, pred_test.reshape(<span class="number">56</span>,), rownames=[<span class="string">'Actual'</span>], colnames=[<span class="string">'Predicted'</span>], margins=<span class="keyword">True</span>))</span><br><span class="line">plot_confusion_matrix(Y_test, pred_test)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_8.png" alt=""></p>
<p>这个模型没有考虑词语的前后联系，所以会出现 not feeling happy 😄 这种错误，接下来用 LSTM 模型来实现这个任务。</p>
<h2 id="版本-2-在-Keras-中使用-LSTM-模型"><a href="#版本-2-在-Keras-中使用-LSTM-模型" class="headerlink" title="版本 2 - 在 Keras 中使用 LSTM 模型"></a>版本 2 - 在 Keras 中使用 LSTM 模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, Dropout, LSTM, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="模型概况"><a href="#模型概况" class="headerlink" title="模型概况"></a>模型概况</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_9.png" alt=""></p>
<h3 id="Keras-和-minibatch"><a href="#Keras-和-minibatch" class="headerlink" title="Keras 和 minibatch"></a>Keras 和 minibatch</h3><p>数据集中所有的句子长度不是统一的，而在 Keras 中要实现小批量梯度下降，某个 minibatch 中所有句子的长度必须全部相同，这样才能输入 LSTM 层进行训练，为了解决这个问题，我们可以将句子进行<strong>填充</strong>，以最长的句子为基准，不足的部分用零向量进行填充，假设最长的句子有 20 个词，那么 “I love you” 这个句子的词向量为 $(e_{i}, e_{love}, e_{you}, \vec{0}, \vec{0}, \ldots, \vec{0})$.</p>
<h3 id="嵌入层"><a href="#嵌入层" class="headerlink" title="嵌入层"></a>嵌入层</h3><p>在 Keras 中，嵌入矩阵是用类似于嵌入层的形式实现的，输入一个由索引值组成的句子，输出句子每个词的词向量。接下来我们会实现一个用预训练词向量初始化过的嵌入层，由于训练集较小，保持嵌入层参数固定不被训练。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.2_10.png" alt=""></p>
<p>首先进行数据的预处理，也就是把句子每个词变成 index，长度不够的用 0 进行填充。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)</span></span><br><span class="line">    X_indices = np.zeros((m,max_len))	<span class="comment"># 这一步其实已经做好了零填充</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = X[i].lower().split()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] = word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure>
<p>接下来就用我们自己的词嵌入矩阵开始构建 Embedding 层啦，方法是：</p>
<ul>
<li>将我们自己的词嵌入矩阵变成 Embedding 层的参数要求的形状<ul>
<li>先用 0 初始化一个正确的形状的矩阵</li>
<li>将词向量填入这个矩阵</li>
</ul>
</li>
<li>定义 <a href="https://keras.io/layers/embeddings/" target="_blank" rel="noopener">Embedding 层</a> </li>
<li>将该层的参数设为我们自己的词嵌入矩阵</li>
</ul>
<p>详细的嵌入层设置可以见这个<a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" target="_blank" rel="noopener">博客</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 以下部分都是为了将我们自己的词嵌入矩阵变成 Embedding 层的参数要求的形状</span></span><br><span class="line">    vocab_len = len(word_to_index) + <span class="number">1</span>                    <span class="comment"># Keras embedding 层输入的规定，估计是注意词汇表的索引值第一个是 1 而不是 0，所以要加一</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]      <span class="comment"># define dimensionality of your GloVe word vectors (= 50)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先用 0 初始化一个正确的形状(vocab_len,emb_dim)的矩阵 </span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim))<span class="comment"># 这个是传入Keras embedding 层的参数，它的形状必须跟 embedding.get_weights() 的形状相同</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 按照索引值将正确的词向量填入这个矩阵</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word] </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从这里开始构建 Keras 的 embedding 层</span></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. </span></span><br><span class="line">    embedding_layer = Embedding(vocab_len,emb_dim,trainable=<span class="keyword">False</span>)<span class="comment"># 这个将该层调为“不可训练”，保持其参数不变</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))<span class="comment"># 这里不太懂？？</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将该层的参数设为我们自己的词嵌入矩阵</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure>
<h3 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h3><p>一切准备就绪！开始建立整个模型，包括 Input 层 -&gt; LSTM 层（返回所有序列值） -&gt; Dropout 层 -&gt; LSTM 层（返回最后一个值）-&gt; Dropout 层 -&gt; softmax（包括 Dense 层和 softmax 激活层） </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojify_V2</span><span class="params">(input_shape, word_to_vec_map, word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义输入层</span></span><br><span class="line">    sentence_indices = Input(shape=input_shape, dtype=<span class="string">'int32'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建嵌入层（参数是自己训练好的词嵌入矩阵）</span></span><br><span class="line">    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将输入向前传播</span></span><br><span class="line">    embeddings = embedding_layer(sentence_indices)   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 继续传播经过一个隐藏单元数为 128 的 LSTM 层</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>, return_sequences=<span class="keyword">True</span>)(embeddings)<span class="comment"># 注意设置 return_sequences=True，返回所有时间序列 </span></span><br><span class="line">    <span class="comment"># 继续经过一个概率值为 0.5 的 Dropout 层</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># 继续传播经过一个隐藏单元数为 128 的 LSTM 层</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>)(X)<span class="comment"># 注意不用设置 return_sequences=True，默认返回最后一个cell的值</span></span><br><span class="line">    <span class="comment"># 继续经过一个概率值为 0.5 的 Dropout 层</span></span><br><span class="line">    X = Dropout(<span class="number">0.5</span>)(X)</span><br><span class="line">    <span class="comment"># 这是 softmax 层的第一部分，先通过一个全连接层</span></span><br><span class="line">    X = Dense(<span class="number">5</span>)(X)</span><br><span class="line">    <span class="comment"># 再通过 softmax 激活函数层</span></span><br><span class="line">    X = Activation(<span class="string">'softmax'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建从 sentence_indices 到 X 的模型</span></span><br><span class="line">    model = Model(input=sentence_indices, outputs=X)</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>让我们看看模型的 summary，maxLen 为 10：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">_________________________________________________________________</span><br><span class="line">Layer (type)                 Output Shape              Param #   </span><br><span class="line">=================================================================</span><br><span class="line">input_1 (InputLayer)         (None, 10)                0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">embedding_2 (Embedding)      (None, 10, 50)            20000050  </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_1 (LSTM)                (None, 10, 128)           91648     </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)          (None, 10, 128)           0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">lstm_2 (LSTM)                (None, 128)               131584    </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)          (None, 128)               0         </span><br><span class="line">_________________________________________________________________</span><br><span class="line">dense_1 (Dense)              (None, 5)                 645       </span><br><span class="line">_________________________________________________________________</span><br><span class="line">activation_1 (Activation)    (None, 5)                 0         </span><br><span class="line">=================================================================</span><br><span class="line">Total params: 20,223,927</span><br><span class="line">Trainable params: 223,877</span><br><span class="line">Non-trainable params: 20,000,050</span><br></pre></td></tr></table></figure>
<p>其中 20000050  个嵌入层的参数是我们预训练好的，不需要训练，所以只需要更新两个 LSTM 层和一个 Dense 层的 223,877 个参数。</p>
<p>接下来编译模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<p>模型输入形状(<code>m</code>, <code>max_len</code>) 的索引值，输出形状为(<code>m</code>, <code>number of classes</code>) 的标签值 onehot 向量，接下来将训练集变成相应的形状。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)</span><br><span class="line">Y_train_oh = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>接下来开始训练，设置 epoch 为 50，batch_size = 32：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train_indices, Y_train_oh, epochs = <span class="number">50</span>, batch_size = <span class="number">32</span>, shuffle=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">......    </span><br><span class="line">Epoch 47/50</span><br><span class="line">132/132 [==============================] - 0s - loss: 0.0597 - acc: 0.9773     </span><br><span class="line">Epoch 48/50</span><br><span class="line">132/132 [==============================] - 0s - loss: 0.0428 - acc: 0.9848     </span><br><span class="line">Epoch 49/50</span><br><span class="line">132/132 [==============================] - 0s - loss: 0.0379 - acc: 0.9848     </span><br><span class="line">Epoch 50/50</span><br><span class="line">132/132 [==============================] - 0s - loss: 0.0418 - acc: 0.9773</span><br></pre></td></tr></table></figure>
<p>让我们在测试集上面试试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先测试集同样需要变一下形状</span></span><br><span class="line">X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)</span><br><span class="line">Y_test_oh = convert_to_one_hot(Y_test, C = <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 进行测试</span></span><br><span class="line">loss, acc = model.evaluate(X_test_indices, Y_test_oh)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Test accuracy = "</span>, acc)</span><br></pre></td></tr></table></figure>
<p>32/56 [================&gt;………….] - ETA: 0s<br>Test accuracy =  0.821428562914</p>
<p>结果还不错！接下来看看哪些地方搞错了！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">C = <span class="number">5</span></span><br><span class="line">y_test_oh = np.eye(C)[Y_test.reshape(<span class="number">-1</span>)] <span class="comment"># np.eye(C)生成5×5的对角矩阵，np.eye(C)[Y_test.reshape(-1)]取[]中每个元素的索引值</span></span><br><span class="line">                                          <span class="comment"># 注意 np.array[np.array] 将会把 [] 中所有的元素索引出来 </span></span><br><span class="line">X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)</span><br><span class="line">pred = model.predict(X_test_indices)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(X_test)):</span><br><span class="line">    x = X_test_indices</span><br><span class="line">    num = np.argmax(pred[i])<span class="comment"># 将 onehot 变成索引</span></span><br><span class="line">    <span class="keyword">if</span>(num != Y_test[i]):</span><br><span class="line">        print(<span class="string">'Expected emoji:'</span>+ label_to_emoji(Y_test[i]) + <span class="string">' prediction: '</span>+ X_test[i] + label_to_emoji(num).strip())</span><br></pre></td></tr></table></figure>
<p>Expected emoji:😄 prediction: she got me a nice present    ❤️<br>Expected emoji:😞 prediction: work is hard    😄<br>Expected emoji:😞 prediction: This girl is messing with me    ❤️<br>Expected emoji:😞 prediction: work is horrible    😄<br>Expected emoji:🍴 prediction: any suggestions for dinner    😄<br>Expected emoji:😄 prediction: you brighten my day    ❤️<br>Expected emoji:😞 prediction: she is a bully    😄<br>Expected emoji:😞 prediction: My life is so boring    ❤️<br>Expected emoji:😄 prediction: will you be my valentine    ❤️<br>Expected emoji:😞 prediction: go away    ⚾<br>Expected emoji:🍴 prediction: I did not have breakfast ❤️</p>
<p>在第一部分我们有一个句子 “not feel good” 总是会失误，原因就是未考虑词语前后联系，看看使用这个模型效果如何：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_test = np.array([<span class="string">'not feeling good'</span>])</span><br><span class="line">X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)</span><br><span class="line">print(x_test[<span class="number">0</span>] +<span class="string">' '</span>+  label_to_emoji(np.argmax(model.predict(X_test_indices))))</span><br></pre></td></tr></table></figure>
<p>not feeling well 😞</p>
<p>成功！！</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>词嵌入对于小训练集的 NLP 问题意义重大</li>
<li>在 Keras 中训练序列化模型需要注意的细节：<ul>
<li>为了使用 mini-batch，我们必须将所有样本填充至一样的长度</li>
<li>嵌入层 Embedding() 可以用自己预训练的词嵌入矩阵进行初始化，这些值即可以固定不训练也可以继续在数据集中微调，如果训练集较小，一般不值得训练</li>
<li>LSTM() 层有一个选项，return_sequences，决定是返回所有  cell 的值还是只最后一个 cell 的值</li>
<li>使用 Dropout() 层进行正则化</li>
</ul>
</li>
</ul>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/词向量/">词向量</a><a class="post-meta__tags" href="/tags/Debiasing-word-vectors/">Debiasing word vectors</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr1.png"><div class="post-qr-code__desc">微信捐赠</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr2.jpg"><div class="post-qr-code__desc">支付宝捐赠</div></div></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/01/02/2018总结/"><i class="fa fa-chevron-left">  </i><span>我的 2018 总结</span></a></div><div class="next-post pull-right"><a href="/2018/12/17/吴恩达机器学习笔记c5w2/"><span>coursera 吴恩达深度学习 Specialization 笔记（course 5 week 2）—— 词嵌入</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=530 height=86 src="//music.163.com/outchain/player?type=2&id=566993785&auto=1&height=66"></iframe></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zODM1Mi8xNDg4MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>