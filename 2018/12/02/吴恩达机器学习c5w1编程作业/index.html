<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="coursera 吴恩达深度学习 Specialization 编程作业（course 5 week 1）"><meta name="keywords" content="吴恩达深度学习笔记,AI,rnn,lstm,梯度剪枝"><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>coursera 吴恩达深度学习 Specialization 编程作业（course 5 week 1） | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#part-1-Building-your-Recurrent-Neural-Network-Step-by-Step"><span class="toc-number">1.</span> <span class="toc-text">part 1- Building your Recurrent Neural Network - Step by Step</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN"><span class="toc-number">1.1.</span> <span class="toc-text">RNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#单个-RNN-前向传播"><span class="toc-number">1.1.1.</span> <span class="toc-text">单个 RNN 前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总的-RNN-前向传播"><span class="toc-number">1.1.2.</span> <span class="toc-text">总的 RNN 前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#单个-RNN-单元反向传播"><span class="toc-number">1.1.3.</span> <span class="toc-text">单个 RNN 单元反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总的-RNN-反向传播"><span class="toc-number">1.1.4.</span> <span class="toc-text">总的 RNN 反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LSTM"><span class="toc-number">1.2.</span> <span class="toc-text">LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#单个-LSTM-单元的前向传播"><span class="toc-number">1.2.1.</span> <span class="toc-text">单个 LSTM 单元的前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总的-LSTM-前向传播"><span class="toc-number">1.2.2.</span> <span class="toc-text">总的 LSTM 前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#单个-LSTM-单元的反向传播"><span class="toc-number">1.2.3.</span> <span class="toc-text">单个 LSTM 单元的反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总的-LSTM-反向传播"><span class="toc-number">1.2.4.</span> <span class="toc-text">总的 LSTM 反向传播</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Part-2-Character-level-language-model-Dinosaurus-land"><span class="toc-number">2.</span> <span class="toc-text">Part 2-Character level language model - Dinosaurus land</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#问题描述"><span class="toc-number">2.1.</span> <span class="toc-text">问题描述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集加载和预处理"><span class="toc-number">2.1.1.</span> <span class="toc-text">数据集加载和预处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型概况"><span class="toc-number">2.1.2.</span> <span class="toc-text">模型概况</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#建立模型组块"><span class="toc-number">2.2.</span> <span class="toc-text">建立模型组块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度剪枝-gradient-clipping"><span class="toc-number">2.2.1.</span> <span class="toc-text">梯度剪枝 gradient clipping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#采样-sampling"><span class="toc-number">2.2.2.</span> <span class="toc-text">采样 sampling</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#建立语言模型"><span class="toc-number">2.3.</span> <span class="toc-text">建立语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#梯度下降"><span class="toc-number">2.3.1.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#训练模型"><span class="toc-number">2.3.2.</span> <span class="toc-text">训练模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Part-3-Improvise-a-Jazz-Solo-with-an-LSTM-Network"><span class="toc-number">3.</span> <span class="toc-text">Part 3-Improvise a Jazz Solo with an LSTM Network</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#问题描述-1"><span class="toc-number">3.1.</span> <span class="toc-text">问题描述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#加载数据集"><span class="toc-number">3.1.1.</span> <span class="toc-text">加载数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型概况-1"><span class="toc-number">3.1.2.</span> <span class="toc-text">模型概况</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#建立模型"><span class="toc-number">3.2.</span> <span class="toc-text">建立模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#生成音乐"><span class="toc-number">3.3.</span> <span class="toc-text">生成音乐</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#生成音乐-1"><span class="toc-number">3.3.1.</span> <span class="toc-text">生成音乐</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">33</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_topimg.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="post-info"><div id="post-title">coursera 吴恩达深度学习 Specialization 编程作业（course 5 week 1）</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-12-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习笔记/">学习笔记</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="part-1-Building-your-Recurrent-Neural-Network-Step-by-Step"><a href="#part-1-Building-your-Recurrent-Neural-Network-Step-by-Step" class="headerlink" title="part 1- Building your Recurrent Neural Network - Step by Step"></a>part 1- Building your Recurrent Neural Network - Step by Step</h1><p>这是这一周编程作业的第一部分，让我们从无到有构建了 RNN 和 LSTM 的前向和反向传播函数，但是作业中给的公式出现很多问题，下面都是更正后的公式，还有一些符号的提法比较模糊，下面也做了说明。</p>
<p>首先引入需要的包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> rnn_utils <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h3 id="单个-RNN-前向传播"><a href="#单个-RNN-前向传播" class="headerlink" title="单个 RNN 前向传播"></a>单个 RNN 前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_1.png" alt=""></p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Wax,xt) + np.dot(Waa,a_prev) + ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya,a_prev) + by)   </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure>
<h3 id="总的-RNN-前向传播"><a href="#总的-RNN-前向传播" class="headerlink" title="总的 RNN 前向传播"></a>总的 RNN 前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_2.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters["Wya"]</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a" and "y" with zeros</span></span><br><span class="line">    a = np.zeros((n_a,m,T_x)) <span class="comment"># 初始化一个存放所有激活值的容器</span></span><br><span class="line">    y_pred = np.zeros((n_y,m,T_x)) <span class="comment"># 初始化一个存放所有y预测值的容器</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next </span></span><br><span class="line">    a_next = a0 <span class="comment"># 将 a_next 初始化为 a&lt;0&gt; </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Append "cache" to "caches" (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure>
<h3 id="单个-RNN-单元反向传播"><a href="#单个-RNN-单元反向传播" class="headerlink" title="单个 RNN 单元反向传播"></a>单个 RNN 单元反向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_3.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass for the RNN-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradient of loss with respect to next hidden state</span></span><br><span class="line"><span class="string">    cache -- python dictionary containing useful values (output of rnn_cell_forward())</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradients of input data, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from cache</span></span><br><span class="line">    (a_next, a_prev, xt, parameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve values from parameters</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of tanh with respect to a_next </span></span><br><span class="line">    dtanh = da_next * (<span class="number">1</span> - a_next**<span class="number">2</span>) <span class="comment"># 指的是代价函数对 (Wax·x^&lt;t&gt;+Waa·a^&lt;t-1&gt;+ba) 的导数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of the loss with respect to Wax</span></span><br><span class="line">    dxt = np.dot(Wax.T, dtanh)</span><br><span class="line">    dWax = np.dot(dtanh, xt.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to Waa </span></span><br><span class="line">    da_prev = np.dot(Waa.T, dtanh)</span><br><span class="line">    dWaa = np.dot(dtanh, a_prev.T)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient with respect to b </span></span><br><span class="line">    dba = np.sum(dtanh, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa, <span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>
<h3 id="总的-RNN-反向传播"><a href="#总的-RNN-反向传播" class="headerlink" title="总的 RNN 反向传播"></a>总的 RNN 反向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_4.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for a RNN over an entire sequence of input data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple containing information from the forward pass (rnn_forward)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches </span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, a0, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes </span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes </span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    dWax = np.zeros((n_a, n_x))</span><br><span class="line">    dWaa = np.zeros((n_a, n_a))</span><br><span class="line">    dba = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    da0 = np.zeros((n_a, m))</span><br><span class="line">    da_prevt = np.zeros((n_a, m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop through all the time steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute gradients at time step t. Choose wisely the "da_next" and the "cache" to use in the backward propagation step. (≈1 line)</span></span><br><span class="line">        <span class="comment"># 这里注意：a^&lt;t&gt; 在这里分两条路影响代价函数 J，所以 da = da[:, :, t] + da_prevt</span></span><br><span class="line">        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Retrieve derivatives from gradients (≈ 1 line)</span></span><br><span class="line">        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[<span class="string">"dxt"</span>], gradients[<span class="string">"da_prev"</span>], gradients[<span class="string">"dWax"</span>], gradients[<span class="string">"dWaa"</span>], gradients[<span class="string">"dba"</span>]</span><br><span class="line">        <span class="comment"># Increment global derivatives w.r.t parameters by adding their derivative at time-step t (≈4 lines)</span></span><br><span class="line">        dx[:, :, t] = dxt</span><br><span class="line">        <span class="comment"># 注意：前向传播中所有 t 的 Wax、Waa、ba 都是相等的，但是每个 t 时间的 dWaxt、dWaa、dba 都是不相等的，</span></span><br><span class="line">        <span class="comment">#      它们都影响了代价函数 J，所以需要把每一个时间 t 求到的 dWaxt、dWaat、dbat 累加起来</span></span><br><span class="line">        dWax += dWaxt</span><br><span class="line">        dWaa += dWaat</span><br><span class="line">        dba += dbat</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set da0 to the gradient of a which has been backpropagated through all time-steps (≈1 line) </span></span><br><span class="line">    da0 = da_prevt</span><br><span class="line">  </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa,<span class="string">"dba"</span>: dba&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>
<h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><h3 id="单个-LSTM-单元的前向传播"><a href="#单个-LSTM-单元的前向传播" class="headerlink" title="单个 LSTM 单元的前向传播"></a>单个 LSTM 单元的前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_5.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span></span><br><span class="line"><span class="string">          c stands for the memory value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>]</span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wi = parameters[<span class="string">"Wi"</span>]</span><br><span class="line">    bi = parameters[<span class="string">"bi"</span>]</span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>]</span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>]</span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt 将 a_prev 和 xt 拼在一起</span></span><br><span class="line">    concat = np.zeros((n_a+n_x, m))</span><br><span class="line">    concat[: n_a, :] = a_prev</span><br><span class="line">    concat[n_a :, :] = xt</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft * c_prev + it * cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wy,a_next) + by)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure>
<h3 id="总的-LSTM-前向传播"><a href="#总的-LSTM-前向传播" class="headerlink" title="总的 LSTM 前向传播"></a>总的 LSTM 前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_6.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters['Wy'] </span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">'Wy'</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros </span></span><br><span class="line">    a = np.zeros((n_a,m,T_x)) <span class="comment"># 存放 a 的容器</span></span><br><span class="line">    c = np.zeros((n_a,m,T_x)) <span class="comment"># 存放 c 的容器</span></span><br><span class="line">    y = np.zeros((n_y,m,T_x)) <span class="comment"># 存放 y 的容器</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a,m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache </span></span><br><span class="line">        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a </span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y </span></span><br><span class="line">        y[:,:,t] = yt</span><br><span class="line">        <span class="comment"># Save the value of the next cell state </span></span><br><span class="line">        c[:,:,t]  = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure>
<h3 id="单个-LSTM-单元的反向传播"><a href="#单个-LSTM-单元的反向传播" class="headerlink" title="单个 LSTM 单元的反向传播"></a>单个 LSTM 单元的反向传播</h3><p>由于这部分给的公式发生了错误，所以结合论坛里的讨论自己进行了公式的推导和更正，可以在<a href="https://chenyichen.xyz/2018/12/01/deeplearning.ai%20%E7%AC%AC%E4%BA%94%E8%AF%BE%E7%AC%AC%E4%B8%80%E5%91%A8%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%20lstm%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E9%83%A8%E5%88%86%E7%9A%84%E5%85%AC%E5%BC%8F%E6%9B%B4%E6%AD%A3%E5%8F%8A%E6%8E%A8%E5%AF%BC/">这篇笔记</a>里找到。</p>
<script type="math/tex; mode=display">
\begin{split}原公式中的“d \Gamma_o^{\langle t \rangle}” &= dZ_o^{<t>}=\frac{\partial J}{\partial Z_o^{<t>}}=\frac{\partial J}{\partial(W_o[a^{<t-1>},x^{<t>}]+b_o)}  \\ 
&= da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*\left(1-\Gamma_o^{\langle t \rangle}\right) \end{split}\tag{7}</script><script type="math/tex; mode=display">
\begin{split}原公式中的“d\widetilde{c}^{\langle t \rangle}”&=dZ_c^{<t>}=\frac{\partial J}{\partial Z_c^{<t>}}=\frac{\partial J}{\partial(W_c[a^{<t-1>},x^{<t>}]+b_c)} \\
&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle}* (1-\tanh(c_{next})^2) * da_{next} \right) \Gamma_u^{\langle t \rangle} * \left(1-\left(\widetilde c^{\langle t \rangle}\right)^2\right) \end{split}\tag{8}</script><script type="math/tex; mode=display">
\begin{split}原公式中的“d\Gamma_u^{\langle t \rangle}”&=dZ_u^{<t>}=\frac{\partial J}{\partial Z_u^{<t>}}=\frac{\partial J}{\partial(W_u[a^{<t-1>},x^{<t>}]+b_u)}  \\
&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle}* (1-\tanh(c_{next})^2) * da_{next}\right)*\Gamma_u^{\langle t \rangle}* \widetilde{c}^{\langle t \rangle} *\left(1-\Gamma_u^{\langle t \rangle}\right)\end{split}\tag{9}</script><script type="math/tex; mode=display">
\begin{split}原公式中的“d\Gamma_f^{\langle t \rangle}”&=dZ_f^{<t>}=\frac{\partial J}{\partial Z_f^{<t>}}=\frac{\partial J}{\partial(W_f[a^{<t-1>},x^{<t>}]+b_f)}  \\
&= \left(dc_{next}+ \Gamma_o^{\langle t \rangle} * (1-\tanh(c_{next})^2)  * da_{next}\right)*\Gamma_f^{\langle t \rangle}* c_{prev}*\left(1-\Gamma_f^{\langle t \rangle}\right)\end{split}\tag{10}</script><script type="math/tex; mode=display">
dW_f = d\Gamma_f^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{11}</script><script type="math/tex; mode=display">
dW_u = d\Gamma_u^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{12}</script><script type="math/tex; mode=display">
dW_c = d\widetilde c^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{13}</script><script type="math/tex; mode=display">
dW_o = d\Gamma_o^{\langle t \rangle} \begin{bmatrix} a_{prev} \\ x_t\end{bmatrix}^T \tag{14}</script><script type="math/tex; mode=display">
db_f=np.sum(d\Gamma_f^{\langle t \rangle},axis=1,keepdims=True)\tag{15}</script><script type="math/tex; mode=display">
db_u=np.sum(d\Gamma_u^{\langle t \rangle},axis=1,keepdims=True)\tag{16}</script><script type="math/tex; mode=display">
db_c=np.sum(d\Gamma_c^{\langle t \rangle},axis=1,keepdims=True)\tag{17}</script><script type="math/tex; mode=display">
db_o=np.sum(d\Gamma_o^{\langle t \rangle},axis=1,keepdims=True)\tag{18}</script><script type="math/tex; mode=display">
da_{prev} = W_f^T[:,:n_a] d\Gamma_f^{\langle t \rangle} + W_u^T[:,:n_a]   d\Gamma_u^{\langle t \rangle}+ W_c^T[:,:n_a] d\widetilde c^{\langle t \rangle} + W_o^T[:,:n_a] d\Gamma_o^{\langle t \rangle} \tag{19}</script><script type="math/tex; mode=display">
dc_{prev} = dc_{next}*\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \tag{20}</script><script type="math/tex; mode=display">
dx^{\langle t \rangle} = W_f^T[:,n_a:] d\Gamma_f^{\langle t \rangle} + W_u^T[:,n_a:]  d\Gamma_u^{\langle t \rangle}+ W_c^T[:,n_a:] d\widetilde c^{\langle t \rangle} + W_o^T[:,n_a:] d\Gamma_o^{\langle t \rangle}\tag{21}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_backward</span><span class="params">(da_next, dc_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the LSTM-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradients of next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    dc_next -- Gradients of next cell state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    cache -- cache storing information from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dxt -- Gradient of input data at time-step t, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dc_prev -- Gradient w.r.t. the previous memory state, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the output gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters) = cache</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from xt's and a_next's shape </span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_a, m = a_next.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gates related derivatives, you can find their values can be found by looking carefully at equations (7) to (10) </span></span><br><span class="line">    dot = da_next * np.tanh(c_next)*ot*(<span class="number">1</span>-ot) <span class="comment"># 也就是 dJ / d(W_o[a^&#123;&lt;t-1&gt;&#125;,x^&#123;&lt;t&gt;&#125;]+b_o),下同</span></span><br><span class="line">    dcct = (dc_next + da_next*ot*(<span class="number">1</span>-np.tanh(c_next)**<span class="number">2</span>))*it*(<span class="number">1</span>-cct**<span class="number">2</span>)</span><br><span class="line">    dit = (dc_next + da_next*ot*(<span class="number">1</span>-np.tanh(c_next)**<span class="number">2</span>))*cct*it*(<span class="number">1</span>-it) <span class="comment"># it 就是更新门 updategate</span></span><br><span class="line">    dft = (dc_next + da_next*ot*(<span class="number">1</span>-np.tanh(c_next)**<span class="number">2</span>))*c_prev*ft*(<span class="number">1</span>-ft)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute parameters related derivatives. Use equations (11)-(14) </span></span><br><span class="line">    dWf = np.dot(dft, np.vstack((a_prev,xt)).T)</span><br><span class="line">    dWi = np.dot(dit, np.vstack((a_prev,xt)).T)</span><br><span class="line">    dWc = np.dot(dcct,np.vstack((a_prev,xt)).T)</span><br><span class="line">    dWo = np.dot(dot,np.vstack((a_prev,xt)).T)</span><br><span class="line">    dbf = np.sum(dft, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dbi = np.sum(dit, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dbc = np.sum(dcct, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dbo = np.sum(dot, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute derivatives w.r.t previous hidden state, previous memory state and input. Use equations (15)-(17). </span></span><br><span class="line">    da_prev = np.dot(parameters[<span class="string">'Wf'</span>][:,:n_a].T,dft) + np.dot(parameters[<span class="string">'Wi'</span>][:,:n_a].T,dit) + np.dot(parameters[<span class="string">'Wo'</span>][:,:n_a].T,dot) + np.dot(parameters[<span class="string">'Wc'</span>][:,:n_a].T,dcct) </span><br><span class="line">    dc_prev = da_next*ot*(<span class="number">1</span>-(np.tanh(c_next))**<span class="number">2</span>)</span><br><span class="line">    dxt = np.dot(parameters[<span class="string">'Wf'</span>][:,n_a:].T,dft) + np.dot(parameters[<span class="string">'Wi'</span>][:,n_a:].T,dit) + np.dot(parameters[<span class="string">'Wo'</span>][:,n_a:].T,dot) + np.dot(parameters[<span class="string">'Wc'</span>][:,n_a:].T,dcct)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Save gradients in dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dc_prev"</span>: dc_prev, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>
<h3 id="总的-LSTM-反向传播"><a href="#总的-LSTM-反向传播" class="headerlink" title="总的 LSTM 反向传播"></a>总的 LSTM 反向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for the RNN with LSTM-cell (over a whole sequence).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Gradients w.r.t the hidden states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    dc -- Gradients w.r.t the memory states, numpy-array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- cache storing information from the forward pass (lstm_forward)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient of inputs, of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t. the previous hidden state, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWf -- Gradient w.r.t. the weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWi -- Gradient w.r.t. the weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWc -- Gradient w.r.t. the weight matrix of the memory gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dWo -- Gradient w.r.t. the weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        dbf -- Gradient w.r.t. biases of the forget gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbi -- Gradient w.r.t. biases of the update gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbc -- Gradient w.r.t. biases of the memory gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dbo -- Gradient w.r.t. biases of the save gate, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches.</span></span><br><span class="line">    (caches, x) = caches</span><br><span class="line">    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes </span></span><br><span class="line">    n_a, m, T_x = da.shape</span><br><span class="line">    n_x, m = x1.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes</span></span><br><span class="line">    dx = np.zeros((n_x, m, T_x))</span><br><span class="line">    da0 = np.zeros((n_a,m))</span><br><span class="line">    da_prevt = np.zeros((n_a,m))</span><br><span class="line">    dc_prevt = np.zeros((n_a,m))</span><br><span class="line">    dWf = np.zeros((n_a, n_a+n_x)) <span class="comment"># 每个时间点 t 的 Wf 都是相等的</span></span><br><span class="line">    dWi = np.zeros((n_a,n_a+n_x))</span><br><span class="line">    dWc = np.zeros((n_a,n_a+n_x))</span><br><span class="line">    dWo = np.zeros((n_a,n_a+n_x))</span><br><span class="line">    dbf = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    dbi = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    dbc = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    dbo = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop back over the whole sequence</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T_x)):</span><br><span class="line">        <span class="comment"># Compute all gradients using lstm_cell_backward</span></span><br><span class="line">        gradients = lstm_cell_backward(da[:,:,t], dc_prevt, caches[t])</span><br><span class="line">        <span class="comment"># Store or add the gradient to the parameters' previous step's gradient</span></span><br><span class="line">        dx[:,:,t] = gradients[<span class="string">'dxt'</span>]</span><br><span class="line">        <span class="comment"># 注意：前向传播中所有 t 的 Wf、Wi、Wo、Wc、ba、bi、bo、bc 都是相等的，</span></span><br><span class="line">        <span class="comment">#      但是每个 t 时间的 dWf、dWi、dWo、dWc、dba、dbi、dbo、dbc 都是不相等的，</span></span><br><span class="line">        <span class="comment">#      它们都影响了代价函数 J，</span></span><br><span class="line">        <span class="comment">#      所以需要把每一个时间 t 求到的 dWf、dWi、dWo、dWc、dba、dbi、dbo、dbc 累加起来</span></span><br><span class="line">        dWf += gradients[<span class="string">'dWf'</span>]</span><br><span class="line">        dWi += gradients[<span class="string">'dWi'</span>]</span><br><span class="line">        dWc += gradients[<span class="string">'dWc'</span>]</span><br><span class="line">        dWo += gradients[<span class="string">'dWo'</span>]</span><br><span class="line">        dbf += gradients[<span class="string">'dbf'</span>]</span><br><span class="line">        dbi += gradients[<span class="string">'dbi'</span>]</span><br><span class="line">        dbc += gradients[<span class="string">'dbc'</span>]</span><br><span class="line">        dbo += gradients[<span class="string">'dbo'</span>]</span><br><span class="line">    <span class="comment"># Set the first activation's gradient to the backpropagated gradient da_prev.</span></span><br><span class="line">    da0 = gradients[<span class="string">'da_prev'</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Store the gradients in a python dictionary</span></span><br><span class="line">    gradients = &#123;<span class="string">"dx"</span>: dx, <span class="string">"da0"</span>: da0, <span class="string">"dWf"</span>: dWf,<span class="string">"dbf"</span>: dbf, <span class="string">"dWi"</span>: dWi,<span class="string">"dbi"</span>: dbi,</span><br><span class="line">                <span class="string">"dWc"</span>: dWc,<span class="string">"dbc"</span>: dbc, <span class="string">"dWo"</span>: dWo,<span class="string">"dbo"</span>: dbo&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>
<h1 id="Part-2-Character-level-language-model-Dinosaurus-land"><a href="#Part-2-Character-level-language-model-Dinosaurus-land" class="headerlink" title="Part 2-Character level language model - Dinosaurus land"></a>Part 2-Character level language model - Dinosaurus land</h1><p>创造一个字符级的语言模型，通过一堆恐龙名字进行训练，用来生成新的恐龙名字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><h3 id="数据集加载和预处理"><a href="#数据集加载和预处理" class="headerlink" title="数据集加载和预处理"></a>数据集加载和预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = open(<span class="string">'dinos.txt'</span>, <span class="string">'r'</span>).read()</span><br><span class="line">data= data.lower()</span><br><span class="line">chars = list(set(data))	<span class="comment"># 去除重复元素</span></span><br><span class="line">data_size, vocab_size = len(data), len(chars)</span><br><span class="line">print(<span class="string">'There are %d total characters and %d unique characters in your data.'</span> % (data_size, vocab_size))</span><br></pre></td></tr></table></figure>
<p>output：There are 19909 total characters and 27 unique characters in your data.</p>
<p>由于是字符级别的，所以一共有 26 个英文字母加上结束符 \n = 27 个字符，将这些字符一一对应存入字典中。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(sorted(chars)) &#125; <span class="comment"># sorted 进行排序</span></span><br><span class="line">ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> enumerate(sorted(chars)) &#125;</span><br><span class="line">print(ix_to_char)</span><br></pre></td></tr></table></figure>
<p>output：{0: ‘\n’, 1: ‘a’, 2: ‘b’, 3: ‘c’, 4: ‘d’, 5: ‘e’, 6: ‘f’, 7: ‘g’, 8: ‘h’, 9: ‘i’, 10: ‘j’, 11: ‘k’, 12: ‘l’, 13: ‘m’, 14: ‘n’, 15: ‘o’, 16: ‘p’, 17: ‘q’, 18: ‘r’, 19: ‘s’, 20: ‘t’, 21: ‘u’, 22: ‘v’, 23: ‘w’, 24: ‘x’, 25: ‘y’, 26: ‘z’}</p>
<h3 id="模型概况"><a href="#模型概况" class="headerlink" title="模型概况"></a>模型概况</h3><ul>
<li>初始化参数</li>
<li>优化循环<ul>
<li>前向传播计算 loss</li>
<li>反向传播计算 loss 对每个参数的梯度</li>
<li>进行梯度剪枝 gradient clipping，防止梯度爆炸</li>
<li>更新参数</li>
</ul>
</li>
<li>返回学习参数</li>
</ul>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_2.png" alt=""></p>
<h2 id="建立模型组块"><a href="#建立模型组块" class="headerlink" title="建立模型组块"></a>建立模型组块</h2><p>两个需要建立的：Gradient clipping 和 采样 sampling</p>
<h3 id="梯度剪枝-gradient-clipping"><a href="#梯度剪枝-gradient-clipping" class="headerlink" title="梯度剪枝 gradient clipping"></a>梯度剪枝 gradient clipping</h3><p>反向传播得到梯度后，在更新参数之前，进行一步梯度剪枝的操作，用以确保不会发生梯度爆炸，也就是把梯度限制在某个区间内，比如 [-10,10]，大于 10 的梯度取 10，小于 -10 的梯度取 -10.</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_7.png" alt=""></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span><span class="params">(gradients, maxValue)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Clips the gradients' values between minimum and maximum.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    gradients -- a dictionary containing the gradients "dWaa", "dWax", "dWya", "db", "dby"</span></span><br><span class="line"><span class="string">    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    gradients -- a dictionary with the clipped gradients.</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    dWaa, dWax, dWya, db, dby = gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWax'</span>], gradients[<span class="string">'dWya'</span>], gradients[<span class="string">'db'</span>], gradients[<span class="string">'dby'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)</span></span><br><span class="line">    <span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax, dWaa, dWya, db, dby]:</span><br><span class="line">        np.minimum(gradient,maxValue,out=gradient)</span><br><span class="line">        np.maximum(gradient,-maxValue,out=gradient)<span class="comment"># 注意！！gradient 在这里只是一个临时变量，我们需要把得到的值 out 输出给它</span></span><br><span class="line"></span><br><span class="line">    gradients = &#123;<span class="string">"dWaa"</span>: dWaa, <span class="string">"dWax"</span>: dWax, <span class="string">"dWya"</span>: dWya, <span class="string">"db"</span>: db, <span class="string">"dby"</span>: dby&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>
<h3 id="采样-sampling"><a href="#采样-sampling" class="headerlink" title="采样 sampling"></a>采样 sampling</h3><p>模型训练好之后，如何生成文字呢？如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_8.png" alt=""></p>
<ul>
<li><p>先输入一个假的输入 $x^{<1>} = 0，a^{<0>} = 0$</0></1></p>
</li>
<li><p>按照公式计算 $\hat y^{<1>}$ 和 $a^{<1>}$</1></1></p>
</li>
<li><p>进行采样：实际上 $\hat y^{<1>}$ 是 $x^{<2>}$ 的预测值，包含了取不同值的概率，我们根据不同的概率进行随机的取值，例如取索引值 3 的概率为 0.16，那么 $x^{<2>}$ 就有 16% 的概率被赋值为 3</2></2></1></p>
<ul>
<li><p>为什么不直接取 $\hat y^{<1>}$ 中概率值最大的赋值给 $x^{<2>}$？论坛看到一个解释：</2></1></p>
<p>Sampling produces variety. Say you have a list of items with probability of occurrence “car” (30%) “airplane” (70%). If you simply choose the argmax of the probability from the list then you always choose airplane which gets boring. If you alternatively “sample” from the list then 70% of the time you get airplane and 30% car. This is really important in longer sentences; if you choose the argmax you end up with the same sentence all the time (exception when multiple words have exactly same probability 50:50), but sampling gives you an interesting mix of sentences from the same list.</p>
</li>
</ul>
</li>
<li><p>将采样得到的 $x^{<n+1>}$ 索引值进行 one-hot 编码赋值给 x 以输入下一个循环。重复第一步直到 y 输出一个结束符 “\n” </n+1></p>
</li>
</ul>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(parameters, char_to_ix)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Sample a sequence of characters according to a sequence of probability distributions output of the RNN</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. </span></span><br><span class="line"><span class="string">    char_to_ix -- python dictionary mapping each character to an index.</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    indices -- a list of length n containing the indices of the sampled characters.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters and relevant shapes from "parameters" dictionary</span></span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    vocab_size = by.shape[<span class="number">0</span>]</span><br><span class="line">    n_a = Waa.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Create the one-hot vector x for the first character (initializing the sequence generation).</span></span><br><span class="line">    x = np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Step 1': Initialize a_prev as zeros </span></span><br><span class="line">    a_prev = np.zeros((n_a,<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)</span></span><br><span class="line">    indices = []	<span class="comment"># 存放每一步生成的索引值</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Idx is a flag to detect a newline character, we initialize it to -1</span></span><br><span class="line">    idx = <span class="number">-1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over time-steps t. At each time-step, sample a character from a probability distribution and append </span></span><br><span class="line">    <span class="comment"># its index to "indices". We'll stop if we reach 50 characters (which should be very unlikely with a well </span></span><br><span class="line">    <span class="comment"># trained model), which helps debugging and prevents entering an infinite loop. </span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    newline_character = char_to_ix[<span class="string">'\n'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (idx != newline_character <span class="keyword">and</span> counter != <span class="number">50</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2: Forward propagate x using the equations (1), (2) and (3)</span></span><br><span class="line">        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)</span><br><span class="line">        z = np.dot(Wya,a)+by</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3: Sample the index of a character within the vocabulary from the probability distribution y</span></span><br><span class="line">        idx = np.random.choice(vocab_size, p=y.ravel())	<span class="comment"># 按照概率进行随机取值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append the index to "indices"</span></span><br><span class="line">        indices.append(idx)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 4: Overwrite the input character as the one corresponding to the sampled index.</span></span><br><span class="line">        x = np.zeros((vocab_size,<span class="number">1</span>))</span><br><span class="line">        x[idx] = <span class="number">1</span>	<span class="comment"># one-hot 编码</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update "a_prev" to be "a"</span></span><br><span class="line">        a_prev = a</span><br><span class="line">        </span><br><span class="line">        counter +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (counter == <span class="number">50</span>): <span class="comment"># 限制生成的单词字长不超过 50，防止无限循环</span></span><br><span class="line">        indices.append(char_to_ix[<span class="string">'\n'</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> indices</span><br></pre></td></tr></table></figure>
<h2 id="建立语言模型"><a href="#建立语言模型" class="headerlink" title="建立语言模型"></a>建立语言模型</h2><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>一些辅助函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smooth</span><span class="params">(loss, cur_loss)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> loss * <span class="number">0.999</span> + cur_loss * <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_sample</span><span class="params">(sample_ix, ix_to_char)</span>:</span></span><br><span class="line">    txt = <span class="string">''</span>.join(ix_to_char[ix] <span class="keyword">for</span> ix <span class="keyword">in</span> sample_ix)</span><br><span class="line">    txt = txt[<span class="number">0</span>].upper() + txt[<span class="number">1</span>:]  <span class="comment"># capitalize first character </span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'%s'</span> % (txt, ), end=<span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_initial_loss</span><span class="params">(vocab_size, seq_length)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> -np.log(<span class="number">1.0</span>/vocab_size)*seq_length</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_a, n_x, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initialize parameters with small random values</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        b --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    Wax = np.random.randn(n_a, n_x)*<span class="number">0.01</span> <span class="comment"># input to hidden</span></span><br><span class="line">    Waa = np.random.randn(n_a, n_a)*<span class="number">0.01</span> <span class="comment"># hidden to hidden</span></span><br><span class="line">    Wya = np.random.randn(n_y, n_a)*<span class="number">0.01</span> <span class="comment"># hidden to output</span></span><br><span class="line">    b = np.zeros((n_a, <span class="number">1</span>)) <span class="comment"># hidden bias</span></span><br><span class="line">    by = np.zeros((n_y, <span class="number">1</span>)) <span class="comment"># output bias</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"Wax"</span>: Wax, <span class="string">"Waa"</span>: Waa, <span class="string">"Wya"</span>: Wya, <span class="string">"b"</span>: b,<span class="string">"by"</span>: by&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_forward</span><span class="params">(parameters, a_prev, x)</span>:</span></span><br><span class="line">    </span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) <span class="comment"># hidden state</span></span><br><span class="line">    p_t = softmax(np.dot(Wya, a_next) + by) <span class="comment"># unnormalized log probabilities for next chars # probabilities for next chars </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, p_t</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_step_backward</span><span class="params">(dy, gradients, parameters, x, a, a_prev)</span>:</span></span><br><span class="line">    </span><br><span class="line">    gradients[<span class="string">'dWya'</span>] += np.dot(dy, a.T)</span><br><span class="line">    gradients[<span class="string">'dby'</span>] += dy</span><br><span class="line">    da = np.dot(parameters[<span class="string">'Wya'</span>].T, dy) + gradients[<span class="string">'da_next'</span>] <span class="comment"># backprop into h</span></span><br><span class="line">    daraw = (<span class="number">1</span> - a * a) * da <span class="comment"># backprop through tanh nonlinearity</span></span><br><span class="line">    gradients[<span class="string">'db'</span>] += daraw</span><br><span class="line">    gradients[<span class="string">'dWax'</span>] += np.dot(daraw, x.T)</span><br><span class="line">    gradients[<span class="string">'dWaa'</span>] += np.dot(daraw, a_prev.T)</span><br><span class="line">    gradients[<span class="string">'da_next'</span>] = np.dot(parameters[<span class="string">'Waa'</span>].T, daraw)</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, gradients, lr)</span>:</span></span><br><span class="line"></span><br><span class="line">    parameters[<span class="string">'Wax'</span>] += -lr * gradients[<span class="string">'dWax'</span>]</span><br><span class="line">    parameters[<span class="string">'Waa'</span>] += -lr * gradients[<span class="string">'dWaa'</span>]</span><br><span class="line">    parameters[<span class="string">'Wya'</span>] += -lr * gradients[<span class="string">'dWya'</span>]</span><br><span class="line">    parameters[<span class="string">'b'</span>]  += -lr * gradients[<span class="string">'db'</span>]</span><br><span class="line">    parameters[<span class="string">'by'</span>]  += -lr * gradients[<span class="string">'dby'</span>]</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(X, Y, a0, parameters, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize x, a and y_hat as empty dictionaries</span></span><br><span class="line">    x, a, y_hat = &#123;&#125;, &#123;&#125;, &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    a[<span class="number">-1</span>] = np.copy(a0)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize your loss to 0</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(len(X)):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Set x[t] to be the one-hot vector representation of the t'th character in X.</span></span><br><span class="line">        <span class="comment"># if X[t] == None, we just have x[t]=0. This is used to set the input for the first timestep to the zero vector. </span></span><br><span class="line">        x[t] = np.zeros((vocab_size,<span class="number">1</span>)) </span><br><span class="line">        <span class="keyword">if</span> (X[t] != <span class="keyword">None</span>):</span><br><span class="line">            x[t][X[t]] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Run one step forward of the RNN</span></span><br><span class="line">        a[t], y_hat[t] = rnn_step_forward(parameters, a[t<span class="number">-1</span>], x[t])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update the loss by substracting the cross-entropy term of this time-step from it.</span></span><br><span class="line">        loss -= np.log(y_hat[t][Y[t],<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">    cache = (y_hat, a, x)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> loss, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(X, Y, parameters, cache)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize gradients as an empty dictionary</span></span><br><span class="line">    gradients = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve from cache and parameters</span></span><br><span class="line">    (y_hat, a, x) = cache</span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">'Waa'</span>], parameters[<span class="string">'Wax'</span>], parameters[<span class="string">'Wya'</span>], parameters[<span class="string">'by'</span>], parameters[<span class="string">'b'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># each one should be initialized to zeros of the same dimension as its corresponding parameter</span></span><br><span class="line">    gradients[<span class="string">'dWax'</span>], gradients[<span class="string">'dWaa'</span>], gradients[<span class="string">'dWya'</span>] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)</span><br><span class="line">    gradients[<span class="string">'db'</span>], gradients[<span class="string">'dby'</span>] = np.zeros_like(b), np.zeros_like(by)</span><br><span class="line">    gradients[<span class="string">'da_next'</span>] = np.zeros_like(a[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagate through time</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(len(X))):</span><br><span class="line">        dy = np.copy(y_hat[t])</span><br><span class="line">        dy[Y[t]] -= <span class="number">1</span></span><br><span class="line">        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t<span class="number">-1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients, a</span><br></pre></td></tr></table></figure>
<p>建立模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Execute one step of the optimization to train the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.</span></span><br><span class="line"><span class="string">    Y -- list of integers, exactly the same as X but shifted one index to the left.</span></span><br><span class="line"><span class="string">    a_prev -- previous hidden state.</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        b --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- value of the loss function (cross-entropy)</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        db -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dby -- Gradients of output bias vector, of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Forward propagate through time (≈1 line)</span></span><br><span class="line">    loss, cache = rnn_forward(X, Y, a_prev, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagate through time (≈1 line)</span></span><br><span class="line">    gradients, a = rnn_backward(X, Y, parameters, cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Clip your gradients between -5 (min) and 5 (max) (≈1 line)</span></span><br><span class="line">    gradients = clip(gradients, <span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update parameters (≈1 line)</span></span><br><span class="line">    parameters = update_parameters(parameters, gradients, learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, gradients, a[len(X)<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(data, ix_to_char, char_to_ix, num_iterations = <span class="number">35000</span>, n_a = <span class="number">50</span>, dino_names = <span class="number">7</span>, vocab_size = <span class="number">27</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Trains the model and generates dinosaur names. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    data -- text corpus</span></span><br><span class="line"><span class="string">    ix_to_char -- dictionary that maps the index to a character</span></span><br><span class="line"><span class="string">    char_to_ix -- dictionary that maps a character to an index</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to train the model for</span></span><br><span class="line"><span class="string">    n_a -- number of units of the RNN cell</span></span><br><span class="line"><span class="string">    dino_names -- number of dinosaur names you want to sample at each iteration. </span></span><br><span class="line"><span class="string">    vocab_size -- number of unique characters found in the text, size of the vocabulary</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- learned parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve n_x and n_y from vocab_size</span></span><br><span class="line">    n_x, n_y = vocab_size, vocab_size</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(n_a, n_x, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize loss (this is required because we want to smooth our loss, don't worry about it)</span></span><br><span class="line">    loss = get_initial_loss(vocab_size, dino_names)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build list of all dinosaur names (training examples).</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"dinos.txt"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        examples = f.readlines()</span><br><span class="line">    examples = [x.lower().strip() <span class="keyword">for</span> x <span class="keyword">in</span> examples]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle list of all dinosaur names</span></span><br><span class="line">    np.random.shuffle(examples)	<span class="comment"># 将样本洗牌</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the hidden state of your LSTM</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 假设 n 个恐龙名字样本</span></span><br><span class="line">        <span class="comment"># 样本1（循环1）-&gt; 样本2（循环2）-&gt;……-&gt; 样本n（循环n）-&gt;样本1（循环n+1）-&gt; ……-&gt;样本n（循环2n）-&gt;……</span></span><br><span class="line">        <span class="comment"># Use the hint above to define one training example (X,Y) (≈ 2 lines)</span></span><br><span class="line">        index = j % len(examples)	<span class="comment"># 目前的循环次数取余样本数</span></span><br><span class="line">        X = [<span class="keyword">None</span>] + [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]] <span class="comment"># [None] 指第一个X&lt;0&gt;,它生成hatY&lt;1&gt;，即X&lt;n&gt;生成hatY&lt;n+1&gt;</span></span><br><span class="line">        Y = X[<span class="number">1</span>:] + [char_to_ix[<span class="string">"\n"</span>]] <span class="comment"># Y 的真实值，与 X 相同，只是没有第一个Y&lt;0&gt;，并且最后加上结束符，比X向右移动了一位</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters</span></span><br><span class="line">        <span class="comment"># Choose a learning rate of 0.01</span></span><br><span class="line">        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Use a latency trick to keep the loss smooth. It happens here to accelerate the training.</span></span><br><span class="line">        loss = smooth(loss, curr_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Every 2000 Iteration, generate "n" characters thanks to sample() to check if the model is learning properly</span></span><br><span class="line">        <span class="keyword">if</span> j % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">            print(<span class="string">'Iteration: %d, Loss: %f'</span> % (j, loss) + <span class="string">'\n'</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># The number of dinosaur names to print</span></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> range(dino_names):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Sample indices and print them</span></span><br><span class="line">                sampled_indices = sample(parameters, char_to_ix, seed)</span><br><span class="line">                print_sample(sampled_indices, ix_to_char)</span><br><span class="line">      </span><br><span class="line">            print(<span class="string">'\n'</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(data, ix_to_char, char_to_ix)</span><br></pre></td></tr></table></figure>
<p>……</p>
<p>Iteration: 34000, Loss: 22.396744</p>
<p>Mavptokekus<br>Ilabaisaurus<br>Itosaurus<br>Macaesaurus<br>Yrosaurus<br>Eiaeosaurus<br>Trodon</p>
<h1 id="Part-3-Improvise-a-Jazz-Solo-with-an-LSTM-Network"><a href="#Part-3-Improvise-a-Jazz-Solo-with-an-LSTM-Network" class="headerlink" title="Part 3-Improvise a Jazz Solo with an LSTM Network"></a>Part 3-Improvise a Jazz Solo with an LSTM Network</h1><p>这次作业是在 Keras 使用 LSTM 进行爵士音乐的生成。</p>
<p>参考 Ji-Sung Kim, 2016, <a href="https://github.com/jisungk/deepjazz" target="_blank" rel="noopener">deepjazz</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> IPython</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> music21 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> grammar <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> qa <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> preprocess <span class="keyword">import</span> * </span><br><span class="line"><span class="keyword">from</span> music_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> data_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model, Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br></pre></td></tr></table></figure>
<h2 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h2><h3 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h3><p>先不管具体的音乐理论和细节，直接加载需要的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X, Y, n_values, indices_values = load_music_utils()</span><br><span class="line">print(<span class="string">'shape of X:'</span>, X.shape)</span><br><span class="line">print(<span class="string">'number of training examples:'</span>, X.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Tx (length of sequence):'</span>, X.shape[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">'total # of unique values:'</span>, n_values)</span><br><span class="line">print(<span class="string">'Shape of Y:'</span>, Y.shape)</span><br></pre></td></tr></table></figure>
<p>shape of X: (60, 30, 78)<br>number of training examples: 60<br>Tx (length of sequence): 30<br>total # of unique values: 78<br>Shape of Y: (30, 60, 78)</p>
<ul>
<li>X：(m, T_x, 78) 其中 m 是样本数，T_x 是音乐片段的时间，78 是 one-hot 向量的长度</li>
<li>Y：(T_y, m, 78) 为了便于 feed 给 LSTM 层，将其维度变一下</li>
<li>n_value：把某个时间的音乐变成 one-hot 向量的长度</li>
<li>indices_values：78 个索引值的字典</li>
</ul>
<h3 id="模型概况-1"><a href="#模型概况-1" class="headerlink" title="模型概况"></a>模型概况</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_6.png" alt=""></p>
<p>每个训练用的序列样本长度可以是不同的，但是这里为了方便向量化，将所有用以训练的样本统一长度为 30 s.</p>
<h2 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h2><p>首先确定模型的超参数，我们必须先确定隐藏层的单元数。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_5.1_5.png" alt=""></p>
<p>看上面这幅图，所谓隐藏层单元数，就是 $W[a^{<t-1>},x^{<t>}]$ 的输出的维度，用 n_a 表示，这里设为 64.</t></t-1></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n_a = <span class="number">64</span></span><br></pre></td></tr></table></figure>
<p>如果我们建立的序列模型是在测试时也给出所有序列，例如输入一个序列，预测一个标签，那么可以简单地使用 Keras 的内置函数进行模型构建，但是对于生成模型来说，我们每次只能知道序列前一个的值，而不知道整个序列，无法直接全部输入模型进行预测，只能一个时间点生成一次，然后让  $x^{\langle t\rangle} = y^{\langle t-1 \rangle}$ 继续生成下一个，而内置的 LSTM 模型的逻辑是，一次将所有时间的序列值丢进去然后得到一个输出。</p>
<p>所以在下面的函数中使用 for 循环调用 LSTM 层 T_x 次，由于每个时间点的 LSTM 层的参数都是一样的，所以我们定义一个<strong>全局的</strong> 层 对象，每个 for 循环都调用这个对象，实现参数共享。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reshapor = Reshape((<span class="number">1</span>, <span class="number">78</span>))                        <span class="comment"># Used in Step 2.B of djmodel(), below</span></span><br><span class="line">LSTM_cell = LSTM(n_a, return_state = <span class="keyword">True</span>)<span class="comment"># return_state 返回 c         # Used in Step 2.C</span></span><br><span class="line">densor = Dense(n_values, activation=<span class="string">'softmax'</span>)     <span class="comment"># Used in Step 2.D</span></span><br></pre></td></tr></table></figure>
<p>建立模型的步骤是：</p>
<ul>
<li><p>建立空列表 outputs 储存每个时间点的输出 $\hat y$ </p>
</li>
<li><p>for t in 1,…, T_x：</p>
<ul>
<li>从输入选择第 t 个时间点的序列值<ul>
<li>使用 <code>x = Lambda(lambda x: X[:,t,:])(X)</code>，输出维度是 (?, 78)</li>
</ul>
</li>
<li>调用 reshapor 把 (?,78)-&gt;(?,1,78)，便于输入 LSTM 层</li>
<li>调用 LSTM_cell 得到输出的 a 和 c<ul>
<li><code>a, _, c = LSTM_cell(input_x, initial_state=[previous hidden state, previous cell state])</code></li>
</ul>
</li>
<li>将隐藏层的激活值 a 通过 softmax 进行输出</li>
<li>把输出存进 outputs</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">djmodel</span><span class="params">(Tx, n_a, n_values)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the model</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Tx -- length of the sequence in a corpus</span></span><br><span class="line"><span class="string">    n_a -- the number of activations used in our model</span></span><br><span class="line"><span class="string">    n_values -- number of unique values in the music data </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a keras model with the </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input of your model with a shape </span></span><br><span class="line">    X = Input(shape=(Tx, n_values)) <span class="comment"># Keras 中 shape 不需要显式地指出样本数 m，第一个位置默认是样本数，打印 X 的形状为 (?, 30, 78)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define s0, initial hidden state for the decoder LSTM</span></span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">'a0'</span>)<span class="comment">#shape 其实是(?,n_a,)</span></span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">'c0'</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Create empty list to append the outputs while you iterate (≈1 line)</span></span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Tx):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.A: select the "t"th time step vector from X. </span></span><br><span class="line">        x = Lambda(<span class="keyword">lambda</span> x: X[:,t,:])(X)<span class="comment"># Lambda 层是对 X 整个进行运算，得到一个结果，所以直接返回 X[:,t,:]</span></span><br><span class="line">        <span class="comment"># Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)</span></span><br><span class="line">        x = reshapor(x) <span class="comment"># (?,78)-&gt;(?,1,78)</span></span><br><span class="line">        <span class="comment"># Step 2.C: Perform one step of the LSTM_cell</span></span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        <span class="comment"># Step 2.D: Apply densor to the hidden state output of LSTM_Cell</span></span><br><span class="line">        out = densor(a)</span><br><span class="line">        <span class="comment"># Step 2.E: add the output to "outputs"</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 3: Create model instance</span></span><br><span class="line">    model = Model(inputs=[X,a0,c0], outputs=outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>建立模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = djmodel(Tx = <span class="number">30</span> , n_a = <span class="number">64</span>, n_values = <span class="number">78</span>)</span><br></pre></td></tr></table></figure>
<p>编译模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">opt = Adam(lr=<span class="number">0.01</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, decay=<span class="number">0.01</span>)<span class="comment">#定义优化器</span></span><br><span class="line"></span><br><span class="line">model.compile(optimizer=opt, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<p>训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">60</span></span><br><span class="line">a0 = np.zeros((m, n_a))</span><br><span class="line">c0 = np.zeros((m, n_a))</span><br><span class="line"></span><br><span class="line">model.fit([X, a0, c0], list(Y), epochs=<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">输出：</span><br><span class="line">......</span><br><span class="line">Epoch 100/100</span><br><span class="line">60/60 [==============================] - 0s - loss: 5.9232 - dense_1_loss_1: 3.7652 - dense_1_loss_2: 1.1579 - dense_1_loss_3: 0.3157 - dense_1_loss_4: 0.0825 - dense_1_loss_5: 0.0493 - dense_1_loss_6: 0.0368 - dense_1_loss_7: 0.0311 - dense_1_loss_8: 0.0285 - dense_1_loss_9: 0.0278 - dense_1_loss_10: 0.0222 - dense_1_loss_11: 0.0213 - dense_1_loss_12: 0.0205 - dense_1_loss_13: 0.0194 - dense_1_loss_14: 0.0199 - dense_1_loss_15: 0.0210 - dense_1_loss_16: 0.0206 - dense_1_loss_17: 0.0192 - dense_1_loss_18: 0.0213 - dense_1_loss_19: 0.0212 - dense_1_loss_20: 0.0219 - dense_1_loss_21: 0.0220 - dense_1_loss_22: 0.0202 - dense_1_loss_23: 0.0217 - dense_1_loss_24: 0.0211 - dense_1_loss_25: 0.0227 - dense_1_loss_26: 0.0202 - dense_1_loss_27: 0.0225 - dense_1_loss_28: 0.0245 - dense_1_loss_29: 0.0252 - dense_1_loss_30: 0.0000e+00 - dense_1_acc_1: 0.0500 - dense_1_acc_2: 0.6833 - dense_1_acc_3: 0.9167 - dense_1_acc_4: 1.0000 - dense_1_acc_5: 1.0000 - dense_1_acc_6: 1.0000 - dense_1_acc_7: 1.0000 - dense_1_acc_8: 1.0000 - dense_1_acc_9: 1.0000 - dense_1_acc_10: 1.0000 - dense_1_acc_11: 1.0000 - dense_1_acc_12: 1.0000 - dense_1_acc_13: 1.0000 - dense_1_acc_14: 1.0000 - dense_1_acc_15: 1.0000 - dense_1_acc_16: 1.0000 - dense_1_acc_17: 1.0000 - dense_1_acc_18: 1.0000 - dense_1_acc_19: 1.0000 - dense_1_acc_20: 1.0000 - dense_1_acc_21: 1.0000 - dense_1_acc_22: 1.0000 - dense_1_acc_23: 1.0000 - dense_1_acc_24: 1.0000 - dense_1_acc_25: 1.0000 - dense_1_acc_26: 1.0000 - dense_1_acc_27: 1.0000 - dense_1_acc_28: 1.0000 - dense_1_acc_29: 1.0000 - dense_1_acc_30: 0.0000e+00</span><br></pre></td></tr></table></figure>
<h2 id="生成音乐"><a href="#生成音乐" class="headerlink" title="生成音乐"></a>生成音乐</h2><p>到目前为止，我们模型的几个层 reshapor、LSTM_cell、densor 中的参数已经学习完毕，存在该对象中，现在我们使用这些参数训练好的层对象来构建一个新的生成模型。</p>
<p>与生成恐龙名字相同，我们仍需要 sampling 操作，因此增加一个将 softmax 的输出变成 one-hot 的层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">music_inference_model</span><span class="params">(LSTM_cell, densor, n_values = <span class="number">78</span>, n_a = <span class="number">64</span>, Ty = <span class="number">100</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uses the trained "LSTM_cell" and "densor" from model() to generate a sequence of values.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    LSTM_cell -- the trained "LSTM_cell" from model(), Keras layer object</span></span><br><span class="line"><span class="string">    densor -- the trained "densor" from model(), Keras layer object</span></span><br><span class="line"><span class="string">    n_values -- integer, umber of unique values</span></span><br><span class="line"><span class="string">    n_a -- number of units in the LSTM_cell</span></span><br><span class="line"><span class="string">    Ty -- integer, number of time steps to generate</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    inference_model -- Keras model instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input of your model with a shape </span></span><br><span class="line">    x0 = Input(shape=(<span class="number">1</span>, n_values))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define s0, initial hidden state for the decoder LSTM</span></span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">'a0'</span>)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">'c0'</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line">    x = x0</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Create an empty list of "outputs" to later store your predicted values (≈1 line)</span></span><br><span class="line">    outputs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Loop over Ty and generate a value at every time step</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(Ty):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.A: Perform one step of LSTM_cell (≈1 line)</span></span><br><span class="line">        a, _, c = LSTM_cell(x,initial_state=[a,c])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)</span></span><br><span class="line">        out = densor(a)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 2.C: Append the prediction "out" to "outputs". out.shape = (None, 78) (≈1 line)</span></span><br><span class="line">        outputs.append(out)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2.D: Select the next value according to "out", and set "x" to be the one-hot representation of the</span></span><br><span class="line">        <span class="comment">#           selected value, which will be passed as the input to LSTM_cell on the next step. We have provided </span></span><br><span class="line">        <span class="comment">#           the line of code you need to do this. </span></span><br><span class="line">        x = Lambda(one_hot)(out)	<span class="comment"># 下一次的输入等于上一次的输出变成 one-hot 向量</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 3: Create model instance with the correct "inputs" and "outputs" (≈1 line)</span></span><br><span class="line">    inference_model = Model(inputs=[x0,a0,c0], outputs = outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inference_model</span><br></pre></td></tr></table></figure>
<p>调用模型进行预测的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_and_sample</span><span class="params">(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, </span></span></span><br><span class="line"><span class="function"><span class="params">                       c_initializer = c_initializer)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Predicts the next value of values using the inference model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    inference_model -- Keras model instance for inference time</span></span><br><span class="line"><span class="string">    x_initializer -- numpy array of shape (1, 1, 78), one-hot vector initializing the values generation</span></span><br><span class="line"><span class="string">    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell</span></span><br><span class="line"><span class="string">    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    results -- numpy-array of shape (Ty, 78), matrix of one-hot vectors representing the values generated</span></span><br><span class="line"><span class="string">    indices -- numpy-array of shape (Ty, 1), matrix of indices representing the values generated</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.</span></span><br><span class="line">    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])</span><br><span class="line">    <span class="comment"># Step 2: Convert "pred" into an np.array() of indices with the maximum probabilities</span></span><br><span class="line">    indices = np.argmax(pred,axis=<span class="number">-1</span>)	<span class="comment"># 将 softmax 输出中概率值最大的定为输出索引值</span></span><br><span class="line">    <span class="comment"># Step 3: Convert indices to one-hot vectors, the shape of the results should be (1, )</span></span><br><span class="line">    results = to_categorical(indices, num_classes=n_values)	<span class="comment"># 将索引值再变成 one-hot 向量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results, indices</span><br></pre></td></tr></table></figure>
<p>看看结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)</span><br><span class="line">print(<span class="string">"np.argmax(results[12]) ="</span>, np.argmax(results[<span class="number">12</span>]))</span><br><span class="line">print(<span class="string">"np.argmax(results[17]) ="</span>, np.argmax(results[<span class="number">17</span>]))</span><br><span class="line">print(<span class="string">"list(indices[12:18]) ="</span>, list(indices[<span class="number">12</span>:<span class="number">18</span>]))</span><br></pre></td></tr></table></figure>
<p>np.argmax(results[12]) = 70<br>np.argmax(results[17]) = 55<br>list(indices[12:18]) = [array([70]), array([71]), array([43]), array([2]), array([10]), array([55])]</p>
<h3 id="生成音乐-1"><a href="#生成音乐-1" class="headerlink" title="生成音乐"></a>生成音乐</h3><p>得到的输出必须经过很好的后处理才能确保听起来好听，输出音乐的质量很大地取决于后处理的质量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out_stream = generate_music(inference_model)</span><br></pre></td></tr></table></figure>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/rnn/">rnn</a><a class="post-meta__tags" href="/tags/lstm/">lstm</a><a class="post-meta__tags" href="/tags/梯度剪枝/">梯度剪枝</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr1.png"><div class="post-qr-code__desc">微信捐赠</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr2.jpg"><div class="post-qr-code__desc">支付宝捐赠</div></div></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/13/用 numpy 进行快速傅立叶变换 fft/"><i class="fa fa-chevron-left">  </i><span>用 numpy 进行快速傅立叶变换 fft</span></a></div><div class="next-post pull-right"><a href="/2018/12/01/deeplearning.ai 第五课第一周编程作业第一部分 lstm 反向传播部分的公式更正及推导/"><span>deeplearning.ai 第五课第一周编程作业第一部分 lstm 反向传播部分的公式更正及推导</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=530 height=86 src="//music.163.com/outchain/player?type=2&id=566993785&auto=1&height=66"></iframe></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zODM1Mi8xNDg4MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>