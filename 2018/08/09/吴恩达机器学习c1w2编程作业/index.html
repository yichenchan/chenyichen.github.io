<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 2）"><meta name="keywords" content="吴恩达深度学习笔记,AI,神经网络,逻辑回归,numpy"><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 2） | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#numpy-使用基础"><span class="toc-number">1.</span> <span class="toc-text">numpy 使用基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sigmoid-函数，np-exp"><span class="toc-number">1.1.</span> <span class="toc-text">sigmoid 函数，np.exp( )</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#求-sigmoid-函数的导数-dZ"><span class="toc-number">1.2.</span> <span class="toc-text">求 sigmoid 函数的导数 (dZ)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reshape-命令重构矩阵"><span class="toc-number">1.3.</span> <span class="toc-text">reshape 命令重构矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#输入矩阵的行向量标准化"><span class="toc-number">1.4.</span> <span class="toc-text">输入矩阵的行向量标准化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#广播（broadcasting）和-softmax-函数"><span class="toc-number">1.5.</span> <span class="toc-text">广播（broadcasting）和 softmax 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#两种损失函数-L1-和-L2"><span class="toc-number">1.6.</span> <span class="toc-text">两种损失函数 L1 和 L2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#用逻辑回归识别猫的图片"><span class="toc-number">2.</span> <span class="toc-text">用逻辑回归识别猫的图片</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#相关包"><span class="toc-number">2.1.</span> <span class="toc-text">相关包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集预处理"><span class="toc-number">2.2.</span> <span class="toc-text">数据集预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#加载原始数据集"><span class="toc-number">2.2.1.</span> <span class="toc-text">加载原始数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#确定图片维度和个数以防止出错"><span class="toc-number">2.2.2.</span> <span class="toc-text">确定图片维度和个数以防止出错</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#重构图片数组变为标准输入矩阵"><span class="toc-number">2.2.3.</span> <span class="toc-text">重构图片数组变为标准输入矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据标准化"><span class="toc-number">2.2.4.</span> <span class="toc-text">数据标准化</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法的一般体系结构"><span class="toc-number">2.3.</span> <span class="toc-text">算法的一般体系结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#构建算法的各个部分"><span class="toc-number">2.4.</span> <span class="toc-text">构建算法的各个部分</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#辅助函数"><span class="toc-number">2.4.1.</span> <span class="toc-text">辅助函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#初始化参数"><span class="toc-number">2.4.2.</span> <span class="toc-text">初始化参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#前向传播和反向传播"><span class="toc-number">2.4.3.</span> <span class="toc-text">前向传播和反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#梯度下降优化参数"><span class="toc-number">2.4.4.</span> <span class="toc-text">梯度下降优化参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#用得到的参数预测数据集的标签"><span class="toc-number">2.4.5.</span> <span class="toc-text">用得到的参数预测数据集的标签</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#把所有的函数聚合到主函数"><span class="toc-number">2.5.</span> <span class="toc-text">把所有的函数聚合到主函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#进一步分析"><span class="toc-number">2.6.</span> <span class="toc-text">进一步分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#学习率的选择"><span class="toc-number">2.6.1.</span> <span class="toc-text">学习率的选择</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#作业结论"><span class="toc-number">2.7.</span> <span class="toc-text">作业结论</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">33</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">29</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_topimg.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="post-info"><div id="post-title">coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 2）</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-09</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习笔记/">学习笔记</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="numpy-使用基础"><a href="#numpy-使用基础" class="headerlink" title="numpy 使用基础"></a>numpy 使用基础</h2><p>Numpy 是 Python 里用于科学计算的模块，由一个<a href="www.numpy.org">开源社区</a>进行维护，下面介绍一些用于神经网络搭建的函数的构建</p>
<h3 id="sigmoid-函数，np-exp"><a href="#sigmoid-函数，np-exp" class="headerlink" title="sigmoid 函数，np.exp( )"></a>sigmoid 函数，np.exp( )</h3><p>回忆：$sigmoid(x)=\frac {1}{1+e^{-x}}$</p>
<p>如果 $ x = (x_1, x_2, …, x_n)$ 是一个行向量，那么 $np.exp(x)$ 会将 $exp( )$ 函数用于 x 的每个元素，输出 $np.exp(x) = (e^{x_1}, e^{x_2}, …, e^{x_n})$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># np.exp的例子</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])<span class="comment">#x 为一个行向量</span></span><br><span class="line">print(np.exp(x)) <span class="comment"># 结果为 (exp(1), exp(2), exp(3))</span></span><br></pre></td></tr></table></figure>
<p>>> [  2.71828183   7.3890561   20.08553692]</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更多向量操作例子</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (x + <span class="number">3</span>)</span><br><span class="line">print(<span class="number">1</span>/x)</span><br></pre></td></tr></table></figure>
<p>>&gt;[4 5 6]</p>
<p>>&gt;[ 1.          0.5         0.33333333]</p>
<script type="math/tex; mode=display">
\text{For } x \in \mathbb {R}^n \text{,     } sigmoid(x) = sigmoid\begin{pmatrix}x_1  \\x_2  \\    ...  \\    x_n  \\\end{pmatrix} = \begin{pmatrix}\frac{1}{1+e^{-x_1}}  \\\frac{1}{1+e^{-x_2}}  \\    ...  \\    \frac{1}{1+e^{-x_n}}  \\\end{pmatrix}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#用 np.exp() 代替 numpy.exp()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">sigmoid(x)</span><br></pre></td></tr></table></figure>
<p>>&gt;array([ 0.73105858,  0.88079708,  0.95257413])</p>
<h3 id="求-sigmoid-函数的导数-dZ"><a href="#求-sigmoid-函数的导数-dZ" class="headerlink" title="求 sigmoid 函数的导数 (dZ)"></a>求 sigmoid 函数的导数 (dZ)</h3><blockquote>
<p>注意：在前面的笔记中，吴恩达的课程视频中写的 dZ 的求法是：$dZ=A-Y$，但是在课程 ppt 和编程作业中的求法是：$dZ=\sigma(x)(1-\sigma(x))$</p>
</blockquote>
<script type="math/tex; mode=display">
sigmoid\_derivative(x) = {\sigma' (x)} = \sigma(x)(1-\sigma(x))=A(1-A)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span></span><br><span class="line"><span class="string">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    s = sigmoid(x)</span><br><span class="line">    ds = s * (<span class="number">1</span> - s)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid_derivative(x) = "</span> + str(sigmoid_derivative(x)))</span><br></pre></td></tr></table></figure>
<p>>&gt;sigmoid_derivative(x) = [ 0.19661193  0.10499359  0.04517666]</p>
<h3 id="reshape-命令重构矩阵"><a href="#reshape-命令重构矩阵" class="headerlink" title="reshape 命令重构矩阵"></a>reshape 命令重构矩阵</h3><ul>
<li>X.shape 命令用于得到一个矩阵或向量的维度（形状）</li>
<li>X.reshape 命令用于把 X 重构为某个维度，例如把 shape (length,height,depth=3) 的图片变成输入，重构为 shape (length∗height∗3,1) 的向量</li>
</ul>
<p>下面的代码把一张图片变为一个向量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    v = image.reshape((image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>],<span class="number">1</span>)) </span><br><span class="line">    <span class="comment">#image.shape[0] 获得图片或数组第一个维度的长度</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values</span></span><br><span class="line">image = np.array([[[ <span class="number">0.67826139</span>,  <span class="number">0.29380381</span>],</span><br><span class="line">        [ <span class="number">0.90714982</span>,  <span class="number">0.52835647</span>],</span><br><span class="line">        [ <span class="number">0.4215251</span> ,  <span class="number">0.45017551</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">0.92814219</span>,  <span class="number">0.96677647</span>],</span><br><span class="line">        [ <span class="number">0.85304703</span>,  <span class="number">0.52351845</span>],</span><br><span class="line">        [ <span class="number">0.19981397</span>,  <span class="number">0.27417313</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">0.60659855</span>,  <span class="number">0.00533165</span>],</span><br><span class="line">        [ <span class="number">0.10820313</span>,  <span class="number">0.49978937</span>],</span><br><span class="line">        [ <span class="number">0.34144279</span>,  <span class="number">0.94630077</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"image2vector(image) = "</span> + str(image2vector(image)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">image2vector(image) = [[ 0.67826139]</span><br><span class="line"> [ 0.29380381]</span><br><span class="line"> [ 0.90714982]</span><br><span class="line"> [ 0.52835647]</span><br><span class="line"> [ 0.4215251 ]</span><br><span class="line"> [ 0.45017551]</span><br><span class="line"> [ 0.92814219]</span><br><span class="line"> [ 0.96677647]</span><br><span class="line"> [ 0.85304703]</span><br><span class="line"> [ 0.52351845]</span><br><span class="line"> [ 0.19981397]</span><br><span class="line"> [ 0.27417313]</span><br><span class="line"> [ 0.60659855]</span><br><span class="line"> [ 0.00533165]</span><br><span class="line"> [ 0.10820313]</span><br><span class="line"> [ 0.49978937]</span><br><span class="line"> [ 0.34144279]</span><br><span class="line"> [ 0.94630077]]</span><br></pre></td></tr></table></figure>
<h3 id="输入矩阵的行向量标准化"><a href="#输入矩阵的行向量标准化" class="headerlink" title="输入矩阵的行向量标准化"></a>输入矩阵的行向量标准化</h3><p>将数据标准化会使得模型表现得更好，因为标准化之后梯度下降收敛速度更快，我们可以通过将输入矩阵 x 每一个<strong>行向量</strong>除以该行向量的模，即 $ \frac{x}{| x|} $</p>
<p>如果 <script type="math/tex">x = \begin{bmatrix}    0 & 3 & 4 \\    2 & 6 & 4 \\\end{bmatrix}</script> 那么 $| x| = np.linalg.norm(x, axis = 1, keepdims = True) =\begin{bmatrix}    5 \\    \sqrt{56} \\\end{bmatrix} $ 而且 $ x_normalized = \frac{x}{| x|} = \begin{bmatrix}    0 &amp; \frac{3}{5} &amp; \frac{4}{5} \\    \frac{2}{\sqrt{56}} &amp; \frac{6}{\sqrt{56}} &amp; \frac{4}{\sqrt{56}} \\\end{bmatrix}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    x_norm = np.linalg.norm(x, ord = <span class="number">2</span>,axis = <span class="number">1</span>,keepdims = <span class="keyword">True</span>)</span><br><span class="line">    <span class="comment">#计算 x 的范数或者说行向量的模，其中 ord = 2 表示范数类型为 2，即平方和的开方，axis =1 表示按行向量处理，keepdims = True 表示保持矩阵的二维特性</span></span><br><span class="line">    x = x / x_norm <span class="comment">#用 x 矩阵除以行向量的模，自动进行广播拓展</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">1</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line">print(<span class="string">"normalizeRows(x) = "</span> + str(normalizeRows(x)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; normalizeRows(x) = [[ 0.          0.6         0.8       ]</span><br><span class="line"> [ 0.13736056  0.82416338  0.54944226]]</span><br></pre></td></tr></table></figure>
<h3 id="广播（broadcasting）和-softmax-函数"><a href="#广播（broadcasting）和-softmax-函数" class="headerlink" title="广播（broadcasting）和 softmax 函数"></a>广播（broadcasting）和 softmax 函数</h3><p>你可以把 softmax 函数看作一个用来标准化的函数，当算法需要进行二元或者更多元分类时</p>
<ul>
<li><p>$ \text{for  } \  x \in \mathbb{R}^{1\times n} \text{,     } softmax(x) = softmax(\begin{bmatrix}  x_1  &amp;&amp;    x_2 &amp;&amp;    …  &amp;&amp;    x_n  \end{bmatrix}) = \begin{bmatrix}     \frac{e^{x_1}}{\sum_{j}e^{x_j}}  &amp;&amp;    \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &amp;&amp;    …  &amp;&amp;    \frac{e^{x_n}}{\sum_{j}e^{x_j}} \end{bmatrix} $</p>
</li>
<li><p>$\text{for a matrix } x \in \mathbb{R}^{m \times n} \text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  </p>
<script type="math/tex; mode=display">
softmax(x) = softmax\begin{bmatrix}
    x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
    x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    x_{m1} & x_{m2} & x_{m3} & \dots  & x_{mn}
\end{bmatrix} \\= \begin{bmatrix}
    \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} & \dots  & \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\
    \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} & \dots  & \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} & \dots  & \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}
\end{bmatrix} = \begin{pmatrix}
    softmax\text{(first row of x)}  \\
    softmax\text{(second row of x)} \\
    ...  \\
    softmax\text{(last row of x)} \\
\end{pmatrix}</script></li>
</ul>
<p>实现代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Calculates the softmax for each row of the input x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Your code should work for a row vector and also for matrices of shape (n, m).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n,m)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    s -- A numpy matrix equal to the softmax of x, of shape (n,m)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对 x 每个元素求 exp()</span></span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 x_exp 每行的和，使用 np.sum(..., axis = 1, keepdims = True)</span></span><br><span class="line">    x_sum = np.sum(x_exp, axis = <span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用广播特性自动计算 softmax 函数</span></span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span> ,<span class="number">0</span>]])</span><br><span class="line">print(<span class="string">"softmax(x) = "</span> + str(softmax(x)))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; softmax(x) = [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04</span><br><span class="line">    1.21052389e-04]</span><br><span class="line"> [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04</span><br><span class="line">    8.01252314e-04]]</span><br></pre></td></tr></table></figure>
<h3 id="两种损失函数-L1-和-L2"><a href="#两种损失函数-L1-和-L2" class="headerlink" title="两种损失函数 L1 和 L2"></a>两种损失函数 L1 和 L2</h3><ol>
<li><p>L1 损失函数定义为：</p>
<script type="math/tex; mode=display">
\begin{align*} & L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}| \end{align*}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L1 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = np.sum(np.abs(y - yhat))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L1 = "</span> + str(L1(yhat,y)))</span><br></pre></td></tr></table></figure>
<p>>&gt; L1 = 1.1</p>
</li>
<li><p>L2 损失函数定义为：</p>
<script type="math/tex; mode=display">
\begin{align*} & L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2 \end{align*}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L2 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    loss = np.dot(y-yhat, y-yhat) <span class="comment"># 直接写成自己和自己做点积</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L2 = "</span> + str(L2(yhat,y)))</span><br></pre></td></tr></table></figure>
<p>>&gt; L2 = 0.43</p>
</li>
</ol>
<h2 id="用逻辑回归识别猫的图片"><a href="#用逻辑回归识别猫的图片" class="headerlink" title="用逻辑回归识别猫的图片"></a>用逻辑回归识别猫的图片</h2><p>实现步骤：</p>
<ul>
<li>建立一个学习算法的一般结构，包括：<ul>
<li>初始化参数</li>
<li>计算代价函数和它对参数的导数</li>
<li>使用梯度下降法迭代参数</li>
</ul>
</li>
<li>把这三个函数用正确的顺序聚合到一个模型主函数中</li>
</ul>
<h3 id="相关包"><a href="#相关包" class="headerlink" title="相关包"></a>相关包</h3><ul>
<li>numpy：python 科学计算基础包</li>
<li>h5py：与存储在 H5 文件中的数据集交互的常用的包 </li>
<li>matplotlib：python 中用来画图像的一个包</li>
<li>PIL 和 scipy：最后用来测试自己的模型使用</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 方便调用</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><p>建立一个简单的判断图片是不是猫的识别算法，数据集包括：</p>
<ul>
<li>m_train 个训练集，包括图片集 train_set_x_orig 和标签集 train_set_y</li>
<li>m_test 个测试集，包括图片集 test_set_x_orig 和标签集 test_set_y</li>
<li>每张图片都是<strong>方形</strong> (height = num_px, width = num_px)，有三个颜色通道，所以数组形状是 (num_px, num_px, 3)</li>
<li>每个图片集都要进行预处理，所以原始数据加上 <strong>_orig</strong>，但是标签集不需要预处理</li>
</ul>
<h4 id="加载原始数据集"><a href="#加载原始数据集" class="headerlink" title="加载原始数据集"></a>加载原始数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(train_set_x_orig[index]) <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</span><br><span class="line"><span class="comment"># np.squeeze() 用于把一个数组的 shape 中为 1 的维度删掉，即让 train_set_y[:, index] 变为一个数</span></span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_1.png" alt=""></p>
<h4 id="确定图片维度和个数以防止出错"><a href="#确定图片维度和个数以防止出错" class="headerlink" title="确定图片维度和个数以防止出错"></a>确定图片维度和个数以防止出错</h4><ul>
<li>训练集的个数：m_train</li>
<li>测试集的个数：m_test</li>
<li>图片（正方形）的尺寸即边长的像素数：num_px</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定维度和个数</span></span><br><span class="line"><span class="comment"># train_set_x_orig 形状为 (m_train, num_px, num_px, 3)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;Number of training examples: m_train = 209</span><br><span class="line">  Number of testing examples: m_test = 50</span><br><span class="line">  Height/Width of each image: num_px = 64</span><br><span class="line">  Each image is of size: (64, 64, 3)</span><br><span class="line">  train_set_x shape: (209, 64, 64, 3)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x shape: (50, 64, 64, 3)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br></pre></td></tr></table></figure>
<h4 id="重构图片数组变为标准输入矩阵"><a href="#重构图片数组变为标准输入矩阵" class="headerlink" title="重构图片数组变为标准输入矩阵"></a>重构图片数组变为标准输入矩阵</h4><p>把尺寸为 (num_px, num_px, 3) 的图片变为 shape 为 (num_px ∗ num_px ∗ 3, 1) 的向量 </p>
<p>把一个 shape 为 (a,b,c,d) 的矩阵变为一个 shape 为 (b∗c∗d, a) 的矩阵的技巧：<code>x_flatten = X.reshape(X.shape[0], -1).T</code></p>
<blockquote>
<p>实际上，reshape() 是按<strong>行</strong>取元素，按<strong>行</strong>放元素</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重构图片数组</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))<span class="comment">#重构后完整性检查</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;train_set_x_flatten shape: (12288, 209)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x_flatten shape: (12288, 50)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br><span class="line">  sanity check after reshaping: [17 31 56 22 33]</span><br></pre></td></tr></table></figure>
<h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><p>为了使得数据在一个合适的尺度上，我们需要将数据标准化，数据标准化的办法见上一篇博文，但是对于图片来说，由于图片的每个像素的 RGB 值介于 0 到 255 之间，所以我们可以将每个特征值除以 255，这样就能将它们标准化了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></span><br></pre></td></tr></table></figure>
<h3 id="算法的一般体系结构"><a href="#算法的一般体系结构" class="headerlink" title="算法的一般体系结构"></a>算法的一般体系结构</h3><ul>
<li>初始化模型参数</li>
<li>通过最优化代价函数学习参数<ul>
<li>计算目前的损失函数（前向传播）</li>
<li>计算现在的梯度（反向传播）</li>
<li>更新参数（梯度下降）</li>
</ul>
</li>
<li>使用学习后的参数对测试集进行预测</li>
<li>分析结果得出结论</li>
</ul>
<h3 id="构建算法的各个部分"><a href="#构建算法的各个部分" class="headerlink" title="构建算法的各个部分"></a>构建算法的各个部分</h3><h4 id="辅助函数"><a href="#辅助函数" class="headerlink" title="辅助函数"></a>辅助函数</h4><p>实现 sigmoid 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid([0, 2]) = "</span> + str(sigmoid(np.array([<span class="number">0</span>,<span class="number">2</span>]))))</span><br></pre></td></tr></table></figure>
<p>>&gt; sigmoid([0, 2]) = [ 0.5         0.88079708]</p>
<h4 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h4><p>使用一系列的 0 初始化我们的参数 w 和 b</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    w = np.zeros((dim,<span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>)) <span class="comment"># 确保 w 的维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int)) <span class="comment">#确保 b 是浮点数或者整数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dim = <span class="number">2</span></span><br><span class="line">w, b = initialize_with_zeros(dim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(w))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(b))</span><br></pre></td></tr></table></figure>
<p>>&gt; w = [[0] [0]]      b = 0</p>
<h4 id="前向传播和反向传播"><a href="#前向传播和反向传播" class="headerlink" title="前向传播和反向传播"></a>前向传播和反向传播</h4><p>步骤：</p>
<ul>
<li>输入 X</li>
<li>计算预测值 $A = \sigma(w^T X + b)$</li>
<li>计算代价函数 $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</li>
<li>计算  $ dw = \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T$</li>
<li>计算 $ db=\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算前向传播和反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播 (从 X 到 COST)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X ) + b) <span class="comment"># 计算预测值                    </span></span><br><span class="line">    cost = (<span class="number">-1</span>/m)*(np.sum(Y*np.log(A) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))) <span class="comment"># 计算代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播 (计算梯度)</span></span><br><span class="line">    dw = (<span class="number">1</span>/m)*np.dot(X,(A-Y).T)</span><br><span class="line">    db = (<span class="number">1</span>/m)*np.sum(A-Y)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,   <span class="comment">#返回梯度 dict</span></span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w, b, X, Y = np.array([[<span class="number">1.</span>],[<span class="number">2.</span>]]), <span class="number">2.</span>, np.array([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">-1.</span>],[<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">-3.2</span>]]), np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]])</span><br><span class="line">grads, cost = propagate(w, b, X, Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cost = "</span> + str(cost))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;dw = [[ 0.99845601]</span><br><span class="line">       [ 2.39507239]]</span><br><span class="line">  db = 0.00145557813678</span><br><span class="line">  cost = 5.80154531939</span><br></pre></td></tr></table></figure>
<h4 id="梯度下降优化参数"><a href="#梯度下降优化参数" class="headerlink" title="梯度下降优化参数"></a>梯度下降优化参数</h4><p>更新参数方法：$ \theta = \theta - \alpha \text{ } d\theta$ ，其中 $\alpha$ 为<strong>学习率</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- 循环的迭代次数</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 代价函数和梯度计算 </span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line">        </span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b - learning_rate*db</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每100次迭代记录一次代价函数到 costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每一百步打印一次代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params, grads, costs = optimize(w, b, X, Y, num_iterations= <span class="number">100</span>, learning_rate = <span class="number">0.009</span>, print_cost = <span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(params[<span class="string">"w"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(params[<span class="string">"b"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; w = [[ 0.19033591]</span><br><span class="line">       [ 0.12259159]]</span><br><span class="line">   b = 1.92535983008</span><br><span class="line">   dw = [[ 0.67752042]</span><br><span class="line">        [ 1.41625495]]</span><br><span class="line">   db = 0.219194504541</span><br></pre></td></tr></table></figure>
<h4 id="用得到的参数预测数据集的标签"><a href="#用得到的参数预测数据集的标签" class="headerlink" title="用得到的参数预测数据集的标签"></a>用得到的参数预测数据集的标签</h4><ul>
<li>先计算预测值 $\hat{Y} = A = \sigma(w^T X + b)$</li>
<li>若 $\hat Y &gt; 0.5$ ，则预测的标签为 1</li>
<li>若 $\hat Y &lt;= 0.5$ ，则预测的标签为 0</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m)) <span class="comment">#初始化</span></span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>) <span class="comment">#确保 w shape 正确</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算图片中是猫的概率</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X) + b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 把概率转化为标签值</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">0</span></span><br><span class="line">  </span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([[<span class="number">0.1124579</span>],[<span class="number">0.23106775</span>]])</span><br><span class="line">b = <span class="number">-0.3</span></span><br><span class="line">X = np.array([[<span class="number">1.</span>,<span class="number">-1.1</span>,<span class="number">-3.2</span>],[<span class="number">1.2</span>,<span class="number">2.</span>,<span class="number">0.1</span>]])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"predictions = "</span> + str(predict(w, b, X)))</span><br></pre></td></tr></table></figure>
<p>>&gt; predictions = [[ 1.  1.  0.]]</p>
<h3 id="把所有的函数聚合到主函数"><a href="#把所有的函数聚合到主函数" class="headerlink" title="把所有的函数聚合到主函数"></a>把所有的函数聚合到主函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用 0 初始化参数</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度下降</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations , learning_rate , print_cost = <span class="keyword">False</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从参数 dict 中获取参数</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测训练集/测试集的标签</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印预测误差</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练模型</span></span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train accuracy: 99.04306220095694 % # 训练集的预测精确度</span><br><span class="line">test accuracy: 70.0 % #测试集的预测精确度</span><br></pre></td></tr></table></figure>
<p>画出错误图形：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture that was wrongly classified.</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(test_set_y[<span class="number">0</span>,index]) + <span class="string">", you predicted that it is a \""</span> + classes[d[<span class="string">"Y_prediction_test"</span>][<span class="number">0</span>,index]].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure>
<p>画出代价函数的下降曲线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>]) <span class="comment"># np.squeeze() 确保它是一维数组</span></span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2.png" alt=""></p>
<h3 id="进一步分析"><a href="#进一步分析" class="headerlink" title="进一步分析"></a>进一步分析</h3><h4 id="学习率的选择"><a href="#学习率的选择" class="headerlink" title="学习率的选择"></a>学习率的选择</h4><p>为了梯度下降正常工作，必须选择合适的学习率。学习率 $\alpha$ 决定了更新参数的快慢，如果学习率太大，我们可能错过最优值，同样，如果太小，我们需要迭代很多次到达最优值，下面比较不同学习率的差别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (hundreds)'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="keyword">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning rate is: 0.01</span><br><span class="line">train accuracy: 99.52153110047847 %</span><br><span class="line">test accuracy: 68.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate is: 0.001</span><br><span class="line">train accuracy: 88.99521531100478 %</span><br><span class="line">test accuracy: 64.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br><span class="line"></span><br><span class="line">learning rate is: 0.0001</span><br><span class="line">train accuracy: 68.42105263157895 %</span><br><span class="line">test accuracy: 36.0 %</span><br><span class="line"></span><br><span class="line">-------------------------------------------------------</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.3.png" alt=""></p>
<p>结论：</p>
<ul>
<li>不同的学习率导致不同的代价函数和预测结果</li>
<li>如果学习率太大（0.01），代价函数可能会上下摆动甚至偏离（尽管在这个例子中 0.01 最后收敛得很好）</li>
<li>更小的学习率不意味着更好的模型，因为有可能出现 <strong>过拟合</strong>，一般出现在训练数据精确度比测试数据高得多时</li>
<li>深度学习中，一般推荐：<ul>
<li>选择更好降低代价函数的学习率</li>
<li>如果发生过拟合，用其他的方式减小过拟合</li>
</ul>
</li>
</ul>
<h3 id="作业结论"><a href="#作业结论" class="headerlink" title="作业结论"></a>作业结论</h3><ul>
<li>预处理数据很重要</li>
<li>先分开构建函数： initialize(), propagate(), optimize()，最后再搭建模型 model()</li>
<li>调整学习率（超参数 的一个例子）对算法影响很大</li>
</ul>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/神经网络/">神经网络</a><a class="post-meta__tags" href="/tags/逻辑回归/">逻辑回归</a><a class="post-meta__tags" href="/tags/numpy/">numpy</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr1.png"><div class="post-qr-code__desc">微信捐赠</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr2.jpg"><div class="post-qr-code__desc">支付宝捐赠</div></div></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/08/11/吴恩达机器学习笔记c1w3/"><i class="fa fa-chevron-left">  </i><span>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 3）—— 浅层神经网络</span></a></div><div class="next-post pull-right"><a href="/2018/08/07/吴恩达机器学习笔记c1w2/"><span>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 2）—— 神经网络基础</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=530 height=86 src="//music.163.com/outchain/player?type=2&id=566993785&auto=1&height=66"></iframe></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zODM1Mi8xNDg4MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>