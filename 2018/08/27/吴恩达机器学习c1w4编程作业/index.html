<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 4）"><meta name="keywords" content="吴恩达深度学习笔记,AI,神经网络"><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 4） | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#第一部分：基本架构"><span class="toc-number">1.</span> <span class="toc-text">第一部分：基本架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#包的引入"><span class="toc-number">1.1.</span> <span class="toc-text">包的引入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总体框架"><span class="toc-number">1.2.</span> <span class="toc-text">总体框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#初始化"><span class="toc-number">1.3.</span> <span class="toc-text">初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#两层神经网路初始化"><span class="toc-number">1.3.1.</span> <span class="toc-text">两层神经网路初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L-层的神经网络的初始化"><span class="toc-number">1.3.2.</span> <span class="toc-text">L 层的神经网络的初始化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#前向传播模块"><span class="toc-number">1.4.</span> <span class="toc-text">前向传播模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性前向传播"><span class="toc-number">1.4.1.</span> <span class="toc-text">线性前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#一层线性-激活前向传播"><span class="toc-number">1.4.2.</span> <span class="toc-text">一层线性-激活前向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L-层模型的前向传播"><span class="toc-number">1.4.3.</span> <span class="toc-text">L 层模型的前向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#计算代价函数"><span class="toc-number">1.5.</span> <span class="toc-text">计算代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#反向传播模块"><span class="toc-number">1.6.</span> <span class="toc-text">反向传播模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#线性反向传播"><span class="toc-number">1.6.1.</span> <span class="toc-text">线性反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#一层线性-激活反向传播"><span class="toc-number">1.6.2.</span> <span class="toc-text">一层线性-激活反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L-层模型的反向传播"><span class="toc-number">1.6.3.</span> <span class="toc-text">L 层模型的反向传播</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#更新参数"><span class="toc-number">1.7.</span> <span class="toc-text">更新参数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#第二部分：图片识别应用"><span class="toc-number">2.</span> <span class="toc-text">第二部分：图片识别应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#包的引入-1"><span class="toc-number">2.1.</span> <span class="toc-text">包的引入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据集"><span class="toc-number">2.2.</span> <span class="toc-text">数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集预处理"><span class="toc-number">2.2.1.</span> <span class="toc-text">数据集预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#加载原始数据集"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">加载原始数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#确定图片维度和个数以防止出错"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">确定图片维度和个数以防止出错</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#重构图片数组变为标准输入矩阵"><span class="toc-number">2.2.1.3.</span> <span class="toc-text">重构图片数组变为标准输入矩阵</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据标准化"><span class="toc-number">2.2.1.4.</span> <span class="toc-text">数据标准化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一般方法"><span class="toc-number">2.3.</span> <span class="toc-text">一般方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#两层的神经网络构建"><span class="toc-number">2.4.</span> <span class="toc-text">两层的神经网络构建</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#L-层的神经网络"><span class="toc-number">2.5.</span> <span class="toc-text">L 层的神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#结果分析"><span class="toc-number">2.6.</span> <span class="toc-text">结果分析</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">33</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">28</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_topimg.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="post-info"><div id="post-title">coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 4）</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-27</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习笔记/">学习笔记</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="第一部分：基本架构"><a href="#第一部分：基本架构" class="headerlink" title="第一部分：基本架构"></a>第一部分：基本架构</h1><p>这是深度学习专项课程第一课第四周的编程作业的第一部分，通过这一部分，可以学到：</p>
<ul>
<li>使用非线性单元比如 ReLU 来提高模型</li>
<li>建立一个更深的神经网络（大于一个隐藏层）</li>
<li>实现一个易用的神经网络类</li>
</ul>
<h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 包的引入</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline <span class="comment"># %符号为ipython的魔法函数，与画图有关，在pycharm中会报错</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="总体框架"><a href="#总体框架" class="headerlink" title="总体框架"></a>总体框架</h2><ul>
<li>初始化一个两层神经网络的参数以及一个 L 层的神经网络的参数</li>
<li><p>实现前向传播模块（下图中的紫色模块）</p>
<ul>
<li>实现某一层的前向传播步骤的<strong>线性部分</strong>（得到 $Z^{[l]}$）</li>
<li>给定激活函数（relu/sigmoid）</li>
<li>组合之前的两步形成一个<strong>[线性—&gt;激活]</strong>前向传播函数</li>
<li>重复<strong>[线性—&gt;ReLU]</strong>前向传播函数 L-1 次（对于 1 到 L-1 层），再加上<strong>[线性—&gt;sigmoid]</strong>在末尾（对于二元分类的最终层 L）。这形成了一个新的 L_model_forward 函数</li>
</ul>
</li>
<li><p>计算损失函数</p>
</li>
<li><p>实现反向传播模块（下图中的红色部分）</p>
<ul>
<li>计算某一层的反向传播函数步骤的<strong>线性部分</strong></li>
<li>计算激活函数的梯度（relu_backward/sigmoid_backward）</li>
<li>组合之前的两步形成一个新的<strong>[线性—&gt;激活]</strong>反向传播函数</li>
<li>重复<strong>[线性—&gt;relu]</strong>反向传播 L-1 次，然后加上<strong>[线性—&gt;sigmoid]</strong>反向传播在末尾。这形成了一个新的 L_model_backward 函数</li>
</ul>
</li>
<li><p>更新参数</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1.png" alt=""></p>
<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><h3 id="两层神经网路初始化"><a href="#两层神经网路初始化" class="headerlink" title="两层神经网路初始化"></a>两层神经网路初始化</h3><ul>
<li>模型结构为：<em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</em></li>
<li>权重 W 采用随机初始化</li>
<li>偏差 b 采用初始化为零的方法</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单隐层神经网络的初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h3 id="L-层的神经网络的初始化"><a href="#L-层的神经网络的初始化" class="headerlink" title="L 层的神经网络的初始化"></a>L 层的神经网络的初始化</h3><p>假设输入 X 维度是（12288，209），则其他的维度如下：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2.png" alt=""></p>
<ul>
<li>模型结构为：<em>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</em></li>
<li>权重 W 采用随机初始化</li>
<li>偏差 b 采用初始化为零</li>
<li>将每层单元个数储存在 list 变量 layer_dims 里面</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L 层神经网络的初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- 包含每一层的维度数据的list数组</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">   </span><br><span class="line">    parameters = &#123;&#125;    <span class="comment"># 先建立一个空的参数dict</span></span><br><span class="line">    L = len(layer_dims)   <span class="comment"># 神经网络的总层数L </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将参数 W1，b1，W2，b1... 初始化</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l],layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l],<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h2 id="前向传播模块"><a href="#前向传播模块" class="headerlink" title="前向传播模块"></a>前向传播模块</h2><ul>
<li>线性前向传播</li>
<li>线性—&gt;激活前向传播，其中激活函数可以是 ReLU 或者 sigmoid</li>
<li>整个模型为：[线性 —&gt; relu] ×（L-1）—&gt; 线性 —&gt; sigmoid</li>
</ul>
<h3 id="线性前向传播"><a href="#线性前向传播" class="headerlink" title="线性前向传播"></a>线性前向传播</h3><ul>
<li>输入上一层的激活值 A_prev，这一层的参数 W，b，</li>
<li>输出这一层的线性值 Z 和 “线性缓存”</li>
<li>“线性缓存”是一个值为 (A_prev,W,b) 的 tuple</li>
<li>$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 之前层的激活值 (或输入数据): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- 权重矩阵: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- 偏差向量, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 这一层激活函数的输入值，也叫“前激活”参数 </span></span><br><span class="line"><span class="string">    cache -- “线性缓存”，值为 (A_prev,W,b) 的 tuple，可以更有效率地计算反向传播</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算 Z 值</span></span><br><span class="line">    Z = np.dot(W,A_prev) + b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A_prev, W, b)	<span class="comment"># “线性缓存”</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure>
<h3 id="一层线性-激活前向传播"><a href="#一层线性-激活前向传播" class="headerlink" title="一层线性-激活前向传播"></a>一层线性-激活前向传播</h3><ul>
<li>输入上一层的激活值 A_prev，这一层的参数 W 和 b，以及激活函数名 (字符串)</li>
<li>输出这一层的激活值 A 和 “前向传播缓存”</li>
<li>“前向传播缓存” = “线性缓存”(A_prev,W,b) + “激活缓存”Z，是一个值为 ( (A_prev,W,b), Z) 的 tuple，简称“缓存”</li>
<li>$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一层线性-激活前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 之前层的激活值 (或输入数据): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- 权重矩阵: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- 偏差向量, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- 在这一层用到的激活函数, 用字符串储存: "sigmoid" 或 "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- 这一层的激活值 </span></span><br><span class="line"><span class="string">    cache -- 前向传播缓存，是一个包含 "linear_cache"（线性缓存，A_prev，W，b） 和 "activation_cache"（激活缓存，Z） 的 tuple：((A_prev,W,b),Z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 判断激活函数是 sigmoid 还是 relu</span></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev,W,b)	<span class="comment"># 先得到这一层的线性值 Z 和“线性缓存” (A_prev,W,b)</span></span><br><span class="line">        A, activation_cache = sigmoid(Z)	<span class="comment"># 再将 Z 激活得到 A 和“激活缓存” Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev,W,b) 	<span class="comment"># 同理</span></span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)	<span class="comment"># 将线性缓存和激活缓存合并成一个“前向传播缓存” tuple</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sigmoid 函数和 relu 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(Z)</span>:</span></span><br><span class="line"></span><br><span class="line">    A = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    cache = Z 	<span class="comment"># 激活缓存 Z</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z)</span>:</span></span><br><span class="line">    </span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    cache = Z	<span class="comment"># 激活缓存 Z</span></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>
<h3 id="L-层模型的前向传播"><a href="#L-层模型的前向传播" class="headerlink" title="L 层模型的前向传播"></a>L 层模型的前向传播</h3><p>对于二元分类，先 [线性-relu激活] L-1 次，再 [线性-sigmoid激活] 一次</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二元分类 L 层模型的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 初始训练集, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- 初始化函数 initialize_parameters_deep() 输出的初始化之后的参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- 最终层即 L 层的激活值</span></span><br><span class="line"><span class="string">    caches -- 包含每一层"前向传播缓存"的 list (一共有 L-1 个, 索引值为 0 到 L-1)</span></span><br><span class="line"><span class="string">              其中每一层的缓存是一个 tuple：((A,W,b),Z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span>	<span class="comment"># 神经网络的层数，由于参数有 W 和 b，故要除以 2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 [线性 -&gt; RELU] 前向传播 L-1 次，并把每一次的缓存加到总缓存 caches 中.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span> + str(l)], parameters[<span class="string">'b'</span> + str(l)], <span class="string">'relu'</span>)	<span class="comment"># 切记是 'relu'字符串 而不是 relu</span></span><br><span class="line">        caches.append(cache)	<span class="comment"># 将每一次的缓存加入总缓存 caches</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实现 [线性 -&gt; sigmoid] 前向传播 1 次，并把这一次的缓存加到总缓存 caches 中.</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span> + str(L)], parameters[<span class="string">'b'</span> + str(L)], <span class="string">'sigmoid'</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于缓存，在对输入进行线性运算时，生成<strong>“线性缓存”</strong>，是一个 (A_prev,W,b) 的 tuple，在对线性值进行激活运算时，生成<strong>“激活缓存”</strong>，是一个值为 Z 的数组，在进行 [线性-激活] 的前向传播时，生成的是<strong>“前向传播缓存”</strong>，简称<strong>“缓存”</strong>，是一个值为 ((A_prev,W,b),Z) 的 tuple</p>
</blockquote>
<h2 id="计算代价函数"><a href="#计算代价函数" class="headerlink" title="计算代价函数"></a>计算代价函数</h2><script type="math/tex; mode=display">
cost = -\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right))</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">  </span><br><span class="line">    m = Y.shape[<span class="number">1</span>]	<span class="comment"># 总样本个数</span></span><br><span class="line"></span><br><span class="line">    cost = <span class="number">-1</span>/m * ( np.dot(Y, np.log(AL).T) + np.dot((<span class="number">1</span>-Y), np.log(<span class="number">1</span>-AL).T) )</span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost)      <span class="comment"># 确保代价函数是一个数而不是数组 (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h2 id="反向传播模块"><a href="#反向传播模块" class="headerlink" title="反向传播模块"></a>反向传播模块</h2><ul>
<li>线性反向传播</li>
<li>线性—&gt;激活反向传播，其中“激活”计算 relu 或者 sigmoid 函数的梯度</li>
<li>整个模型为：[线性 —&gt; relu] ×（L-1）—&gt; 线性 —&gt; sigmoid</li>
</ul>
<h3 id="线性反向传播"><a href="#线性反向传播" class="headerlink" title="线性反向传播"></a>线性反向传播</h3><p>已知 $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$ 和该层 “线性缓存”(A_prev,W,b)，求 $dW^{[l]}, db^{[l]} ,dA^{[l-1]}$ ，如下图所示</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.3.png" alt=""></p>
<p>计算公式为：</p>
<script type="math/tex; mode=display">
Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\\
dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T}\\
db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}\\
dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 线性反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- 代价函数对某l层线性输出 Z 的偏导 (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- 该l层的前向传播“线性缓存” tuple：(A_prev, W, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- 代价函数对前一层 l-1 层的激活值的偏导数，与 A_prev 的形状相同</span></span><br><span class="line"><span class="string">    dW -- 代价函数对当前层 l 层的权重 W 的偏导数，形状与 W 相同</span></span><br><span class="line"><span class="string">    db -- 代价函数对当前层 l 层的偏差 b 的偏导数，形状与 b 相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache	<span class="comment"># 将该层“线性缓存”提取出来</span></span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]		<span class="comment"># 总样本数m</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据公式计算 dW，db，dA_prev</span></span><br><span class="line">    dW = <span class="number">1</span>/m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<h3 id="一层线性-激活反向传播"><a href="#一层线性-激活反向传播" class="headerlink" title="一层线性-激活反向传播"></a>一层线性-激活反向传播</h3><ul>
<li>已知 $dA^{[l]} = \frac{\partial \mathcal{L} }{\partial A^{[l]}}$ 和 $g(.)$ 和该层 “前向传播缓存”((A_prev,W,b),Z)，先求 $dZ^{[l]}$, 再求 $dW^{[l]}, db^{[l]} ,dA^{[l-1]}$ </li>
<li>$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]})$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一层线性-激活反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- 当前层l层的激活值梯度 </span></span><br><span class="line"><span class="string">    cache -- “前向传播缓存”tuple：((A_prev,W,b),Z)</span></span><br><span class="line"><span class="string">    activation -- 在这一层用到的激活函数, 用字符串储存: "sigmoid" 或 "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- 代价函数对前一层 l-1 层的激活值的偏导数，与 A_prev 的形状相同</span></span><br><span class="line"><span class="string">    dW -- 代价函数对当前层 l 层的权重 W 的偏导数，形状与 W 相同</span></span><br><span class="line"><span class="string">    db -- 代价函数对当前层 l 层的偏差 b 的偏导数，形状与 b 相同</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache	<span class="comment"># 从“前向传播缓存”中提取“线性缓存”和“激活缓存”</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)	<span class="comment"># 先用 dA 求得 dZ</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)	  <span class="comment"># 再运行线性反向传播</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)		<span class="comment">#同理</span></span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<p>其中 dA 求得 dZ 的函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现单个 relu 单元的反向传播</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- 某l层的激活值</span></span><br><span class="line"><span class="string">    cache -- 该层的“激活缓存” Z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- 代价函数对l层的 Z 的梯度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    Z = cache	<span class="comment"># 将 Z 值从“激活缓存”取出</span></span><br><span class="line">    dZ = np.array(dA, copy=<span class="keyword">True</span>) <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据 relu 函数，当 Z&lt;=0，dZ 也为零</span></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, cache)</span>:</span></span><br><span class="line">    </span><br><span class="line">    Z = cache</span><br><span class="line">    </span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-Z))</span><br><span class="line">    dZ = dA * s * (<span class="number">1</span>-s)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dZ</span><br></pre></td></tr></table></figure>
<h3 id="L-层模型的反向传播"><a href="#L-层模型的反向传播" class="headerlink" title="L 层模型的反向传播"></a>L 层模型的反向传播</h3><p>对于二元分类，先 [sigmoid—&gt;线性] 一次，再 [relu—&gt;线性] L-1 次</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.4.png" alt=""></p>
<ul>
<li>反向传播初始化，即求 dAL 可以使用公式：$dAL=-\frac{Y}{AL}-\frac{1-Y}{1-AL}$ </li>
<li>将梯度数据存入名为 grads 的 dict 中：$grads[“dW” + str(l)] = dW^{[l]}$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L 层模型的反向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- 前向传播的最终激活值向量</span></span><br><span class="line"><span class="string">    Y -- 真实的“标签”向量 (包含 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- 总缓存，包含每一层的“前向传播缓存”，用 sigmoid 函数进行激活的缓存是 cache[L-1]，用 relu 函数进行激活的缓存是 cache[l] (for l in range(L-1),即 l = 0,1...,L-2)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- 包含代价函数对 A,W,b 的梯度的 dict</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;	<span class="comment"># 先建立一个储存梯度的空 dict</span></span><br><span class="line">    L = len(caches)	<span class="comment"># 神经网络的总层数</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]	<span class="comment"># 训练集样本的个数</span></span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># 让 Y 和 AL 的形状相同</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播的初始化，即求 dAL</span></span><br><span class="line">    dAL =  - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))	<span class="comment"># 逐元素相除</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第 L 层先 [sigmoid -&gt; 线性] 反向传播一次. 输入：dAL, current_cache. 输出：grads["dAL-1"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]	  <span class="comment">#从总缓存 caches 中取出当前缓存 </span></span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L<span class="number">-1</span>)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">'sigmoid'</span>)	 <span class="comment"># 进行一次反向传播，，并把dAL-1，dWL，dbL 放入字典中</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 继续反向传播，从 l=L-2 循环到 l=0，故从 l+1 开始</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):	  <span class="comment"># 将 range() 倒序</span></span><br><span class="line">        <span class="comment"># 第 l 层: [relu -&gt; 线性] 反向传播求梯度.</span></span><br><span class="line">        <span class="comment"># 输入：grads["dA" + str(l + 1)], current_cache. 输出：grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]	<span class="comment"># 取出当前层的缓存</span></span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">'dA'</span> + str(L<span class="number">-1</span>)], current_cache, <span class="string">'relu'</span>)	<span class="comment"># 输入的是 dAL-1</span></span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<h2 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h2><script type="math/tex; mode=display">
W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \\
b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用梯度下降更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有初始化后的参数的字典</span></span><br><span class="line"><span class="string">    grads -- 包含所有参数的梯度的字典</span></span><br><span class="line"><span class="string">    learning_rate -- 学习率</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 包含所有更新过的参数的字典</span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = Wl 的值</span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = bl 的值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span>	<span class="comment"># 神经网络的层数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用公式更新参数</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h1 id="第二部分：图片识别应用"><a href="#第二部分：图片识别应用" class="headerlink" title="第二部分：图片识别应用"></a>第二部分：图片识别应用</h1><p>这是深度学习专项课程第一课第四周的编程作业的第二部分，通过这一部分，可以学到：</p>
<ul>
<li>学习如何使用第一部分中构建的辅助函数来建立我们需要的任何结构的模型</li>
<li>用不同的模型结构进行实验观察每一种的表现</li>
<li>认识到在从头开始构建神经网络之前构建辅助函数使得任务更加容易</li>
</ul>
<h2 id="包的引入-1"><a href="#包的引入-1" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v3 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>数据集包括：</p>
<ul>
<li>m_train 个训练集，包括图片集 train_set_x_orig 和标签集 train_set_y</li>
<li>m_test 个测试集，包括图片集 test_set_x_orig 和标签集 test_set_y</li>
<li>每张图片都是<strong>方形</strong> (height = num_px, width = num_px)，有三个颜色通道，所以数组形状是 (num_px, num_px, 3)</li>
<li>每个图片集都要进行预处理，所以原始数据加上 <strong>_orig</strong>，但是标签集不需要预处理</li>
</ul>
<h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><h4 id="加载原始数据集"><a href="#加载原始数据集" class="headerlink" title="加载原始数据集"></a>加载原始数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(train_set_x_orig[index]) <span class="comment"># 画图</span></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</span><br><span class="line"><span class="comment"># np.squeeze() 用于把一个数组的 shape 中为 1 的维度删掉，即让 train_set_y[:, index] 变为一个数</span></span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_1.png" alt=""></p>
<h4 id="确定图片维度和个数以防止出错"><a href="#确定图片维度和个数以防止出错" class="headerlink" title="确定图片维度和个数以防止出错"></a>确定图片维度和个数以防止出错</h4><ul>
<li>训练集的个数：m_train</li>
<li>测试集的个数：m_test</li>
<li>图片（正方形）的尺寸即边长的像素数：num_px</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确定维度和个数</span></span><br><span class="line"><span class="comment"># train_set_x_orig 形状为 (m_train, num_px, num_px, 3)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;Number of training examples: m_train = 209</span><br><span class="line">  Number of testing examples: m_test = 50</span><br><span class="line">  Height/Width of each image: num_px = 64</span><br><span class="line">  Each image is of size: (64, 64, 3)</span><br><span class="line">  train_set_x shape: (209, 64, 64, 3)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x shape: (50, 64, 64, 3)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br></pre></td></tr></table></figure>
<h4 id="重构图片数组变为标准输入矩阵"><a href="#重构图片数组变为标准输入矩阵" class="headerlink" title="重构图片数组变为标准输入矩阵"></a>重构图片数组变为标准输入矩阵</h4><p>把尺寸为 (num_px, num_px, 3) 的图片变为 shape 为 (num_px ∗ num_px ∗ 3, 1) 的向量 </p>
<p>把一个 shape 为 (a,b,c,d) 的矩阵变为一个 shape 为 (b∗c∗d, a) 的矩阵的技巧：<code>x_flatten = X.reshape(X.shape[0], -1).T</code></p>
<blockquote>
<p>实际上，reshape() 是按<strong>行</strong>取元素，按<strong>行</strong>放元素</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重构图片数组</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))<span class="comment">#重构后完整性检查</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;train_set_x_flatten shape: (12288, 209)</span><br><span class="line">  train_set_y shape: (1, 209)</span><br><span class="line">  test_set_x_flatten shape: (12288, 50)</span><br><span class="line">  test_set_y shape: (1, 50)</span><br><span class="line">  sanity check after reshaping: [17 31 56 22 33]</span><br></pre></td></tr></table></figure>
<h4 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h4><p>为了使得数据在一个合适的尺度上，我们需要将数据标准化，对于图片来说，由于图片的每个像素的 RGB 值介于 0 到 255 之间，所以我们可以将每个特征值除以 255，这样就能将它们标准化了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></span><br></pre></td></tr></table></figure>
<h2 id="一般方法"><a href="#一般方法" class="headerlink" title="一般方法"></a>一般方法</h2><ul>
<li>初始化参数 / 定义超参数</li>
<li><p>进行 num_iterations 次循环：</p>
<ul>
<li>前向传播</li>
<li>计算代价函数</li>
<li>反向传播</li>
<li>更新参数</li>
</ul>
</li>
<li><p>用训练过的参数来预测标签值</p>
</li>
</ul>
<h2 id="两层的神经网络构建"><a href="#两层的神经网络构建" class="headerlink" title="两层的神经网络构建"></a>两层的神经网络构建</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.5.png" alt=""></p>
<p>会用到的之前构建的函数有：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>确定每层的单元数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># 输入向量维度</span></span><br><span class="line">n_h = <span class="number">7</span>		<span class="comment"># 隐藏层单元数</span></span><br><span class="line">n_y = <span class="number">1</span>		<span class="comment"># 输出层单元数</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure>
<p>将构建的函数组合起来形成主函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个两层的神经网络: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 输入数据, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- 真实的标签向量 (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- 每一层的单元数 (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- 梯度下降的迭代次数</span></span><br><span class="line"><span class="string">    learning_rate -- 梯度下降学习率</span></span><br><span class="line"><span class="string">    print_cost -- 如果值为 True 则每一百步打印一次代价函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 包含更新后的参数 W1，W2，b1，b2 的字典</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []	<span class="comment"># 用来记录每百步的代价函数的值</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]	<span class="comment"># 样本数</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数字典中取出 W1，W2，b1，b2</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降循环 num_iterations 次</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向传播，输出下一层的激活值和“前向传播缓存”</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">'relu'</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算代价函数</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播初始化，即求得 dAL</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播，获得前一层的激活值梯度和这一层的参数梯度</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">'sigmoid'</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">'relu'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将参数梯度都存入 grads 字典</span></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 从参数字典中取出参数并赋值以便下一次迭代</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每百步打印一次代价函数</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)	<span class="comment"># 方便等下打印图表</span></span><br><span class="line">       </span><br><span class="line">    <span class="comment"># 打印出代价函数的图表</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters	<span class="comment"># 返回更新过的参数</span></span><br></pre></td></tr></table></figure>
<p>用我们的数据集进行测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.7.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.8.png" alt=""></p>
<p>看看预测训练集的精确度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure>
<p>>&gt; Accuracy: 1.0</p>
<p>再看看预测测试集的精确度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure>
<p>>&gt; Accuracy: 0.72</p>
<p>我们可以看到比第二周的逻辑回归的 70% 的精确度要稍高，接下来我们建立一个 L 层的神经网络进行试验。</p>
<h2 id="L-层的神经网络"><a href="#L-层的神经网络" class="headerlink" title="L 层的神经网络"></a>L 层的神经网络</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.9.png" alt=""></p>
<p>可以用到的之前构建的函数有：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>确定每层的单元数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>]	<span class="comment"># 这是一个四层的模型</span></span><br></pre></td></tr></table></figure>
<p>将辅助函数组合到主函数中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现一个 L 层的神经网络: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- 输入数据向量, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- 真实的标签向量 (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- 包含输入向量维度和每层的单元数的列表, 长度为层数 + 1</span></span><br><span class="line"><span class="string">    learning_rate -- 梯度下降学习率</span></span><br><span class="line"><span class="string">    num_iterations -- 梯度下降循环迭代次数</span></span><br><span class="line"><span class="string">    print_cost -- 若为 True，则每百步打印一次代价函数 cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- 模型训练出来的参数，可用于预测标签值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    costs = []	<span class="comment"># 用来记录代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数初始化</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 梯度下降循环</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment"># 前向传播: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        AL, caches = L_model_forward(X, parameters)	  <span class="comment"># 注意在前向传播的过程中会得到总缓存 caches</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算代价函数</span></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">    </span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)	  <span class="comment"># 注意反向传播时要用上前向传播的总缓存 caches</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">       </span><br><span class="line">        <span class="comment"># 每百步打印一次代价函数值</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># 画出代价函数图表</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>用数据集训练模型试试看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>得到的结果如下：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.10.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.11.png" alt=""></p>
<p>看看预测训练集的精确度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br></pre></td></tr></table></figure>
<p>>&gt; Accuracy: 0.985645933014</p>
<p>看看预测测试集的精确度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure>
<p>>&gt; Accuracy: 0.8</p>
<p>我们可以看到 4 层神经网络的 80% 精确度比逻辑回归的 70% 和 2 层神经网络的 72% 精确度要高很多！</p>
<p>还可以拿自己的图片进行预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先将自己的图片加到 image 文件夹</span></span><br><span class="line">my_image = <span class="string">"my_image.jpg"</span> <span class="comment"># 将这个改成你图片的名字 </span></span><br><span class="line">my_label_y = [<span class="number">1</span>] <span class="comment"># 图片的真实标签值 (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class="line"></span><br><span class="line">fname = <span class="string">"images/"</span> + my_image</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="keyword">False</span>))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">my_image = my_image/<span class="number">255.</span></span><br><span class="line">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your L-layer model predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.13.png" alt=""></p>
<p>找了十张动物图片来进行测试，前五张不是猫，后五张是猫，测试结果是：第一张熊猫识别错误，第二张北极熊识别错误，第三张大象识别正确，第四张兔子识别正确，第五张非洲狮识别正确（其实对这张最没信心，因为外形和猫实在是太像了，反而识别正确了==），第六张猫识别错误，第七张猫识别错误，第八张猫识别正确，第九张猫识别正确，第十张猫识别正确，粗略看来，这十张图的正确率有六成，模型仍需改进！</p>
<h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><p>试着打印出分类错误的图片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.12.png" alt=""></p>
<p>我们可以发现有以下特征的图片更难判断正确：</p>
<ul>
<li>猫的身体在不寻常的位置</li>
<li>猫的颜色与背景颜色相似</li>
<li>特殊的猫的颜色和种类</li>
<li>相机角度</li>
<li>图片的明亮度</li>
<li>大小变化（猫在图片中很大或者很小）</li>
</ul>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/神经网络/">神经网络</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr1.png"><div class="post-qr-code__desc">微信捐赠</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr2.jpg"><div class="post-qr-code__desc">支付宝捐赠</div></div></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/08/29/吴恩达机器学习笔记c2w1/"><i class="fa fa-chevron-left">  </i><span>coursera 吴恩达深度学习 Specialization 笔记（course 2 week 1）—— 正则化等</span></a></div><div class="next-post pull-right"><a href="/2018/08/15/吴恩达机器学习笔记c1w4/"><span>coursera 吴恩达深度学习 Specialization 笔记（course 1 week 4）—— 深度神经网络</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=530 height=86 src="//music.163.com/outchain/player?type=2&id=566993785&auto=1&height=66"></iframe></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zODM1Mi8xNDg4MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>