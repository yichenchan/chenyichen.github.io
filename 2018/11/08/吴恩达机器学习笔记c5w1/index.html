<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="coursera 吴恩达深度学习 Specialization 笔记（course 5 week 1）—— 循环神经网络 RNN"><meta name="keywords" content="吴恩达深度学习笔记,AI,rnn"><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>coursera 吴恩达深度学习 Specialization 笔记（course 5 week 1）—— 循环神经网络 RNN | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#序列化模型"><span class="toc-number">1.</span> <span class="toc-text">序列化模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#序列化模型的例子"><span class="toc-number">1.1.</span> <span class="toc-text">序列化模型的例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#标注"><span class="toc-number">1.2.</span> <span class="toc-text">标注</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#循环神经网络"><span class="toc-number">2.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#为什么不能用标准神经网络"><span class="toc-number">2.1.</span> <span class="toc-text">为什么不能用标准神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是循环神经网络-Recurrent-Neural-Network"><span class="toc-number">2.2.</span> <span class="toc-text">什么是循环神经网络 Recurrent  Neural Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN-的前向传播"><span class="toc-number">2.3.</span> <span class="toc-text">RNN 的前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#前向传播简化"><span class="toc-number">2.3.1.</span> <span class="toc-text">前向传播简化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN-的反向传播"><span class="toc-number">2.4.</span> <span class="toc-text">RNN 的反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#更多-RNN-的类型"><span class="toc-number">2.5.</span> <span class="toc-text">更多 RNN 的类型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#语言模型和序列生成"><span class="toc-number">3.</span> <span class="toc-text">语言模型和序列生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是语言模型"><span class="toc-number">3.1.</span> <span class="toc-text">什么是语言模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何用-RNN-构建语言模型"><span class="toc-number">3.2.</span> <span class="toc-text">如何用 RNN 构建语言模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#采样新序列"><span class="toc-number">3.3.</span> <span class="toc-text">采样新序列</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#更多的-RNN-类型"><span class="toc-number">4.</span> <span class="toc-text">更多的 RNN 类型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RNN-中的梯度消失问题"><span class="toc-number">4.1.</span> <span class="toc-text">RNN 中的梯度消失问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#门控回归单元-Gated-Recurrent-Unit-GRU"><span class="toc-number">4.2.</span> <span class="toc-text">门控回归单元 Gated Recurrent Unit (GRU)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#简化版-GRU-原理"><span class="toc-number">4.2.1.</span> <span class="toc-text">简化版 GRU 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#完全的-GRU-单元"><span class="toc-number">4.2.2.</span> <span class="toc-text">完全的 GRU 单元</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#长短时记忆网络-Long-Short-Term-Memory-LSTM"><span class="toc-number">4.3.</span> <span class="toc-text">长短时记忆网络 Long Short Term Memory (LSTM)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#GRU-和-LSTM-的选择"><span class="toc-number">4.3.1.</span> <span class="toc-text">GRU 和 LSTM 的选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#双向-RNN-BIdirectional-RNN-BRNN"><span class="toc-number">4.4.</span> <span class="toc-text">双向 RNN BIdirectional RNN (BRNN)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度-RNN"><span class="toc-number">4.5.</span> <span class="toc-text">深度 RNN</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">34</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">29</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_topimg.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="post-info"><div id="post-title">coursera 吴恩达深度学习 Specialization 笔记（course 5 week 1）—— 循环神经网络 RNN</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-11-08</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习笔记/">学习笔记</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>在“序列化模型”这门课的第一周，我们会学习到循环神经网络，这种模型对时间数据表现得非常好，它有几个变种，包括 LSTM，GRU 和 双向 RNN。</p>
<h1 id="序列化模型"><a href="#序列化模型" class="headerlink" title="序列化模型"></a>序列化模型</h1><h2 id="序列化模型的例子"><a href="#序列化模型的例子" class="headerlink" title="序列化模型的例子"></a>序列化模型的例子</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_1.png" alt=""></p>
<a id="more"></a>
<h2 id="标注"><a href="#标注" class="headerlink" title="标注"></a>标注</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_2.png" alt=""></p>
<p>假如我们输入一句话 x，如果要找出其中的人名，那么输出标签对应人名的位置取 1.</p>
<ul>
<li>$T_x,T_y$ 表示这个序列的长度</li>
<li>上标 $(i)$ 表示第 i 个样本，上标 <code>&lt; t&gt;</code>表示这个序列的第 t 个位置  </li>
</ul>
<p>为了表示输入句子 x，我们需要建立一个字典，在这个例子里用的是词汇量为 10,000 的字典，对于现代的自然语言处理应用而言, 这种规模属于非常小的了，对于商业级的应用，字典规模一般为 3 到 5 万词汇，10 万级词汇的字典也比较常见。构建这个字典的一个方法就是去训练集中查找，找到出现频率最高的 1 万个词，另一种方式是查找一些网上的字典，将其中包含的英语中最常见的 1 万词作为拟构建的字典。</p>
<p>然后对输入句子进行 <strong>one-hot</strong> 编码，句子里的任意一个词 t，设为 $x^{&lt;   t  &gt;}$，都将表示为一个 one-hot 向量，one-hot 的意思指只有一位为 1 其余位全是 0。例如  $x^{&lt;   1  &gt;}$ 代表的单词 Harry， 就被表示为一个向量：向量的其余位全为 0，除了在第 4075 位上有一个 1 ，因为 Harry 这个单词就在词汇表中的第 4075 位上。 </p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_3.png" alt=""></p>
<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="为什么不能用标准神经网络"><a href="#为什么不能用标准神经网络" class="headerlink" title="为什么不能用标准神经网络"></a>为什么不能用标准神经网络</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_4.png" alt=""></p>
<p>之所以不能用上图中的标准神经网络，是因为有一些问题：</p>
<ul>
<li>不同样本的输入输出长度可能不同</li>
<li>无法共享从句子不同地方学习到的特征</li>
</ul>
<h2 id="什么是循环神经网络-Recurrent-Neural-Network"><a href="#什么是循环神经网络-Recurrent-Neural-Network" class="headerlink" title="什么是循环神经网络 Recurrent  Neural Network"></a>什么是循环神经网络 Recurrent  Neural Network</h2><p>读取第一个词语 $x^{&lt; 1&gt;}$，输入一个神经网络，形成第一个神经网络的隐藏层，然后得到第一个词语的输出 $\hat y^{&lt; 1&gt;}$，第一个神经网络的激活值 $a^{&lt; 1&gt;}$，乘上一个参数后继续加到下一个词语的神经网络中供其使用，然后再继续往下传递，于是每一步都可以使用到先前的信息进行预测。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_5.png" alt=""></p>
<p>这个模型的一个缺点就是： 在序列中对某一时间的预测仅使用之前的输入， 而不使用序列中之后的信息，具体来说，当我们预测 y3 时，它不会使用关于词语 x4, x5, x6 等的信息。使用双向 RNN 可以解决这个问题。</p>
<h2 id="RNN-的前向传播"><a href="#RNN-的前向传播" class="headerlink" title="RNN 的前向传播"></a>RNN 的前向传播</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_6.png" alt=""></p>
<script type="math/tex; mode=display">
a^{< 0>}= \vec 0\\a ^ { <  1 > } = g_1 \left( W _ { a a } a ^ { <  1 > } + W _ { a x } x ^ { <  1 > } + b _ { a } \right) \leftarrow tanh/relu\\
\hat { y } ^ { <  1 > } = g_2 \left( W _ { y a } a ^ { <  1 > } + b _ { y } \right) \leftarrow sigmoid \\a ^ { <  t > } = g \left( W _ { a a } a ^ { <  t > } + W _ { a x } x ^ { <  t > } + b _ { a } \right)\\
\hat { y } ^ { <  t > } = g \left( W _ { y a } a ^ { <  t > } + b _ { y } \right)</script><h3 id="前向传播简化"><a href="#前向传播简化" class="headerlink" title="前向传播简化"></a>前向传播简化</h3><script type="math/tex; mode=display">
a^{< t>}=g(W_a[a^{< t-1>}, x^{< t>}]+b_a)</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_7.png" alt=""></p>
<h2 id="RNN-的反向传播"><a href="#RNN-的反向传播" class="headerlink" title="RNN 的反向传播"></a>RNN 的反向传播</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_8.png" alt=""></p>
<p>红线是反向传播的路径，绿线是前向传播的路径，一个很酷的名字，基于时间的反向传播。</p>
<h2 id="更多-RNN-的类型"><a href="#更多-RNN-的类型" class="headerlink" title="更多 RNN 的类型"></a>更多 RNN 的类型</h2><p>目前为止，我们学习的都是输入序列和输出序列模型长度相同的模型，但是它们很多时候是不同的。</p>
<ul>
<li>one-to-one：标准神经网络</li>
<li>many-to-many<ul>
<li>$T_x=T_y$: 比如输入一句话输出其中人名</li>
<li>$T_x \not= T_y$：比如机器翻译，输入法语，输出英语</li>
</ul>
</li>
<li>many-to-one：例如情感分析，输入一句话输，出情感值</li>
<li>one-to-many：例如音乐生成，输入一个和弦，输出一首歌的音符序列</li>
</ul>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_11.png" alt=""></p>
<h1 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h1><h2 id="什么是语言模型"><a href="#什么是语言模型" class="headerlink" title="什么是语言模型"></a>什么是语言模型</h2><p>给定任何句子序列 $y^{&lt;  1 &gt;},y^{&lt;  2 &gt;},…,y^{&lt;  T_y &gt;}$，它都能给出这个特定句子的概率 $P(y^{&lt; 1&gt;},y^{&lt; 2&gt;},…,y^{&lt; T_y&gt;})$，也就是说你在任何一个时空听到一句话，它是 $y^{&lt; 1&gt;},y^{&lt; 2&gt;},…,y^{&lt; T_y&gt;}$的概率，例如下面这个语音识别的例子。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_12.png" alt=""></p>
<h2 id="如何用-RNN-构建语言模型"><a href="#如何用-RNN-构建语言模型" class="headerlink" title="如何用 RNN 构建语言模型"></a>如何用 RNN 构建语言模型</h2><ul>
<li>训练集：一个很大的语料库 corpus</li>
<li>首先将一句话标记化 Tokenize<ul>
<li>每一个单词被转化为一个 one-hot 向量</li>
<li>句子末尾可以加一个 <code>&lt; EOS&gt;</code> (end of sentence) 标记表示这是句子的末尾</li>
<li>对于不在语料库中的单词，使用一个统一标记 <code>&lt; UN&gt;</code> (unknown)</li>
</ul>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_13.png" alt=""></p>
<h2 id="采样新序列"><a href="#采样新序列" class="headerlink" title="采样新序列"></a>采样新序列</h2><p>我们训练语言模型的时候使用下面的结构：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_14.png" alt=""></p>
<p>但是在采样的时候，或者说要生成一个随机选择的句子，我们直接将前一层的预测值输入下一层作为下一层的输入。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_15.png" alt=""></p>
<p>目前我们建立的是词一级的语言模型，</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_16.png" alt=""></p>
<p>我们还可以建立字符级的语言模型，找出句子里所有可能出现的字符，并用来定义词汇表。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_17.png" alt=""></p>
<p>于是序列变成训练数据的每个字符而不是每个单词，这样做不用担心出现字符表中未出现的字符，但是最终会有更长的序列，需要更多计算资源。</p>
<h1 id="更多的-RNN-类型"><a href="#更多的-RNN-类型" class="headerlink" title="更多的 RNN 类型"></a>更多的 RNN 类型</h1><h2 id="RNN-中的梯度消失问题"><a href="#RNN-中的梯度消失问题" class="headerlink" title="RNN 中的梯度消失问题"></a>RNN 中的梯度消失问题</h2><p>有两句话：</p>
<ul>
<li>The <strong>cat</strong> which already ate  a bunch of food that was delicious <strong>was</strong> full.</li>
<li>The <strong>cats</strong> which already ate  a bunch of food that was delicious <strong>were</strong> full.</li>
</ul>
<p>cat 搭配 was，cats 搭配 were，但是他们之间隔的非常远，具有很长的依赖关系，对于 RNN 来说很难处理这种长依赖关系，为什么？</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_18.png" alt=""></p>
<p>与深度网络类似，较深的 RNN 也会产生<strong>梯度消失</strong>的问题，导致那么从输出 y 得到的梯度很难反向传播去影响前期层的权重，所以很难让句子前方的文字与后面的文字产生依赖关系。这导致了 RNN 的附近效应，意味着序列某个位置的的值主要受它附近的值的影响，很难做到通过各种方式反向传播到序列的开始，从而去修改神经网络在序列前期做的计算，所以这是 RNN 算法的一个缺点。</p>
<p>RNN 也可能产生梯度爆炸的问题，导致参数变得很大，一个解决方案是 gradient clipping，也就是如果梯度大于某些临界值，重新缩放某些梯度向量，使其不那么大。</p>
<p>下面是一些解决 RNN 梯度消失的方法。</p>
<h2 id="门控回归单元-Gated-Recurrent-Unit-GRU"><a href="#门控回归单元-Gated-Recurrent-Unit-GRU" class="headerlink" title="门控回归单元 Gated Recurrent Unit (GRU)"></a>门控回归单元 Gated Recurrent Unit (GRU)</h2><p>我们首先用一个图解释普通的 RNN 单元是如何工作：</p>
<script type="math/tex; mode=display">
a ^ { <  t > } = g \left( W _ { a } \left[ a ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { a } \right)\\
g()=tanh()</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_19.png" alt=""></p>
<h3 id="简化版-GRU-原理"><a href="#简化版-GRU-原理" class="headerlink" title="简化版 GRU 原理"></a>简化版 GRU 原理</h3><p>仍然是为了解决类似 The <strong>cat</strong> which already ate  a bunch of food that was delicious <strong>was</strong> full. 这种句子前后<strong>长期依赖</strong>的问题，我们引入一个新的变量 c = memory cell，即记忆细胞，GRU 中主要的计算公式为：</p>
<script type="math/tex; mode=display">
c^{< t>}=a^{< t>}\\
\begin{array} { l }  { \tilde { c } ^ { <  t > } = \tanh \left( W _ { c } \left[ c ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { c } \right) } \\
{ \Gamma _ { u } = \sigma \left( W _ { u } \left[ c ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { u } \right) } \\
{ c ^ { <  t > } = \Gamma _ { u } * \tilde { c } ^ { <  t > } + \left( 1 - \Gamma _ { u } \right) * c ^ { <  t - 1 > } } \end{array}</script><ul>
<li>$c^{&lt; t&gt;}$ 是记忆细胞的值，等于某个时间的激活值</li>
<li>$ \tilde { c } ^ { &lt;  t &gt; }$ 是用以取代 $c^{&lt; t&gt;}$ 的候选值，我们可以理解为现在这个 t 时间点的信息</li>
<li>$\Gamma _ { u }$ 是一个“门 (gate)”，由于是 sigmoid 函数的输出，所以是一个介于 0～1 之间的值，但是大部分值都非常接近 1 或者非常接近 0，u 代表 update，实现一个类似筛选器的作用，它决定了我们是否用 $ \tilde { c } ^ { &lt;  t &gt; }$ 取代 $c^{&lt; t-1&gt;}​$。</li>
<li>* 表示逐元素相乘</li>
<li>${ c ^ { &lt;  t &gt; } = \Gamma _ { u } <em> \tilde { c } ^ { &lt;  t &gt; } + \left( 1 - \Gamma _ { u } \right) </em> c ^ { &lt;  t - 1 &gt; } } $ 这是一种类似于滑动平均的公式，$ \tilde { c } ^ { &lt;  t &gt; }$ 可以理解为现在这个 t 时间点的信息，当门值 $\Gamma _u$ 为 1 时，我们将过去积累的信息 $c^{&lt; t-1&gt;}$ 丢弃，更新为现在的信息 $ \tilde { c } ^ { &lt;  t &gt; }$，当门值 $\Gamma _u$ 为 0 时，这个单元将记住过去积累的信息。而 $\Gamma _u$ 的值到底是 1 还是 0，就由模型自己学习。</li>
</ul>
<p>分析：cat 这个词是第三人称单数，对后面谓语有影响，所以到这个词时，门值 $\Gamma _ { u }=1$，根据最后一个式子，$c^{&lt; t&gt;}=\tilde c^{&lt; t&gt;}$，即在此处我们的记忆细胞的信息发生了一次更新，好，它现在记住了这里有个第三人称单数的 cat，接着到 which、already、ate 等单词时，他们的门值 $\Gamma _ { u }=0$，$c^{&lt; t&gt;}= c^{&lt; t-1&gt;}$，也就是说，这些词对谓语 was 不存在影响，记忆细胞的值不需要更新，仍旧等于上一个旧值。也就是说 cat 这个词的信息可以一直往后产生影响。</p>
<p>用一个相似的图来表示 GRU 的原理：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_20.png" alt=""></p>
<h3 id="完全的-GRU-单元"><a href="#完全的-GRU-单元" class="headerlink" title="完全的 GRU 单元"></a>完全的 GRU 单元</h3><script type="math/tex; mode=display">
\begin{array} { l }  { \tilde { c } ^ { <  t > } = \tanh \left( W _ { c } \left[\Gamma_r* c ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { c } \right) } \\
{ \Gamma _ { u } = \sigma \left( W _ { u } \left[ c ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { u } \right) } \\
{ \Gamma _ { r } = \sigma \left( W _ { r } \left[ c ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { r } \right) } \\
{ c ^ { <  t > } = \Gamma _ { u } * \tilde { c } ^ { <  t > } + \left( 1 - \Gamma _ { u } \right) * c ^ { <  t - 1 > } } \end{array}</script><ul>
<li>新增加的门 $\Gamma_r$ (r 代表 relevant)，说明了 $c^{&lt; t-1&gt;}$ 对于计算候选值 $ \tilde { c } ^ { &lt;  t &gt; }$ 有多大的<strong>相关性</strong></li>
</ul>
<h2 id="长短时记忆网络-Long-Short-Term-Memory-LSTM"><a href="#长短时记忆网络-Long-Short-Term-Memory-LSTM" class="headerlink" title="长短时记忆网络 Long Short Term Memory (LSTM)"></a>长短时记忆网络 Long Short Term Memory (LSTM)</h2><p>LSTM 与 GRU 相比具有三个“门”—— 更新门、遗忘门和输出门。</p>
<script type="math/tex; mode=display">
\begin{array} { l } { \tilde { c } ^ { <  t > } = \tanh \left( W _ { c } \left[ a ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { c } \right) } \\ 更新门 \ \ { \Gamma _ { u } = \sigma \left( W _ { u } \left[ a ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { u } \right) } \\遗忘门 \ \  { \Gamma _ { f } = \sigma \left( W _ { f } \left[ a ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { f } \right) } \\ 输出门 \ \ { \Gamma _ { o } = \sigma \left( W _ { o } \left[ a ^ { <  t - 1 > } , x ^ { <  t > } \right] + b _ { o } \right) } \\ { c ^ { <  t > } = \Gamma _ { u } * \tilde { c } ^ { <  t > } + \Gamma _ { f } * c ^ { <  t - 1 > } } \\ { a ^ { <  t > } = \Gamma _ { o } * \tanh c ^ { <  t > } } \end{array}</script><p>一个长短时记忆单元：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_21.png" alt=""></p>
<p>我们可以将几个 LSTM 单元用以下的方式结合起来：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_22.png" alt=""></p>
<p>过去时间的信息 $c^{&lt; t&gt;}$ 在红线这条独立的路径中向前传播，所以之前的信息能维持很长时间。</p>
<p>LSTM 有一个变体“窥空连接 (peephole connection)”，即 $c^{&lt; t-1&gt;}$ 会影响门控的数值：</p>
<script type="math/tex; mode=display">
{ \Gamma _ { u } = \sigma \left( W _ { u } \left[ a ^ { <  t - 1 > } , x ^ { <  t > } , c^{< t-1>} \right] + b _ { u } \right) } \\{ \Gamma _ { f } = \sigma \left( W _ { f } \left[ a ^ { <  t - 1 > } , x ^ { <  t > } , c^{< t-1>} \right] + b _ { f } \right) } \\{ \Gamma _ { o } = \sigma \left( W _ { o } \left[ a ^ { <  t - 1 > } , x ^ { <  t > }, c^{< t-1>} \right] + b _ { o } \right) }</script><h3 id="GRU-和-LSTM-的选择"><a href="#GRU-和-LSTM-的选择" class="headerlink" title="GRU 和 LSTM 的选择"></a>GRU 和 LSTM 的选择</h3><p>什么时候你应该使用 LSTM，什么时候使用 GRU？</p>
<p>在深度学习的历史中，LSTM 要远远早于 GRU，GRU 是一个相对近期的发明，用来作为复杂的 LSTM 模型的简化。在不同的问题上不同的算法各有千秋，所以，不存在一个普适的优秀算法 。GRU 的优点是其模型的简单性，因此更适用于构建较大的网络，它只有两个门控，从计算角度看，它的效率更高，可扩展性有利于构筑较大的模型；但是 LSTM 更加的强大和灵活，因为它具有三个门控。</p>
<p>LSTM 是经过历史检验的方法，因此，如果要选取一个，大多数人会把 LSTM 作为默认第一个去尝试的方法。但在过去几年 GRU 的势头越来越猛， 越来越多的团队同时也用 GRU，因为其简单而且效果可以和 LSTM 比拟，可以更容易的将其扩展到更大的问题。</p>
<h2 id="双向-RNN-BIdirectional-RNN-BRNN"><a href="#双向-RNN-BIdirectional-RNN-BRNN" class="headerlink" title="双向 RNN BIdirectional RNN (BRNN)"></a>双向 RNN BIdirectional RNN (BRNN)</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_23.png" alt=""></p>
<p>为了在某个时间点的预测获得序列前部分和后部分的信息，例如为了知道 teddy 到底是指人名还是“泰迪熊”，只凭借该词前面的信息是无法知道的，必须利用该词后面的信息。</p>
<p>BRNN 的结构如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_24.png" alt=""></p>
<p>为了预测 y3 的值，我们会利用到所有的信息，如下图的黄色线条显示了信息的流动过程：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_25.png" alt=""></p>
<p>图中的每个方块既可以是 GRU 单元，也可以是 LSTM 单元。</p>
<p>双向 RNN 的缺点是需要整个数据序列，然后才能在任何地方进行预测。例如, 如果要构建语音识别系统，使用 BRNN 需要等待人停止说话，得到整短话，才可以实际处理它。但对于许多自然语言处理应用，可以得到整个句子，标准 BRNN 算法实际上是非常有效的。</p>
<h2 id="深度-RNN"><a href="#深度-RNN" class="headerlink" title="深度 RNN"></a>深度 RNN</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.1_27.png" alt=""></p>
<p>$a^{[t]}$ 表示第 t 层，这一层所有的参数都是相同的 $W_a^{[t]},b_a^{[t]}$.</p>
<p>由于存在时间这个维度，就算很少的层数网络都变得非常大，所以 RNN 模型的层数都不会很深。</p>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/rnn/">rnn</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr1.png"><div class="post-qr-code__desc">微信捐赠</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr2.jpg"><div class="post-qr-code__desc">支付宝捐赠</div></div></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/12/01/deeplearning.ai 第五课第一周编程作业第一部分 lstm 反向传播部分的公式更正及推导/"><i class="fa fa-chevron-left">  </i><span>deeplearning.ai 第五课第一周编程作业第一部分 lstm 反向传播部分的公式更正及推导</span></a></div><div class="next-post pull-right"><a href="/2018/11/06/吴恩达机器学习c4w4编程作业/"><span>coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 4）</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=530 height=86 src="//music.163.com/outchain/player?type=2&id=566993785&auto=1&height=66"></iframe></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zODM1Mi8xNDg4MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>