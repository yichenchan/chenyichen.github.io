<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="coursera 吴恩达深度学习 Specialization 笔记（course 2 week 2）—— 优化算法的改进"><meta name="keywords" content="吴恩达深度学习笔记,AI,神经网络优化,优化算法"><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>coursera 吴恩达深度学习 Specialization 笔记（course 2 week 2）—— 优化算法的改进 | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#小批量梯度下降算法-mini-batch-gradient-descent"><span class="toc-number">1.</span> <span class="toc-text">小批量梯度下降算法 (mini-batch gradient descent)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法介绍"><span class="toc-number">1.1.</span> <span class="toc-text">算法介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#理解算法"><span class="toc-number">1.2.</span> <span class="toc-text">理解算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何选择-mini-batch-的-size"><span class="toc-number">1.3.</span> <span class="toc-text">如何选择 mini-batch 的 size</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#指数加权平均-Exponentially-weighted-averages"><span class="toc-number">2.</span> <span class="toc-text">指数加权平均 (Exponentially weighted averages)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是指数加权平均"><span class="toc-number">2.1.</span> <span class="toc-text">什么是指数加权平均</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#理解指数加权平均"><span class="toc-number">2.2.</span> <span class="toc-text">理解指数加权平均</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何实现"><span class="toc-number">2.3.</span> <span class="toc-text">如何实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#小技巧：偏差修正"><span class="toc-number">2.4.</span> <span class="toc-text">小技巧：偏差修正</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#动量梯度下降算法-Gradient-descent-with-momentum"><span class="toc-number">3.</span> <span class="toc-text">动量梯度下降算法 (Gradient descent with momentum)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#普通梯度下降的困境"><span class="toc-number">3.1.</span> <span class="toc-text">普通梯度下降的困境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#动量梯度下降步骤"><span class="toc-number">3.2.</span> <span class="toc-text">动量梯度下降步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法解释"><span class="toc-number">3.3.</span> <span class="toc-text">算法解释</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#均方根传递梯度下降算法——RMSprop-算法-Root-Mean-Square-prop"><span class="toc-number">4.</span> <span class="toc-text">均方根传递梯度下降算法——RMSprop 算法 (Root Mean Square prop)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSprop-实现步骤"><span class="toc-number">4.1.</span> <span class="toc-text">RMSprop 实现步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#算法理解"><span class="toc-number">4.2.</span> <span class="toc-text">算法理解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自适应矩估计梯度下降算法——Adam-算法-Adaptive-Moment-Estimation"><span class="toc-number">5.</span> <span class="toc-text">自适应矩估计梯度下降算法——Adam 算法 (Adaptive Moment Estimation)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam-实现步骤"><span class="toc-number">5.1.</span> <span class="toc-text">Adam 实现步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#超参数的默认值"><span class="toc-number">5.2.</span> <span class="toc-text">超参数的默认值</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#学习率衰减-learning-rate-decay"><span class="toc-number">6.</span> <span class="toc-text">学习率衰减 (learning rate decay)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么需要学习率衰减"><span class="toc-number">6.1.</span> <span class="toc-text">为什么需要学习率衰减</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何实现-1"><span class="toc-number">6.2.</span> <span class="toc-text">如何实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#局部最优问题-the-problem-of-local-optima"><span class="toc-number">7.</span> <span class="toc-text">局部最优问题 (the problem of local optima)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是局部最优"><span class="toc-number">7.1.</span> <span class="toc-text">什么是局部最优</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么不用担心局部最优问题"><span class="toc-number">7.2.</span> <span class="toc-text">为什么不用担心局部最优问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#停滞区问题-Plateaus"><span class="toc-number">7.3.</span> <span class="toc-text">停滞区问题 (Plateaus)</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">37</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_topimg.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="post-info"><div id="post-title">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 2）—— 优化算法的改进</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-02</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习笔记/">学习笔记</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本周主要学习不同的参数优化方式，提高模型的训练速度。</p>
<h2 id="小批量梯度下降算法-mini-batch-gradient-descent"><a href="#小批量梯度下降算法-mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降算法 (mini-batch gradient descent)"></a>小批量梯度下降算法 (mini-batch gradient descent)</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><p>向量化可以有效率地同时计算 m 个样本，但是当样本数为几百万时，速度依然会很慢，每一次迭代都必须先处理几百万的数据集才能往前一步，所以我们可以使用这个算法进行加速。</p>
<p>首先我们将训练集划分为一个一个微小的训练集，也就是小批量训练集 (mini-batch)，比如每一个微小训练集只有 1000 个样本：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_2.png" alt=""></p>
<a id="more"></a>
<p>我们把第 t 个批次的数据用上标 {t} 表示，例如 $X^{\{t\}},Y^{\{t\}}$ </p>
<p>$for \quad t = 1,…,5000: (每一步用 (X^{\{t\}},Y^{\{t\}}) 做一次梯度下降)\\ \quad X^{\{t\}} 为输入的前向传播 (1000 个样例)\\ \quad 计算代价函数 J^{\{t\}}\\ \quad 反向传播计算梯度 (只用  X^{\{t\}} 和 Y^{\{t\}})\\ \quad 更新参数 $   </p>
<p>以上是对训练集的一次迭代，可以得到 5000 次梯度逼近，继续使用另一个 for 循环进行多次迭代。</p>
<h3 id="理解算法"><a href="#理解算法" class="headerlink" title="理解算法"></a>理解算法</h3><p>小批量梯度下降 (mini-batch gradient descent) 和批量梯度下降 (batch gradient descent) 中代价函数的变化如下图：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_3.png" alt=""></p>
<p>梯度下降有三种类型，根据每个批次的样本数不同划分：</p>
<ul>
<li>若 minibatch 尺寸 = m，称之为：批量梯度下降 (BGD) ( X{t} , Y{t} ) = ( X , Y ) </li>
<li>若 minibatch 尺寸 = 1，称之为：随机梯度下降 (SGD)  ( X{t}, Y{t} ) = ( X{1}, Y{1} ), ( X{2}, Y{2} ) ,…, ( X{m}, Y{m} ) </li>
<li>若 minibatch 尺寸介于 1 到 m 之间：小批量梯度下降 (MBGD)</li>
</ul>
<p>在实际中一般采用第三种，因为第一种和第二种都有缺点：</p>
<ul>
<li>随机梯度下降失去了使用向量加速的机会，而且会发生震荡，但是速度比批量梯度下降快</li>
<li>批量梯度下降的缺点是如果训练集太大，在每次的迭代上要花费太长的时间</li>
<li>小批量梯度下降，则既可以利用到向量化，又可以不用等待整个训练集都遍历一遍就可以进行梯度下降，而且震荡要比随机梯度下降要小，是最好的方案</li>
</ul>
<p>下图展示了不同梯度下降的过程，蓝线是批量梯度下降，绿线是小批量梯度下降，紫线是随机梯度下降。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_4.png" alt=""></p>
<h3 id="如何选择-mini-batch-的-size"><a href="#如何选择-mini-batch-的-size" class="headerlink" title="如何选择 mini-batch 的 size"></a>如何选择 mini-batch 的 size</h3><p>最小批包含的样本数是一个可调的超参数，用如下原则来确定：</p>
<ul>
<li>如果训练集很小（m &lt; 2000）：直接使用批量梯度下降</li>
<li>如果训练集很大：一般选择 <strong>64 ～ 512</strong> 作为每个批次的大小，由于计算机内存的布局和访问方式，把它设为 2 的幂数代码运行会更快，故一般选择 <strong>64，126，256，512</strong> 这几个值</li>
<li>确保 minibatch 的 X{t} 和 Y{t} 能够放进 CPU 和 GPU 的内存</li>
</ul>
<h2 id="指数加权平均-Exponentially-weighted-averages"><a href="#指数加权平均-Exponentially-weighted-averages" class="headerlink" title="指数加权平均 (Exponentially weighted averages)"></a>指数加权平均 (Exponentially weighted averages)</h2><h3 id="什么是指数加权平均"><a href="#什么是指数加权平均" class="headerlink" title="什么是指数加权平均"></a>什么是指数加权平均</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_5.png" alt=""></p>
<p>上图表示的是伦敦一年 365 天气温的散点图，第 t 天的气温值为 $\theta_t$，给出如下公式计算<strong>指数加权平均</strong> (统计学中称为“指数加权滑动平均”)：</p>
<script type="math/tex; mode=display">
v_t=\beta v_{t-1}+(1-\beta)\theta_t</script><p>其中 $v_t$ 可以近似认为是前 $\frac{1}{1-\beta}$ 天的气温平均值，例如：</p>
<ul>
<li>若 $\beta = 0.9$ 则 $v_t$ 可认为计算的是前 10 天的平均气温值，如红线所示</li>
<li>若 $\beta = 0.98$ 则 $v_t$ 可认为计算的是前 50 天的平均气温值，如绿线所示</li>
<li>若 $\beta = 0.5$ 则 $v_t$ 可认为计算的是前 2 天的平均气温值，如黄线所示</li>
</ul>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_6.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_7.png" alt=""></p>
<h3 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_8.png" alt=""></p>
<h3 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h3><script type="math/tex; mode=display">
v_0=0\\v_1=\beta v_0+(1-\beta)\theta_1\\v_2=\beta v_1+(1-\beta)\theta_2\\v_3=\beta v_2+(1-\beta)\theta_3\\...</script><p>伪代码：</p>
<p>$v_\theta=0\\repeat\{\\ \quad \quad \quad get \ next \ \theta_t \\  \quad \quad \quad v_\theta:=\beta v_\theta+(1-\beta)\theta_t\\ \quad \quad \quad \}$ </p>
<h3 id="小技巧：偏差修正"><a href="#小技巧：偏差修正" class="headerlink" title="小技巧：偏差修正"></a>小技巧：偏差修正</h3><p>当 $\beta=0.98$ 时，我们预计会得到绿色那条线： </p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_6.png" alt=""></p>
<p>但是实际上我们会得到下图紫色那条线：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_9.png" alt=""></p>
<p>根据公式 $v_t=\beta v_{t-1}+(1-\beta)\theta_t$ ：<br>$v_0=0\\v1=0.98v_0+0.02\theta_1=0.02\theta_1\\v_2=0.98v_1+0.02\theta_2=0.98*0.02\theta_1+0.02\theta_2$</p>
<p>可以看到 $v_2$ 的值将远小于 $\theta_1$ 和 $\theta_2$ ，造成估计的偏差，有一种方法可以在估算的初期将偏差修正：</p>
<script type="math/tex; mode=display">
v_t=\frac{v_t}{1-\beta^t}</script><p>$t=2:1-\beta ^t=1-0.98^2=0.0396\\\frac{v_2}{0.0396}=\frac{0.0196 \theta_1 + 0.02\theta_2}{0.0396} \rightarrow \theta_1 和 \theta_2的加权平均数，从而消除了偏差$  </p>
<p>当 $t​$ 逐渐增大，$\beta^t​$ 的值将趋于 0，修正偏差基本无影响，所以绿线和紫线在后半段几乎重合。</p>
<h2 id="动量梯度下降算法-Gradient-descent-with-momentum"><a href="#动量梯度下降算法-Gradient-descent-with-momentum" class="headerlink" title="动量梯度下降算法 (Gradient descent with momentum)"></a>动量梯度下降算法 (Gradient descent with momentum)</h2><h3 id="普通梯度下降的困境"><a href="#普通梯度下降的困境" class="headerlink" title="普通梯度下降的困境"></a>普通梯度下降的困境</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_10.png" alt=""></p>
<ul>
<li>红点是梯度下降最小点，对于普通的梯度下降，路径如蓝线所示，朝着最小值缓慢地震荡前进，这种震荡会减慢学习的速度</li>
<li>不能使用太大的学习率，否则会产生超调，产生发散，无法收敛到最小点</li>
<li>在上下方向希望学习慢一点，因为不希望出现那些震荡，水平方向希望学习快一点，因为希望快速从左到右</li>
</ul>
<h3 id="动量梯度下降步骤"><a href="#动量梯度下降步骤" class="headerlink" title="动量梯度下降步骤"></a>动量梯度下降步骤</h3><p>$v_{dW}=0(维度和dW相同)\\v_{db}=0(维度和db相同)\\在第 t 次迭代：\\ \quad \quad 计算当前小批次的 dW，db \\ \quad \quad v_{dW}=\beta v_{dW}+(1-\beta)dW \\ \quad \quad v_{db}=\beta v_{db}+(1-\beta)db \\ \quad \quad W:=W-\alpha v_{dW},b:=b-\alpha v_{db} $    </p>
<ul>
<li>超参数 $\alpha, \beta$ ， 其中 $\beta = 0.9$ 效果最好，相当于计算前十次迭代的梯度平均值</li>
<li>一般不需要对 $v_{dW},v_{db}$ 进行偏差修正 </li>
<li>另一个版本为 $v_{dW}=\beta v_{dW}+dW,v_{db}=\beta v_{db}+db$ 也是正确的，但吴恩达不推荐</li>
</ul>
<h3 id="算法解释"><a href="#算法解释" class="headerlink" title="算法解释"></a>算法解释</h3><p>解释：这些操作可以让梯度下降变得平滑，因为 $v_{dW}$ 计算的是这一次迭代之前的若干次的梯度的平均值，震荡的路径在纵轴方向是 $\nearrow \searrow \nearrow \searrow$ ，这些梯度一正一负，相互抵消，在纵轴方向的平均值趋于 0 ，减弱了震荡，而横轴方向是 $\rightarrow \rightarrow\rightarrow\rightarrow$ ，都指向了右边，所以横轴方向的平均值依然很大。</p>
<p>形象解释：把代价函数图像想象成一个碗，有一个球滑向碗的最低点，把导数项 dW 和 db 想象成球滚下时的加速度，而把动量项 $v_{dW},v_{db}$ 想象成球的速度。</p>
<p>导数项给了球一个加速度，然后球向下滚，因为有加速度，所以它滚得越来越快，因为$\beta$ 是一个略小于1的数，可以把它看作摩擦力，让球不至于无限加速下去。与梯度下降中每一步都独立于之前步骤所不同的是，现在你的球可以向下滚并获得动量，沿碗向下加速并获得动量。</p>
<h2 id="均方根传递梯度下降算法——RMSprop-算法-Root-Mean-Square-prop"><a href="#均方根传递梯度下降算法——RMSprop-算法-Root-Mean-Square-prop" class="headerlink" title="均方根传递梯度下降算法——RMSprop 算法 (Root Mean Square prop)"></a>均方根传递梯度下降算法——RMSprop 算法 (Root Mean Square prop)</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_11.png" alt=""></p>
<p>仍然是这个震荡的问题，为了便于理解，假设只有两个参数 w 和 b，我们可以看到要减小震荡，必须使得纵轴即参数 b 的学习速度很慢，而横轴即参数 w 的学习速度很快，我们仍然使用指数加权平均的知识来实现梯度下降。</p>
<h3 id="RMSprop-实现步骤"><a href="#RMSprop-实现步骤" class="headerlink" title="RMSprop 实现步骤"></a>RMSprop 实现步骤</h3><p>$ S_{dW}=0(维度和dW相同)\\S_{db}=0(维度和db相同)\\在第 t 次迭代：\\ \quad \quad 计算当前小批次的 dW，db \\ \quad \quad S_{dW}=\beta’ S_{dW}+(1-\beta’)(dW)^2 \\ \quad \quad S_{db}=\beta’ S_{db}+(1-\beta’)(db)^2 \\ \quad \quad W:=W-\alpha \frac{dW}{\sqrt{S_{dW}}+\varepsilon },b:=b-\alpha \frac{db}{\sqrt{S_{db}}+\varepsilon } $ </p>
<ul>
<li>$\beta’$ 是为了和动量算法中的  $\beta$  相区分</li>
<li>$(dW)^2$ 是逐元素平方</li>
<li>$\varepsilon$ 是一个非常小的数比如 $10^{-8}$ 确保分母不为零</li>
</ul>
<h3 id="算法理解"><a href="#算法理解" class="headerlink" title="算法理解"></a>算法理解</h3><p>如上图所示，我们希望纵轴参数 b 的梯度很小，学习更慢，减小震荡，希望横轴参数 w 的梯度很大，学习更快，也就是希望作为分母的 $\sqrt{S_{dW}}$ 很小，而 $\sqrt{S_{db}}$ 很大。由于纵轴方向上是震荡的，路径更加偏向 b 轴一些，所以 $(db)^2$ 会相对更大，从而$\sqrt{S_{db}}$ 会相对更大，而 $(dW)^2$ 会相对更小，从而 $\sqrt{S_{dW}}$ 相对更小。</p>
<h2 id="自适应矩估计梯度下降算法——Adam-算法-Adaptive-Moment-Estimation"><a href="#自适应矩估计梯度下降算法——Adam-算法-Adaptive-Moment-Estimation" class="headerlink" title="自适应矩估计梯度下降算法——Adam 算法 (Adaptive Moment Estimation)"></a>自适应矩估计梯度下降算法——Adam 算法 (Adaptive Moment Estimation)</h2><p>这是一种结合了动量算法和 RMSprop 两者优点的算法，被广泛使用且已经被证明在很多不同种类的神经网络架构中都十分有效。</p>
<h3 id="Adam-实现步骤"><a href="#Adam-实现步骤" class="headerlink" title="Adam 实现步骤"></a>Adam 实现步骤</h3><p>$v_{dW}=0,S_{dW}=0,v_{db}=0,S_{db}=0   \leftarrow  初始化\\ 在第 t 次迭代：\\ \quad  计算当前小批次的 dW，db \\   \quad v_{dW}=\beta_1 v_{dW}+(1-\beta_1)dW  ，v_{db}=\beta_1 v_{db}+(1-\beta_1)db \leftarrow  动量算法\\ \quad S_{dW}=\beta_2 S_{dW}+(1-\beta_2)(dW)^2，S_{db}=\beta_2 S_{db}+(1-\beta_2)(db)^2 \leftarrow RMSprop\\  \quad v_{dW}^{correct}=\frac{v_{dW}}{1-\beta_1^t}，v_{db}^{correct}=\frac{v_{db}}{1-\beta_1^t}   \leftarrow v 的偏差修正   \\ \quad  S_{dW}^{correct}=\frac{S_{dW}}{1-\beta_2^t}，S_{db}^{correct}=\frac{v_{db}}{1-\beta_2^t}     \leftarrow S 的偏差修正   \\ \quad W:=W-\alpha \frac{v_{dW}^{correct}}{\sqrt{S_{dW}^{correct}}+\varepsilon },b:=b-\alpha \frac{v_{db}^{correct}}{\sqrt{S_{db}^{correct}}+\varepsilon }   \leftarrow 更新参数$   </p>
<h3 id="超参数的默认值"><a href="#超参数的默认值" class="headerlink" title="超参数的默认值"></a>超参数的默认值</h3><ul>
<li>学习率 $\alpha$ ：尝试不同值比较效果</li>
<li>$\beta_1：0.9$   </li>
<li>$\beta_2:0.999$ </li>
<li>$\varepsilon:10^{-8}$    </li>
</ul>
<blockquote>
<p>参数为 $\beta_1$ 的梯度值的指数加权平均 $v$ 称为第一阶的矩，参数为 $\beta_2$ 的梯度平方值的指数加权平均 $S$ 被称为第二阶的矩</p>
</blockquote>
<h2 id="学习率衰减-learning-rate-decay"><a href="#学习率衰减-learning-rate-decay" class="headerlink" title="学习率衰减 (learning rate decay)"></a>学习率衰减 (learning rate decay)</h2><h3 id="为什么需要学习率衰减"><a href="#为什么需要学习率衰减" class="headerlink" title="为什么需要学习率衰减"></a>为什么需要学习率衰减</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_12.png" alt=""></p>
<p>假如我们采用固定步长，如蓝色路径所示，那么它会逐步向最小值靠近，但不会完全收敛到这点，所以算法会在最小值周围浮动，但是却永远不会真正收敛。</p>
<p>如果慢慢地衰减学习率，刚开始学习率取值较大，学习速度较快，但随着学习率的的逐渐衰减，步长也会逐渐减小，所以最后将围绕着离极值点更近的的区域摆动，不会继续漂流远离。</p>
<h3 id="如何实现-1"><a href="#如何实现-1" class="headerlink" title="如何实现"></a>如何实现</h3><p>1 次迭代（1 epoch）：遍历完一次数据集，完成一次梯度更新</p>
<p>学习率衰减就是使学习率随着迭代次数 epoch _num 单调递减，比如下面这个公式：</p>
<script type="math/tex; mode=display">
\alpha = \frac{1}{1+decay\_rate*epoch\_num}\alpha_0</script><ul>
<li>decay_rate 为衰减率，是一个超参数</li>
<li>epoch_num 是迭代次数</li>
<li>$\alpha_0$ 是初始学习率，也是一个超参数</li>
</ul>
<p>代一些具体值看看学习率衰减的情况：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_13.png" alt=""></p>
<p>还有一些其他的学习率衰减公式，比如下面这个：</p>
<script type="math/tex; mode=display">
\alpha = 0.95 ^{epoch\_num}\alpha_0</script><ul>
<li>使得学习率呈指数级下降</li>
</ul>
<p>还有：</p>
<script type="math/tex; mode=display">
\alpha = \frac{k}{\sqrt{epoch\_num}}\alpha_0 \ \ 或者\frac{k}{\sqrt{t}}\alpha_0</script><p>还有采用离散阶梯函数的学习率衰减，比如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_14.png" alt=""></p>
<p>有时候人们还会使用手动的梯度衰减，但是比较少。</p>
<h2 id="局部最优问题-the-problem-of-local-optima"><a href="#局部最优问题-the-problem-of-local-optima" class="headerlink" title="局部最优问题 (the problem of local optima)"></a>局部最优问题 (the problem of local optima)</h2><p>深度学习早期，人们担心优化算法会陷入局部最优之中。</p>
<h3 id="什么是局部最优"><a href="#什么是局部最优" class="headerlink" title="什么是局部最优"></a>什么是局部最优</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_15.png" alt=""></p>
<p>假设上图中是参数 w1 和 w2关于代价函数的三维图像，人们担心在对这些参数进行优化的时候会优化到图中蓝点所在的位置，这些值只是在“谷底”，而不是真正的最小值，是局部最优。</p>
<h3 id="为什么不用担心局部最优问题"><a href="#为什么不用担心局部最优问题" class="headerlink" title="为什么不用担心局部最优问题"></a>为什么不用担心局部最优问题</h3><p>在训练一个神经网络时，代价函数中大部分梯度为零的点并不是上图中的局部最优，而是<strong>鞍点 (saddle point)</strong>，这种点周围的函数从一个方向看是凸函数，从另一个是凹函数，就像马鞍一样，如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_16.png" alt=""></p>
<p>如果一个点要是局部最优点，那么这个点周围的函数需要在所有方向看都是凸函数或者都是凹函数，但是由于神经网络的参数有上万乃至上百万个，如果某个点要成为局部最优，那么在所有的上百万个维度中函数的方向都得是一样的凹函数或者凸函数，这件事发生的概率非常非常低。更有可能碰到的是鞍点，某些方向的曲线向上弯曲，某些方向的向下弯曲，并非所有的曲线都向上或者向下弯曲，所以在高维空间中，不用担心发生局部最优问题！</p>
<h3 id="停滞区问题-Plateaus"><a href="#停滞区问题-Plateaus" class="headerlink" title="停滞区问题 (Plateaus)"></a>停滞区问题 (Plateaus)</h3><p>停滞区：导数长时间接近于零的一段区域</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_17.png" alt=""></p>
<p>当沿着马鞍面向下移动，移动到鞍点，但鞍点不是我们需要的最小值点，需要继续往下移动。但是这个地方梯度为零或者接近于零，曲面很平，需要花费很长时间缓慢地在这个停滞区内找到这个点而不能继续往下，当左侧或右侧有随机扰动，才能继续往下走，这会让学习速度变得非常慢。</p>
<p>解决办法：采用 动量算法、RMSprop算法、Adam算法等可以改善这个情况。</p>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/神经网络优化/">神经网络优化</a><a class="post-meta__tags" href="/tags/优化算法/">优化算法</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr1.png"><div class="post-qr-code__desc">微信捐赠</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr2.jpg"><div class="post-qr-code__desc">支付宝捐赠</div></div></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2018/09/04/吴恩达机器学习c2w2编程作业/"><i class="fa fa-chevron-left">  </i><span>coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 2）</span></a></div><div class="next-post pull-right"><a href="/2018/09/01/吴恩达机器学习c2w1编程作业/"><span>coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 1）</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=530 height=86 src="//music.163.com/outchain/player?type=2&id=566993785&auto=1&height=66"></iframe></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zODM1Mi8xNDg4MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>