<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="coursera 吴恩达深度学习 Specialization 笔记（course 5 week 3）—— 序列模型和注意力机制"><meta name="keywords" content="吴恩达深度学习笔记,AI,注意力机制"><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>coursera 吴恩达深度学习 Specialization 笔记（course 5 week 3）—— 序列模型和注意力机制 | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#sequence-to-sequence-模型"><span class="toc-number">1.</span> <span class="toc-text">sequence to sequence 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本模型"><span class="toc-number">1.1.</span> <span class="toc-text">基本模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何挑出可能性最大的输出序列"><span class="toc-number">1.2.</span> <span class="toc-text">如何挑出可能性最大的输出序列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Beam-search-集束搜索"><span class="toc-number">1.3.</span> <span class="toc-text">Beam search 集束搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#第一步"><span class="toc-number">1.3.1.</span> <span class="toc-text">第一步</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#第二步"><span class="toc-number">1.3.2.</span> <span class="toc-text">第二步</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#第三步"><span class="toc-number">1.3.3.</span> <span class="toc-text">第三步</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#集束搜索改进"><span class="toc-number">1.4.</span> <span class="toc-text">集束搜索改进</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#长度归一化"><span class="toc-number">1.4.1.</span> <span class="toc-text">长度归一化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#如何选择束宽-B"><span class="toc-number">1.4.2.</span> <span class="toc-text">如何选择束宽 B</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#集束搜索的错误分析"><span class="toc-number">1.5.</span> <span class="toc-text">集束搜索的错误分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#单个样例分析"><span class="toc-number">1.5.1.</span> <span class="toc-text">单个样例分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#错误分析过程"><span class="toc-number">1.5.2.</span> <span class="toc-text">错误分析过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型评估——-Bleu-Score"><span class="toc-number">1.6.</span> <span class="toc-text">模型评估—— Bleu Score</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Unigram（单词）-Bleu-score"><span class="toc-number">1.6.1.</span> <span class="toc-text">Unigram（单词） Bleu score</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bigram（双词词组）-Bleu-score"><span class="toc-number">1.6.2.</span> <span class="toc-text">bigram（双词词组） Bleu score</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#n-gram（n-词词组）的-Bleu-score"><span class="toc-number">1.6.3.</span> <span class="toc-text">n-gram（n 词词组）的 Bleu score</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#组合-Bleu-score"><span class="toc-number">1.6.4.</span> <span class="toc-text">组合 Bleu score</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#注意力模型-Attention-Model"><span class="toc-number">2.</span> <span class="toc-text">注意力模型 Attention Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#长序列翻译存在的问题"><span class="toc-number">2.1.</span> <span class="toc-text">长序列翻译存在的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#注意力模型-intuition"><span class="toc-number">2.2.</span> <span class="toc-text">注意力模型 intuition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#如何计算注意力权重"><span class="toc-number">2.3.</span> <span class="toc-text">如何计算注意力权重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#注意力模型的例子"><span class="toc-number">2.4.</span> <span class="toc-text">注意力模型的例子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#语音识别"><span class="toc-number">3.</span> <span class="toc-text">语音识别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#语音识别问题"><span class="toc-number">3.1.</span> <span class="toc-text">语音识别问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#语音识别的-CTC-联结主义时间分类-损失函数"><span class="toc-number">3.2.</span> <span class="toc-text">语音识别的 CTC(联结主义时间分类) 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#触发词-激活词-检测系统"><span class="toc-number">3.3.</span> <span class="toc-text">触发词 (激活词) 检测系统</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">37</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">30</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_topimg.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="post-info"><div id="post-title">coursera 吴恩达深度学习 Specialization 笔记（course 5 week 3）—— 序列模型和注意力机制</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-02-11</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习笔记/">学习笔记</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>本周介绍了注意力机制 (Attention mechanism)，这个机制可以帮助模型理解给定一个输入后应该“关注”哪儿，同时学习如何处理语音识别和音频数据。</p>
<h2 id="sequence-to-sequence-模型"><a href="#sequence-to-sequence-模型" class="headerlink" title="sequence to sequence 模型"></a>sequence to sequence 模型</h2><h3 id="基本模型"><a href="#基本模型" class="headerlink" title="基本模型"></a>基本模型</h3><p>将法语变成英语：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_1.png" alt=""></p>
<p>输入编码器，然后用解码器输出：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_2.png" alt=""></p>
<a id="more"></a>
<p>另一个例子是图像标注解释，目标是输入一张图片，输出这张图片的解释文字，首先将图片通过一个预训练好的神经网络，例如 AlexNet，它可以学习输入图片的编码结果，最后输出一个图片的特征序列，这个网络可以作为上图中的编码器部分。将这个 4096 维的特征序列输入解码器，输出文字。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_3.png" alt=""></p>
<h3 id="如何挑出可能性最大的输出序列"><a href="#如何挑出可能性最大的输出序列" class="headerlink" title="如何挑出可能性最大的输出序列"></a>如何挑出可能性最大的输出序列</h3><p>首先我们看看序列-序列模型和语言模型的区别。</p>
<p>我们使用语言模型估计一个句子的概率 $P(y^{<1>},…,y^{&lt; T_ y  &gt;})$ :</1></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_4.png" alt=""></p>
<p>而机器翻译模型的解码器部分就和语言模型一样，而绿色的编码器部分输出相当于语言模型中的 $a^{&lt; 0  &gt;}$，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_5.png" alt=""></p>
<p>由于编码器的输出相当于 $a^{&lt; 0  &gt;}$，所以基本的语言模型变成了在 $x^{<1>},x^{<2>},…,x^{&lt; T _ x  &gt;}$的条件下的语言模型，称之为“条件语言模型”。</2></1></p>
<p>当使用该模型时，输入一个法语句子 x，在这个句子的条件下生成对应的英语句子，但问题是我们每次都可能得到不同的翻译，我们需要做的是找到一个句子使得这个条件概率最大化。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_6.png" alt=""></p>
<p>如何找到可能性最大的句子？一般使用定向搜索 beam search，见下节。</p>
<p>为何不使用贪婪搜索？</p>
<p>贪婪搜索指的是先选出可能性最大的第一个词 $\hat y^{<1>}​$，然后选出可能性最大的第二个单词，然后第三，第四……假设我们有两个句子：</1></p>
<ul>
<li>Jane is visiting Africa in Septemper.</li>
<li>Jane is going to be visiting Africa in September.</li>
</ul>
<p>它们是一句法语的两种翻译，明显第一种更好，但是当前两个单词是 Jane is 时，going 出现的概率比 visiting 会更大，因为 going 是更加常见的单词，所以贪婪搜索是不合适的。</p>
<h3 id="Beam-search-集束搜索"><a href="#Beam-search-集束搜索" class="headerlink" title="Beam search 集束搜索"></a>Beam search 集束搜索</h3><p>假设我们要实现输入法语 Jane visite l’Afrique en september. 最佳输出为 Jane is visiting Africa in September. 如何在众多输出中选出这个最佳的句子？</p>
<p>我们有一个 10000 词的词汇表 {a,aaron,…,in,…,jane,…,september,…,zulu}，设置束宽 beam width 为 B = 3. 即一次挑选出三个可能的词。</p>
<h4 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h4><p>首先我们将编码器的输出输入到解码器里，得到 $ \hat y^{<1>}$，它是一个 10000 维的向量，代表了给定法语单词 x，输出第一个词的概率 $P(y^{<1>}|x)$。</1></1></p>
<script type="math/tex; mode=display">
\hat y^{<1>}=\begin{bmatrix}  P(“a”|x) \\...\\ P(“in”|x) \\...\\ P(“jane”|x)\\...\\P(“september”|x)\\...\\P(“zulu”|x) \end{bmatrix}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_7.png" alt=""></p>
<p>根据 $P(y^{<1>}|x)​$，挑出概率值最大的 B (这里取 3) 个单词，例如 in、jane、September。</1></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_8.png" alt=""></p>
<h4 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h4><p>接下来我们确定第二个单词是什么。</p>
<p>对于 $y^{<1>}=“in”$，我们将单词 in 采样到下一层，得到 $ \hat y^{<2>}$，它同样是一个 10000  维的向量，代表了给定 $x,y^{<1>}$，输出第二个词的概率 $P(y^{<2>}|x,“in”）$。</2></1></2></1></p>
<script type="math/tex; mode=display">
\hat y^{<2>}=\begin{bmatrix}  P(“a”|x,y^{<1>}) \\...\\ P(“in”|x,y^{<1>}) \\...\\ P(“jane”|x,y^{<1>})\\...\\P(“september”|x,y^{<1>})\\...\\P(“zulu”|x,y^{<1>}) \end{bmatrix}</script><p>接着计算前两个单词的联合概率 $P(“in”,y^{<2>}|x)=P(“in”|x)P(y^{<2>}|x,“in”)​$，也即在给定法语单词 x 的情况下，输出 $“in”,y^{<2>}​$ 的概率。</2></2></2></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_10.png" alt=""></p>
<p>接着对 $y^{<1>}=“jane”​$，进行同样的操作，得到 $P(“jane”,y^{<2>}|x)​$。</2></1></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_11.png" alt=""></p>
<p>接着对 $y^{<1>}=“September”​$，进行同样的操作，得到 $P(“September”,y^{<2>}|x)​$。</2></1></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_12.png" alt=""></p>
<p>对于三个单词 in、jane、September 而言，每个单词后面有 10000 种可能，所以总共有 30000 种可能，在这 30000 种可能里根据 $P(y^{<1>},y^{<2>}|x)​$，挑选出 B = 3 种概率最大的组合，如下图红框所示。</2></1></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_14.png" alt=""></p>
<h4 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h4><p>现在我们内存里储存了 “in september”、“jane is”、“jane visits” 三种组合的 $P(y^{<1>},y^{<2>}|x)​$。</2></1></p>
<p>重复第二步的操作直到生成结束符<code>&lt;EOS&gt;</code>：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_15.png" alt=""></p>
<blockquote>
<p>当 B = 1 时，集束搜索退化为贪婪搜索。</p>
</blockquote>
<h3 id="集束搜索改进"><a href="#集束搜索改进" class="headerlink" title="集束搜索改进"></a>集束搜索改进</h3><h4 id="长度归一化"><a href="#长度归一化" class="headerlink" title="长度归一化"></a>长度归一化</h4><p>集束搜索的目标是找到使得 $P(y^{<1>},…,y^{&lt; T _ y  &gt;}|x)$ 最大的句子 y，即：</1></p>
<script type="math/tex; mode=display">
arg \space \max \limits_{y} P(y^{<1>},...,y^{<T_y>}|x)= arg \ \max\limits_y\prod_{t=1}^{T_y}P(y^{<t>}|x,y^{<1>},...,y^{<t-1>}) \\=P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})...P(y^{<T_y>}|x,y^{<1>},...,y^{<T_y>})</script><p>由于上式是由许多个小于1 的概率值相乘，所以最后会得到一个非常非常小的数，导致数值下溢，意思是无法用浮点数准确表达，所以在实际中我们会将上式取对数，即：</p>
<script type="math/tex; mode=display">
arg \ \max\limits_y\sum_{t=1}^{T_y}logP(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><p>还有一个问题就是，由于上式是由多个概率相乘得到的，如果一个句子越长，那么乘得项就越多，最后的概率也越低，也就是说，这个函数倾向于选择更短的输出，我们可以进行一个改动：</p>
<script type="math/tex; mode=display">
arg \ \max\limits_y \frac{1}{T^{\alpha}_y} \sum_{t=1}^{T_y}logP(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><ul>
<li>其中 $\alpha$ 决定了对长度进行归一化的柔和程度，1 表示完全的归一化，0 表示完全没有归一化</li>
</ul>
<h4 id="如何选择束宽-B"><a href="#如何选择束宽-B" class="headerlink" title="如何选择束宽 B"></a>如何选择束宽 B</h4><p>一般 B 越大，结果更好，但是速度更慢，在生产系统中，B 在 10~100 左右，而研究人员为了发论文很多使用 1000~3000 的束宽 B. </p>
<h3 id="集束搜索的错误分析"><a href="#集束搜索的错误分析" class="headerlink" title="集束搜索的错误分析"></a>集束搜索的错误分析</h3><h4 id="单个样例分析"><a href="#单个样例分析" class="headerlink" title="单个样例分析"></a>单个样例分析</h4><p>当我们对算法进行测试时，我们需要将模型生成的翻译句子与人工翻译的句子进行对比，算法有时候会生成错误的句子，这种错误的来源有两个方面：有可能是集束搜索产生了错误，束宽 B 选择的不正确；也有可能是 RNN 模型导致了错误。为了知道到底是哪个部分出现了问题并集中时间改进那个部分，我们需要对结果进行误差分析。</p>
<p>例如我们要翻译下面这个法语句子：Jane visite l’Afrique en septembre.</p>
<p>高质量的人类翻译：Jane visits Africa in September.（将这个句子记为 $y^*$）</p>
<p>RNN + 集束搜索生成的错误翻译：Jane visits Africa last September. （将这个句子记为 $\hat y$）</p>
<p>现在我们需要计算得到人类翻译的句子的概率 $P(y^*|x)$ 和得到模型预测的句子的概率 $P(\hat y|x)$，比较两者的大小。</p>
<p>如何计算呢?</p>
<p>我们首先看看 RNN 计算的是什么，</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_16.png" alt=""></p>
<script type="math/tex; mode=display">
\hat y^{<1 >}=\begin{bmatrix}  P(“a”|x) \\...\\ P(“in”|x) \\...\\ P(“jane”|x)\\...\\P(“september”|x)\\...\\P(“zulu”|x) \end{bmatrix} ,\hat y^{<2 >}=\begin{bmatrix}  P(“a”|x,y^{<1 >}) \\...\\ P(“in”|x,y^{<1 >}) \\...\\ P(“jane”|x,y^{<1 >})\\...\\P(“september”|x,y^{<1 >})\\...\\P(“zulu”|x,y^{<1 >}) \end{bmatrix}......</script><p>假设要计算得到 “Jane visits Africa in September.” 这个句子的概率就是：</p>
<script type="math/tex; mode=display">
P(“Jane \ visits \ Africa \ in \ September."|x) \\ =P("jane"|x)P("visits"|x,"jane")\cdot\cdot\cdot P(<EOS>|x,"jane",...,"september")\\ = \hat y^{<1>}["jane"] \cdot \hat y^{<2>}["visits"] \cdot \hat y^{<3>}["africa"]\cdot \cdot \cdot \hat y^{<1>}["<EOS>"]</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_17.png" alt=""></p>
<blockquote>
<p>注意：实际上，由于 $ y ^ * $ 和 $ \hat y $ 长度不同，为了比较  $ P(y ^ *| x) $ 和  $ P(\hat y |x)  $ 的大小，必须求长度<strong>归一化后</strong>的概率，这里先省略这一步</p>
</blockquote>
<p>接下来有两种情况：</p>
<ul>
<li><p>情况一：$P(y^*|x) &gt;P(\hat y |x)$ </p>
<p>集束搜索是根据  $P(y|x) $ 决定最终输出的， $ P(y^*|x) &gt;P(\hat y |x) $，按理说最终集束搜索应该会选择概率值更大的  $y^* $，但是真实的输出是  $\hat y $，这就说明在这种情况下，集束搜索出问题了，没有给你最大化  $P(y|x) $ 的那个句子 y.</p>
</li>
<li><p>情况二：$P(y^*|x) \leqslant P(\hat y |x)$</p>
<p>虽然  $y^* $ 是比  $\hat y $ 更好的翻译，但是 RNN 预测得到句子的概率  $P(y^*|x) \leqslant P(\hat y |x) $，也就是说 RNN 更有可能输出更差的  $\hat y $ 而不是更好的  $y ^* $，这就说明在这种情况下，RNN 模型是有问题的。</p>
</li>
</ul>
<h4 id="错误分析过程"><a href="#错误分析过程" class="headerlink" title="错误分析过程"></a>错误分析过程</h4><p>查看开发集并找出算法在开发集中的错误翻译（“错误”指得是跟人类的翻译比较差的多），记录每个样例的 $P(y^*|x)$ 和 $P(\hat y |x)$，并进行比较，按照上述两种情况找出导致这个样例子错误的原因，集束搜索还是 RNN？记录下来。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_18.png" alt=""></p>
<p>这样就能通过错误分析找出哪一部分错误是集束搜索引起的，哪一部分错误是 RNN 模型引起的。当你发现集束搜索引起了更多的错误时，我们因该努力增加束宽 B，反之，如果发现 RNN 是错误所在，可以试着加上正则化，获取更多的训练数据，或者换一个网络结构。</p>
<h3 id="模型评估——-Bleu-Score"><a href="#模型评估——-Bleu-Score" class="headerlink" title="模型评估—— Bleu Score"></a>模型评估—— Bleu Score</h3><p>对于一个法语句子，可能有多个很好的英语翻译，那么如何在有几个质量好的翻译结果的情况下 评估机器翻译系统呢？对于只有一个正确答案的图像识别， 只需要测量准确度就行了，那么如何在有多个好结果的情况下测量准确度呢？传统的方法是利用 BLEU（双语评估替补） 指数。 </p>
<p>假设我们有一个法语句子和两个人类的参考翻译：</p>
<ul>
<li>French: Le chat est sur le tapis.</li>
<li>参考 1 : The cat is on the mat.</li>
<li>参考 2 : There is a cat on the mat. </li>
</ul>
<h4 id="Unigram（单词）-Bleu-score"><a href="#Unigram（单词）-Bleu-score" class="headerlink" title="Unigram（单词） Bleu score"></a>Unigram（单词） Bleu score</h4><p>假设模型输出的结果为：</p>
<ul>
<li>the the the the the the the </li>
</ul>
<p>一种计算结果精确度 Precision 的方式为：看这个句子的每个单词是否出现在参考翻译（1 或 2）中，如果出现了则记为 1，否则记为 0，将所有单词的计数相加再除以句子的单词总数就是精确度了，则上面那个句子的 Precision 为 $\frac{7}{7}$ ，但是明显它是一个很糟糕的翻译，所以有改进的精确度计算方式：对于句子的每个单词（重复的不算），看该词<strong>最多</strong>出现在参考翻译中的次数，例如 the 在参考 1 中出现 2 次，在参考 2 中出现 1 次，所以 the 这个单词的得分最多是 2，而这个句子所有单词都是 the，那么总得分还是 2，所以修正后的精确度变为 $\frac{2}{7}$ .</p>
<h4 id="bigram（双词词组）-Bleu-score"><a href="#bigram（双词词组）-Bleu-score" class="headerlink" title="bigram（双词词组） Bleu score"></a>bigram（双词词组） Bleu score</h4><p>假设翻译结果：</p>
<ul>
<li>The cat the cat on the mat.</li>
</ul>
<p>找出句子所有双词词组：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">在句子中出现次数 Count</th>
<th style="text-align:center">在任何一个参考翻译中出现的最大次数 Count_clip</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">the cat</td>
<td style="text-align:center">2</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">cat the</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">cat on</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">on the</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">the mat</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">总和</td>
<td style="text-align:center">6</td>
<td style="text-align:center">4</td>
</tr>
</tbody>
</table>
</div>
<p>最后的双词组精确度就是 $\frac{4}{6}$.</p>
<h4 id="n-gram（n-词词组）的-Bleu-score"><a href="#n-gram（n-词词组）的-Bleu-score" class="headerlink" title="n-gram（n 词词组）的 Bleu score"></a>n-gram（n 词词组）的 Bleu score</h4><script type="math/tex; mode=display">
P_n=\frac{\sum\limits_{n-gram \in \hat y}Count_{clip}(n-gram)}{\sum\limits_{n-gram \in \hat y}Count(n-gram)}</script><ul>
<li>$P_n$ 指的是 n 词词组的 Bleu score</li>
<li>其中 $Count_{clip}$ 指的是某个词组在任何一个参考翻译中出现的最大次数，$Count$ 指的是某个词组在句子中出现的总次数</li>
</ul>
<h4 id="组合-Bleu-score"><a href="#组合-Bleu-score" class="headerlink" title="组合 Bleu score"></a>组合 Bleu score</h4><script type="math/tex; mode=display">
Combined  \ Bleu \ score = BP*e^{\frac{1}{n}\sum\limits_n P_n}</script><ul>
<li>其中 BP 指的是 brevity penalty 简短惩罚因子，如果输出非常简短的翻译，就很容易得到高精准度，因为可能大部分输出的字都在参考表里，但是我们不希望翻译是非常短的，因此，简短惩罚就是一个调整因素，用来惩罚翻译系统中输出的非常简短的翻译。<script type="math/tex; mode=display">
BP = \left\{\begin{matrix}
1 & 如果输出句子长度大于参考句子长度\\ 
e^{1-{输出长度}/{参考句子长度}} &  其他情况
\end{matrix}\right.</script></li>
</ul>
<h2 id="注意力模型-Attention-Model"><a href="#注意力模型-Attention-Model" class="headerlink" title="注意力模型 Attention Model"></a>注意力模型 Attention Model</h2><h3 id="长序列翻译存在的问题"><a href="#长序列翻译存在的问题" class="headerlink" title="长序列翻译存在的问题"></a>长序列翻译存在的问题</h3><p>传统的机器翻译采用 编码器-解码器 的模型，先将输入语句全部读入编码器得到一个编码向量，然后再通过解码器进行输出。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_16.png" alt=""></p>
<p>对于一个很长的句子而言，人类的翻译方式是先读一部分翻译一部分，再读一部分翻译一部分，模型采用的方式是先将<strong>整个</strong>句子进行读取再进行输出，但是模型无法记住特别长的句子，所以对于 编码器-解码器 翻译模型来说，模型的表现随着句子长度先增大后减小，如下图中的蓝线。而引入一种模拟人一段一段翻译的“注意力模型”，可以使得机器翻译系统的表现不随序列长度增大而减小，如下图绿线。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_19.png" alt=""></p>
<h3 id="注意力模型-intuition"><a href="#注意力模型-intuition" class="headerlink" title="注意力模型 intuition"></a>注意力模型 intuition</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_21.png" alt=""></p>
<p>其中：</p>
<ul>
<li>$t’$ 为输入序列，$t$ 为输出序列</li>
<li>$ \alpha ^ {&lt; t ,  t’  &gt;}$ 是注意力权重，表示 $y^{&lt; t  &gt;}$ 应该放在 $a^{&lt; t’  &gt;}$ 上的“注意力”的多少，且 $\sum\limits_{t’} \alpha^{&lt;t,  t’ &gt;}=1$ </li>
<li>$ c^{&lt; t  &gt;}=\sum\limits_{t’} \alpha^{&lt; t ,  t’  &gt;}a ^ {&lt; t’ &gt;} $  指的是整个上下文应该放在第 t 个输出的“注意力”</li>
<li>$a^{&lt; t’   &gt;}=(\stackrel{\rightarrow}{a}^{&lt; t’  &gt;},\stackrel{\leftarrow}{a}^{&lt;  t’ &gt;})$ 指双向 RNN 在 t‘ 时刻的激活值</li>
</ul>
<h3 id="如何计算注意力权重"><a href="#如何计算注意力权重" class="headerlink" title="如何计算注意力权重"></a>如何计算注意力权重</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_22.png" alt=""></p>
<p>为了得到每个词的注意力权重，我们可以将 $s^{&lt; t -1  &gt;}$ 和所有的激活值 $a^{&lt;  t’  &gt;}$ 输入一个只有一层的小型神经网络，得到 $e ^ {&lt; t , t’  &gt;}$，那么 $\alpha ^  {&lt; t ,  t’ &gt;}$ 就是 $e ^ {&lt; t , t’  &gt;}$ 的 softmax 值，然后交给模型自己去训练。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_23.png" alt=""></p>
<script type="math/tex; mode=display">
\alpha^{<t, t^{\prime}>}=\frac{\exp \left(e^{<t, t^{\prime}>}\right)}{\sum_{t^{\prime}=1}^{T_{X}} \exp \left(e^{<t, t^{\prime}>}\right)}</script><p>这个算法的一个问题是需要二次时间成本。</p>
<h3 id="注意力模型的例子"><a href="#注意力模型的例子" class="headerlink" title="注意力模型的例子"></a>注意力模型的例子</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_24.png" alt=""></p>
<p>上图是注意力权重的可视化，我们可以看到机器翻译输入和输出对应的词语具有很高的注意力权重。</p>
<h2 id="语音识别"><a href="#语音识别" class="headerlink" title="语音识别"></a>语音识别</h2><h3 id="语音识别问题"><a href="#语音识别问题" class="headerlink" title="语音识别问题"></a>语音识别问题</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_25.png" alt=""></p>
<p>以前语音识别系统基于“音素”，例如”The”有一个”de”音 “e”音，”Quick”有一个”ku”音 “u”音 “ik”音 “k”音，语言学家假设音素是声音的基本单元。然而，用端到端的深度学习，我们发现用音素表示声音不再是必要的，你可以开发一个系统,，输入一个音频片段，并直接输出文字，而不需要使用像这样的手工设计的表示方法。更大的数据集使这成为可能，很多学术论文，是基于上千个小时的数据集的，最好的商业性的系统是在超过 1 万甚至 10 万小时的音频数据集上训练的。标注的声频数据集，和深度学习算法极大地推动语音识别地进步。</p>
<h3 id="语音识别的-CTC-联结主义时间分类-损失函数"><a href="#语音识别的-CTC-联结主义时间分类-损失函数" class="headerlink" title="语音识别的 CTC(联结主义时间分类) 损失函数"></a>语音识别的 CTC(联结主义时间分类) 损失函数</h3><p>由于语音识别一般输入远远大于输出，如何定义损失函数？CTC 损失函数允许 RNN 生成这样的输出序列：“ttt_h_eee_ _ _ &lt;空格 &gt; _ _ _ qqq _ _” ，其中 _ 指空白字符，基本原则是：<strong>允许折叠那些没有被空白字符隔开的重复字符</strong>。所以上述输出变为“the q”。所以例如“the quick brown fox.” 这种短句子，我们可以往其中插入空白字符 _ 和字母重复来扩充到需要的长度。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_26.png" alt=""></p>
<h3 id="触发词-激活词-检测系统"><a href="#触发词-激活词-检测系统" class="headerlink" title="触发词 (激活词) 检测系统"></a>触发词 (激活词) 检测系统</h3><p>什么是触发词检测系统？</p>
<p>用 Alexa 这个词唤醒亚马逊的 echo， 用“小度你好”唤醒的百度的 BaiduDuerOS，用 Hey Siri 唤醒苹果的 Siri 智能语音助手，用 ok Google 来唤醒 Google Home.</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_27.png" alt=""></p>
<p>将训练语音输入模型，标签设置为触发词对应的时间点为 1 （为了保持 0 和 1 的平衡，可以令触发词后的一段时间都为 1），然后进行训练。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_5.3_28.png" alt=""></p>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/注意力机制/">注意力机制</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr1.png"><div class="post-qr-code__desc">微信捐赠</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="/img/qr2.jpg"><div class="post-qr-code__desc">支付宝捐赠</div></div></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="next-post pull-right"><a href="/2019/02/10/用 python 进行简单滤波器设计/"><span>用 python 进行简单滤波器设计</span><i class="fa fa-chevron-right"></i></a></div></nav><div class="post-adv"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=530 height=86 src="//music.163.com/outchain/player?type=2&id=566993785&auto=1&height=66"></iframe></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zODM1Mi8xNDg4MA=="><script>(function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
})(document, 'script');</script></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 - 2019 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>