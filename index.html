<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。"><meta name="keywords" content=""><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>chenyichen's blog | 在AI小白之路上踽踽独行 | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">17</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">15</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><nav id="nav" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/skater11.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="site-info"><div id="site-title">艺琛的 Livehouse</div><div id="site-sub-title">chenyichen's blog | 在AI小白之路上踽踽独行</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/yichenchan" target="_blank"><i class="fa fa-github"></i></a><a class="social-icon" href="https://weibo.com/5101047894/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo" target="_blank"><i class="fa fa-weibo"></i></a><a class="social-icon" href="c840098794@gmail.com" target="_blank"><i class="fa fa-email"></i></a><a class="social-icon" href="https://steamcommunity.com/id/840098794/" target="_blank"><i class="fa fa-steam"></i></a><a class="social-icon search"><i class="fa fa-search"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/10/吴恩达机器学习笔记c3w2/">coursera 吴恩达深度学习 Specialization 笔记（course 3 week 2）—— 机器学习策略（二）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a></span><div class="content"><h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p>如果你想得到一个训练算法来做人类可以做的任务，但是训练的算法还没有达到人类的效果，你就需要手动地检查算法中的错误，来知道你下一步该做什么，这个过程叫做错误分析。</p>
<h3 id="如何进行错误分析"><a href="#如何进行错误分析" class="headerlink" title="如何进行错误分析"></a>如何进行错误分析</h3><p>假设训练一个分类猫的算法，准确率为 90%，错误率为 10%，我们对分错的样本进行检查，发现分错的样本中有一些把狗认成了猫，这时是否应该将重心转移到使分类器对狗的表现更好？</p>
<p>错误分析：</p>
<ul>
<li>拿到约 <strong>100</strong> 张分类错误的开发集图片并进行手动检测<ul>
<li>一般不需要检测上万张图片，几百张就足够了！</li>
</ul>
</li>
<li>输出错误的图片里多少张狗</li>
</ul>
<p>假设只有 5 张，那么在错误图片中只有 5% 的狗，如果在狗的问题上花了大量的时间，那么就算是最好的情况，也<strong>最多</strong>只是把错误率从 10% 降到 9.5%，所以并不值得。但是假设 100 张错误图片里有 50 张狗的图片，那么可以确定很值得将时间花在狗身上，因为错误率<strong>最多</strong>可能从 10% 降到 5%.</p></div><a class="more" href="/2018/09/10/吴恩达机器学习笔记c3w2/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/08/吴恩达机器学习笔记c3w1/">coursera 吴恩达深度学习 Specialization 笔记（course 3 week 1）—— 机器学习策略（一）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a></span><div class="content"><p>现在来到 coursera 的 deeplearning.ai 课程的第三课，这门课程叫 Structuring Machine Learning Projects 结构化机器学习项目，将会学习到构建机器学习项目的一些策略和基本套路，防止南辕北辙。</p>
<h2 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化 (Orthogonalization)"></a>正交化 (Orthogonalization)</h2><h3 id="何为正交化"><a href="#何为正交化" class="headerlink" title="何为正交化"></a>何为正交化</h3><p>如同老式电视机一样，一个旋钮控制一个确定的功能，如屏幕横向伸长或纵向伸长，而不会一个旋钮控制两个属性。</p>
<h3 id="ML-的正交化"><a href="#ML-的正交化" class="headerlink" title="ML 的正交化"></a>ML 的正交化</h3><p>我们希望某个旋钮能单独让下面的某个步骤运行良好而不影响其他，这就叫正交化。</p>
<ul>
<li>首先训练集要拟合的很好<ul>
<li>否则减小偏差</li>
</ul>
</li>
<li>如果训练集运行良好，则希望开发集运行良好<ul>
<li>否则减小方差</li>
</ul>
</li>
<li>如果在训练集和开发集运行良好，则希望在测试集运行良好<ul>
<li>否则采用更大的开发集</li>
</ul>
</li>
<li>最后希望在真实世界表现良好<ul>
<li>否则调整开发集或者改变代价函数</li>
</ul>
</li>
</ul></div><a class="more" href="/2018/09/08/吴恩达机器学习笔记c3w1/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/06/吴恩达机器学习笔记c2w3/">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 3）—— 调参方法、批量归一化和深度学习框架</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/超参数/">超参数</a></span><div class="content"><p>本周主要学习超参数的系统调整，批量归一化和深度学习框架的相关知识。</p>
<h2 id="超参数的调整"><a href="#超参数的调整" class="headerlink" title="超参数的调整"></a>超参数的调整</h2><h3 id="调参的优先级"><a href="#调参的优先级" class="headerlink" title="调参的优先级"></a>调参的优先级</h3><ul>
<li>第一优先级：学习率 $\alpha$ </li>
<li>第二优先级：动量算法参数 $\beta$ (默认值0.9)，最小批的个数 mini-batch size，每层的隐藏单元数 hidden units</li>
<li>第三优先级：神经网络的层数 layers，学习率衰减 learning rate decay</li>
<li>几乎不需要调优：Adam 算法中的 $\beta_1,\beta_2,\varepsilon$，几乎每次只用默认值 $0.9,0.999,10^{-8}$    </li>
</ul>
<h3 id="调参的注意事项"><a href="#调参的注意事项" class="headerlink" title="调参的注意事项"></a>调参的注意事项</h3><ol>
<li><p>随机取样</p>
<p>当需要调整多个超参数时，尽量在网格中随机取样（下图右），而不是规则抽样（下图左）</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_1.png" alt=""></p>
</li>
</ol></div><a class="more" href="/2018/09/06/吴恩达机器学习笔记c2w3/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/04/吴恩达机器学习c2w2编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 2）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/优化算法/">优化算法</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/梯度下降/">梯度下降</a></span><div class="content"><p>本次作业实现了几种梯度下降算法，比较了它们的不同。</p>
<h2 id="普通梯度下降-BGD"><a href="#普通梯度下降-BGD" class="headerlink" title="　普通梯度下降 (BGD)"></a>　普通梯度下降 (BGD)</h2><p>所谓普通梯度下降就是一次处理所有的 m 个样本，也叫批量梯度下降 (Batch Gradient Descent)，公式为：</p>
<script type="math/tex; mode=display">
W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\\
b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script></div><a class="more" href="/2018/09/04/吴恩达机器学习c2w2编程作业/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/02/吴恩达机器学习笔记c2w2/">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 2）—— 优化算法的改进</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络优化/">神经网络优化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/优化算法/">优化算法</a></span><div class="content"><p>本周主要学习不同的参数优化方式，提高模型的训练速度。</p>
<h2 id="小批量梯度下降算法-mini-batch-gradient-descent"><a href="#小批量梯度下降算法-mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降算法 (mini-batch gradient descent)"></a>小批量梯度下降算法 (mini-batch gradient descent)</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><p>向量化可以有效率地同时计算 m 个样本，但是当样本数为几百万时，速度依然会很慢，每一次迭代都必须先处理几百万的数据集才能往前一步，所以我们可以使用这个算法进行加速。</p>
<p>首先我们将训练集划分为一个一个微小的训练集，也就是小批量训练集 (mini-batch)，比如每一个微小训练集只有 1000 个样本：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_2.png" alt=""></p></div><a class="more" href="/2018/09/02/吴恩达机器学习笔记c2w2/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/01/吴恩达机器学习c2w1编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 1）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络优化/">神经网络优化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/正则化/">正则化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/初始化/">初始化</a></span><div class="content"><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>这次编程作业对比了三种不同的初始化方法的不同，三种方法分别是“零初始化”、“随机初始化”、“He 初始化”。</p>
<p>这是所用的数据，我们要将红点和蓝点分类：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_1.png" alt=""></p></div><a class="more" href="/2018/09/01/吴恩达机器学习c2w1编程作业/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/01/beck观后感/">《Beck》观后感</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/读书观影笔记/">读书观影笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/摇滚/">摇滚</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/音乐/">音乐</a></span><div class="content"><p>　　我终于找到我的本命番了，它的名字叫《Beck》，为啥我知道这是本命番呢，因为这是第一部我看到最后迟迟舍不得看完的番，一共２６集，每天睡前看几集，看了有十几天，不敢一次看完，因为看完了我的梦就碎了，是的，这是一个追梦的故事，我相信任何热爱摇滚，热爱音乐，热爱吉他，渴望或者已经开始玩乐队的人，都会对这部番有着深深的感动和发自内心的共鸣。因为这部番，我拿起了好久没碰的木琴，并开始挑选我的第一把电吉他……</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_topimg.jpg" alt=""></p></div><a class="more" href="/2018/09/01/beck观后感/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/08/29/吴恩达机器学习笔记c2w1/">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 1）—— 正则化等</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络优化/">神经网络优化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/正则化/">正则化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/初始化/">初始化</a></span><div class="content"><p>deeplearning.ai 的第二个课程名为 <strong>改进深度神经网络：超参数调整，正则化和优化 (Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization)</strong> ，这门课程教你使深度学习表现更好的“魔法”，而不是把神经网络当作一个黑箱，你会理解什么使得神经网络表现更好，而且能更系统地得到好的结果，你还会学到一些 tensorflow 知识。通过三周的学习，你将能够：</p>
<ul>
<li>了解构建深度学习应用程序的行业最佳实践</li>
<li>能够有效地使用常见的神经网络技巧，包括初始化，L2 正则化和丢失正则化，批量归一化，梯度检查</li>
<li>能够实现和应用各种优化算法，例如小批量梯度下降，动量，PMSprop 和 Adam，并检查它们的收敛性</li>
<li>了解如何设置训练/开发/测试集和分析偏差/方差</li>
<li>能够在 TensorFlow 上实现神经网络</li>
</ul></div><a class="more" href="/2018/08/29/吴恩达机器学习笔记c2w1/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/08/27/吴恩达机器学习c1w4编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 4）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-27</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络/">神经网络</a></span><div class="content"><h1 id="第一部分：基本架构"><a href="#第一部分：基本架构" class="headerlink" title="第一部分：基本架构"></a>第一部分：基本架构</h1><p>这是深度学习专项课程第一课第四周的编程作业的第一部分，通过这一部分，可以学到：</p>
<ul>
<li>使用非线性单元比如 ReLU 来提高模型</li>
<li>建立一个更深的神经网络（大于一个隐藏层）</li>
<li>实现一个易用的神经网络类</li>
</ul>
<h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 包的引入</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline <span class="comment"># %符号为ipython的魔法函数，与画图有关，在pycharm中会报错</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure></div><a class="more" href="/2018/08/27/吴恩达机器学习c1w4编程作业/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/08/15/吴恩达机器学习笔记c1w4/">coursera 吴恩达深度学习 Specialization 笔记（course 1 week 4）—— 深度神经网络</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络/">神经网络</a></span><div class="content"><p>本周主要介绍<strong>深度神经网络</strong> (Deep Neural Networks)。</p>
<h2 id="深度神经网络概况"><a href="#深度神经网络概况" class="headerlink" title="深度神经网络概况"></a>深度神经网络概况</h2><h3 id="什么是深度神经网络"><a href="#什么是深度神经网络" class="headerlink" title="什么是深度神经网络"></a>什么是深度神经网络</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.1.png" alt=""></p>
<p>所谓深浅取决于神经网络的层数，例如左上角的逻辑回归模型是一个“最浅的”神经网络，而右下角的神经网络具有五个隐藏层，可以算得上是深度神经网络。</p></div><a class="more" href="/2018/08/15/吴恩达机器学习笔记c1w4/#more">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>