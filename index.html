<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。"><meta name="keywords" content=""><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>chenyichen's blog | 在AI小白之路上踽踽独行 | 艺琛的 Livehouse</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">22</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">19</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><nav id="nav" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/skater11.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">艺琛的 Livehouse</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="site-info"><div id="site-title">艺琛的 Livehouse</div><div id="site-sub-title">chenyichen's blog | 在AI小白之路上踽踽独行</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/yichenchan" target="_blank"><i class="fa fa-github"></i></a><a class="social-icon" href="https://weibo.com/5101047894/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo" target="_blank"><i class="fa fa-weibo"></i></a><a class="social-icon" href="c840098794@gmail.com" target="_blank"><i class="fa fa-email"></i></a><a class="social-icon" href="https://steamcommunity.com/id/840098794/" target="_blank"><i class="fa fa-steam"></i></a><a class="social-icon search"><i class="fa fa-search"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/19/吴恩达机器学习c4w2编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 2）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-19</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/cnn/">cnn</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/Keras/">Keras</a></span><div class="content"><h2 id="Keras-教程"><a href="#Keras-教程" class="headerlink" title="Keras 教程"></a>Keras 教程</h2><ul>
<li>Keras 是一个高级神经网络框架，用 Python 写成，在 TensorFlow 和 CNTK 等一些更低级的框架上运行，拥有比 tensorflow 更高的抽象</li>
<li>Keras 能够快速搭建和试验不同的模型</li>
<li>Keras 比低级框架限制更多，无法实现一些非常复杂的模型，但是对一些普通模型运行很好</li>
</ul>
<p>这里用 Keras 实现一个识别图片中的人是否开心的算法。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_1.png" alt=""></p>
<p>先引入我们需要的包：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> kt_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p>加载数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">Y_train = Y_train_orig.T</span><br><span class="line">Y_test = Y_test_orig.T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_3.png" alt=""></p>
<p>在 Keras 中训练测试模型有四个步骤：</p>
<ul>
<li><p>建立模型</p>
<ul>
<li><p>自己定义一个 model() 函数进行前向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HappyModel</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the HappyModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- 输入图片的维度（不包括图片的数量！）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero-Padding: pads the border of X_input with zeroes</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CONV -&gt; BN -&gt; RELU Block applied to X</span></span><br><span class="line">    X = Conv2D(<span class="number">32</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">1</span>, <span class="number">1</span>), name = <span class="string">'conv0'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn0'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MAXPOOL</span></span><br><span class="line">    X = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">'max_pool'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FLATTEN X (means convert it to a vector) + FULLYCONNECTED</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>, name=<span class="string">'fc'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model. This creates your Keras model instance, you'll use this instance to train/test the model.</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'HappyModel'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel = HappyModel(X_train.shape[<span class="number">1</span>:])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>编译模型</p>
<ul>
<li><p>model.compile(optimizer = “…”, loss = “…”, metrics = [“accuracy”])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.compile(optimizer = <span class="string">'Adam'</span>, loss = <span class="string">'binary_crossentropy'</span>, metrics = [<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>在训练数据上训练模型</p>
<ul>
<li><p>model.fit(x = …, y = …, epochs = …, batch_size = …)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.fit(x = X_train, y = Y_train, epochs = <span class="number">40</span>, batch_size = <span class="number">16</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_4.png" alt=""></p>
</li>
</ul>
</li>
<li><p>在测试数据上测试模型</p>
<ul>
<li><p>model.evaluate(x = …, y = …)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">preds = happyModel.evaluate(x = X_test, y = Y_test)</span><br><span class="line">print(preds)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_5.png" alt=""></p>
</li>
</ul>
</li>
</ul>
<p>最后可以达到 98% 的训练集精确度，90% 的训练集精确度。</p>
<p>我们可以用自己的图片进行测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">'images/45.jpg'</span>	<span class="comment"># 用自己的图片路径</span></span><br><span class="line"></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">imshow(img)</span><br><span class="line"></span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line"></span><br><span class="line">print(happyModel.predict(x))</span><br><span class="line"><span class="keyword">if</span> happyModel.predict(x) == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">'这个人不开心哦 :('</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'这个人在笑哦 :)'</span>)</span><br></pre></td></tr></table></figure>
<p>我们还可以获得模型的概况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.summary()</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_6.png" alt=""></p>
<p>还可以将模型流程图打印出来并保存为位图文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(happyModel, to_file=<span class="string">'HappyModel.png'</span>)</span><br><span class="line">SVG(model_to_dot(happyModel).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_7.png" alt=""></p>
<h2 id="用-Keras-创建-ResNet-残差网络"><a href="#用-Keras-创建-ResNet-残差网络" class="headerlink" title="用 Keras 创建 ResNet 残差网络"></a>用 Keras 创建 ResNet 残差网络</h2><p>这节会用残差网络创建一个非常深的卷积网络。</p>
<ul>
<li>实现基本的残差网络组块</li>
<li>将这些组块合在一起实现一个最先进的神经网络图像分类器</li>
</ul>
<h3 id="包"><a href="#包" class="headerlink" title="包"></a>包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> resnets_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line">K.set_learning_phase(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="非常深的神经网络的问题所在"><a href="#非常深的神经网络的问题所在" class="headerlink" title="非常深的神经网络的问题所在"></a>非常深的神经网络的问题所在</h3><ul>
<li>优点：很深的网络能学习到非常多层次的抽象特征，从边缘到复杂特征。</li>
<li>缺点：训练时会产生梯度下降，使得训练速度极慢。</li>
</ul>
<h3 id="残差网络的基本-block-组块"><a href="#残差网络的基本-block-组块" class="headerlink" title="残差网络的基本 block 组块"></a>残差网络的基本 block 组块</h3><p>有两种主要的组块在残差网络中，取决于输入/输出维度是否相同。</p>
<ol>
<li><p>The identity block （输入/输出维度相同）</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_8.png" alt=""></p>
<p>First component of main path: </p>
<ul>
<li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. Use 0 as the seed for the random initialization. </li>
<li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>Second component of main path:</p>
<ul>
<li>The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is “same” and its name should be <code>conv_name_base + &#39;2b&#39;</code>. Use 0 as the seed for the random initialization. </li>
<li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>Third component of main path:</p>
<ul>
<li>The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2c&#39;</code>. Use 0 as the seed for the random initialization. </li>
<li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li>
</ul>
<p>Final step: </p>
<ul>
<li>The shortcut and the input are added together.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The identity block 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X, f, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Second component of main path </span></span><br><span class="line">    X = Conv2D(filters = F2, kernel_size = (f,f), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'same'</span>, name = conv_name_base + <span class="string">'2b'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path </span></span><br><span class="line">    X = Conv2D(filters = F3, kernel_size = (<span class="number">1</span>,<span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2c'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2c'</span> )(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation </span></span><br><span class="line">    X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">"relu"</span>)(X)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<blockquote>
<p>最后的 X 加上 X_shoutcut 应该使用 Keras 内置的 Add()([]) 函数，否则后面会报错：’Tensor’ object has no attribute ‘_keras_history’</p>
</blockquote>
</li>
<li><p>The convolutional block（输入/输出维度不相同）</p>
<p>当输入输出维度不相同时，在 shourtcut 这条路径使用一个卷积操作来使得维度相同。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_9.png" alt=""></p>
<p>各层的细节：</p>
<p>First component of main path:</p>
<ul>
<li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. </li>
<li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>Second component of main path:</p>
<ul>
<li>The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be <code>conv_name_base + &#39;2b&#39;</code>.</li>
<li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>Third component of main path:</p>
<ul>
<li>The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be <code>conv_name_base + &#39;2c&#39;</code>.</li>
<li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li>
</ul>
<p>Shortcut path:</p>
<ul>
<li>The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;1&#39;</code>.</li>
<li>The BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;1&#39;</code>. </li>
</ul>
<p>Final step: </p>
<ul>
<li>The shortcut and the main path values are added together.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>实现的代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The convolutional block 函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X, f, filters, stage, block, s = <span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the convolutional block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    s -- Integer, specifying the stride to be used</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value</span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path </span></span><br><span class="line">    X = Conv2D(F1, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path</span></span><br><span class="line">    X = Conv2D(F2, (f, f), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">'same'</span>, name = conv_name_base + <span class="string">'2b'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path </span></span><br><span class="line">    X = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), name = conv_name_base + <span class="string">'2c'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### SHORTCUT PATH ####</span></span><br><span class="line">    X_shortcut = Conv2D(F3, (<span class="number">1</span>,<span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">'1'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'1'</span> )(X_shortcut)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = X = Add()([X,X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="建立一个-50-层的残差网络模型"><a href="#建立一个-50-层的残差网络模型" class="headerlink" title="建立一个 50 层的残差网络模型"></a>建立一个 50 层的残差网络模型</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.2_10.png" alt=""></p>
<p>这个模型的细节为：</p>
<ul>
<li>Zero-padding pads the input with a pad of (3,3)</li>
<li>Stage 1:<ul>
<li>The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”.</li>
<li>BatchNorm is applied to the channels axis of the input.</li>
<li>MaxPooling uses a (3,3) window and a (2,2) stride.</li>
</ul>
</li>
<li>Stage 2:<ul>
<li>The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”.</li>
<li>The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”.</li>
</ul>
</li>
<li>Stage 3:<ul>
<li>The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”.</li>
<li>The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”.</li>
</ul>
</li>
<li>Stage 4:<ul>
<li>The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”.</li>
<li>The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”.</li>
</ul>
</li>
<li>Stage 5:<ul>
<li>The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”.</li>
<li>The 2 identity blocks use three set of filters of size [512, 512, 2048], “f” is 3 and the blocks are “b” and “c”.</li>
</ul>
</li>
<li>The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”.</li>
<li>The flatten doesn’t have any hyperparameters or name.</li>
<li>The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be <code>&#39;fc&#39; + str(classes)</code>.</li>
</ul>
<p>实现代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50 层的残差网络模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape = <span class="params">(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>)</span>, classes = <span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the popular ResNet50 the following architecture:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3</span></span><br><span class="line"><span class="string">    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string">    classes -- integer, number of classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">64</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">2</span>, <span class="number">2</span>), name = <span class="string">'conv1'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X) <span class="comment">#  kernel_initializer = glorot_uniform 表示 Xavier 均匀初始化</span></span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">'bn_conv1'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage = <span class="number">2</span>, block=<span class="string">'a'</span>, s = <span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 3 </span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], stage = <span class="number">3</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">'d'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 4</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'c'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'d'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'e'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">'f'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 5 </span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block = <span class="string">'a'</span>, s = <span class="number">2</span> )</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], stage = <span class="number">5</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># AVGPOOL</span></span><br><span class="line">    X = AveragePooling2D((<span class="number">2</span>,<span class="number">2</span>), name = <span class="string">'avg_pool'</span>)(X)</span><br><span class="line">   </span><br><span class="line">    <span class="comment"># output layer</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(classes, activation=<span class="string">'softmax'</span>, name=<span class="string">'fc'</span> + str(classes), kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)	<span class="comment">#  kernel_initializer = glorot_uniform 表示 Xavier 均匀初始化</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create model</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">'ResNet50'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h3 id="进行训练和预测"><a href="#进行训练和预测" class="headerlink" title="进行训练和预测"></a>进行训练和预测</h3><ul>
<li><p>建立模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = ResNet50(input_shape = (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>), classes = <span class="number">6</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>编译模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练模型</p>
<ul>
<li><p>载入数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert training and test labels to one hot matrices</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, Y_train, epochs = <span class="number">2</span>, batch_size = <span class="number">32</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>进行预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">preds = model.evaluate(X_test, Y_test)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="测试自己的图片"><a href="#测试自己的图片" class="headerlink" title="测试自己的图片"></a>测试自己的图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">'images/my_image.jpg'</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line">print(<span class="string">'Input image shape:'</span>, x.shape)</span><br><span class="line">my_image = scipy.misc.imread(img_path)</span><br><span class="line">imshow(my_image)</span><br><span class="line">print(<span class="string">"class prediction vector [p(0), p(1), p(2), p(3), p(4), p(5)] = "</span>)</span><br><span class="line">print(model.predict(x))</span><br></pre></td></tr></table></figure>
<h3 id="对模型进行总览"><a href="#对模型进行总览" class="headerlink" title="对模型进行总览"></a>对模型进行总览</h3><ul>
<li><p>表格</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
</li>
<li><p>位图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(model, to_file=<span class="string">'model.png'</span>)</span><br><span class="line">SVG(model_to_dot(model).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>实际中非常深的普通网络不起作用，因为梯度下降它们很难训练</li>
<li>跳跃连接帮忙解决跳跃连接问题，它们也使得残差网络很容易学习 identity function</li>
<li>有两种主要的组块：The identity block and the convolutional block</li>
<li>把这些组块堆叠起来可以形成很深的残差网络</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/17/吴恩达机器学习笔记c4w2/">coursera 吴恩达深度学习 Specialization 笔记（course 4 week 2）—— 深度卷积网络的案例研究</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-17</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/cnn/">cnn</a></span><div class="content"><p>本周我们直接从研究论文里学习深度 CNN 的一些使用技巧和方法。</p>
<h2 id="outline"><a href="#outline" class="headerlink" title="outline"></a>outline</h2><ul>
<li><p>一些经典的网络模型：</p>
<ul>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>VGGNet</li>
</ul>
</li>
<li><p>残差卷积网络 ResNet</p>
</li>
<li>初始神经网络实例</li>
</ul>
<h2 id="经典卷积网络模型"><a href="#经典卷积网络模型" class="headerlink" title="经典卷积网络模型"></a>经典卷积网络模型</h2><h3 id="LeNet－５"><a href="#LeNet－５" class="headerlink" title="　LeNet－５"></a>　LeNet－５</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_2.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_1.png" alt=""></p>
<ul>
<li>60000 个参数</li>
<li>从左到右 $n_H,n_W$ 减小，$n_C$ 增大</li>
<li>一个至今使用的模式：conv -&gt; pool -&gt; conv -&gt; pool -&gt; FC -&gt; FC -&gt; output</li>
<li>这篇论文中使用的是 sigmoid/tanh 函数，而不是 ReLU</li>
<li>由于论文较久远，吴恩达建议只读论文的第二章，快速看一下第三章</li>
</ul>
<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_3.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_4.png" alt=""></p>
<ul>
<li>与 LetNet 很相似，但是大得多，大概六千万个参数</li>
<li>使用 ReLU 激活函数</li>
<li>使用两块 GPU 训练</li>
<li>局部响应归一层 LRN</li>
</ul>
<h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_5.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_6.png" alt=""></p>
<ul>
<li>超过一亿三千万个参数，十分庞大的网络</li>
<li>结构非常统一，缺点是网络过于庞大</li>
</ul>
<p>建议先读 AlexNet 的论文，再读 VGG-16 的论文，最后读最难的 LeNet。</p>
<h2 id="残差网络-ResNet"><a href="#残差网络-ResNet" class="headerlink" title="残差网络 ResNet"></a>残差网络 ResNet</h2><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_7.png" alt=""></p>
<p>训练网络很深时会发生梯度消失或者梯度爆炸，利用“跳跃连接” ( skip connection)，可以训练很深很深的残差网络。</p>
<h3 id="残差结构-residual-block"><a href="#残差结构-residual-block" class="headerlink" title="残差结构 residual block"></a>残差结构 residual block</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_8.png" alt=""></p>
<ul>
<li>绿色的叫<strong>主路径 (main  path)</strong></li>
<li>紫色的叫<strong>快捷路径 (short cut)</strong> 或 <strong>跳跃连接 (skip connection)</strong></li>
<li>最后两者合一得到的 $a^{[l+1]} = g(z^{[l+2]}+a^{[l]})$ </li>
</ul>
<h3 id="残差网络-Residual-Network"><a href="#残差网络-Residual-Network" class="headerlink" title="残差网络 Residual Network"></a>残差网络 Residual Network</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_9.png" alt=""></p>
<h3 id="性能对比"><a href="#性能对比" class="headerlink" title="性能对比"></a>性能对比</h3><p>对于一个普通网络 (plain network)，随着层数的增大，理论上训练误差应该减小，但是实际上是先减小后增大，如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_10.png" alt=""></p>
<p>而如果使用 ResNet，即使训练一个 1000 层的网络，性能也不会先增后降。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_11.png" alt=""></p>
<h3 id="为什么-ResNet-有用"><a href="#为什么-ResNet-有用" class="headerlink" title="为什么 ResNet 有用"></a>为什么 ResNet 有用</h3><p>假设我们在一个网络后面加上一个残差结构，如下图所示。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_12.png" alt=""></p>
<p> 所有的激活函数使用的都是 ReLU，所以 $a \geq 0 $ </p>
<p>$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(w^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$</p>
<p>加入使用正则化使权重衰减，w 和 b 变得很小，如果没有残差结构，那么可能会发生梯度消失，但是如果有残差结构，那么就算 w 和 b 衰减到 0，$a^{[l+2]}=g(a^{[l]})=ReLU(a^{[l]})=a^{[l]}$，所以额外增加两层也不会损害神经网络的表现。</p>
<p>要实现跳跃连接，我们必须假设 $z^{[l+2]}$ 和 $a^{[l]}$ 是同维度，这样才能相加，所以深度残差网络里面很多  <strong>same 卷积</strong>，保证卷积之后维度不变。</p>
<p>但是有时候 $z^{[l+2]}$ 和 $a^{[l]}$ 维度不同，比如 256 和 128，我们可以加上一个系数 $W_S$：</p>
<p>$a^{[l+2]}=g(w^{[l+2]}a^{[l+1]}+b^{[l+2]}+W_sa^{[l]})$</p>
<p>此处$W_S$ 是一个 256×128 的矩阵。  </p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_13.png" alt=""></p>
<h2 id="Inception-网络"><a href="#Inception-网络" class="headerlink" title="Inception 网络"></a>Inception 网络</h2><h3 id="1-1-卷积"><a href="#1-1-卷积" class="headerlink" title="1*1 卷积"></a>1*1 卷积</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_14.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_15.png" alt=""></p>
<p>1×1 卷积本质上就是一个全连接神经网络，逐一作用于这 6×6=36 个不同的位置，这个小神经网络接收一行 32 个数的输入，然后输出过滤器数个输出值。这个有时被称作“网中网”。</p>
<p>一个实例：</p>
<p>我们可以用 1×1 卷积来缩小通道数：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_16.png" alt=""></p>
<h3 id="Inception-network-引例"><a href="#Inception-network-引例" class="headerlink" title="Inception network 引例"></a>Inception network 引例</h3><p>在设计卷积网络的某一层的时候，有可能会用到1×1的卷积层或者3×3的卷积层或者5×5的卷积层或者池化层，Inception 网络在某一层将这些全部用上，使得网络更复杂，效果也更好。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_17.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_18.png" alt=""></p>
<p>我们将输出进行不同的卷积操作，但是都进行 “same padding”，使得输出维度都一样，最后将所有的输出堆叠起来，这就形成了 Inception 网络的一个模块。</p>
<h3 id="计算成本问题"><a href="#计算成本问题" class="headerlink" title="计算成本问题"></a>计算成本问题</h3><p>我们看 5×5 的卷积核。</p>
<ol>
<li><p>像上图一样直接将 5×5 的卷积核与 28×28×192 的输入进行卷积，得到 28×28×32 的输出</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_19.png" alt=""></p>
<p>最后进行的乘法运算次数为 28×28×32×5×5×192 约等于 1.2亿 次</p>
</li>
<li><p>在进行 5×5 卷积之前进行一个 1×1 卷积（<strong>瓶颈层</strong>），先将通道数减到 16，再进行 5×5 卷积，得到 28×28×32 的输出</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_20.png" alt=""></p>
<p>最后的运算次数为 28×28×16×192×1×1 + 28×28×32×5×5×16 约为 1240万 次，是第一种方法的十分之一运算量。</p>
</li>
</ol>
<h3 id="Inception-模块"><a href="#Inception-模块" class="headerlink" title="Inception 模块"></a>Inception 模块</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_21.png" alt=""></p>
<h3 id="Inception-网络-1"><a href="#Inception-网络-1" class="headerlink" title="Inception 网络"></a>Inception 网络</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_22.png" alt=""></p>
<ul>
<li><p>红色部分是一个一个 Inception 模块</p>
</li>
<li><p>绿色的旁支是以隐藏层作为输入的 softmax 函数，防止网络太深了过拟合</p>
</li>
<li><p>这个网络是谷歌开发的 gooleNet，用来纪念 Lecun 的 LeNet</p>
</li>
<li><p>Inception 这个名字来源于《盗梦空间 (Inception)》表情包：），作者们把这个图片当成了构建更加有深度的神经网络的动机：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_23.png" alt=""></p>
</li>
</ul>
<h2 id="搭建卷积网络的小技巧"><a href="#搭建卷积网络的小技巧" class="headerlink" title="搭建卷积网络的小技巧"></a>搭建卷积网络的小技巧</h2><h3 id="使用开源资源"><a href="#使用开源资源" class="headerlink" title="使用开源资源"></a>使用开源资源</h3><ul>
<li>使用文献中的网络结构</li>
<li>使用开源的网络结构算法实现</li>
<li>使用人家预训练好的模型然后在自己的数据集上进行微调</li>
</ul>
<h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p>如果你想实现一个计算机视觉应用而不想从零开始训练权重，实现更快的方式通常是下载已经训练好权重的网络结构，把这个作为预训练迁移到你感兴趣的新任务上，计算机视觉的研究社区已经很擅长把许多数据库发布在网络上，如 ImageNet、MSCOCO、PASCAL 等数据库，许多计算机视觉的研究者已经在上面训练了自己的算法，有时训练要耗费好几周时间，占据许多 GPU，事实上有人已经做过这种训练，这意味着你可以下载这些开源的权重为你自己的神经网络做好的初始化开端，且可以用迁移学习来迁移知识，从这些大型公共数据库迁移知识到你自己的问题上。</p>
<p>假如我们要识别家里的两只猫，训练数据量不够，我们可以从网上下载一些训练好的模型，比如在有 1000 类物体的 ImageNet 数据库上训练的模型。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_24.png" alt=""></p>
<p>将紫色框住的部分全部“冻结住”，可训练参数为零，将这部分看成一个模块或者函数，我们需要做的是把 1000 分类的 softmax 改成 3 分类的然后训练 softmax 这部分的参数即可。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_25.png" alt=""></p>
<p>当你有更大的带标签数据集，你可以冻结更少的层数。只冻结前面这些层，然后训练后面这些层，（1）你可以用最后几层的权重，作为初始化开始做梯度下降，把 softmax 改成自己需要的（2）或者你也可以去掉最后几层，然后用自己的新神经元和新 softmax 输出。原则是：你数据越多，所冻结的层数可以越少，自己训练的层数可以越多。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_26.png" alt=""></p>
<p>最后，如果你有许多数据，你可以用该开源网络和权重来初始化整个网络然后训练。</p>
<h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在计算机视觉领域，更多的数据对几乎所有的计算机视觉工作都有好处，但是对于绝大多数问题，总是不能获得足够多的数据，这就需要数据增强。</p>
<ol>
<li><p>普通数据增强方式</p>
<p>镜像 mirroring</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_27.png" alt=""></p>
<p>随机裁剪 random cropping</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_28.png" alt=""></p>
<p>旋转 rotation</p>
<p>剪切 shearing</p>
<p>局部弯曲 local warping</p>
<p>……</p>
</li>
<li><p>色彩变化 color shifting</p>
<p>给三个颜色通道 R G B 增加不同的值，可以使用 PCA 主成份分析算法来实现。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_29.png" alt=""></p>
</li>
</ol>
<h4 id="数据增强线程"><a href="#数据增强线程" class="headerlink" title="数据增强线程"></a>数据增强线程</h4><p>数据存在硬盘中，在进入 CPU/GPU 训练之前使用几个 CPU 线程进行失真处理（数据增强），训练线程和失真线程可以并行处理。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_30.png" alt=""></p>
<h3 id="目前计算机视觉的状态"><a href="#目前计算机视觉的状态" class="headerlink" title="目前计算机视觉的状态"></a>目前计算机视觉的状态</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_31.png" alt=""></p>
<p>机器学习的两种知识来源：</p>
<ul>
<li>带标签的数据</li>
<li>手工构建的 特征/网络结构/其他组件</li>
</ul>
<p>当我们拥有大量的数据，我们只需要简单的算法和更少的手工构建部分，而当我们数据量不够时，就需要更多的手工构建部分，需要花更多时间在设计网络结构上。传统的计算机视觉论文，相当依赖复杂的手工构建，就算近些年随着数据量的增加，手工构建的数量明显减少，但还是有很多手工设计的网络架构，所以超参数选择在计算机视觉比起其他领域更加复杂。</p>
<h3 id="在基准数据集上做的好-赢得比赛-的技巧"><a href="#在基准数据集上做的好-赢得比赛-的技巧" class="headerlink" title="在基准数据集上做的好 / 赢得比赛 的技巧"></a>在基准数据集上做的好 / 赢得比赛 的技巧</h3><p>这些方法在比赛时可以使得结果准确度增加一些微弱的优势，但是会增加很多运算成本，所以这些东西并不会真的用在实际的服务客户的系统上。</p>
<ul>
<li><p>Ensembling 总效果</p>
<ul>
<li>独立训练几个网络然后取输出值的平均值</li>
</ul>
</li>
<li><p>multi-crop at test time 在测试时多重裁剪</p>
<ul>
<li><p>在测试图像的多个裁剪子图像中运行分类器然后求结果平均值</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.2_32.png" alt=""></p>
</li>
</ul>
</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/10/16/吴恩达机器学习c4w1编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 4 week 1）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-10-16</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/cnn/">cnn</a></span><div class="content"><h1 id="cnn-辅助函数的构建"><a href="#cnn-辅助函数的构建" class="headerlink" title="cnn 辅助函数的构建"></a>cnn 辅助函数的构建</h1><p>这周的编程作业内容是使用 numpy 实现卷积层和池化层，包括前向传播和方向传播。</p>
<h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="作业大纲"><a href="#作业大纲" class="headerlink" title="作业大纲"></a>作业大纲</h2><ul>
<li>卷积函数，包括：<ul>
<li>零填充</li>
<li>卷积窗口</li>
<li>卷积前向传播</li>
<li>卷积反向传播</li>
</ul>
</li>
<li>池化函数，包括：<ul>
<li>池化前向传播</li>
<li>创造 mask</li>
<li>Distribute value</li>
<li>池化反向传播</li>
</ul>
</li>
</ul>
<p>注意：每一步前向传播都需要储存一些参数在 cache 中，以便方向传播时可以用</p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>先构建两个辅助函数。</p>
<h3 id="零填充-Zero-Padding"><a href="#零填充-Zero-Padding" class="headerlink" title="零填充 Zero-Padding"></a>零填充 Zero-Padding</h3><p>该辅助函数的作用是在该图片周围加零，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_1.png" alt=""></p>
<ul>
<li>它使图片在通过卷积层时尺寸不会减小，对深层网络尤其有效</li>
<li>使我们保留边缘的重要信息</li>
</ul>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X, pad)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    在图片的长宽方向填充 pad 宽的 0 像素值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- 代表 m 张图片的形状为 (m, n_H, n_W, n_C) 的数组</span></span><br><span class="line"><span class="string">    pad -- 整数，图片水平方向和竖直方向的填充值</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># np.pad(填充对象, ((维度1的前方，维度1的后方),(..,..),(..,..),(..,..)), '填充方式', constant_values = (前方填充值, 后方填充值)) </span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>,<span class="number">0</span>),(pad,pad),(pad,pad),(<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>, constant_values = (<span class="number">0</span>,<span class="number">0</span>) )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br></pre></td></tr></table></figure>
<h3 id="单步卷积运算"><a href="#单步卷积运算" class="headerlink" title="单步卷积运算"></a>单步卷积运算</h3><ul>
<li>取出输入向量</li>
<li>在输入的每个位置使用过滤器进行卷积</li>
<li>输出另一个向量</li>
</ul>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_single_step</span><span class="params">(a_slice_prev, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将过滤器与一小片图片进行卷积操作</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_slice_prev -- 形状为 (f, f, n_C_prev) 的之前图片的一小片</span></span><br><span class="line"><span class="string">    W -- 形状为 (f, f, n_C_prev) 形状与过滤器一样的权重参数矩阵</span></span><br><span class="line"><span class="string">    b -- 形状为 (1, 1, 1) 的偏差参数矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 标量，过滤器与一小片图片卷积的结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 先逐元素相乘</span></span><br><span class="line">    s = np.multiply(a_slice_prev, W)</span><br><span class="line">    <span class="comment"># 再全部相加</span></span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    <span class="comment"># 加上偏差 b</span></span><br><span class="line">    Z = Z + float(b) <span class="comment"># b 为矩阵，先用 float() 转为标量再相加</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure>
<h3 id="卷积神经网络——前向传播"><a href="#卷积神经网络——前向传播" class="headerlink" title="卷积神经网络——前向传播"></a>卷积神经网络——前向传播</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_2.png" alt=""></p>
<p>切片方法：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_3.png" alt=""></p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_prev, W, b, hparameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    卷积函数的前向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- 形状为 (m, n_H_prev, n_W_prev, n_C_prev)，前一层的激活值</span></span><br><span class="line"><span class="string">    W -- 形状为 (f, f, n_C_prev, n_C)，权重矩阵</span></span><br><span class="line"><span class="string">    b -- 形为 (1, 1, 1, n_C)，偏差矩阵</span></span><br><span class="line"><span class="string">    hparameters -- 包含步长 "stride" 和填充 "pad" 的字典</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- 形为 (m, n_H, n_W, n_C) 的卷积输出</span></span><br><span class="line"><span class="string">    cache -- 反向传播中要用到的缓存值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 A_prev 的 shape 中获取维度信息  </span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从 W 的 shape 中获取维度信息</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = np.shape(W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取步长和填充信息</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用公式计算输出的维度信息，int() 可用于向下取整</span></span><br><span class="line">    n_H = int((n_H_prev+<span class="number">2</span>*pad-f)/stride) + <span class="number">1</span></span><br><span class="line">    n_W = int((n_W_prev+<span class="number">2</span>*pad-f)/stride) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用 0 初始化输出矩阵</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输入进行填充</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># 对 m 个训练图像的循环</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i,:,:,:]             <span class="comment"># 选出第 i 个图像</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                         <span class="comment"># 对输出向量高度方向的循环</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                     <span class="comment"># 对输出向量宽度方向的循环</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):                 <span class="comment"># 对输出向量通道数（过滤器个数）的循环</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到循环到 (i,h,w,c) 时候对应的图像“小片”</span></span><br><span class="line">                    vert_start = h*stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w*stride </span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 进行切片操作</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line">                    <span class="comment"># 将对应的图像小片和过滤器进行卷积得到 (i,h,w,c) 处的值</span></span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])</span><br><span class="line">                                          </span><br><span class="line">    <span class="comment"># 确保输出维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将一些信息储存在缓存中以便反向传播可以用</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化层减小了输入的高度和宽度，帮助减少计算量，使得特征检测器在输入中的位置更加不变。</p>
<ul>
<li>max pooling</li>
<li>average pooling</li>
</ul>
<p>池化层没有参数学习，只需要确定超参数，例如滤波器的大小。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_prev, hparameters, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    池化层的前向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "f" and "stride"</span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出输入的维度信息</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出过滤器的参数 </span></span><br><span class="line">    f = hparameters[<span class="string">"f"</span>]</span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义输出的维度</span></span><br><span class="line">    n_H = int(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = int(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化输出矩阵</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))              </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                           <span class="comment"># 对所有训练样本循环</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                     <span class="comment"># 对高度方向循环</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):                 <span class="comment"># 对宽度方向循环</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range (n_C):            <span class="comment"># 对输出通道数的循环</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到切片索引值</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 对输入进行切片</span></span><br><span class="line">                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 进行池化操作</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.max(a_prev_slice)</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.average(a_prev_slice)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 缓存</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保证输出维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure>
<h2 id="cnn-中的反向传播"><a href="#cnn-中的反向传播" class="headerlink" title="cnn 中的反向传播"></a>cnn 中的反向传播</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>这部分是可选项，在课程中也没有给详细的推导过程和解释，具体的解释可以参考以下几个网站：</p>
<ul>
<li><a href="https://grzegorzgwardys.wordpress.com/2016/04/22/8/" target="_blank" rel="noopener">参考1</a></li>
<li><a href="http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/" target="_blank" rel="noopener">参考2</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/6494810.html" target="_blank" rel="noopener">参考3</a></li>
</ul>
<p>在参考 1 中我们可以知道，$dA^{[l-1]}$ 就是将 $dZ^{[l]}$ 与翻转 180 度的过滤器矩阵进行卷积的结果，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_4.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_5.jpg" alt=""></p>
<p>这正是下面式子所表示的，+= 就是将四个叠加起来，问题是下面的式子中的 W 并没有旋转 180 度……这点一直无法解释……</p>
<script type="math/tex; mode=display">
d A^{[l-1]} + = \sum _ { h = 0 } ^ { n _ { l I } } \sum _ { w = 0 } ^ { n _ { W } } W _ { c } \times d Z ^{[l]} _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
d W _ { c } + = \sum _ { h = 0 } ^ { n _ { H } } \sum _ { w = 0 } ^ { n _ { W } } a _ { s l i c e } \times d Z _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
d b = \sum _ { h } \sum _ { w } d Z _ { h w }</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db[:,:,:,c] += dZ[i, h, w, c]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward(), output of conv_forward()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),</span></span><br><span class="line"><span class="string">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    dW -- gradient of the cost with respect to the weights of the conv layer (W)</span></span><br><span class="line"><span class="string">          numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    db -- gradient of the cost with respect to the biases of the conv layer (b)</span></span><br><span class="line"><span class="string">          numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (A_prev, W, b, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = np.shape(W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters"</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    pad = hparameters[<span class="string">'pad'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from dZ's shape</span></span><br><span class="line">    (m, n_H, n_W, n_C) = np.shape(dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev, dW, db with the correct shapes</span></span><br><span class="line">    dA_prev = np.zeros(np.shape(A_prev))                           </span><br><span class="line">    dW = np.zeros(np.shape(W))   </span><br><span class="line">    db = np.zeros(np.shape(b))   </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pad A_prev and dA_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    dA_prev_pad = zero_pad(dA_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select ith training example from A_prev_pad and dA_prev_pad</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i,:,:,:]</span><br><span class="line">        da_prev_pad = dA_prev_pad[i,:,:,:]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice"</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the slice from a_prev_pad</span></span><br><span class="line">                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Update gradients for the window and the filter's parameters using the code formulas given above</span></span><br><span class="line">                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br><span class="line">                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br><span class="line">                    db[:,:,:,c] += dZ[i, h, w, c]</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span></span><br><span class="line">        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure>
<h3 id="池化层-1"><a href="#池化层-1" class="headerlink" title="池化层"></a>池化层</h3><p>虽然池化层没有参数，但是还是需要将梯度反向传播到池化层的上一层，以便反向传播能继续下去。</p>
<h4 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max pooling"></a>max pooling</h4><p>对于 max pooling 而言，只有原来的最大值才对最终的代价函数有影响，所以我们只需要计算代价函数对这个最大值的梯度即可，其他的置为零，首先创造一个蒙板函数：</p>
<script type="math/tex; mode=display">
X = \left[ \begin{array} { l l } { 1 } & { 3 } \\ { 4 } & { 2 } \end{array} \right] \quad \rightarrow \quad M = \left[ \begin{array} { l l } { 0 } & { 0 } \\ { 1 } & { 0 } \end{array} \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a mask from an input matrix x, to identify the max entry of x.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Array of shape (f, f)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    mask = (x == np.max(x)) <span class="comment"># x 中等于 np.max(x) 的都为真，其他为假</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure>
<h4 id="average-pooling"><a href="#average-pooling" class="headerlink" title="average pooling"></a>average pooling</h4><p>由于 average pooling 中过滤器中每个值都对最终结果有影响，所以这每个值的梯度都是下一层梯度的平均一份，因为它们每个数对最终代价函数的贡献都是一样的，所以我们将一个梯度分散为若干个相等的梯度：</p>
<script type="math/tex; mode=display">
d Z = 1 \quad \rightarrow \quad d Z = \left[ \begin{array} { l l } { 1 / 4 } & { 1 / 4 } \\ { 1 / 4 } & { 1 / 4 } \end{array} \right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span><span class="params">(dz, shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Distributes the input value in the matrix of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dz -- input scalar</span></span><br><span class="line"><span class="string">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Array of size (n_H, n_W) for which we distributed the value of dz</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shape (≈1 line)</span></span><br><span class="line">    (n_H, n_W) = shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the value to distribute on the matrix (≈1 line)</span></span><br><span class="line">    average = n_H * n_W</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a matrix where every entry is the "average" value (≈1 line)</span></span><br><span class="line">    a = dz * np.ones((n_H, n_W)) / average</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>
<h4 id="合并到一个函数"><a href="#合并到一个函数" class="headerlink" title="合并到一个函数"></a>合并到一个函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span><span class="params">(dA, cache, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    实现池化层的反向传播</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A</span></span><br><span class="line"><span class="string">    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters </span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从 cache 取出参数</span></span><br><span class="line">    (A_prev, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出超参数</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    f = hparameters[<span class="string">'f'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 取出 dA_prev 和 dA 的维度信息</span></span><br><span class="line">    m, n_H_prev, n_W_prev, n_C_prev = np.shape(A_prev)</span><br><span class="line">    m, n_H, n_W, n_C = np.shape(dA)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 dA_prev 初始化</span></span><br><span class="line">    dA_prev = np.zeros(np.shape(A_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                        <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 对每个训练样例单独操作</span></span><br><span class="line">        a_prev = A_prev[i, :,:,:]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop on the vertical axis</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop on the horizontal axis</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels (depth)</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 找到切片索引值</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride </span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 用两种方式计算反向传播</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 进行切片</span></span><br><span class="line">                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                        <span class="comment"># 创造目前切片的蒙板，使得该切片中最大值置 1，其他置 0</span></span><br><span class="line">                        mask = create_mask_from_window(a_prev_slice)</span><br><span class="line">                        <span class="comment"># 将 dA[i,h,w,c] 这个位置的值乘上蒙板得到 [i,h,w,c] 这个位置反向传播的结果，最后根据链式法则将所有支路相加，也就是 +=</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i,h,w,c]</span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># 首先得到 [i,h,w,c] 这个特定位置的梯度值 dA[i,h,w,c]</span></span><br><span class="line">                        da = dA[i,h,w,c]</span><br><span class="line">                        <span class="comment"># 过滤器形状</span></span><br><span class="line">                        shape = (f,f)</span><br><span class="line">                        <span class="comment"># 将 dA[i,h,w,c] 这个值分散得到该位置反向传播结果，根据链式法则将所有位置的结果相加</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)</span><br><span class="line">                        </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev</span><br></pre></td></tr></table></figure>
<h1 id="cnn-的应用"><a href="#cnn-的应用" class="headerlink" title="cnn 的应用"></a>cnn 的应用</h1><p>这部分使用 tensorflow 来构建一个分类器。</p>
<h2 id="包的引入和数据集"><a href="#包的引入和数据集" class="headerlink" title="　包的引入和数据集"></a>　包的引入和数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> cnn_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">＃ 加载数据集</span><br><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure>
<p>仍然是识别手势的数据集：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_6.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片示例</span></span><br><span class="line">index = <span class="number">6</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(Y_train_orig[:, index])))</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_7.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集信息</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br><span class="line">conv_layers = &#123;&#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_4.1_8.png" alt=""></p>
<h2 id="创建占位符"><a href="#创建占位符" class="headerlink" title="创建占位符"></a>创建占位符</h2><p>首先需要创建输入数据的占位符，以便在运行 sess 时可以喂数据进去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建占位符</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_H0, n_W0, n_C0, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_H0 -- scalar, height of an input image</span></span><br><span class="line"><span class="string">    n_W0 -- scalar, width of an input image</span></span><br><span class="line"><span class="string">    n_C0 -- scalar, number of channels of the input</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype "float"</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype "float"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    X = tf.placeholder(tf.float32, shape = [<span class="keyword">None</span>,n_H0, n_W0, n_C0 ]) <span class="comment"># 第一个参数是数据类型，第二个是占位符形状</span></span><br><span class="line">    Y = tf.placeholder(tf.float32, shape = [<span class="keyword">None</span>, n_y ])</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure>
<h2 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h2><ul>
<li>初始化使用 W = tf.get_variable(“W”, [1,2,3,4], initializer = …)</li>
<li>初始化器使用 tf.contrib.layers.xavier_initializer(seed = 0)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [4, 4, 3, 8]</span></span><br><span class="line"><span class="string">                        W2 : [2, 2, 8, 16]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, W2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                              <span class="comment"># 这句不用管，确保我们的输出和教程一样</span></span><br><span class="line"></span><br><span class="line">    W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>)) </span><br><span class="line">    W2 = tf.get_variable(<span class="string">"W2"</span>, [<span class="number">2</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">16</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>))</span><br><span class="line">   </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>卷积层 （步长 1，same 填充） -&gt; RELU 激活 -&gt; maxpool 池化（8×8过滤器，8×8步长，same 填充） -&gt; 卷积层（步长 1，same 填充）-&gt; RELU 激活 -&gt; maxpool 池化（4×4 过滤器，4×4 步长）-&gt;拍扁 -&gt; 全连接层（输出结点 6 个，不需要调用 softmax，因为在 tensorflow 中，softmax 和代价函数被整合进一个函数中）</p>
<p>使用的函数为：</p>
<ul>
<li>卷积层：<code>tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = &#39;SAME&#39;)</code>  <ul>
<li>X 为输入，W1 为过滤器，strides 必须为 [1,s,s,1]，s 为步长，padding 类型为 same </li>
</ul>
</li>
<li>maxpool 池化层：<code>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = &#39;SAME&#39;)</code> <ul>
<li>f 为过滤器尺寸</li>
</ul>
</li>
<li>relu：<code>tf.nn.relu(Z1)</code> <ul>
<li>Z1 可以是任意形状</li>
</ul>
</li>
<li>拍扁：<code>tf.contrib.layers.flatten(P)</code><ul>
<li>返回一个 [batch_size,k] 的张量，也就是说会保留样本个数那个维度</li>
</ul>
</li>
<li>全连接层：<code>tf.contrib.layers.fully_connected(F, num_outputs)</code> <ul>
<li>num_outputs 为输出层结点个数</li>
<li>注意：tensorflow 会自动帮我们初始化全连接层的参数并在训练模型的时候自动训练，所以不用初始参数</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "W2"</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># CONV2D: stride of 1, padding 'SAME'</span></span><br><span class="line">    Z1 = tf.nn.conv2d(X, W1, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A1 = tf.nn.relu(Z1)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 8x8, sride 8, padding 'SAME'</span></span><br><span class="line">    P1 = tf.nn.max_pool(A1, ksize = [<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># CONV2D: filters W2, stride 1, padding 'SAME'</span></span><br><span class="line">    Z2 = tf.nn.conv2d(P1, W2, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A2 = tf.nn.relu(Z2)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 4x4, stride 4, padding 'SAME'</span></span><br><span class="line">    P2 = tf.nn.max_pool(A2, ksize = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># FLATTEN</span></span><br><span class="line">    P2 = tf.contrib.layers.flatten(P2)</span><br><span class="line">    <span class="comment"># FULLY-CONNECTED without non-linear activation function (not not call softmax).</span></span><br><span class="line">    <span class="comment"># 6 neurons in output layer. Hint: one of the arguments should be "activation_fn=None" </span></span><br><span class="line">    Z3 = tf.contrib.layers.fully_connected(P2, <span class="number">6</span>, activation_fn = <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure>
<h2 id="计算代价函数"><a href="#计算代价函数" class="headerlink" title="计算代价函数"></a>计算代价函数</h2><ul>
<li>计算所有样例的损失函数：<code>tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y)</code> <ul>
<li>返回所有样例的损失函数的一个向量</li>
</ul>
</li>
<li>计算损失函数均值（代价函数）：<code>tf.reduce_mean()</code>  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h2 id="总模型"><a href="#总模型" class="headerlink" title="总模型"></a>总模型</h2><ul>
<li>创造占位符</li>
<li>初始化参数（全连接层不要）</li>
<li>前向传播</li>
<li>计算损失</li>
<li>创建优化器</li>
<li>小批量梯度下降</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 总模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.009</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          num_epochs = <span class="number">100</span>, minibatch_size = <span class="number">64</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer ConvNet in Tensorflow:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    train_accuracy -- real number, accuracy on the train set (X_train)</span></span><br><span class="line"><span class="string">    test_accuracy -- real number, testing accuracy on the test set (X_test)</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    ops.reset_default_graph()	<span class="comment"># 重置默认的计算图</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)       <span class="comment"># 不用管，保持结果一致 (tensorflow seed)</span></span><br><span class="line">    seed = <span class="number">3</span>                    <span class="comment"># 不用管，保持结果一致 (numpy seed)</span></span><br><span class="line">    (m, n_H0, n_W0, n_C0) = X_train.shape             </span><br><span class="line">    n_y = Y_train.shape[<span class="number">1</span>]                            </span><br><span class="line">    costs = []                  <span class="comment"># 记录代价函数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 根据输入数据形状创建占位符</span></span><br><span class="line">    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化参数</span></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播: 在 tensorflow 计算图中构建前向传播</span></span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 代价函数计算: 往计算图中增加代价函数</span></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播: 定义优化器. Use an AdamOptimizer that minimizes the cost.</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化全局参数</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">     </span><br><span class="line">    <span class="comment"># 开始会话 sess 计算计算图</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 运行全局初始化</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 进行训练循环</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">            minibatch_cost = <span class="number">0.</span></span><br><span class="line">            num_minibatches = int(m / minibatch_size) <span class="comment"># 计算小批量的个数</span></span><br><span class="line">            seed = seed + <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)	<span class="comment"># 划分小批量</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 选择小批量</span></span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line">                <span class="comment"># 运行会话以执行 optimizer and the cost, the feedict should contain a minibatch for (X,Y).</span></span><br><span class="line">                _ , temp_cost= sess.run([optimizer,cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)               </span><br><span class="line">                </span><br><span class="line">                minibatch_cost += temp_cost / num_minibatches</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每五次迭代打印一次 cost</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, minibatch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(minibatch_cost)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># plot the cost</span></span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the correct predictions</span></span><br><span class="line">        predict_op = tf.argmax(Z3, <span class="number">1</span>)	<span class="comment"># 找到 Z3 中最大值的索引号，1 为按行取</span></span><br><span class="line">        correct_prediction = tf.equal(predict_op, tf.argmax(Y, <span class="number">1</span>))	<span class="comment"># 相等则为 1 否则为 0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate accuracy on the test set</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">        print(accuracy)</span><br><span class="line">        train_accuracy = accuracy.eval(&#123;X: X_train, Y: Y_train&#125;)	<span class="comment"># accuracy.eval() 相当于 sess.run(accuracy)</span></span><br><span class="line">        test_accuracy = accuracy.eval(&#123;X: X_test, Y: Y_test&#125;)</span><br><span class="line">        print(<span class="string">"Train Accuracy:"</span>, train_accuracy)</span><br><span class="line">        print(<span class="string">"Test Accuracy:"</span>, test_accuracy)                              </span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> train_accuracy, test_accuracy, parameters</span><br></pre></td></tr></table></figure>
<h2 id="Tensorflow-使用感想"><a href="#Tensorflow-使用感想" class="headerlink" title="Tensorflow 使用感想"></a>Tensorflow 使用感想</h2><p>在 tensorflow 中，计算图中的任何值都不会被计算出来，除非你使用 <code>sess.run(feed_dict)</code>或者 <code>tensor.eval(feed_dict)</code>，在求值的时候，从要求的值往前推，把这一条线上所有需要的 placeholder 找出来然后填入 feed_dict。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/28/吴恩达机器学习笔记c4w1/">coursera 吴恩达深度学习 Specialization 笔记（course 4 week 1）—— 卷积神经网络基础</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-28</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/cnn/">cnn</a></span><div class="content"><h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><h3 id="计算机视觉的问题"><a href="#计算机视觉的问题" class="headerlink" title="计算机视觉的问题"></a>计算机视觉的问题</h3><ul>
<li><p>图像分类：例如猫分类器</p>
</li>
<li><p>物体检测：例如自动驾驶不仅需要识别出图片中是否有车，还要计算出这张图片中汽车的位置</p>
</li>
<li><p>神经风格转换：如下图</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_4.1_1.png" alt=""></p>
</li>
</ul></div><a class="more" href="/2018/09/28/吴恩达机器学习笔记c4w1/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/26/坎坷选导师/">坎坷开学路</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/个人随想/">个人随想</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/日记/">日记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/研究生生活/">研究生生活</a></span><div class="content"><p>今天是9月26号，在哈工大深圳校区的荔园写下这篇日记，距离开学已经过去了十天左右的时间，本来以为可以开开心心入学，没想到碰到了许多倒霉事儿：</p></div><a class="more" href="/2018/09/26/坎坷选导师/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/10/吴恩达机器学习笔记c3w2/">coursera 吴恩达深度学习 Specialization 笔记（course 3 week 2）—— 机器学习策略（二）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a></span><div class="content"><h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p>如果你想得到一个训练算法来做人类可以做的任务，但是训练的算法还没有达到人类的效果，你就需要手动地检查算法中的错误，来知道你下一步该做什么，这个过程叫做错误分析。</p>
<h3 id="如何进行错误分析"><a href="#如何进行错误分析" class="headerlink" title="如何进行错误分析"></a>如何进行错误分析</h3><p>假设训练一个分类猫的算法，准确率为 90%，错误率为 10%，我们对分错的样本进行检查，发现分错的样本中有一些把狗认成了猫，这时是否应该将重心转移到使分类器对狗的表现更好？</p>
<p>错误分析：</p>
<ul>
<li>拿到约 <strong>100</strong> 张分类错误的开发集图片并进行手动检测<ul>
<li>一般不需要检测上万张图片，几百张就足够了！</li>
</ul>
</li>
<li>输出错误的图片里多少张狗</li>
</ul>
<p>假设只有 5 张，那么在错误图片中只有 5% 的狗，如果在狗的问题上花了大量的时间，那么就算是最好的情况，也<strong>最多</strong>只是把错误率从 10% 降到 9.5%，所以并不值得。但是假设 100 张错误图片里有 50 张狗的图片，那么可以确定很值得将时间花在狗身上，因为错误率<strong>最多</strong>可能从 10% 降到 5%.</p></div><a class="more" href="/2018/09/10/吴恩达机器学习笔记c3w2/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/08/吴恩达机器学习笔记c3w1/">coursera 吴恩达深度学习 Specialization 笔记（course 3 week 1）—— 机器学习策略（一）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a></span><div class="content"><p>现在来到 coursera 的 deeplearning.ai 课程的第三课，这门课程叫 Structuring Machine Learning Projects 结构化机器学习项目，将会学习到构建机器学习项目的一些策略和基本套路，防止南辕北辙。</p>
<h2 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化 (Orthogonalization)"></a>正交化 (Orthogonalization)</h2><h3 id="何为正交化"><a href="#何为正交化" class="headerlink" title="何为正交化"></a>何为正交化</h3><p>如同老式电视机一样，一个旋钮控制一个确定的功能，如屏幕横向伸长或纵向伸长，而不会一个旋钮控制两个属性。</p>
<h3 id="ML-的正交化"><a href="#ML-的正交化" class="headerlink" title="ML 的正交化"></a>ML 的正交化</h3><p>我们希望某个旋钮能单独让下面的某个步骤运行良好而不影响其他，这就叫正交化。</p>
<ul>
<li>首先训练集要拟合的很好<ul>
<li>否则减小偏差</li>
</ul>
</li>
<li>如果训练集运行良好，则希望开发集运行良好<ul>
<li>否则减小方差</li>
</ul>
</li>
<li>如果在训练集和开发集运行良好，则希望在测试集运行良好<ul>
<li>否则采用更大的开发集</li>
</ul>
</li>
<li>最后希望在真实世界表现良好<ul>
<li>否则调整开发集或者改变代价函数</li>
</ul>
</li>
</ul></div><a class="more" href="/2018/09/08/吴恩达机器学习笔记c3w1/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/06/吴恩达机器学习笔记c2w3/">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 3）—— 调参方法、批量归一化和深度学习框架</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/超参数/">超参数</a></span><div class="content"><p>本周主要学习超参数的系统调整，批量归一化和深度学习框架的相关知识。</p>
<h2 id="超参数的调整"><a href="#超参数的调整" class="headerlink" title="超参数的调整"></a>超参数的调整</h2><h3 id="调参的优先级"><a href="#调参的优先级" class="headerlink" title="调参的优先级"></a>调参的优先级</h3><ul>
<li>第一优先级：学习率 $\alpha$ </li>
<li>第二优先级：动量算法参数 $\beta$ (默认值0.9)，最小批的个数 mini-batch size，每层的隐藏单元数 hidden units</li>
<li>第三优先级：神经网络的层数 layers，学习率衰减 learning rate decay</li>
<li>几乎不需要调优：Adam 算法中的 $\beta_1,\beta_2,\varepsilon$，几乎每次只用默认值 $0.9,0.999,10^{-8}$    </li>
</ul>
<h3 id="调参的注意事项"><a href="#调参的注意事项" class="headerlink" title="调参的注意事项"></a>调参的注意事项</h3><ol>
<li><p>随机取样</p>
<p>当需要调整多个超参数时，尽量在网格中随机取样（下图右），而不是规则抽样（下图左）</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_1.png" alt=""></p></div><a class="more" href="/2018/09/06/吴恩达机器学习笔记c2w3/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/04/吴恩达机器学习c2w2编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 2）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/优化算法/">优化算法</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/梯度下降/">梯度下降</a></span><div class="content"><p>本次作业实现了几种梯度下降算法，比较了它们的不同。</p>
<h2 id="普通梯度下降-BGD"><a href="#普通梯度下降-BGD" class="headerlink" title="　普通梯度下降 (BGD)"></a>　普通梯度下降 (BGD)</h2><p>所谓普通梯度下降就是一次处理所有的 m 个样本，也叫批量梯度下降 (Batch Gradient Descent)，公式为：</p>
<script type="math/tex; mode=display">
W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\\
b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script></div><a class="more" href="/2018/09/04/吴恩达机器学习c2w2编程作业/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/02/吴恩达机器学习笔记c2w2/">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 2）—— 优化算法的改进</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络优化/">神经网络优化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/优化算法/">优化算法</a></span><div class="content"><p>本周主要学习不同的参数优化方式，提高模型的训练速度。</p>
<h2 id="小批量梯度下降算法-mini-batch-gradient-descent"><a href="#小批量梯度下降算法-mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降算法 (mini-batch gradient descent)"></a>小批量梯度下降算法 (mini-batch gradient descent)</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><p>向量化可以有效率地同时计算 m 个样本，但是当样本数为几百万时，速度依然会很慢，每一次迭代都必须先处理几百万的数据集才能往前一步，所以我们可以使用这个算法进行加速。</p>
<p>首先我们将训练集划分为一个一个微小的训练集，也就是小批量训练集 (mini-batch)，比如每一个微小训练集只有 1000 个样本：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_2.png" alt=""></p></div><a class="more" href="/2018/09/02/吴恩达机器学习笔记c2w2/#more">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>