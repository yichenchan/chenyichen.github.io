<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。"><meta name="keywords" content=""><meta name="author" content="陈艺琛,undefined"><meta name="copyright" content="陈艺琛"><title>chenyichen's blog | 在AI小白之路上踽踽独行 | 陈艺琛的滑板公园</title><link rel="shortcut icon" href="/img/logo1.png"><link rel="stylesheet" href="/css/index.css?version=1.5.6"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css?version=1.5.6"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?20c8efd323cd63b9f6bf846113eb6f60";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avater.jpg"></div><div class="author-info__name text-center">陈艺琛</div><div class="author-info__description text-center">a student of HITsz————坚持滑板，热爱摇滚，偶尔弹吉他，喜欢阅读，抽空打游戏，在AI小白之路上踽踽独行。</div><div class="follow-button"><a href="https://web.okjike.com/user/9e0ec001-4bb6-4cab-af94-ea8b2f6067ef/post" target="_blank">在即刻上关注我</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">17</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">15</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">3</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">友链</div><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的博客</a><a class="author-info-links__name text-center" href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank">网易机器学习公开课</a><a class="author-info-links__name text-center" href="https://developers.google.com/machine-learning/crash-course/?hl=zh-cn" target="_blank">谷歌机器学习速成</a><a class="author-info-links__name text-center" href="http://www.hitsz.edu.cn/index.html" target="_blank">哈工大深圳主页</a></div></div></div><nav id="nav" style="background-image: url(https://raw.githubusercontent.com/yichenchan/blogimg/master/img/skater11.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">陈艺琛的滑板公园</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a><a class="site-page" href="/">主页</a><a class="site-page" href="/categories/学习笔记">学习笔记</a><a class="site-page" href="/categories/读书观影笔记">读书观影笔记</a><a class="site-page" href="/categories/个人随想">个人随想</a><a class="site-page" href="/categories/爱好和生活">爱好和生活</a><a class="site-page" href="/gallery">相册</a><a class="site-page" href="/archives">所有博客</a><a class="site-page" href="/tags">特色标签</a><a class="site-page" href="/about">关于我</a></span></div><div id="site-info"><div id="site-title">陈艺琛的滑板公园</div><div id="site-sub-title">chenyichen's blog | 在AI小白之路上踽踽独行</div><div id="site-social-icons"><a class="social-icon" href="https://github.com/yichenchan" target="_blank"><i class="fa fa-github"></i></a><a class="social-icon" href="https://weibo.com/5101047894/profile?rightmod=1&amp;wvr=6&amp;mod=personinfo" target="_blank"><i class="fa fa-weibo"></i></a><a class="social-icon" href="c840098794@gmail.com" target="_blank"><i class="fa fa-email"></i></a><a class="social-icon" href="https://steamcommunity.com/id/840098794/" target="_blank"><i class="fa fa-steam"></i></a><a class="social-icon search"><i class="fa fa-search"></i></a></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/10/吴恩达机器学习笔记c3w2/">coursera 吴恩达深度学习 Specialization 笔记（course 3 week 2）—— 机器学习策略（二）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-10</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a></span><div class="content"><h2 id="错误分析"><a href="#错误分析" class="headerlink" title="错误分析"></a>错误分析</h2><p>如果你想得到一个训练算法来做人类可以做的任务，但是训练的算法还没有达到人类的效果，你就需要手动地检查算法中的错误，来知道你下一步该做什么，这个过程叫做错误分析。</p>
<h3 id="如何进行错误分析"><a href="#如何进行错误分析" class="headerlink" title="如何进行错误分析"></a>如何进行错误分析</h3><p>假设训练一个分类猫的算法，准确率为 90%，错误率为 10%，我们对分错的样本进行检查，发现分错的样本中有一些把狗认成了猫，这时是否应该将重心转移到使分类器对狗的表现更好？</p>
<p>错误分析：</p>
<ul>
<li>拿到约 <strong>100</strong> 张分类错误的开发集图片并进行手动检测<ul>
<li>一般不需要检测上万张图片，几百张就足够了！</li>
</ul>
</li>
<li>输出错误的图片里多少张狗</li>
</ul>
<p>假设只有 5 张，那么在错误图片中只有 5% 的狗，如果在狗的问题上花了大量的时间，那么就算是最好的情况，也<strong>最多</strong>只是把错误率从 10% 降到 9.5%，所以并不值得。但是假设 100 张错误图片里有 50 张狗的图片，那么可以确定很值得将时间花在狗身上，因为错误率<strong>最多</strong>可能从 10% 降到 5%.</p>
<p>有时候我们可以并行考虑好几个 idea 是否值得付出努力。</p>
<p>假设有下面几个提高猫咪分类器性能的 idea：</p>
<ul>
<li>提高对狗的识别</li>
<li>提高对其他猫科动物的识别</li>
<li>提高模型对模糊图像的性能</li>
</ul>
<p>如何评估这几个方案，可以列一张表，记录每个分类错误的图片分属于哪一个错误类别，然后计算百分比：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_1.png" alt=""></p>
<p>通过上面的分析我们可以知道现在应该专注于提高对模糊的图片（43%）或者猫科动物图片（61%）的准确度，但仅仅是一个参考，因为这取决于获得这些数据<strong>容不容易</strong>，假如要获得猫科动物的图片需要大量的时间，那么这一方面就可以先放放。</p>
<h3 id="清除错误标记（标签）"><a href="#清除错误标记（标签）" class="headerlink" title="清除错误标记（标签）"></a>清除错误标记（标签）</h3><p>当你检查数据集时发现一些输出标签 Y 标记错误，需不需要花时间修正这些标签。</p>
<h4 id="对于训练集"><a href="#对于训练集" class="headerlink" title="对于训练集"></a>对于训练集</h4><ul>
<li>深度学习算法在训练集上对于一些随机错误非常稳健，也就是说基本不需要管训练集上的一些随机的错误标记</li>
<li>但是注意算法对系统误差比较敏感，如果标记员一直把白色的狗标记为猫，那么训练出来的分类器也会学着这么做</li>
</ul>
<h4 id="对于开发-测试集"><a href="#对于开发-测试集" class="headerlink" title="对于开发/测试集"></a>对于开发/测试集</h4><p>推荐的方法是在错误分析时增加一列“因为标签错误而分类错误”的统计：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_2.png" alt=""></p>
<p>如上如图，加入标签错误是 6%，那么我们值得花时间去纠正这 6% 的样本的标签吗？</p>
<p>假设算法的总错误率是 10% 或 2%，考虑这两种情况下标签错误对评估的影响：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>算法的总错误率</th>
<th>10%</th>
<th>2%</th>
</tr>
</thead>
<tbody>
<tr>
<td>由标签错误引起的错误率</td>
<td>0.6%</td>
<td>0.6%</td>
</tr>
<tr>
<td>由其他因素引起的错误率</td>
<td>9.4%</td>
<td>1.4%</td>
</tr>
</tbody>
</table>
</div>
<p>当总错误率为 10% 时，并不值得花时间去纠正这些错误的标签，因为即使纠正了也只提高 0.6% 的准确度，但是当总错误率为 2% 时，如果我们不纠正标签，那么他们引起的错误就占了一大半，这时纠正开发集中的标签错误就很有必要了。</p>
<h3 id="纠正开发集-测试集的原则"><a href="#纠正开发集-测试集的原则" class="headerlink" title="纠正开发集/测试集的原则"></a>纠正开发集/测试集的原则</h3><ul>
<li>对开发集和测试集进行一样的处理，确保他们来自相同的分布，当你纠正开发集中的一些问题，将这个过程也运用到测试集中</li>
<li>考虑检查你算法预测正确的和预测错误的样本</li>
<li>训练集 和 开发/测试集可以服从稍微不同的分布</li>
</ul>
<h3 id="建立第一个模型的建议"><a href="#建立第一个模型的建议" class="headerlink" title="建立第一个模型的建议"></a>建立第一个模型的建议</h3><ul>
<li>设立开发/测试集和评估指标</li>
<li><strong>快速地建立一个不那么复杂的初始模型</strong></li>
<li>使用偏差/方差分析/错误分析进行模型的迭代</li>
</ul>
<h2 id="训练集-和-开发-测试集-的不匹配"><a href="#训练集-和-开发-测试集-的不匹配" class="headerlink" title="训练集 和 开发/测试集 的不匹配"></a>训练集 和 开发/测试集 的不匹配</h2><p>深度学习算法都希望有大量的训练数据，这导致很多团队将能找到的任何数据都塞进训练集，只为有更多的训练数据，即使很多这种数据来自于与开发集和测试集不同的分布。因此在深度学习时代，越来越多的团队正在使用的，训练数据并非来自与开发集和测试集相同的分布。</p>
<h3 id="在不同的分布上进行训练和测试"><a href="#在不同的分布上进行训练和测试" class="headerlink" title="在不同的分布上进行训练和测试"></a>在不同的分布上进行训练和测试</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_3.png" alt=""></p>
<p>要训练一个猫咪分类器，目前手上有两种数据，一种是从网上爬下来的十分精美的图片 200000 个，另一种是用户上传的质量很差的图片 10000 张，这两种数据的分布差异非常大，但是我们更关心的是右边用户上传的数据，是我们的“靶子”，现在有两种数据集分配方案。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_4.png" alt=""></p>
<p>第一种是将所有图片混合洗牌，其中 205000 张作为训练集，剩下的 5000 张在开发/测试集之间平分。这种分法有一个很大的缺点！按照这种分法，开发集和测试集中按照比例可以算出用户上传的照片只有约 119 张，大部分都是网上爬的图片，并没有很好地反映我们所关心的数据（用户的图片），也就是说靶子放错了！</p>
<p>而正确的第二种分法是：将所有网上爬的 200000 张图片<strong>加上用户的 5000 张作为训练集</strong>，而剩下的用户的 5000 张平分为开发集和测试集，这样一来，你的开发集就瞄准了正确的目标。但缺点就是使得训练集分布和开发集分布有些不同，但是长期来说性能更好。</p>
<p>总结：</p>
<ul>
<li>在某些情况下，可以允许训练集和开发/测试集来自不同的分布，以可以获得更大的训练集</li>
<li>但是注意：训练集中<strong>也需要</strong>包含足够的开发集分布数据，来避免数据不匹配问题！</li>
</ul>
<h3 id="数据分布不匹配时的偏差-方差分析方法"><a href="#数据分布不匹配时的偏差-方差分析方法" class="headerlink" title="数据分布不匹配时的偏差/方差分析方法"></a>数据分布不匹配时的偏差/方差分析方法</h3><p> 当你的训练集、开发集、测试集来自不同的分布时，偏差和方差的分析方法也会相应变化。</p>
<h4 id="提出问题"><a href="#提出问题" class="headerlink" title="提出问题"></a>提出问题</h4><p>对于一个猫分类器，假设人类误差是 0，且训练集和开发集数据来自不同分布，训练集误差为 1%，开发集误差为 10%，那么它们之间 9% 的差距到底是因为 (1)模型泛化能力不足导致方差太大，还是因为 (2)训练集数据分布不同？</p>
<h4 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h4><p>为了找出原因，我们定义一个新的数据集：训练-开发集 (training-dev set).</p>
<p>训练-开发集：将所有训练集数据洗牌，取出一小块作为训练-开发集，它的分布与训练集相同。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_5.png" alt=""></p>
<blockquote>
<p>分布相同：</p>
<ul>
<li>训练集 $\leftrightarrow$ 训练-开发集</li>
<li>开发集 $\leftrightarrow$ 测试集</li>
</ul>
</blockquote>
<p>划分好之后，计算出模型在如下各个数据集上的误差如下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">例子</th>
<th style="text-align:center">1</th>
<th style="text-align:center">2</th>
<th style="text-align:center">3</th>
<th style="text-align:center">4</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">训练集误差</td>
<td style="text-align:center">1%</td>
<td style="text-align:center">1%</td>
<td style="text-align:center">10%</td>
<td style="text-align:center">10%</td>
</tr>
<tr>
<td style="text-align:left">训练-开发集误差</td>
<td style="text-align:center">9%</td>
<td style="text-align:center">1.5%</td>
<td style="text-align:center">11%</td>
<td style="text-align:center">11%</td>
</tr>
<tr>
<td style="text-align:left">开发集误差</td>
<td style="text-align:center">10%</td>
<td style="text-align:center">10%</td>
<td style="text-align:center">12%</td>
<td style="text-align:center">20%</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>例子1：训练集误差 1%，训练-开发集误差 9%，两者同分布都差距这么大，说明模型泛化能力不好，属于高方差问题。</li>
<li>例子2：在同样是在没训练过的数据集上预测，与训练集分布一样的训练-开发集误差很小，而与训练集分布不同的开发集（你所关心的）却误差很大，算法未在你所关心的分布上训练的很好，这就是<strong>数据不匹配</strong>问题 (mismatch problem)。</li>
<li>例子3：人类误差 0，训练集误差却高达 10%，这是明显是高偏差问题。</li>
<li>例子4：10% 说明高偏差；11%～20% 说明数据不匹配程度相当大。</li>
</ul>
<h4 id="train-sets-和-dev-test-sets-不匹配问题的通用的解决方案"><a href="#train-sets-和-dev-test-sets-不匹配问题的通用的解决方案" class="headerlink" title="train sets 和 dev/test sets 不匹配问题的通用的解决方案"></a>train sets 和 dev/test sets 不匹配问题的通用的解决方案</h4><p>一个模型可能的误差类型：</p>
<ul>
<li><p><strong>人类误差</strong> (训练集分布)  $&lt;\xlongequal{反映了不同分布的识别难度大小}&gt; $ 人类误差 (开发集分布) </p>
<p>$\Updownarrow$ 反映可避免偏差</p>
</li>
<li><p><strong>训练集误差</strong> (训练集分布)</p>
<p> $\Updownarrow$ 反映方差情况</p>
</li>
<li><p><strong>训练-开发集误差</strong> (训练集分布)</p>
<p> $\Updownarrow$ 反映（训练集和开发集）数据不匹配情况</p>
</li>
<li><p><strong>开发集误差</strong> (开发/测试集分布)</p>
<p> $\Updownarrow$ 反映算法对开发集的过拟合情况，因为如果这个差距太大，则也许将神经网络</p>
<p> $\Updownarrow$ 调的太偏向开发集了，此时需要一个更大的开发集</p>
</li>
<li><p><strong>测试集误差</strong> (开发/测试集分布)</p>
</li>
</ul>
<h3 id="如何改善数据不匹配问题"><a href="#如何改善数据不匹配问题" class="headerlink" title="如何改善数据不匹配问题"></a>如何改善数据不匹配问题</h3><ul>
<li>人工分析误差，试着理解训练集与开发/测试集之间的差异<ul>
<li>举个例子，在识别车载语音识别时，可能开发集中很多车辆噪音，而训练集没有，这是由实际情况决定的</li>
</ul>
</li>
<li>想方设法使得训练集与之更相似：可以收集更多与开发/测试集相似的数据加入训练集，或者进行人工合成数据加入训练集，只要合成的数据让人类觉得很真实，那么就可以骗过算法<ul>
<li>比如模拟车载噪音数据，将训练集中干净的音频加上一段噪音合成出噪音下的说话声。但是注意：一小段<strong>重复</strong>用来合成的汽车噪音会使得学习算法对这段声音产生过拟合</li>
<li>再比如无人驾驶中的汽车识别，可以通过三维建模画出我们需要的汽车样子。但是注意：建模所用的汽车模型非常有限，加入只有 20 种，那么神经网络很可能对这 20 种汽车产生过拟合</li>
</ul>
</li>
</ul>
<h2 id="迁移学习-Transfer-learning"><a href="#迁移学习-Transfer-learning" class="headerlink" title="迁移学习 (Transfer learning)"></a>迁移学习 (Transfer learning)</h2><h3 id="什么叫迁移学习"><a href="#什么叫迁移学习" class="headerlink" title="什么叫迁移学习"></a>什么叫迁移学习</h3><p>深度学习中最有力的方法之一，是有时你可以把在一个任务中神经网络学习到的东西，应用到另一个任务中去。比如，你可以让神经网络学习去识别物体，比如猫，然后用学习到的（一部分）知识来帮助你更好地识别X射线的结果。这就是所谓的迁移学习。</p>
<h3 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h3><p>假如我们我们用猫狗的图片训练了一个如下的图片识别的神经网络：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_6.png" alt=""></p>
<p>如果我们想把这个模型应用到另一个场景，比如 x 光片的诊断，实现方法是移除这个神经网络的最后一层和其相关的权重，如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_7.png" alt=""></p>
<p>要实现迁移学习，要做的是：</p>
<ul>
<li>将数据集换成 x 光诊断的图片</li>
<li>随机初始化新的最后一层的权重 W 和 偏差 b</li>
<li>重新训练该网络<ul>
<li>如果数据量小：可以只训练最后一层的参数，保留其他的参数</li>
<li>如果数据量大：可以训练所有层</li>
</ul>
</li>
</ul>
<p><strong>预训练 (pre training)</strong>：我们把猫狗图片训练的模型用到其他地方，那么用猫狗图片对模型的训练就称之为预训练。预训练对模型的权重进行了预初始化，其他的训练是在预初始化权重的基础上继续训练。</p>
<p><strong>微调 (fine tuning)</strong>：在预训练的基础上继续训练，比如上面的再用 x 光诊断图片继续训练，对权重进行更新，称之为“微调”。</p>
<p>有时候你也可以将最后一层移除后增加一些新的层，然后对新建的几层重新训练：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_8.png" alt=""></p>
<h3 id="为什么迁移学习有效？"><a href="#为什么迁移学习有效？" class="headerlink" title="为什么迁移学习有效？"></a>为什么迁移学习有效？</h3><p>是因为从从大规模的图像识别数据集中学习到的边界检测，曲线检测，明暗对象检测等低层次的信息，或许能够帮助你的学习算法更好地去进行放射扫描结果的诊断。它会在图像的组织结构和自然特性中学习到很多信息，其中一些信息会非常的有帮助。所以当神经网络学会了图像识别意味着它可能学习到了以下信息：关于不同图片的点，线，曲面等等信息，在不同图片中看起来是什么样子的。或许关于一个物体对象的很小的细节，都能够帮助你在放射信息诊断的神经网络中，学习得更快一些或者减少学习需要的数据。</p>
<h3 id="什么时候采用迁移学习？"><a href="#什么时候采用迁移学习？" class="headerlink" title="什么时候采用迁移学习？"></a>什么时候采用迁移学习？</h3><ul>
<li><p>任务 A 有着比任务 B 多得多的数据，可以从 A 迁移到 B</p>
<ul>
<li><p>例如，假设你在一个图像识别任务中拥有一百万个样本，这就意味着大量数据中的低层次特征信息或者大量的有帮助的特征信息在神经网络的前几层被学习到了。但是对于放射扫描结果的诊断任务，或许仅仅是100个 X 光扫描样本。所以你从图像识别中学习到的大量的信息可以被用于迁移并且这些信息会有效地帮助你处理好放射诊疗。</p>
</li>
<li><p>总结：只能大数据模型迁移到小数据模型，而不能小数据模型迁移到大数据模型。</p>
</li>
</ul>
</li>
<li><p>任务 A 和任务 B 有着相同的输入</p>
<ul>
<li>例如识别猫狗和诊断 X 光输入的都是图片</li>
</ul>
</li>
<li><p>当你认为任务 A 中的低层次特征会帮助到任务 B </p>
<ul>
<li>例如你判断两个任务有某种可以互通的特征，比如两者都是识别物体或都是语音识别</li>
</ul>
</li>
</ul>
<h2 id="多任务学习-multi-task-learning"><a href="#多任务学习-multi-task-learning" class="headerlink" title="多任务学习 (multi-task learning)"></a>多任务学习 (multi-task learning)</h2><p>在多任务学习中，几个任务一起进行，让神经网络同时做几件事情。</p>
<h3 id="如何实现-1"><a href="#如何实现-1" class="headerlink" title="如何实现"></a>如何实现</h3><p>在自动驾驶时拍下一张照片需要识别出照片中是否存在一些物体，如果有就置 1，否则为 0 ：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_9.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">行人</th>
<th style="text-align:center">0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">汽车</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">交通牌</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">信号灯</td>
<td style="text-align:center">0</td>
</tr>
</tbody>
</table>
</div>
<p>于是输出标签变为（4，m）的矩阵：</p>
<script type="math/tex; mode=display">
Y=\begin{bmatrix}
 |&  |&  & | & \\ 
 y^{(1)}& y^{(2)} &,... , & y^{(m)} & \\ 
 |& | &  & | & 
\end{bmatrix}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_10.png" alt=""></p>
<p>搭建上图中的网络进行训练，最后一层有四个单元，表示识别的四种物体的概率，那么代价函数可以表示为：</p>
<script type="math/tex; mode=display">
J=\frac{1}{m} \sum\limits^{m}_{i=1}\sum\limits^{4}_{j=1}L(\hat y^{(i)}_j,y^{(i)}_j)</script><ul>
<li><p>有时候 4 个标签会有缺失（下图中的 “？”），你仍然可以进行训练，只需要省略缺失项， $\sum\limits^{4}_{j=1}$ 只对有标签的值进行求和即可</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.2_11.png" alt=""></p>
</li>
<li><p>该方法相当于将四个独立的神经网络合在一起，但是效果更好</p>
</li>
</ul>
<h3 id="什么时候适用多任务学习"><a href="#什么时候适用多任务学习" class="headerlink" title="什么时候适用多任务学习"></a>什么时候适用多任务学习</h3><ul>
<li>你要训练的任务共享一些低层次的特征<ul>
<li>例如行人、交通灯、汽车、交通牌这些物体都是道路特征</li>
</ul>
</li>
<li>非硬性：多任务中每个任务的数据量相当相似<ul>
<li>如果你集中在某一任务上，其他任务比这单一任务有多得多的数据</li>
</ul>
</li>
<li>如果你可以训练一个足够大的网络来做好所有的任务<ul>
<li>如果神经网络不够大，与单独训练每个任务相比，多任务训练会损害准确率</li>
</ul>
</li>
</ul>
<h2 id="端到端深度学习-end-to-end-deep-learning"><a href="#端到端深度学习-end-to-end-deep-learning" class="headerlink" title="端到端深度学习 (end-to-end deep learning)"></a>端到端深度学习 (end-to-end deep learning)</h2><h3 id="什么是“端到端深度学习”"><a href="#什么是“端到端深度学习”" class="headerlink" title="什么是“端到端深度学习”"></a>什么是“端到端深度学习”</h3><p>一个由许多阶段组成的学习系统，输入 x，输出 y，如果我们将其中的许多阶段用单个神经网络进行替代，直接建立从输入端 x 到输出端 y 的映射，就叫做“端到端深度学习”。</p>
<h3 id="一些例子"><a href="#一些例子" class="headerlink" title="一些例子"></a>一些例子</h3><ul>
<li><p>例子 1</p>
<p>加入要建立一个语音识别系统，那么一般思路为：</p>
<p>输入音频 x $\xrightarrow{MFCC算法}$ 提取低级特征 $\xrightarrow{ML算法}$ 找到音素 $\xrightarrow{组合}$ 单词  $\xrightarrow{剪辑}$ 输出文字 y                    </p>
<p>而端到端深度学习的实现方式为直接建立输入语音 x 和文字脚本 y 的映射：</p>
<p>输入音频 x   $\xrightarrow{ \quad \quad\quad\quad\quad\quad\quad 端到端深度学习\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}$ 输出文字 y   </p>
</li>
<li><p>例子 2</p>
<p>机器翻译系统，传统机器学习思路：</p>
<p>英语 $\rightarrow$ 文本分析 $\rightarrow$ 提取特征 $\rightarrow$ … $\rightarrow$ 中文</p>
<p>端到端机器学习使用大量的英文和中文对应文本的数据集进行直接映射：</p>
<p>英文$\xrightarrow{ \quad \quad\quad\quad\quad 端到端深度学习\quad\quad\quad\quad}$ 中文 </p>
</li>
</ul>
<h3 id="end-to-end-DL-的优缺点"><a href="#end-to-end-DL-的优缺点" class="headerlink" title="end-to-end DL 的优缺点"></a>end-to-end DL 的优缺点</h3><ol>
<li>优点<ul>
<li>端到端数据学习让数据“说话”，从 X $\rightarrow$ Y 的映射中更好地学习到数据内在的统计学特征，而不是被迫去反映人的先见<ul>
<li>例如传统机器学习利用到语言中“音素”的概念，但是这个概念是是一个人造的产物，无法确定让算法使用“音素”是不是有利的</li>
</ul>
</li>
<li>需要更少的人工设计组件，简化工作流程</li>
</ul>
</li>
<li>缺点<ul>
<li>为了得到 X 到 Y 的映射，需要非常大量的 (X $\rightarrow$Y) 数据</li>
<li>它排除了一些具有潜在用途的手工设计组件<ul>
<li>如果数据量不够，那么一个精心手工设计的系统实际上可以向算法中注入人类关于这个问题的知识，是很有帮助的</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="什么时候需要运用端到端深度学习"><a href="#什么时候需要运用端到端深度学习" class="headerlink" title="什么时候需要运用端到端深度学习"></a>什么时候需要运用端到端深度学习</h3><p>关键问题：你是否有充足的数据去学习出具有能够映射 X 到 Y 所需复杂度的方程？</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/08/吴恩达机器学习笔记c3w1/">coursera 吴恩达深度学习 Specialization 笔记（course 3 week 1）—— 机器学习策略（一）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-08</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a></span><div class="content"><p>现在来到 coursera 的 deeplearning.ai 课程的第三课，这门课程叫 Structuring Machine Learning Projects 结构化机器学习项目，将会学习到构建机器学习项目的一些策略和基本套路，防止南辕北辙。</p>
<h2 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化 (Orthogonalization)"></a>正交化 (Orthogonalization)</h2><h3 id="何为正交化"><a href="#何为正交化" class="headerlink" title="何为正交化"></a>何为正交化</h3><p>如同老式电视机一样，一个旋钮控制一个确定的功能，如屏幕横向伸长或纵向伸长，而不会一个旋钮控制两个属性。</p>
<h3 id="ML-的正交化"><a href="#ML-的正交化" class="headerlink" title="ML 的正交化"></a>ML 的正交化</h3><p>我们希望某个旋钮能单独让下面的某个步骤运行良好而不影响其他，这就叫正交化。</p>
<ul>
<li>首先训练集要拟合的很好<ul>
<li>否则减小偏差</li>
</ul>
</li>
<li>如果训练集运行良好，则希望开发集运行良好<ul>
<li>否则减小方差</li>
</ul>
</li>
<li>如果在训练集和开发集运行良好，则希望在测试集运行良好<ul>
<li>否则采用更大的开发集</li>
</ul>
</li>
<li>最后希望在真实世界表现良好<ul>
<li>否则调整开发集或者改变代价函数</li>
</ul>
</li>
</ul>
<p>当你发现某个步骤表现不好的时候，找到一个特定的旋钮进行调节，从而解决问题。</p>
<h2 id="设置目标"><a href="#设置目标" class="headerlink" title="设置目标"></a>设置目标</h2><h3 id="设置单一化的评估指标"><a href="#设置单一化的评估指标" class="headerlink" title="设置单一化的评估指标"></a>设置单一化的评估指标</h3><ul>
<li>精确率 precision：模型识别出是猫的集合中，有多少百分比真正是猫</li>
<li>召回率 recall：对于全部是猫的图片有多少能正确识别出来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_1.png" alt=""></p>
<p>A、B 分类器分别有两个指标 P 和 R，各有长处，不好判断，于是可以计算它们的调和平均数，变成一个单一指标 F1 score，公式为：</p>
<script type="math/tex; mode=display">
F1=\frac{2}{\frac{1}{P}+\frac{1}{R}}</script><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_2.png" alt=""></p>
<p>再比如我们知道各个算法在不同国家的错误率，我们求这些错误率的平均值，得到一个单一指标，从而知道 C 算法性能最好。</p>
<h3 id="优化指标和满足指标"><a href="#优化指标和满足指标" class="headerlink" title="优化指标和满足指标"></a>优化指标和满足指标</h3><p>优化指标 optimizing metric：指这个指标越大（越小）越好，需要它表现得<strong>尽可能</strong>好，一般一个问题只需要一个优化指标，其他的是满足指标。</p>
<p>满足指标 satisficing metric：不需要尽可能好，只需要到达某个门槛即可，只要到达了这个门槛就不关心它的值到底是多少。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_3.png" alt=""></p>
<p>上图中的精确度指标是优化指标，运行时间是满足指标，假设我们设置门槛为 100ms，则首先淘汰 C 分类器，然后由于 B 的优化指标更好，我们需要精确度越大越好，所以最好的分类器是 B，也就是说我们要选出运行时间在 100ms 以内且精确度最大的分类器。</p>
<h3 id="开发集和测试集的设置"><a href="#开发集和测试集的设置" class="headerlink" title="开发集和测试集的设置"></a>开发集和测试集的设置</h3><h4 id="指导原则"><a href="#指导原则" class="headerlink" title="指导原则"></a>指导原则</h4><ul>
<li>开发集给我们树立了一个靶子，选择的开发集和测试集要能够反映出将来需要预测的数据和你认为重要的数据</li>
<li>开发集和测试集应该来自相同的分布</li>
<li>训练集和开发集可以来自不同的分布</li>
</ul>
<p>设定开发集和评估方法就像放置一个目标，然后团队通过各种尝试接近这个目标，使得模型在这个开发集和评估方法上做得更好。如果开发集和测试集来自不同的地方，当你的团队花了几个月靠拢开发集，最后用测试集做测试时，会发现表现并不好。就像让你的团队用数个月的时间瞄准一个目标，但是数个月后你说：“等等，我要测试一下，我要换个目标！”</p>
<p>假设现在有来自世界八个地区的数据，如果将其中四个地区的数据作为 dev sets，另外四个地区的数据作为 test sets，是完全错误的，会导致开发集和测试集的数据分布不同，正确的做法应该是将所有地区的数据混合再分成开发集和测试集，保证数据分布相同。</p>
<p>总结：找到将来你需要的预测的数据，同时放入开发集和测试集中，瞄准你需要的目标。</p>
<h4 id="数据划分比例"><a href="#数据划分比例" class="headerlink" title="数据划分比例"></a>数据划分比例</h4><ul>
<li>几百到上万个样本：训练集/测试集=70/30；训练集/开发集/测试集=60/20/20</li>
<li>上百万个样本：训练集/开发集/测试集=98/1/1<ul>
<li>假如一百万个样本，拿一万当开发集，一万当测试集即可</li>
</ul>
</li>
<li>测试集有时候可以用开发集代替，但是有测试集会更安心，而且需要足够大才能有足够的自信来评估整个模型表现</li>
</ul>
<h3 id="什么时候重新定义指标"><a href="#什么时候重新定义指标" class="headerlink" title="什么时候重新定义指标"></a>什么时候重新定义指标</h3><ol>
<li>当你的算法由于某些特殊情况影响了用户体验或者说用户有特殊的需要时</li>
</ol>
<p>例如我们设置指标为：开发集上猫咪分类的错误率越低越好。算法 A 错误率：3%；算法 B 错误率：5%；根据我们的原有指标来看，明显算法 A 更好。但是现在算法 A 会在推荐猫咪图片时，错误地推荐一些色情图片，这对用户来说是无法忍受的，而 B 算法就不会，所以这种情况下，我们的评估指标发生了变化，B 算法更符合我们的需要。</p>
<p>那么如何修改评价指标？</p>
<script type="math/tex; mode=display">
Error=\frac{1}{\sum\limits_iw^{(i)}}\frac{1}{m_{dev}}\sum\limits_{i=1}^{m_{dev}}w^{(i)}I\{y_{predict}^{(i)} \neq y^{(i)} \}\\
w^{(i)}= \left\{
\begin{aligned}1 && 如果x^{(i)}不是色情图片\\10 &  & 如果x^{(i)}是色情图片\end{aligned}\right.</script><ul>
<li>$m_{dev}$ 为开发集的个数</li>
<li>$y_{predict}^{(i)}$ 为第 i 个样本的预测值，取值为 1 或 0</li>
<li>$I\{y_{predict}^{(i)} \neq y^{(i)} \}$ 如果括号里为真，则值为 1，即错误数加 1</li>
<li>如果某个图片是色情图片，则权重 w=10， 将会给它产生更大的误差值，也就是说假如一个图片是色情图片，则它相当于十个错误图片，会大大提高错误率</li>
<li>$\frac{1}{\sum\limits_iw^{(i)}}$ 是归一化操作，使得 error 介于 0～1 之间</li>
</ul>
<ol>
<li>实际情况发生了变化而当前的算法无法满足实际需要时</li>
</ol>
<p>例如你用网上找到十分精美的图片用作训练集和开发集，来开发猫咪识别算法，但是用户使用时上传各种各样的图片，可能取景不好，可能猫没照全，可能图像模糊，也就是说实际情况发生了改变，我们的目标也发生了改变，这个时候你需要从实际情况获取更多的数据，开始修改你的指标或者开发/测试集了，让它们实际中做得更好。</p>
<h3 id="机器学习的两个正交化步骤"><a href="#机器学习的两个正交化步骤" class="headerlink" title="机器学习的两个正交化步骤"></a>机器学习的两个正交化步骤</h3><ul>
<li>立靶子：讨论如何定义一个指标来评估分类器</li>
<li>射靶子：如何很好地满足这个指标</li>
</ul>
<h2 id="与人类表现相比较"><a href="#与人类表现相比较" class="headerlink" title="与人类表现相比较"></a>与人类表现相比较</h2><h3 id="为什么要和人类表现相比较"><a href="#为什么要和人类表现相比较" class="headerlink" title="为什么要和人类表现相比较"></a>为什么要和人类表现相比较</h3><ul>
<li>机器学习算法快速发展，慢慢在某些领域变得和人类相比有竞争力</li>
<li>在某些领域，机器学习效率比人类更高</li>
</ul>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_4.png" alt=""></p>
<p>绿线指<strong>贝叶斯误差 (Bayers error)</strong>，它指从 x 映射到 y 的最好的理论函数，永远无法超越，不管是人类还是机器。</p>
<p>当机器的表现超越人类之后进展放慢，原因有：</p>
<ul>
<li>人类的误差在许多任务中与贝叶斯误差相差不远，超越人类之后没有那么大改善空间</li>
<li>当表现超越人类之后，就没有什么工具来进行提高了</li>
</ul>
<p>当机器的表现不如人类，我们可以用以下工具来进行提高：</p>
<ul>
<li>从人类得到标签进行学习</li>
<li>人类可以对偏差进行分析，改进算法</li>
<li>可以得到更好的偏差/方差分析，即得到一个改进的标准</li>
</ul>
<h3 id="用人类标准评估偏差"><a href="#用人类标准评估偏差" class="headerlink" title="用人类标准评估偏差"></a>用人类标准评估偏差</h3><p>方法：用人类的误差的最低值作为贝叶斯误差的一个估计值。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_5.png" alt=""></p>
<p>第一个例子训练集误差跟人类误差差距较大，故专注于减小偏差，第二个例子开发集误差跟训练集误差差距更大，所以专注于减小方差。吴恩达把训练集上的误差和人类误差之间的差值称之为“可回避的偏差”，也就是偏差的下限。所以左边的例子可回避偏差为 7%，右边的为 0.5%，左边的偏差可提高空间很大。</p>
<h3 id="更好地定义“人类表现”"><a href="#更好地定义“人类表现”" class="headerlink" title="更好地定义“人类表现”"></a>更好地定义“人类表现”</h3><p>假设看一张 x 光片，不同的人类误差如下：</p>
<ul>
<li>普通人：3%</li>
<li>普通医生：1%</li>
<li>有经验的医生：0.7%</li>
<li>一个有经验的医生团队讨论：0.5% $\rightarrow$ 作为贝叶斯误差的估计值</li>
</ul>
<p>那么如何定义人类表现呢？搞清楚你的目的：</p>
<ul>
<li>如果是为了超过一个普通人的水平，那么将人类表现定为 1%</li>
<li>如果目的是作为贝叶斯误差的代替，那么定义为 0.5% 更好</li>
</ul>
<p>举个例子：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_6.png" alt=""></p>
<p>第一种情况，无论人类表现定义为哪种，可避免偏差都很大，所以专注于减小偏差，第种，无论人类表现定义为哪种，都专注于减小方差，第三种情况由于训练集误差只有 0.7%，所以人类表现的最好定义是 0.5%。</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_3.1_7.png" alt=""></p>
<p>总之：人类误差（贝叶斯误差的估计值）和训练集误差之间的的差距代表了可避免的偏差，训练集误差和开发集误差之间的差距代表了方差。</p>
<h3 id="ML-明显超过人类的领域"><a href="#ML-明显超过人类的领域" class="headerlink" title="ML 明显超过人类的领域"></a>ML 明显超过人类的领域</h3><ul>
<li>在线广告</li>
<li>产品推荐</li>
<li>物流（预测运输时间）</li>
<li>贷款批准</li>
<li>某些医学领域</li>
<li>某些语音识别</li>
<li>某些图像识别</li>
</ul>
<p>人类更擅长的是自然感知任务，包括视觉，语音识别，自然语言处理等；机器更适合处理结构化数据相关任务。</p>
<h2 id="如何提高模型表现"><a href="#如何提高模型表现" class="headerlink" title="如何提高模型表现"></a>如何提高模型表现</h2><ul>
<li>如何减小可避免偏差<ul>
<li>训练更大的模型</li>
<li>训练更久/更高的优化算法<ul>
<li>动量算法、RMSprop、Adam</li>
</ul>
</li>
<li>改变神经网络结构/超参数搜寻<ul>
<li>使用 RNN/CNN</li>
</ul>
</li>
</ul>
</li>
<li>如何减小方差<ul>
<li>更多数据</li>
<li>正则化<ul>
<li>L2、dropout、数据集增强</li>
</ul>
</li>
<li>改变神经网络架构/超参数搜寻</li>
</ul>
</li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/06/吴恩达机器学习c2w2编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 2）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/优化算法/">优化算法</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/梯度下降/">梯度下降</a></span><div class="content"><p>本次作业实现了几种梯度下降算法，比较了它们的不同。</p>
<h2 id="普通梯度下降-BGD"><a href="#普通梯度下降-BGD" class="headerlink" title="　普通梯度下降 (BGD)"></a>　普通梯度下降 (BGD)</h2><p>所谓普通梯度下降就是一次处理所有的 m 个样本，也叫批量梯度下降 (Batch Gradient Descent)，公式为：</p>
<script type="math/tex; mode=display">
W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}\\
b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]}</script></div><a class="more" href="/2018/09/06/吴恩达机器学习c2w2编程作业/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/04/吴恩达机器学习笔记c2w3/">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 3）—— 调参方法、批量归一化和深度学习框架</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/超参数/">超参数</a></span><div class="content"><p>本周主要学习超参数的系统调整，批量归一化和深度学习框架的相关知识。</p>
<h2 id="超参数的调整"><a href="#超参数的调整" class="headerlink" title="超参数的调整"></a>超参数的调整</h2><h3 id="调参的优先级"><a href="#调参的优先级" class="headerlink" title="调参的优先级"></a>调参的优先级</h3><ul>
<li>第一优先级：学习率 $\alpha$ </li>
<li>第二优先级：动量算法参数 $\beta$ (默认值0.9)，最小批的个数 mini-batch size，每层的隐藏单元数 hidden units</li>
<li>第三优先级：神经网络的层数 layers，学习率衰减 learning rate decay</li>
<li>几乎不需要调优：Adam 算法中的 $\beta_1,\beta_2,\varepsilon$，几乎每次只用默认值 $0.9,0.999,10^{-8}$    </li>
</ul>
<h3 id="调参的注意事项"><a href="#调参的注意事项" class="headerlink" title="调参的注意事项"></a>调参的注意事项</h3><ol>
<li><p>随机取样</p>
<p>当需要调整多个超参数时，尽量在网格中随机取样（下图右），而不是规则抽样（下图左）</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_2.3_1.png" alt=""></p>
</li>
</ol></div><a class="more" href="/2018/09/04/吴恩达机器学习笔记c2w3/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/04/吴恩达机器学习笔记c2w2/">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 2）—— 优化算法的改进</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/优化算法/">优化算法</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络优化/">神经网络优化</a></span><div class="content"><p>本周主要学习不同的参数优化方式，提高模型的训练速度。</p>
<h2 id="小批量梯度下降算法-mini-batch-gradient-descent"><a href="#小批量梯度下降算法-mini-batch-gradient-descent" class="headerlink" title="小批量梯度下降算法 (mini-batch gradient descent)"></a>小批量梯度下降算法 (mini-batch gradient descent)</h2><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><p>向量化可以有效率地同时计算 m 个样本，但是当样本数为几百万时，速度依然会很慢，每一次迭代都必须先处理几百万的数据集才能往前一步，所以我们可以使用这个算法进行加速。</p>
<p>首先我们将训练集划分为一个一个微小的训练集，也就是小批量训练集 (mini-batch)，比如每一个微小训练集只有 1000 个样本：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.2_2.png" alt=""></p></div><a class="more" href="/2018/09/04/吴恩达机器学习笔记c2w2/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/01/beck观后感/">《Beck》观后感</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/读书观影笔记/">读书观影笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/摇滚/">摇滚</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/音乐/">音乐</a></span><div class="content"><p>　　我终于找到我的本命番了，它的名字叫《Beck》，为啥我知道这是本命番呢，因为这是第一部我看到最后迟迟舍不得看完的番，一共２６集，每天睡前看几集，看了有十几天，不敢一次看完，因为看完了我的梦就碎了，是的，这是一个追梦的故事，我相信任何热爱摇滚，热爱音乐，热爱吉他，渴望或者已经开始玩乐队的人，都会对这部番有着深深的感动和发自内心的共鸣。因为这部番，我拿起了好久没碰的木琴，并开始挑选我的第一把电吉他……</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/beck%E8%A7%82%E5%90%8E%E6%84%9F_topimg.jpg" alt=""></p></div><a class="more" href="/2018/09/01/beck观后感/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/09/01/吴恩达机器学习c2w1编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 2 week 1）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-09-01</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络优化/">神经网络优化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/正则化/">正则化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/初始化/">初始化</a></span><div class="content"><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>这次编程作业对比了三种不同的初始化方法的不同，三种方法分别是“零初始化”、“随机初始化”、“He 初始化”。</p>
<p>这是所用的数据，我们要将红点和蓝点分类：</p>
<p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl_code_2.1_1.png" alt=""></p></div><a class="more" href="/2018/09/01/吴恩达机器学习c2w1编程作业/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/08/29/吴恩达机器学习笔记c2w1/">coursera 吴恩达深度学习 Specialization 笔记（course 2 week 1）—— 正则化等</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-29</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络优化/">神经网络优化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/正则化/">正则化</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/初始化/">初始化</a></span><div class="content"><p>deeplearning.ai 的第二个课程名为 <strong>改进深度神经网络：超参数调整，正则化和优化 (Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization)</strong> ，这门课程教你使深度学习表现更好的“魔法”，而不是把神经网络当作一个黑箱，你会理解什么使得神经网络表现更好，而且能更系统地得到好的结果，你还会学到一些 tensorflow 知识。通过三周的学习，你将能够：</p>
<ul>
<li>了解构建深度学习应用程序的行业最佳实践</li>
<li>能够有效地使用常见的神经网络技巧，包括初始化，L2 正则化和丢失正则化，批量归一化，梯度检查</li>
<li>能够实现和应用各种优化算法，例如小批量梯度下降，动量，PMSprop 和 Adam，并检查它们的收敛性</li>
<li>了解如何设置训练/开发/测试集和分析偏差/方差</li>
<li>能够在 TensorFlow 上实现神经网络</li>
</ul></div><a class="more" href="/2018/08/29/吴恩达机器学习笔记c2w1/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/08/27/吴恩达机器学习c1w4编程作业/">coursera 吴恩达深度学习 Specialization 编程作业（course 1 week 4）</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-27</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络/">神经网络</a></span><div class="content"><h1 id="第一部分：基本架构"><a href="#第一部分：基本架构" class="headerlink" title="第一部分：基本架构"></a>第一部分：基本架构</h1><p>这是深度学习专项课程第一课第四周的编程作业的第一部分，通过这一部分，可以学到：</p>
<ul>
<li>使用非线性单元比如 ReLU 来提高模型</li>
<li>建立一个更深的神经网络（大于一个隐藏层）</li>
<li>实现一个易用的神经网络类</li>
</ul>
<h2 id="包的引入"><a href="#包的引入" class="headerlink" title="包的引入"></a>包的引入</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 包的引入</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline <span class="comment"># %符号为ipython的魔法函数，与画图有关，在pycharm中会报错</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure></div><a class="more" href="/2018/08/27/吴恩达机器学习c1w4编程作业/#more">阅读更多</a><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2018/08/15/吴恩达机器学习笔记c1w4/">coursera 吴恩达深度学习 Specialization 笔记（course 1 week 4）—— 深度神经网络</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-08-15</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/学习笔记/">学习笔记</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/吴恩达深度学习笔记/">吴恩达深度学习笔记</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/AI/">AI</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/神经网络/">神经网络</a></span><div class="content"><p>本周主要介绍<strong>深度神经网络</strong> (Deep Neural Networks)。</p>
<h2 id="深度神经网络概况"><a href="#深度神经网络概况" class="headerlink" title="深度神经网络概况"></a>深度神经网络概况</h2><h3 id="什么是深度神经网络"><a href="#什么是深度神经网络" class="headerlink" title="什么是深度神经网络"></a>什么是深度神经网络</h3><p><img src="https://raw.githubusercontent.com/yichenchan/blogimg/master/img/dl4.1.png" alt=""></p>
<p>所谓深浅取决于神经网络的层数，例如左上角的逻辑回归模型是一个“最浅的”神经网络，而右下角的神经网络具有五个隐藏层，可以算得上是深度神经网络。</p></div><a class="more" href="/2018/08/15/吴恩达机器学习笔记c1w4/#more">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2018 By 陈艺琛</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">这里是我的一亩自耕田，记录自己的学习过程，生活随想和读书笔记，感谢您的参观！</div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.6"></script><script src="/js/fancybox.js?version=1.5.6"></script><script src="/js/sidebar.js?version=1.5.6"></script><script src="/js/copy.js?version=1.5.6"></script><script src="/js/fireworks.js?version=1.5.6"></script><script src="/js/transition.js?version=1.5.6"></script><script src="/js/scroll.js?version=1.5.6"></script><script src="/js/head.js?version=1.5.6"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/search/local-search.js"></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>